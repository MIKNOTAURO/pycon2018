[{"by":"coloneltcb","descendants":0,"id":17024670,"kids":"None","score":1,"text":"Live at 3:55 p.m. ET SAN FRANCISCO--(BUSINESS WIRE)-- Twilio (TWLO), the leading cloud communications platform, today reported financial results for its first quarter ended March 31, 2018. \u201cWe are honored that a growing list of companies around the world are placing their trust in Twilio. Our first quarter results exhibited broad-based strength across multiple areas of our business, especially with continued expansion with existing customers,\u201d said Jeff Lawson, Twilio\u2019s Co-Founder and Chief Executive Officer. \u201cOur continued devotion to innovation was highlighted by the launch of Twilio Flex - the first fully programmable cloud contact center application platform.\u201d First Quarter 2018 Financial Highlights Key Metrics and Recent Business Highlights Outlook Twilio is providing guidance for the second quarter ending June 30, 2018 and full year ending December 31, 2018 as follows: Conference Call Information Twilio will host a conference call today, May 8, 2018, to discuss first quarter 2018 financial results, as well as the second quarter and full year 2018 outlook, at 2 p.m. Pacific Time, 5 p.m. Eastern Time. A live webcast of the conference call, as well as a replay of the call, will be available at https:\/\/investors.twilio.com. The conference call can also be accessed by dialing (844) 453-4207, or +1 (647) 253-8638 (outside the U.S. and Canada). The conference ID is 7672639. Following the completion of the call through 11:59 p.m. Eastern Time on May 15, 2018, a replay will be available by dialing (800) 585-8367 or +1 (416) 621-4642 (outside the U.S. and Canada) and entering passcode 7672639. Twilio has used, and intends to continue to use, its investor relations website as a means of disclosing material non-public information and for complying with its disclosure obligations under Regulation FD. About Twilio Inc. More than 2 million developers around the world have used\u00a0Twilio\u00a0to unlock the magic of communications to improve any human experience.\u00a0Twilio\u00a0has democratized communications channels like voice, text, chat, and video by virtualizing the world\u2019s telecommunications infrastructure through APIs that are simple enough for any developer to use, yet robust enough to power the world\u2019s most demanding applications. By making communications a part of every software developer's toolkit,\u00a0Twilio\u00a0is enabling innovators across every industry \u2014 from emerging leaders to the world\u2019s largest organizations \u2014 to reinvent how companies engage with their customers. Forward-Looking Statements This press release and the accompanying conference call contain forward-looking statements within the meaning of the federal securities laws, which statements involve substantial risks and uncertainties. Forward-looking statements generally relate to future events or our future financial or operating performance. In some cases, you can identify forward-looking statements because they contain words such as \u201cmay,\u201d \u201cwill,\u201d \u201cshould,\u201d \u201cexpects,\u201d \u201cplans,\u201d \u201canticipates,\u201d \u201ccould,\u201d \u201cintends,\u201d \u201ctarget,\u201d \u201cprojects,\u201d \u201ccontemplates,\u201d \u201cbelieves,\u201d \u201cestimates,\u201d \u201cpredicts,\u201d \u201cpotential\u201d or \u201ccontinue\u201d or the negative of these words or other similar terms or expressions that concern our expectations, strategy, plans or intentions. Forward-looking statements contained in this press release include, but are not limited to, statements about: Twilio\u2019s outlook for the quarter ending June 30, 2018 and full year ending December 31, 2018 and Twilio\u2019s expectations regarding its products and solutions. You should not rely upon forward-looking statements as predictions of future events. The outcome of the events described in these forward-looking statements is subject to known and unknown risks, uncertainties, and other factors that may cause Twilio\u2019s actual results, performance, or achievements to differ materially from those described in the forward-looking statements, including, among other things: adverse changes in general economic or market conditions; changes in the market for communications; Twilio\u2019s ability to adapt its products to meet evolving market and customer demands and rapid technological change; Twilio\u2019s ability to generate sufficient revenues to achieve or sustain profitability; Twilio\u2019s ability to retain customers and attract new customers; Twilio\u2019s limited operating history, which makes it difficult to evaluate its prospects and future operating results; Twilio\u2019s ability to effectively manage its growth; and Twilio\u2019s ability to compete effectively in an intensely competitive market. The forward-looking statements contained in this press release are also subject to additional risks, uncertainties, and factors, including those more fully described in Twilio\u2019s most recent filings with the Securities and Exchange Commission, including its Form 10-K for the year ended December 31, 2017 filed on March 1, 2018. Further information on potential risks that could affect actual results will be included in the subsequent periodic and current reports and other filings that Twilio makes with the Securities and Exchange Commission from time to time. Moreover, Twilio operates in a very competitive and rapidly changing environment, and new risks and uncertainties may emerge that could have an impact on the forward-looking statements contained in this press release. Forward-looking statements represent Twilio\u2019s management\u2019s beliefs and assumptions only as of the date such statements are made. Twilio undertakes no obligation to update any forward-looking statements made in this press release to reflect events or circumstances after the date of this press release or to reflect new information or the occurrence of unanticipated events, except as required by law. Use of Non-GAAP Financial Measures To provide investors and others with additional information regarding Twilio\u2019s results, the following non-GAAP financial measures are disclosed: non-GAAP gross profit and gross margin, non-GAAP operating expenses, non-GAAP loss from operations and operating margin, non-GAAP net loss attributable to common stockholders, and non-GAAP net loss per share attributable to common stockholders, basic and diluted. Non-GAAP Gross Profit and Non-GAAP Gross Margin. For the periods presented, Twilio defines non-GAAP gross profit and non-GAAP gross margin as GAAP gross profit and GAAP gross margin, respectively, adjusted to exclude stock-based compensation and amortization of acquired intangibles. Non-GAAP Operating Expenses. For the periods presented, Twilio defines non-GAAP operating expenses (including categories of operating expenses) as GAAP operating expenses (and categories of operating expenses) adjusted to exclude, as applicable, stock-based compensation, amortization of acquired intangibles, acquisition-related expenses, and payroll taxes related to stock-based compensation. Non-GAAP Loss from Operations and Non-GAAP Operating Margin. For the periods presented, Twilio defines non-GAAP loss from operations and non-GAAP operating margin as GAAP loss from operations and GAAP operating margin, respectively, adjusted to exclude stock-based compensation, amortization of acquired intangibles, acquisition-related expenses, and payroll taxes related to stock-based compensation. Non-GAAP Net Loss Attributable to Common Stockholders and Non-GAAP Net Loss Per Share Attributable to Common Stockholders, Basic and Diluted. For the periods presented, Twilio defines non-GAAP net loss attributable to common stockholders and non-GAAP net loss per share attributable to common stockholders, basic and diluted, as GAAP net loss attributable to common stockholders and GAAP net loss per share attributable to common stockholders, basic and diluted, respectively, adjusted to exclude stock-based compensation, amortization of acquired intangibles, acquisition-related expenses, and payroll taxes related to stock-based compensation. Twilio\u2019s management uses the foregoing non-GAAP financial information, collectively, to evaluate its ongoing operations and for internal planning and forecasting purposes. Twilio\u2019s management believes that non-GAAP financial information, when taken collectively, may be helpful to investors because it provides consistency and comparability with past financial performance, facilitates period-to-period comparisons of results of operations, and assists in comparisons with other companies, many of which use similar non-GAAP financial information to supplement their GAAP results. Non-GAAP financial information is presented for supplemental informational purposes only, and should not be considered a substitute for financial information presented in accordance with GAAP, and may be different from similarly-titled non-GAAP measures used by other companies. Whenever Twilio uses a non-GAAP financial measure, a reconciliation is provided to the most directly comparable financial measure stated in accordance with GAAP. Investors are encouraged to review the related GAAP financial measures and the reconciliation of these non-GAAP financial measures to their most directly comparable GAAP financial measures. With respect to Twilio\u2019s guidance as provided under \u201cOutlook\u201d above, Twilio has not reconciled its expectations as to non-GAAP loss from operations to GAAP loss from operations or non-GAAP net loss per share to GAAP net loss per share because stock-based compensation expense cannot be reasonably calculated or predicted at this time. Accordingly, a reconciliation is not available without unreasonable effort. Operating Metrics Twilio reviews a number of operating metrics to evaluate its business, measure performance, identify trends, formulate business plans, and make strategic decisions. These include the number of Active Customer Accounts, Base Revenue, and Dollar-Based Net Expansion Rate. Number of Active Customer Accounts. Twilio believes that the number of Active Customer Accounts is an important indicator of the growth of its business, the market acceptance of its platform and future revenue trends. Twilio defines an Active Customer Account at the end of any period as an individual account, as identified by a unique account identifier, for which Twilio has recognized at least $5 of revenue in the last month of the period. Twilio believes that use of its platform by customers at or above the $5 per month threshold is a stronger indicator of potential future engagement than trial usage of its platform or usage at levels below $5 per month. A single organization may constitute multiple unique Active Customer Accounts if it has multiple account identifiers, each of which is treated as a separate Active Customer Account. Base Revenue. Twilio monitors Base Revenue as one of the more reliable indicators of future revenue trends. Base Revenue consists of all revenue other than revenue from large Active Customer Accounts that have never entered into 12-month minimum revenue commitment contracts with Twilio, which the Company refers to as Variable Customer Accounts. While almost all of Twilio\u2019s customers exhibit some level of variability in the usage of its products, based on the experience of Twilio\u2019s management, Twilio believes that Variable Customer Accounts are more likely to have significant fluctuations in usage of its products from period to period, and therefore that revenue from Variable Customer Accounts may also fluctuate significantly from period to period. This behavior is best evidenced by the decision of such customers not to enter into contracts with Twilio that contain minimum revenue commitments, even though they may spend significant amounts on the use of the Company\u2019s products and they may be foregoing more favorable terms often available to customers that enter into committed contracts with Twilio. This variability adversely affects Twilio\u2019s ability to rely upon revenue from Variable Customer Accounts when analyzing expected trends in future revenue. For historical periods through March 31, 2016, Twilio defined a Variable Customer Account as an Active Customer Account that (i) had never signed a minimum revenue commitment contract with the Company for a term of at least 12 months and (ii) has met or exceeded 1% of the Company\u2019s revenue in any quarter in the periods presented through March 31, 2016. To allow for consistent period-to-period comparisons, in the event a customer qualified as a Variable Customer Account as of March 31, 2016, or a previously Variable Customer Account ceased to be an Active Customer Account as of such date, Twilio included such customer as a Variable Customer Account in all periods presented. For reporting periods starting with the three months ended June 30, 2016, Twilio defines a Variable Customer Account as a customer account that (a) has been categorized as a Variable Customer Account in any prior quarter, as well as (b) any new customer account that (i) has never signed a minimum revenue commitment contract with Twilio for a term of at least 12 months and (ii) meets or exceeds 1% of the Company\u2019s revenue in a quarter. Once a customer account is deemed to be a Variable Customer Account in any period, they remain a Variable Customer Account in subsequent periods unless they enter into a minimum revenue commitment contract with Twilio for a term of at least 12 months. Dollar-Based Net Expansion Rate. Twilio\u2019s ability to drive growth and generate incremental revenue depends, in part, on the Company\u2019s ability to maintain and grow its relationships with existing Active Customer Accounts and to increase their use of the platform. An important way in which Twilio tracks its performance in this area is by measuring the Dollar-Based Net Expansion Rate for Active Customer Accounts, other than Variable Customer Accounts. Twilio\u2019s Dollar-Based Net Expansion Rate increases when such Active Customer Accounts increase their usage of a product, extend their usage of a product to new applications or adopt a new product. Twilio\u2019s Dollar-Based Net Expansion Rate decreases when such Active Customer Accounts cease or reduce their usage of a product or when the Company lowers usage prices on a product. As our customers grow their businesses and extend the use of our platform, they sometimes create multiple customer accounts with us for operational or other reasons. As such, for reporting periods starting with the three months ended December 31, 2016, when we identify a significant customer organization (defined as a single customer organization generating more than 1% of revenue in a quarterly reporting period) that has created a new Active Customer Account, this new Active Customer Account is tied to, and revenue from this new Active Customer Account is included with, the original Active Customer Account for the purposes of calculating this metric. Twilio believes that measuring Dollar-Based Net Expansion Rate on revenue generated from Active Customer Accounts, other than Variable Customer Accounts, provides a more meaningful indication of the performance of the Company\u2019s efforts to increase revenue from existing customer accounts. Twilio\u2019s Dollar-Based Net Expansion Rate compares the revenue from Active Customer Accounts, other than Variable Customer Accounts, in a quarter to the same quarter in the prior year. To calculate the Dollar-Based Net Expansion Rate, the Company first identifies the cohort of Active Customer Accounts, other than Variable Customer Accounts, that were Active Customer Accounts in the same quarter of the prior year. The Dollar-Based Net Expansion Rate is the quotient obtained by dividing the revenue generated from that cohort in a quarter, by the revenue generated from that same cohort in the corresponding quarter in the prior year. When Twilio calculates Dollar-Based Net Expansion Rate for periods longer than one quarter, it uses the average of the applicable quarterly Dollar-Based Net Expansion Rates for each of the quarters in such period. Source: Twilio Inc.  View source version on businesswire.com: https:\/\/www.businesswire.com\/news\/home\/20180508006508\/en\/  .bwalignc {text-align: center !important;} .bwalignl {text-align: left !important;} .bwalignr {text-align: right !important;} .bwdoublebottom {border-bottom-color: black !important; border-bottom-style: double !important; border-bottom-width: 2.25pt !important;} .bwlistitemmargb {margin-bottom: 10.0px !important;} .bwnowrap {white-space: nowrap !important;} .bwpadb1 {padding-bottom: 2.0px !important;} .bwpadb3 {padding-bottom: 4.0px !important;} .bwpadl0 {padding-left: 0.0px !important;} .bwpadl1 {padding-left: 5.0px !important;} .bwpadl2 {padding-left: 10.0px !important;} .bwpadl3 {padding-left: 15.0px !important;} .bwpadl6 {padding-left: 30.0px !important;} .bwpadl8 {padding-left: 40.0px !important;} .bwpadr0 {padding-right: 0.0px !important;} .bwsinglebottom {border-bottom-color: black !important; border-bottom-style: solid !important; border-bottom-width: 1.0pt !important;} .bwtablemarginb {margin-bottom: 10.0px !important;} .bwvertalignb {vertical-align: bottom !important;} .bwvertalignt {vertical-align: top !important;} ;} ","time":1525811149,"title":"Twilio Announces First Quarter 2018 Results","type":"story","url":"https:\/\/finance.yahoo.com\/news\/twilio-announces-first-quarter-2018-201500154.html","label":7,"label_name":"random"},{"by":"rbanffy","descendants":0,"id":17024655,"kids":"None","score":3,"text":"Google today announced Chrome OS is getting Linux support. As a result, Chromebooks will soon be able to run Linux apps and execute Linux commands. A preview of Linux on the Pixelbook will be released first, with support for more devices coming soon. One of Google\u2019s goals this year is to make it possible for developers to code on Chromebooks. Want Chrome OS to run the Linux terminal, Android Studio, Git, Sublime, Vim, or Android Studio? All of that will be possible this year. \u201cJust go to wherever you normally get those apps, whether it\u2019s on the websites or through apt-get in the Linux terminal, and seamless get those apps like any other Linux distribution,\u201d Chrome OS director of product management Kan Liu told VentureBeat. (Separately, Google also announced Android Studio for Chrome OS. The company has it already working internally and a preview is coming later this year.) Support for Linux apps means developers will finally be able to use a Google device to develop for Google\u2019s platforms, rather than having to depend on Windows, Mac, or Linux machines. And because Chrome OS doesn\u2019t just run Chrome OS-specific apps anymore, developers will be able to create, test, and run any Android or web app for phones, tablets, and laptops all on their Chromebooks. Without having to switch devices, you can run your favorite IDE \u2014 as long as there is a Debian Linux version (for the curious, Google is specifically using Debian Stretch here \u2014 code in your favorite language and launch projects to Google Cloud with the command line. I expected the news to receive the biggest applause from the audience, and that was indeed the case. Among enthusiasts and power users, the biggest criticism of Chrome OS has been the inability to build on the platform. \u201cI\u2019m pretty excited about it myself,\u201d Liu shared with VentureBeat. \u201cI can actually now self-publish on Chrome OS. I can literally pull up the source code for Chrome OS and build Chrome OS within Chrome OS. And with Android Studio, even though it\u2019s still relatively early, I can build Android apps on Chrome OS, deploy it to the exact same device \u2014 which is something you\u2019ve never been able to do with Android before.\u201d  The Chrome OS operating system is based on the Linux kernel, so this has technically been possible before. In fact, some users have managed to pull it off with tools like crouton, albeit with the tradeoff of disabling some Chrome OS security features. But now, Google wants to officially support Linux apps so Chrome OS security can remaining intact.  Here\u2019s how it works. \u201cWe put the Linux app environment within a security sandbox, running inside a virtual machine,\u201d Liu told VentureBeat. \u201cWe made sure the user experience is seamless to the user. Whether you use a web app, whether you are using an Android app, or whether you are using a Linux app, the window treatment and the way you launch the app from the launcher is the same.\u201d   The virtual machine is specifically designed for Chromebooks. A Google spokesperson estimated that the VM \u201cstarts up in a second.\u201d It also integrates completely with Chromebook features, meaning windows can be moved around and files can be opened directly from apps. \u201cFrom a UI perspective, we use Wayland to make sure that it\u2019s completely seamless to the user. You don\u2019t see the windows from the Linux side; it\u2019s just running within a Chrome OS window. For all the window treatment from Chrome OS windows, you get on the Linux side as well.\u201d For example, if you want to use Visual Studio Code, you will head to the Chrome OS launcher, open the Linux app right there, and it will open the app in a Chrome OS window. You\u2019ll be able to run files from GitHub, edit them, save them, and so on. \u201cThe files are actually inside the VM as well,\u201d Liu explained. \u201cFrom a security standpoint, that\u2019s what makes it safer. But, you\u2019ll be able to access files from both sides. If you were ever to get malware in the Linux side, for example, it can\u2019t contaminate the rest of Chrome OS.\u201d   Google also announced and showed off adb support. You will be able to debug Android apps, although support is still finnicky.   To be clear, Linux app support is still very early \u2014 that\u2019s why it\u2019s being made available as a preview first, and only on the Pixelbook (you can switch to the Dev channel to try it, but there are stability issues). The goal is to bring it to all Chromebooks, although there are some requirements, including some extra storage space (300MB for Linux and more for the apps) and Linux kernel version 4.4 or higher. Chrome OS won\u2019t have Linux app support on by default \u2014 users will have to explicitly enable it. \u201cThe average user probably doesn\u2019t need the Linux terminal and the ability to code,\u201d Liu noted. \u201cSo it\u2019s not going to be on by default, but for the people that care about that, they can just turn it on.\u201d Once a power user tries to enable that switch in settings, they\u2019ll be told whether their device is supported, if space needs to be freed up, and so on. Chromebooks have gotten more powerful over the years, so it makes plenty of sense to let them run Android apps and Linux apps. For the former, Google pointed to two apps as examples of more engagement on Chrome OS. Evernote is seeing its users spend triple the time on large screen devices\u2019 and quadruple the time when using the Google Pixelbook, while for Squid, Chromebooks have accounted for over 7 percent of its active devices but 21 percent of their revenue over the last 30 days. With Linux apps on the way, Chromebooks could not just be a platform with more engagement and revenue for Android apps, but one that developers actually use day-to-day.  Follow us on social:","time":1525811064,"title":"Chrome OS is getting Linux app support","type":"story","url":"https:\/\/venturebeat.com\/2018\/05\/08\/chrome-os-is-getting-linux-app-support\/amp\/?__twitter_impression=true","label":9,"label_name":"tech"},{"by":"john58","descendants":0,"id":17024641,"kids":"None","score":1,"text":"Helping you build wealth Entertainment company Walt Disney (DIS) reported a 23% jump in earnings for the second quarter, helped by double-digit revenue growth in parks and resorts, as well as studio entertainment. Net favorable impact of the tax act lowered effective income tax rate, in turn lifting the bottom-line. Earnings and revenue came in above street\u00a0expectations. With revenue increasing 9% to $14.55 billion during the quarter, the company\u2019s earnings climbed 23% to $2.94 billion or $1.95 per share. Excluding certain items affecting comparability, EPS rose 23% to $1.84.\n Revenue benefited from an increase in the number of visitors to its domestic and international parks and resorts, a shift in the timing of the Easter holiday, attendance growth at Walt Disney World Resort and at Disneyland Paris.\n Following the results, the stock inched up 0.45% in after hours.\n (The story will be updated shortly along with the AlphaGraphic)\n Published in EARNINGS, MERGERS & DEALS and OTHER INDUSTRIES \n            Comments are closed.         \r\n\t\t\tGet our newsletter delivered to your inbox\r\n\t\t Helping you build wealth \nCONTACT\u00a0|\u00a0PRIVACY POLICY\n  Copyright 2018 Alphastreet Inc.   \u00a0  |   \u00a0 All rights reserved","time":1525811012,"title":"Disney posts upbeat Q1 results","type":"story","url":"https:\/\/news.alphastreet.com\/disney-posts-upbeat-q1-results\/","label":7,"label_name":"random"},{"by":"wjn0","descendants":0,"id":17024639,"kids":"None","score":1,"text":"I was sitting in the nearly empty restaurant of the Westin Hotel in Alexandria, Virginia, getting ready for a showdown with the federal government that I had been trying to avoid for more than seven years. The Obama administration was demanding that I reveal the confidential sources I had relied on for a chapter about a botched CIA operation in my 2006 book, \u201cState of War.\u201d I had also written about the CIA operation for the New York Times, but the paper\u2019s editors had suppressed the story at the government\u2019s request. It wasn\u2019t the only time they had done so. Bundled against the freezing wind, my lawyers and I were about to reach the courthouse door when two news photographers launched into a perp-walk shoot. As a reporter, I had witnessed this classic scene dozens of times, watching in bemusement from the sidelines while frenetic photographers and TV crews did their business. I never thought I would be the perp, facing those whirring cameras. As I walked past the photographers into the courthouse that morning in January 2015, I saw a group of reporters, some of whom I knew personally. They were here to cover my case, and now they were waiting and watching me. I felt isolated and alone. My lawyers and I took over a cramped conference room just outside the courtroom of U.S. District Judge Leonie Brinkema, where we waited for her to begin the pretrial hearing that would determine my fate. My lawyers had been working with me on this case for so many years that they now felt more like friends. We often engaged in gallows humor about what it was going to be like for me once I went to jail. But they had used all their skills to make sure that didn\u2019t happen and had even managed to keep me out of a courtroom and away from any questioning by federal prosecutors. Until now. My case was part of a broader crackdown on reporters and whistleblowers that had begun during the presidency of George W. Bush and continued far more aggressively under the Obama administration, which had already prosecuted more leak cases than all previous administrations combined. Obama officials seemed determined to use criminal leak investigations to limit reporting on national security. But the crackdown on leaks only applied to low-level dissenters; top officials caught up in leak investigations, like former CIA Director David Petraeus, were still treated with kid gloves. Initially, I had succeeded in the courts, surprising many legal experts. In the U.S. District Court for the Eastern District of Virginia, Brinkema had sided with me when the government repeatedly subpoenaed me to testify before a grand jury. She had ruled in my favor again by quashing a trial subpoena in the case of Jeffrey Sterling, a former CIA officer who the government accused of being a source for the story about the ill-fated CIA operation. In her rulings, Brinkema determined that there was a \u201creporter\u2019s privilege\u201d \u2014 at least a limited one \u2014 under the First Amendment that gave journalists the right to protect their sources, much as clients and patients can shield their private communications with lawyers and doctors. But the Obama administration appealed her 2011 ruling quashing the trial subpoena, and in 2013, the 4th Circuit Court of Appeals, in a split decision, sided with the administration, ruling that there was no such thing as a reporter\u2019s privilege. In 2014, the Supreme Court refused to hear my appeal, allowing the 4th Circuit ruling to stand. Now there was nothing legally stopping the Justice Department from forcing me to either reveal my sources or be jailed for contempt of court. But even as I\u00a0was losing in the courts, I was gaining ground in the court of public opinion. My decision to go to the Supreme Court had captured the attention of the nation\u2019s political and media classes. Instead of ignoring the case, as they had for years, the national media now framed it as a major constitutional battle over press freedom. That morning in Alexandria, my lawyers and I learned that the prosecutors were frustrated by my writing style. In \u201cState of War: The Secret History of the CIA and the Bush Administration,\u201d I didn\u2019t include attribution for many passages. I didn\u2019t explicitly say where I was getting my information, and I didn\u2019t identify what information was classified and what wasn\u2019t. That had been a conscious decision; I didn\u2019t want to interrupt the narrative flow of the book with phrases explaining how I knew each fact, and I didn\u2019t want to explicitly say how I had obtained so much sensitive information. If prosecutors couldn\u2019t point to specific passages to prove I had relied on confidential sources who gave me classified information, their criminal case against Sterling might fall apart. When I walked into the courtroom that morning, I thought the prosecutors might demand that I publicly identify specific passages in my book where I had relied on classified information and confidential sources. If I didn\u2019t comply, they could ask the judge to hold me in contempt and send me to jail. I was worried, but I felt certain that the hearing would somehow complete the long, strange arc I had been living as a national security investigative reporter for the past 20 years. As I took the stand, I thought about how I had ended up here, how much press freedom had been lost, and how drastically the job of national security reporting had changed in the post-9\/11 era.  From top left to bottom right: Aldrich Ames, John I. Millis, John Deutch, Wen Ho Lee. Photo: AP, Getty Images I started covering the CIA in 1995. The Cold War was over, the CIA was downsizing, and CIA officer Aldrich Ames had just been unmasked as a Russian spy. A whole generation of senior CIA officials was leaving Langley. Many wanted to talk. I was the first reporter many of them had ever met. As they emerged from their insular lives at the CIA, they had little concept of what information would be considered newsworthy. So I decided to show more patience with sources than I ever had before. I had to learn to listen and let them talk about whatever interested them. They had fascinating stories to tell. In addition to their experiences in spy operations, many had been involved in providing intelligence support at presidential summit meetings, treaty negotiations, and other official international conferences. I realized that these former CIA officers had been backstage at some of the most historic events over the last few decades and thus had a unique and hidden perspective on what had happened behind the scenes in American foreign policy. I began to think of these CIA officers like the title characters in Tom Stoppard\u2019s play \u201cRosencrantz and Guildenstern Are Dead,\u201d in which Stoppard reimagines \u201cHamlet\u201d from the viewpoint of two minor characters who fatalistically watch Shakespeare\u2019s play from the wings. While covering the CIA for the Los Angeles Times and later the New York Times, I found that patiently listening to my sources paid off in unexpected ways. During one interview, a source was droning on about a minor bureaucratic battle inside the CIA when he briefly referred to how then-President Bill Clinton had secretly given the green light to Iran to covertly ship arms to Bosnian Muslims during the Balkan wars. The man had already\u00a0resumed talking about his bureaucratic turf war when I realized what he had just said and interrupted him, demanding that he go back\u00a0to Iran. That led me to write a series of stories that prompted the House of Representatives to create a special select committee to investigate the covert Iran-Bosnia arms pipeline. Another source surprised me by volunteering a copy of the CIA\u2019s secret history of the agency\u2019s involvement in the 1953 coup in Iran. Up until then, the CIA had insisted that many of the agency\u2019s internal documents from the coup had long since been lost or destroyed. But one incident left me questioning whether I should continue as a national security reporter. In 2000, John Millis, a former CIA officer who had become staff director of the House Intelligence Committee, summoned me to his small office on Capitol Hill. After he closed the door, he took out a classified report by the CIA\u2019s inspector general and read it aloud, slowly, as I sat next to him. He repeated passages when I asked, allowing me to transcribe the report verbatim. The report concluded that top CIA officials had impeded an internal investigation into evidence that former CIA Director John Deutch had mishandled large volumes of classified material, placing it on personal computers in his home. The story was explosive, and it angered top CIA officials. Several months later, Millis killed himself. His death shook me badly. I didn\u2019t believe that my story had played a role, but as I watched a crowd of current and former CIA officials stream into the church in suburban Virginia where his funeral was held, I wondered whether I was caught up in a game that was turning deadly. (I have never before disclosed that Millis was the source for the Deutch story, but his death more than 17 years ago makes me believe there is no longer any purpose to keeping his identity a secret. In an interview for this story, Millis\u2019s widow, Linda Millis, agreed that there was no reason to continue hiding his role as my source, adding: \u201cI don\u2019t believe there is any evidence that [leaking the Deutch report] had anything to do with John\u2019s death.\u201d) Another painful but important lesson came from my coverage of the case of Wen Ho Lee, a Chinese-American scientist at Los Alamos National Laboratory, who in 1999 was suspected by the government of spying for China. After the government\u2019s espionage case against him collapsed, I was heavily criticized \u2014 including in an editor\u2019s note\u00a0in the New York Times \u2014 for having written stories that lacked sufficient caveats about flaws and holes in the government\u2019s case. The editor\u2019s note said that we should \u201chave pushed harder to uncover weaknesses in the FBI case against Dr. Lee,\u201d and that \u201cin place of a tone of journalistic detachment from our sources, we occasionally used language that adopted the sense of alarm that was contained in official reports and was being voiced to us by investigators, members of Congress and administration officials with knowledge of the case.\u201d In hindsight, I believe the criticism was valid. That bitter experience almost led me to leave the Times. Instead, I decided to stay. In the end, it made me much more skeptical of the government.  CIA Director George Tenet at FBI headquarters in Washington on Feb. 20, 2001. Photo: Rick Bowmer\/AP I discovered that there was, in effect, a marketplace of secrets in Washington, in which White House officials and other current and former bureaucrats, contractors, members of Congress, their staffers, and journalists all traded information. This informal black market helped keep the national security apparatus running smoothly, limiting nasty surprises for all involved. The revelation that this secretive subculture existed, and that it allowed a reporter to glimpse the government\u2019s dark side, was jarring. It felt a bit like being in the Matrix. Once it became known that you were covering this shadowy world, sources would sometimes appear in mysterious ways. In one case, I received an anonymous phone call from someone with highly sensitive information who had read other stories I had written. The information from this new source was very detailed and valuable, but the person refused to reveal her identity and simply said she would call back. The source called back several days later with even more information, and after several calls, I was able to convince her to call at a regular time so I would be prepared to talk. For the next few months, she called once every week at the exact same time and always with new information. Because I didn\u2019t know who the source was, I had to be cautious with the information and never used any of it in stories unless I could corroborate it with other sources. But everything the source told me checked out. Then after a few months, she abruptly stopped calling. I never heard from her again, and I never learned her identity. Disclosures of confidential information to the press were generally tolerated as facts of life in this secret subculture. The media acted as a safety valve, letting insiders vent by leaking. The smartest officials realized that leaks to the press often helped them, bringing fresh eyes to stale internal debates. And the fact that the press was there, waiting for leaks, lent some discipline to the system. A top CIA official once told me that his rule of thumb for whether a covert operation should be approved was, \u201cHow will this look on the front page of the New York Times?\u201d If it would look bad, don\u2019t do it. Of course, his rule of thumb was often ignored. For decades, official Washington did next to nothing to stop leaks. The CIA or some other agency would feign outrage over the publication of a story it didn\u2019t like. Officials launched leak investigations but only went through the motions before abandoning each case. It was a charade that both government officials and reporters understood. As part of my legal case, my lawyers filed Freedom of Information Act requests with several government agencies seeking documents those agencies had about me. All the agencies refused to provide any documents related to my current leak case, but eventually the FBI began to turn over reams of documents about old leak investigations that had been conducted years earlier on other stories I had written. I was stunned to learn of them. The documents revealed that the FBI gave code names to its leak investigations. One set of documents identified an investigation code-named \u201cBRAIN STORM\u201d; another, code-named \u201cSERIOUS MONEY,\u201d involved a story I did in 2003 about how the Iraqi regime of Saddam Hussein had tried to reach a secret, last-minute deal with the Bush administration to avoid war. Yet the government had closed all these leak investigations without taking action against my sources or me, at least as far as I know. Even after 9\/11, government officials had a limited appetite for aggressively pursuing leak cases, and Justice Department and FBI officials had little interest in getting assigned to leak investigations. They knew they were dead-end cases. A June 19, 2003, FBI memo about BRAIN STORM shows it shared the fate of virtually all leak investigations of that era. The FBI\u2019s Washington field office \u201chas covered all logical leads, and no viable suspect has been identified,\u201d the memo noted. \u201cBased upon this situation, WFO is referring this matter back to FBIHQ for additional input and\/or presenting this case to DOJ for closure.\u201d One reason that officials didn\u2019t want to conduct aggressive leak investigations was that they regularly engaged in quiet negotiations with the press to try to stop the publication of sensitive national security stories. Government officials seemed to understand that a get-tough approach to leaks might lead to the breakdown of this informal arrangement. At the time, I usually went along with these negotiations. About a year before 9\/11, for instance, I learned that the CIA had sent case officers to Afghanistan to meet with Ahmed Shah Massoud, the leader of the rebel Northern Alliance, which was fighting the Taliban government. The CIA officers had been sent to try to convince Massoud to help the Americans go after Osama bin Laden, who was then living in Afghanistan under the Taliban\u2019s protection. When I called the CIA for comment, then-CIA Director George Tenet called me back personally to ask me not to run the story. He told me the disclosure would threaten the safety of the CIA officers in Afghanistan. I agreed. I finally wrote the story after 9\/11, but I later wondered whether it had been a mistake to hold it before the attacks on New York City and Washington. Independent investigations of 9\/11 later concluded that the CIA\u2019s effort to target bin Laden before the attacks had been half-hearted. If I had reported the story before 9\/11, the CIA would have been angry, but it might have led to a public debate about whether the United States was doing enough to capture or kill bin Laden. That public debate might have forced the CIA to take the effort to get bin Laden more seriously. My experience with that story and subsequent ones made me much less willing to go along with later government requests to hold or kill stories. And that ultimately set me on a collision course with the editors at the New York Times, who were still quite willing to cooperate with the government.  President George W. Bush\u00a0addresses the nation\u00a0from the Oval Office of the White House in Washington, D.C., on March 19, 2003, announcing U.S. military airstrikes in Iraq. Photo: Alex Wong\/Getty Images After the 9\/11 attacks, the Bush administration began asking the press to kill stories more frequently. They did it so often that I became convinced the administration was invoking national security to quash stories that were merely politically embarrassing. In late 2002, for instance, I called the CIA for comment on a story about the existence of a secret CIA prison in Thailand that had just been created to house Al Qaeda detainees, including Abu Zubaydah. In response, Bush administration officials called the Times and got the paper to kill the story. I disagreed with the paper\u2019s decision because I believed that the White House was just trying to cover up the fact that the CIA had begun to set up secret prisons. I finally reported the information a year later. (In 2014, the Senate Intelligence Committee\u2019s report on the CIA\u2019s torture program provided new insight into the consequences of the killed Thailand story. \u201cIn November 2002, after the CIA learned that a major U.S. newspaper knew that Abu Zubaydah was in Country [redacted], senior CIA officials, as well as Vice President Cheney, urged the newspaper not to publish the information,\u201d the 2014 report states. \u201cWhile the U.S. newspaper did not reveal Country [redacted] as the location of Abu Zubaydah, the fact that it had the information, combined with previous media interest, resulted in the decision to close Detention Site Green.\u201d) By 2002, I was also starting to clash with the editors over our coverage of the Bush administration\u2019s claims about pre-war intelligence on Iraq. My stories raising questions about the intelligence, particularly the administration\u2019s claims of a link between Iraq and Al Qaeda, were being cut, buried, or held out of the paper altogether. One of the few stories I managed to get on the front page cast doubt on reports that an Iraqi intelligence officer had met with 9\/11 plotter Mohamed Atta in Prague before the attacks on New York and Washington. But Doug Frantz, then the investigations editor in New York, felt that he had to sneak it onto Page 1. \u201cGiven the atmosphere among the senior editors at The Times, I was concerned that the story would not make it to page 1 on a day when everyone was convened around the table,\u201d Frantz emailed me recently. \u201cSo I decided that it was too important to appear inside the paper and went ahead and offered it on a\u00a0Sunday, a day when the senior editors weren\u2019t often involved in the discussion.\u201d Then-Executive Editor Howell Raines was believed by many at the paper to prefer stories that supported the case for war. But Raines now says he was not pro-war, and that he did not object to putting my Prague story on the front page. \u201cI never told anyone at any level on the Times that I wanted stories that supported the war,\u201d he\u00a0told me in an email. Meanwhile, Judy Miller, an intense reporter who was based in New York but had sources at the highest levels of the Bush administration, was writing story after story that seemed to document the existence of Iraq\u2019s weapons of mass destruction. Her stories were helping to set the political agenda in Washington. Miller and I were friends \u2014 in fact, I was probably one of her closest friends in the Washington bureau at the time. In the year before 9\/11, Miller worked on a remarkable series of stories about Al Qaeda that offered clear warnings about its new power and intent. In the months after 9\/11, she and I both scrambled to document Al Qaeda\u2019s role in the attacks and the counterterrorism response by the United States. We were both part of a team that won the 2002 Pulitzer Prize for Explanatory Reporting for our coverage of terrorism and 9\/11. But in the months leading up to the March 2003 invasion of Iraq, while Miller and other Times reporters were landing a string of big stories that dazzled the editors, I was getting frustrated that so few of my sources in the intelligence community were willing to talk to me about what they thought of the Bush administration\u2019s case for war. I kept hearing quiet complaints that the White House was pressuring CIA analysts to cook the books and deliver intelligence reports that followed the party line on Iraq. But when I pressed, few were willing to provide specifics. Intermediaries would sometimes tell me that they were receiving anguished calls from CIA analysts, but when I asked to talk to them, they refused. After weeks of reporting in late 2002 and early 2003, I was able to get enough material to start writing stories that revealed that intelligence analysts were skeptical of the Bush administration\u2019s evidence for going to war, particularly the administration\u2019s assertions that there were links between Saddam\u2019s regime and Al Qaeda. But after I filed the first story, it sat in the Times computer system for days, then weeks, untouched by editors. I asked several editors about the story\u2019s status, but no one knew. Finally, the story ran, but it was badly cut and buried deep inside the paper. I wrote another one, and the same thing happened. I tried to write more, but I started to get the message. It seemed to me that the Times didn\u2019t want these stories. What angered me most was that while they were burying my skeptical stories, the editors were not only giving banner headlines to stories asserting that Iraq had weapons of mass destruction, they were also demanding that I help match stories from other publications about Iraq\u2019s purported WMD programs. I grew so sick of this that when the Washington Post reported that Iraq had turned over nerve gas to terrorists, I refused to try to match the story. One mid-level editor in the Washington bureau yelled at me for my refusal. He came to my desk carrying a golf club while berating me after I told him that the story was bullshit and I wasn\u2019t going to make any calls on it. As a small protest, I put a sign on my desk that said, \u201cYou furnish the pictures, I\u2019ll furnish the war.\u201d It was New York Journal publisher William Randolph Hearst\u2019s supposed line to artist Frederic Remington, whom he had sent to Cuba to illustrate the \u201ccrisis\u201d there before the Spanish-American War. I don\u2019t think my editors even noticed the sign.  U.S. Marines on a mission\u00a0on the outskirts of Baghdad, Iraq, on April 6, 2003. Photo: Gilles Bassigna\/Gamma-Rapho\/Getty Images I have to admit, it was strange doing an interview naked, but that\u2019s what a key source demanded. In March 2003, I flew to Dubai to interview a very nervous man. It had taken weeks of negotiations, through a series of intermediaries, to arrange our meeting. We agreed on a luxury hotel in Dubai, the modern capital of Middle Eastern intrigue. Just before we were scheduled to meet, however, the source imposed new demands. We would have to talk in the hotel\u2019s steam room, naked. He wanted to make sure he wasn\u2019t being recorded. That also made it impossible for me to take notes until after our meeting. But it was worth it. He told me the story of how Qatar had given sanctuary to Khalid Shaikh Mohammed in the 1990s, when he was wanted in connection with a plot to blow up American airliners. Qatari officials had given KSM a government job and then had apparently warned him when the FBI and CIA were closing in, allowing him to escape to Afghanistan, where he joined forces with bin Laden and became the mastermind behind the 9\/11 plot. I was later able to confirm the story, which was especially significant because Qatar was home to the forward headquarters of U.S. Central Command, the military command in charge of the invasion of Iraq. After the story ran, I felt revitalized. That spring, just as the U.S.-led invasion of Iraq began, I called the CIA for comment on a story about a harebrained CIA operation to turn over nuclear blueprints to Iran. The idea was that the CIA would give the Iranians flawed blueprints, and Tehran would use them to build a bomb that would turn out to be a dud. The problem was with the execution of the secret plan. The CIA had taken Russian nuclear blueprints it had obtained from a defector and then had American scientists riddle them with flaws. The CIA then asked another Russian to approach the Iranians. He was supposed to pretend to be trying to sell the documents to the highest bidder. But the design flaws in the blueprints were obvious. The Russian who was supposed to hand them over feared that the Iranians would quickly recognize the errors, and that he would be in trouble. To protect himself when he dropped off the documents at an Iranian mission in Vienna, he included a letter warning that the designs had problems. So the Iranians received the nuclear blueprints and were also warned to look for the embedded flaws. Several CIA officials believed that the operation had either been mismanaged or at least failed to achieve its goals. By May 2003, I confirmed the story through a number of sources, wrote up a draft, and called the CIA public affairs office for comment. Instead of responding to me, the White House immediately called Washington Bureau Chief Jill Abramson and demanded a meeting. The next day, Abramson and I went to the West Wing of the White House to meet with National Security Adviser Condoleezza Rice. In her office, just down the hall from the Oval Office, we sat across from Rice and George Tenet, the CIA director, along with two of their aides. Rice stared straight at me. I had received information so sensitive that I had an obligation to forget about the story, destroy my notes, and never make another phone call to discuss the matter with anyone, she said. She told Abramson and me that the New York Times should never publish the story. I tried to turn the tables. I asked Tenet a few questions about the Iranian program and got him to confirm the story, and also provide some details I hadn\u2019t heard before. The only point he disputed was that the operation had been mismanaged. Rice argued that the operation was an alternative to a full-scale invasion of Iran, like the war that President George W. Bush had just launched in Iraq. \u201cYou criticize us for going to war for weapons of mass destruction,\u201d I recall her saying. \u201cWell, this is what we can do instead.\u201d (Years later, when Rice testified in the Sterling trial, a copy of the \u201ctalking points\u201d she had prepared for our meeting was entered into evidence, though I don\u2019t remember her actually saying many of these things.) Abramson told Rice and Tenet that the decision on whether to run the story was up to Times Executive Editor Howell Raines. After the meeting, Abramson and I stopped for lunch. We were both stunned by the full-court press we had just endured. But I also recognized that I had just gotten high-level confirmation for the story \u2014 better confirmation than I could ever have imagined. Just after Abramson and I met with Tenet and Rice, the Jayson Blair scandal erupted, forcing Raines into an intense battle to save his job. Blair may have been the immediate cause of the crisis, but among the staff at the Times, Blair was merely the trigger that allowed resentment that had built up against Raines over his management style to come out into the open. Abramson recalls that after our meeting with Rice, she took the Iran story to both Raines and then-Managing Editor Gerald Boyd. \u201cThey gave me a swift no\u201d about publishing the story, Abramson told me recently. She said that she told Raines and Boyd that Rice was willing to discuss the story with them on a secure phone line that they could use from a facility on Manhattan\u2019s East Side, but she says they never asked to take that step, and she didn\u2019t push them to do so. Raines disputes this. \u201cI was not informed of this meeting [with Rice and Tenet], nor do I recall being involved with your story in any way,\u201d he said in an email. (Boyd died in 2006.) Raines left the paper in early June 2003. Joe Lelyveld, the retired executive editor, briefly came back to run the Times on an interim basis. I talked to him by phone about the Iran story, but he didn\u2019t really have time to deal with it. When Bill Keller was named executive editor in the summer of 2003, he agreed to discuss the story with Abramson and me. Abramson, meanwhile, had been promoted to managing editor, Keller\u2019s No. 2. After I went over the story with him, Keller decided not to publish it. I tried over the next year to get him to change his mind, but I couldn\u2019t. The spiking of the Iran story, coming so soon after the internal fights over WMD coverage, left me depressed. I began to think about whether to write a book that would include the Iran story and document the war on terror more broadly in a way I didn\u2019t believe I had been able to do in the Times.  U.S. federal prosecutor Patrick Fitzgerald speaks during a press conference at the Department of Justice on Oct. 28, 2005, in Washington, D.C. Photo: Mandel Ngan\/AFP\/Getty Images In December 2003, the Justice Department appointed Patrick Fitzgerald, then the U.S. attorney in Chicago, to be a special counsel to investigate allegations that top Bush White House officials had illegally leaked Plame\u2019s covert identity as a CIA officer. Critics claimed that the Bush White House had sold her out to the press as retribution against her Iraq war critic husband, former U.S. diplomat Joseph Wilson. Without thinking about the long-term consequences, many in the media cheered Fitzgerald on, urging him to aggressively go after top Bush administration officials to find out who was the source of the leak. Anti-Bush liberals saw the Plame case and the Fitzgerald leak investigation as a proxy fight over the war in Iraq, rather than as a potential threat to press freedom. Fitzgerald, an Inspector Javert-like prosecutor whose special counsel status meant that no one at the Justice Department could rein him in, started subpoenaing reporters all over Washington and demanding they testify before a grand jury. There was hardly a murmur of dissent from liberals as Fitzgerald pressed one prominent reporter after another for information. Only Judy Miller went to jail rather than cooperate. (She eventually testified after she received a waiver from her source, I. Lewis \u201cScooter\u201d Libby, a top aide to Vice President Dick Cheney.) Fitzgerald became famous as a tough, no-nonsense prosecutor, and the fact that he had run roughshod over the Washington press corps didn\u2019t hurt his reputation. He went on to become a partner in one of America\u2019s premier law firms. The Plame case eventually faded away, but it had set a dangerous precedent. Fitzgerald had successfully subpoenaed reporters and forced them to testify and in the process, had become the Justice Department\u2019s biggest star. He had demolished the political, social, and legal constraints that previously made government officials reluctant to go after journalists and their sources. He became a role model for career prosecutors, who saw that you could rise to the top of the Justice Department by going after reporters and their sources. White House officials, meanwhile, saw that there wasn\u2019t as much political blowback from targeting reporters and conducting aggressive leak investigations as they had expected. The decadesold informal understanding between the government and the press \u2014 that the government would only go through the motions on leak investigations \u2014 was dead. Michael Hayden, nominated to be the new CIA director, looks on during a personnel announcement on May 8, 2006, in the Oval Office of the White House. Photo: Roger Wollenberg-Pool\/Getty Images In the summer of 2003, the New York Times named a new Washington bureau chief: Philip Taubman, an old friend of Bill Keller\u2019s. Taubman had been the Times\u2019s Moscow bureau chief when Keller won a Pulitzer Prize as a correspondent there. Now Taubman was Keller\u2019s man in Washington. Taubman and I developed a friendly relationship. He had covered national security and intelligence matters earlier in his career, and he seemed eager for scoops. But by 2004, I began to disagree with some of his decisions. That spring, I learned that the Bush administration had discovered that Ahmad Chalabi, the neoconservatives\u2019 golden boy in Iraq, had told an Iranian intelligence official that the National Security Agency had broken Iranian codes. That was a huge betrayal by the man some senior Bush administration officials had once considered installing as the leader of Iraq. But after I called the CIA and NSA for comment, NSA Director Michael Hayden called Taubman and asked him not to run the story. Hayden argued that even though Chalabi had told the Iranians that the U.S. had broken their codes, it wasn\u2019t clear the Iranians believed him, and they were still using the same communications systems. Taubman agreed, and we sat on the story until the CIA public affairs office called and told him that someone else was reporting it, and that we should no longer feel bound not to publish. I was upset that I had lost an exclusive, and I believed that Hayden\u2019s arguments against publication had been designed simply to save the White House from embarrassment over Chalabi. In the spring of 2004, just as the Plame case was heating up and starting to change the dynamics between the government and the press, I met with a source who told me cryptically that there was something really big and really secret going on inside the government. It was the biggest secret the source had ever heard. But it was something the source was too nervous to discuss with me. A new fear of aggressive leak investigations was filtering down. I decided to stay in touch with the source and raise the issue again. Over the next few months, I met with the source repeatedly, but the person never seemed willing to divulge what the two of us had begun to refer to as \u201cthe biggest secret.\u201d Finally, in the late summer of 2004, as I was leaving a meeting with the source, I said I had to know what the secret was. Suddenly, as we were standing at the source\u2019s front door, everything spilled out. Over the course of about 10 minutes, the source provided a detailed outline of the NSA\u2019s massive post-9\/11 domestic spying program, which I later learned was code-named Stellar Wind. The source told me that the NSA had been wiretapping Americans without search warrants, without court approval. The NSA was also collecting the phone and email records of millions of Americans. The operation had been authorized by the president. The Bush administration was engaged in a massive domestic spying program that was probably illegal and unconstitutional, and only a handful of carefully selected people in the government knew about it. I left that meeting shocked, but as a reporter, I was also elated. I knew that this was the story of a lifetime. The NSA had lived by strict rules against domestic spying for 30 years, ever since the Church Committee investigations of intelligence abuses in the 1970s had led to a series of reforms. One reform measure, the Foreign Intelligence Surveillance Act of 1978, made it illegal for the NSA to eavesdrop on Americans without the approval of a secret FISA court. My source had just revealed to me that the Bush administration was secretly ignoring the law requiring search warrants from the FISA court. I quickly began to think about how I could confirm the story and fortunately found the right person, a source who didn\u2019t usually like to volunteer much information but was sometimes willing to confirm things I had heard elsewhere. As we sat alone in a quiet bar, I told the source what I had heard about the NSA program, and it was immediately clear that the source knew the same secret and was troubled by it. The source explained many of the technical details of the Bush administration\u2019s secret NSA domestic spying program to me, describing how the NSA had latched onto giant gateway switches along the borders between the domestic and international telecommunications networks, so it could vacuum up all international phone traffic and email messages sent or received by Americans. As I worked to find more people to talk to about the story, I realized that the reporter sitting next to me in the Washington bureau, Eric Lichtblau, was hearing similar things. Lichtblau covered the Justice Department. When he first came to the paper in 2002, I had been jealous of his abilities as a reporter, especially his success at developing sources. I sometimes let my resentment get the better of me; I recall one meeting with Abramson in which I was openly dismissive of an exclusive story Lichtblau was working on. But he never held it against me, and we struck up a friendship and started working on stories together. Lichtblau had heard from a source that something potentially illegal was going on at DOJ, that officials seemed to be ignoring the law requiring warrants for national security wiretaps, and that Attorney General John Ashcroft might be involved. Lichtblau and I compared notes, and we realized we were probably hearing about the same story. We decided to work together. We both kept digging, talking to more people. We started doing some interviews together and discovered that we had very different reporting styles. While I liked to let a source talk about whatever was on their mind, Lichtblau liked to get right to the point, and sometimes badgered sources to cough up information. Our approaches were complementary, and we inadvertently developed a good cop-bad cop routine. Lichtblau would often give our sources colorful nicknames, which made it easier for us to talk without revealing their identities. He called one early source on the NSA story \u201cVomit Guy\u201d because when he told the source what he wanted to talk about, the source told Lichtblau he was so upset about the topic that he wanted to throw up. By the fall of 2004, we had a draft of a story. I felt it was time to go through the front door, so I decided on impulse to try to bluff my way to the top of the NSA. I called the NSA\u2019s press spokesperson, Judy Emmel, and told her I had to talk to Hayden immediately. I said it was urgent, and I couldn\u2019t tell her what it was about. She got Hayden on the phone right away. I was shocked that my bluff had worked, but now that I had Hayden, I had to think fast about what I wanted to ask him. I decided to read him the first few paragraphs of the draft Lichtblau and I were writing. Lichtblau was sitting next to me, staring intently as I read Hayden the top of the story on the phone. I was sitting in front of my computer, ready to transcribe whatever Hayden would say. After I read the first few paragraphs, Hayden let out an audible gasp and then stammered for a moment. Finally, he said that whatever the NSA was doing was legal and operationally effective. I pressed him further, but he refused to say more and hung up. Hayden had all but confirmed the story. It seemed obvious from his response that he knew exactly what I was talking about and had begun to defend his actions before deciding to end the conversation. After explaining to Lichtblau what Hayden had just said, I walked over to Taubman\u2019s office to tell him the news. \u201cI thought it was a terrific scoop, but knew we would be faced with some tough questions about whether publication might undermine U.S. efforts to prevent another 9\/11 style attack,\u201d Taubman emailed me recently. Within days, Hayden called Taubman and asked him not to run the NSA story. Taubman listened, but was noncommittal. That was the beginning of what turned out to be more than a year of negotiations between the Times and the Bush administration, as officials repeatedly sought to kill the NSA story. A few days later, Taubman and I went to the Old Executive Office Building, next to the White House, to meet with acting CIA Director John McLaughlin, who had recently replaced Tenet, and McLaughlin\u2019s chief of staff, John Moseman. We met them in the office the CIA director maintains in the OEB to be close to the White House. The meeting, the first of many between the Times and the government over the NSA story, was odd. In contrast to my meeting with Tenet and Rice on the Iran story, when they had confirmed the story while asking the paper to kill it, McLaughlin and Moseman refused to acknowledge that the NSA story was true, even as they asked us not to print it. They kept speaking in hypothetical terms, saying that if such a program existed, it would be important to the United States that it remain secret, and American newspapers shouldn\u2019t report on such things. I had now been through this routine with the Bush administration several times, and their dire warnings about national security no longer impressed me. They had cried wolf too many times to be credible. Taubman didn\u2019t give them an answer about whether the Times would publish the story, telling them it would be up to Keller. He also demanded that they warn us if they found out any other news organization was onto the same story. In his 2016 memoir, \u201cPlaying to the Edge: American Intelligence in the Age of Terror,\u201d Hayden recalls that what he had heard from McLaughlin and Moseman convinced him that he could negotiate with Taubman, but not with me. \u201cTaubman seemed to be thoughtful and reflective throughout. Risen was described as obnoxious, argumentative, and combative, commenting only to rebut with a constant theme of the public\u2019s right to know,\u201d Hayden writes. \u201cContemporaneous notes indicated that Taubman understood the seriousness of the question, while Risen doesn\u2019t give a shit, frankly.\u201d Hayden writes that as a result of that assessment, \u201cwe became pretty forthcoming \u2014 with Taubman.\u201d As Lichtblau and I continued to report, we realized that we had to get a better understanding of how American and international telecommunications networks worked. I spent a day in the library at Georgetown University, poring over technical journals and academic works on the telecommunications industry. I called AT&T\u2019s headquarters and told the company\u2019s spokesperson I was interested in learning more about the infrastructure of the telephone system, particularly the big switches that brought telephone and internet traffic into the United States. I did not tell the spokesperson why I was taking an interest in such an arcane issue, other than that it was for a story in the New York Times. At first, the spokesperson was very friendly and cooperative, and said he would be happy to have me talk with some of AT&T\u2019s technical experts, adding that he might be able to arrange a tour of their facilities. But I never heard from him again. I called back several times, but he didn\u2019t return my calls. I finally figured that someone from the Bush administration had admonished AT&T not to talk to me.  Bill Keller, former executive editor\u00a0of the New York Times, in 2013. Photo: Andrew Harrer\/Bloomberg\/Getty Images We were operating against the backdrop of the 2004 presidential\u00a0race between George W. Bush and John Kerry. With a week or two to go before the election, Lichtblau and I, along with Corbett and Taubman, went to New York for a meeting with Keller and Abramson to decide whether the story would be published. We sat in the back alcove of Keller\u2019s office in the old Times building on 43rd Street. It was a comfortable, book-lined nook that I had visited once before, when I had tried to get Keller to change his mind and publish the long-spiked CIA-Iran story. Lichtblau, Corbett, and I argued strongly that the NSA story should be published. In that small room, we launched into an intense debate over whether to publish the story, dominated by the inherent tension between national security and the public\u2019s right to know. A key issue was the legality of the NSA program. Keller seemed skeptical of our sources\u2019 arguments that the program was illegal and possibly unconstitutional. There were some tense exchanges. I told Keller I thought this was the kind of story that had helped make the New York Times great in the 1970s, when Seymour Hersh had uncovered a series of intelligence abuses. Keller seemed unimpressed; as I recall, he called the comparison between the NSA story and Hersh\u2019s earlier work \u201cfacile.\u201d (I don\u2019t think my comment was facile, but it was probably arrogant.) As the meeting wore on and Keller was unconvinced by each reason we gave for running the story, I grew more desperate to find some argument that might change his mind. Finally, I said that if we didn\u2019t run the story before the election, a key source might go elsewhere and another news outlet might publish it. That was exactly the wrong thing to say to Keller. He got his back up, wondering aloud whether the source had a political agenda. He said he wouldn\u2019t be pressured into running the story before the election because he didn\u2019t want to let the potential political impact affect his journalistic decision. I pointed out that if he decided not to run the story before the election, that would also have an impact, but he seemed to ignore my comment. By the end of the meeting, he said he had decided not to run the story. In a recent interview, Keller acknowledged that my telling him that a source might go elsewhere with the story influenced his decision. \u201cThat set off alarm bells in my head,\u201d Keller recalled, adding that he thought \u201cwe have a critical source with an animus.\u201d Keller now also says that the overall climate in the country in 2004 provides important context for his decision not to run the story. In a 2013 interview with then-Times Public Editor Margaret Sullivan, he expanded on that, saying that \u201cthree years after 9\/11, we, as a country, were still under the influence of that trauma, and we, as a newspaper, were not immune. It was not a kind of patriotic rapture. It was an acute sense that the world was a dangerous place.\u201d Keller\u2019s rejection was a setback. But after the election, Lichtblau and I convinced the editors to let us start working on the story again. As we looked for more sources, we began to feel the chilling effect of the government\u2019s new approach to leak investigations. Within the small group of people in the government who knew about the NSA program, many also knew by now that we were investigating it and were afraid to talk to us. On a snowy night in December 2004, we drove to the home of one official who Lichtblau believed knew about the NSA program. When the official opened the door, he recognized Lichtblau and quickly realized why we were there. He started berating us for showing up unannounced, told us to leave immediately, and shut the door. He seemed worried that someone might have seen us outside his house. The paper once again began meetings with top administration officials who wanted to stop us from running the story. In the weeks after the election, Lichtblau, Taubman, and I went to the Justice Department to meet with Deputy Attorney General James Comey and White House Counsel Alberto Gonzales. Ashcroft had just resigned, and while it had not yet been announced, it was clear that Gonzales was about to replace him as attorney general. Now it was up to Gonzales to convince us to kill the story. But once the meeting started, Gonzales barely said anything; it seemed that the administration was temporarily happy to have gotten through the election without our story being published, and the tone in the room was more relaxed than usual. Comey did most of the talking. While he admitted that he had some qualms about the program, he kept insisting that it was too important to publicly disclose and that we shouldn\u2019t run our story. (Comey did not reveal that he and several other top Justice Department officials, along with then-FBI Director Robert Mueller, had nearly resigned over certain aspects of the program earlier in 2004.) Meanwhile, Hayden, who clearly had decided to make Taubman the focus of his lobbying campaign to stop the Times from publishing the story, invited him, but not Lichtblau or me, to NSA headquarters and allowed Taubman to talk with NSA officials directly involved in the domestic spying program. Afterward, Taubman told Lichtblau and me that he couldn\u2019t tell us what he had learned. Today, Taubman says Hayden\u2019s purpose \u201cwas to read me into the program on an off-the-record basis so I would have a better understanding of how it worked and why disclosure of it would be damaging to U.S. national security.\u201d \u201cWhen I returned to the bureau, I recall that Eric and you, not surprisingly, were eager to hear about the meeting,\u201d Taubman told me in an email. \u201cI described my visit in general, but said I had agreed not to tell anyone about the technical details I had learned, but would employ my knowledge by telling you if I saw anything in your draft story that I thought was incorrect.\u201d Keller now says that Taubman\u2019s relationship with Hayden played an important role in the decision to not run the story. \u201cCertainly one factor was that Taubman knew Hayden pretty well, and he trusted him,\u201d Keller told me. \u201cHayden invited Taubman out to where the guys were actually doing the NSA program. When somebody gives you that kind of access and says lives will be at risk, you take them seriously.\u201d Meanwhile, the White House decided to enlist members of the \u201cGang of Eight,\u201d the handful of congressional leaders who had been secretly briefed on the program while the rest of Congress was kept in the dark. Then-Rep. Jane Harman, the ranking Democrat on the House Intelligence Committee, called Taubman and made the case that the New York Times should not run the story. When I asked Taubman about this recently, he suggested that Harman\u2019s call came as a result of his discussions with the government. Taubman recalls telling either Hayden or Rice that the Times needed to hear from the leaders of the congressional intelligence committees who knew about the program. \u201cJane Harman agreed to talk to me, on the condition that the call be off-the-record. She told me that she and her colleagues, Democrat and Republican, strongly supported the NSA effort and requested that the [Times] not disclose it.\u201d By mid-December 2004, the story had been re-reported, so Lichtblau, Corbett, and I began pushing again to get it into the paper. Instead of traveling to New York this time, we held a series of closed-door meetings with Taubman in his office in Washington. The additional reporting and rewriting did not sway him. He accepted the Bush administration\u2019s arguments that the piece would harm national security. He killed the story. This time, Keller was not directly involved in our meetings. The NSA story now seemed permanently dead. I was about to start a long-scheduled leave to write a book about the CIA and the Bush administration. I was furious that the Times had killed both the Iran and the NSA stories, and angry that the White House was successfully suppressing the truth. I told myself that if I kept going along with decisions to cut, bury, or outright kill so many stories, as I had the last few years, I wouldn\u2019t be able to respect myself. I decided to put the Iran and the NSA stories in my book. I was pretty sure that meant I would be fired from the Times. It was nerve-wracking, but my wife, Penny, stood firm. \u201cI won\u2019t respect you if you don\u2019t do it,\u201d she told me. That sealed my decision. Throughout early 2005, I worked at home on \u201cState of War,\u201d which was scheduled to be published by Free Press, an imprint of Simon and Schuster, in early 2006. After I wrote the chapter about the NSA domestic spying program, I called Lichtblau and asked him to come to my house. When he arrived, I told him to sit down, read the chapter, and let me know whether it was OK to put the story in my book. After he finished reading, he joked that I had buried the lede, but I sharply reminded him that writing a book was different from writing a news story. He gave his approval to include it in the book, since he knew the story was dead at the Times. He only asked that I mention him by name in the chapter \u2014\u00a0and spell his name correctly. While I was on book leave, Lichtblau was in an agonizing position. Barred by his editors from working on the NSA story, he was instead assigned to cover the debate in Congress over the reauthorization of the Patriot Act. But Lichtblau knew that the debate over how to strike the proper balance between national security and civil liberties was a charade so long as the existence of the NSA\u2019s domestic spying program was hidden from public view. The White House allowed Congress to publicly debate the balance, even while George W. Bush had already secretly decided what that balance would be. \u201cKnowing about the NSA program, I found it increasingly awkward to write about all the back-and-forth haranguing with a straight face,\u201d Lichtblau later wrote in his 2008 book, \u201cBush\u2019s Law: The Remaking of American Justice.\u201d \u201cAfter getting back to the office from one congressional hearing that I covered on the Patriot Act that spring of 2005, I walked straight over to Rebecca Corbett\u2019s desk in frustration to suggest that maybe someone else should cover the whole debate in Congress; in light of what we knew, I told her, I no longer felt comfortable covering what seemed a bit like a Washington game of three-card monte. \u2026 I was stuck on the story.\u201d While covering one congressional hearing, Lichtblau listened as Harman called for tighter restrictions on the Patriot Act to prevent abuses of civil liberties. Lichtblau knew that Harman had been briefed on the NSA program and had called the Times to kill our story, so he followed her out into the hall to talk about it. But when he asked her how she could square her demands for limits on the Patriot Act with what she knew about the NSA program, Harman chided him for raising the matter. \u201cShooing away her aides, she grabbed me by the arm and drew me a few feet away to a more remote section of the Capitol corridor,\u201d he wrote in his book. \u201c\u2018You should not be talking about that here,\u2019 she scolded me in a whisper. \u2018They don\u2019t even know about that,\u2019 she said, gesturing to her aides, who were now looking on at the conversation with obvious befuddlement. \u2018The Times did the right thing by not publishing that story.\u2019\u201d I returned from book leave in May 2005 and finished my manuscript later that summer. In the late summer or early fall, after I turned in the last chapters to my publisher and the editing process at Free Press was virtually complete, I decided to let the Times know what I was doing. I emailed Jill Abramson, who by then was in New York, and told her that I was putting the NSA and the Iran stories in my book. The reaction was swift. Within minutes, Taubman was standing near my desk, grimly demanding to talk. We went into his office. He said firmly that I was being insubordinate and rebelling against the editorial decisions of the Times. \u201cMy view was that you and The Times had joint ownership of the story, that the top news executives, after careful consideration, had decided to hold the story, and that you did not have a unilateral right to publish it in your book,\u201d Taubman recalled recently. \u201cAt the time, I was concerned that you were moving ahead with a volatile decision you had made absent consultation with me or Bill.\u201d Taubman also recalls that he was angry because he believed I had misled him before I went on book leave into thinking I was going to write a biography of George Tenet.\u00a0He\u2019s probably right. He wanted me to take the NSA story out of my book. I responded that I wanted the NSA story to be published both in the Times and in my book. We began to talk almost every day about how to resolve our impasse. Initially, I suggested the paper run the NSA story when my book came out, under the kind of arrangement that the Washington Post seemed to have with Bob Woodward. The Post regularly excerpted Woodward\u2019s books on its front page, giving the paper Woodward\u2019s scoops and his books enormous publicity. That proposal went nowhere. Eventually, Taubman countered that the paper would only consider running the NSA story if I first agreed to remove it from my book and thus, give the paper the chance to reconsider its publication without any undue pressure. But I knew the only reason the Times would even consider running the NSA story was if I kept it in my book. We were at loggerheads, and the clock was ticking toward the January 2006 publication of \u201cState of War.\u201d When I told Lichtblau what was going on, he joked, \u201cYou don\u2019t just have a gun to their heads. You\u2019ve got an Uzi.\u201d While researching this personal account, I was surprised to learn that Abramson recalls that my email to her was not what started a debate among the Times senior editors about what to do about the NSA story and my book. Abramson says that by the time I emailed her, she already knew that I was putting the NSA story in my book. She says that another reporter in the Washington bureau had told her earlier that I was going to do it, and she had already told Bill Keller about my plans. She said she told Keller that \u201cthey would look like idiots\u201d if they still were holding the story when it appeared in my book. \u201cThe classified program will be known publicly when Jim\u2019s book is published anyway. So what could possibly be the point of continuing to hold it?\u201d Abramson recalls telling him. \u201cI had previously revisited publishing with Keller once or twice \u2014 maybe more,\u201d Abramson told me recently. \u201cI wanted the story published.\u201d  Eric Lichtblau in Derwood, Maryland. Photo: Brooks Kraft LLC\/Corbis\/Getty Images That fall, I became so concerned that the Times would not run the NSA story and that I would be fired that I secretly met with another national news organization about a job. I told a senior editor there that I had a major story that the Times had refused to run under pressure from the White House. I didn\u2019t tell him anything about the story, but I said if they hired me, I would give the story to them. The senior editor replied that their publication would never run a piece if the White House raised objections on national security grounds. I left that meeting more depressed than ever. After a long series of contentious conversations stretching over several weeks in the fall of 2005, I finally reached an uneasy compromise with the editors. They would let Lichtblau and me start working on the NSA story again, and the paper would resume talks with the Bush administration over whether to publish it. But if the paper once again decided not to run the story, I had to take it out of my book. I agreed to those conditions, but I secretly knew that it was already too late to take the chapter out of my book, and I had no intention of doing so. I was gambling that the Times would run the story before the book was published. But I also knew that if the editors didn\u2019t run it, I would probably be out of a job. Curiously, the Times editors seemed to shrug off the Iran story, even though they knew it was going to be in the book too. Maybe the NSA story was fresher in their minds. We didn\u2019t have any significant discussions about whether to publish the Iran story in the paper, and the editors never complained to me about my decision to publish it in my book. (In 2014, Jill Abramson said in an interview with \u201c60 Minutes\u201d that she regretted that she didn\u2019t push harder to get the Times to publish the CIA-Iran story.) The editors began a new round of meetings with Bush administration officials, who were apparently surprised that the Times was resurrecting the NSA story. I was excluded from these conversations. In each of the meetings in which they sought to convince the editors not to run the story, Bush administration officials repeatedly said the NSA program was the crown jewel of the nation\u2019s counterterrorism programs, and that it saved American lives by stopping terrorist attacks. The meetings dragged on through the fall of 2005. Michael Hayden, now the principal deputy director of the Office of National Intelligence, often took the lead, and continued to meet with Philip Taubman. In one meeting, Taubman and Bill Keller received a secret briefing in which officials described the counterterrorism successes of the program. But when the two editors returned to the Washington bureau, they told Lichtblau and me that their briefing was off the record and so secret that they couldn\u2019t share what they\u2019d heard. Lichtblau and I eventually realized that Bush administration officials had been misleading Keller and Taubman. The officials had told them that under the NSA\u2019s secret domestic spying program, the agency didn\u2019t actually listen to any phone calls or read any emails without court-approved search warrants. They had insisted that the agency was only vacuuming up metadata, obtaining phone calling logs and email addresses. The content of the phone calls and email messages\u00a0was not being monitored, the officials had told the editors. That was not true, but the government had been trying to convince Keller and Taubman that Lichtblau and I had been exaggerating the scope of the story. It took time, but Lichtblau and I were finally able to persuade Keller and Taubman that they had been misled. In our recent interview, Keller said that once he realized the administration had been disingenuous with him, he started to change his mind about publishing the story. It was also critically important that Lichtblau had developed a new source who said that some Bush administration officials had expressed fears that they might face prosecution for their involvement in the secret NSA spying operation. There had also been an intense debate at the highest levels of the Bush administration about the legality of some aspects of the program. That raised fundamental questions about the reassurances the Times editors had received from administration officials about the program\u2019s standing. In the late fall of 2005, I also got an important lead from a new source, but the tip was so cryptic I didn\u2019t know what to make of it at the time. It came when a senior official agreed to see me, but only on the condition that our interview be conducted with other officials present. During that meeting, the official repeatedly and loudly expressed total ignorance of any secret NSA domestic spying program. But as I was leaving and stood up to shake hands, the official pulled me close and whispered, so quietly that none of the others in the room could hear, \u201cCheck out when Ashcroft was sick.\u201d Lichtblau and I struggled for weeks to figure out what that tip meant. By late November 2005, Keller seemed to be leaning toward publishing the NSA story, but the editors were moving so slowly that I was getting nervous that they wouldn\u2019t make up their minds before my book came out in January. I was more anxious than I had ever been in my life. I couldn\u2019t sleep and began to develop high blood pressure. I tried to distract myself by going to the movies, but I was so stressed that I would usually walk out after five or 10 minutes. I also had to keep meeting with key sources on the NSA story to persuade them to stick with me and not take the story elsewhere. I urged them to be patient, though I was running out of patience myself. During meetings on the NSA story with Lichtblau and Rebecca Corbett, I was so fatigued and stressed that I would often lie down and shut my eyes on the couch in Corbett\u2019s office. With the clock ticking and the publication of my book looming, Taubman asked me to arrange a new round of meetings with the very few Democratic congressional leaders who knew about the NSA program. He wanted them to tell him that it was OK to run the story. Both Lichtblau and I found this request troubling. I met with one Democrat who agreed to call Taubman, but the congressperson only told him the story was accurate, not whether the Times should run it. Taubman wanted more, so I went to see Nancy Pelosi, then the House minority leader, who had previously been the ranking Democrat on the House Intelligence Committee. After she read the story, I asked her if she would call Taubman. Without confirming the story, she said simply, \u201cThe New York Times is a big institution. It can make its own decisions.\u201d Then, after one final round of meetings with the White House in early December, Keller said he had decided to run the story. He called the White House and told them his decision. President Bush then called Arthur Sulzberger, the Times\u2019s publisher, and asked for a personal meeting and a chance to convince Sulzberger to overrule Keller. It was intimidating stuff, but I was confident that Sulzberger would view this as a chance to live up to the legacy of his father, who had published the Pentagon Papers in the face of threats from the Nixon White House. Sulzberger, Keller, and Taubman went to the Oval Office to meet Bush. Lichtblau and I were not invited to the meeting and were not even allowed to meet with Sulzberger to brief him on the story beforehand. Keller later said that Bush told Sulzberger he would have \u201cblood on your hands\u201d if he published the NSA story. Keller also said the meeting didn\u2019t change his or Sulzberger\u2019s minds about publishing the story. Keller and the other editors began to express confidence that the story would run, yet there was still no date for publication. In fact, the White House was trying to schedule more meetings with the editors to try one last time to change their minds. I was frantic; it was December, my book was coming out in early January, and the story still hadn\u2019t run in the Times. Finally, Lichtblau came in with new information that prompted the Times to publish the story. Just days after the Bush-Sulzberger Oval Office meeting, a source told Lichtblau that the White House had considered getting a court-ordered injunction to prevent the Times from publishing the story. This was electric news, because the last time that had happened at the Times was during the Pentagon Papers case in the 1970s, one of the most important events in the history of the newspaper. The debate about whether to run the story was over. By that afternoon, the piece was ready to go. But there was one last thing: Keller included a line in the story saying that the article had been held for a year at the request of the Bush administration, which had argued it would harm national security. The final version of the story stated: \u201cThe White House asked The New York Times not to publish this article, arguing that it could jeopardize continuing investigations and alert would-be terrorists that they might be under scrutiny. After meeting with senior administration officials to hear their concerns, the newspaper delayed publication for a year to conduct additional reporting.\u201d We had one advantage over the Times of the early 1970s during the Pentagon Papers crisis, and that was the internet. It would be much more difficult for the White House to go to court to stop publication because we could quickly put the story online. So soon after Keller called the White House to tell them the story was running, the NSA story was posted on the New York Times website. It ran on the front page on December 16, 2005. Lichtblau, Corbett, Taubman, and I sat in Taubman\u2019s office, listening over a speaker phone as Keller made the final decision to post the story. As the call with Keller ended, I let out a long sigh. Taubman looked at me. \u201cWhat\u2019s wrong?\u201d he asked. \u201cNothing. I\u2019m just relieved.\u201d In the years since, Taubman has reconsidered his decision-making on the NSA story, notably because of the leaks of NSA documents by former NSA contractor Edward Snowden, which revealed in greater detail the massive scale of the domestic surveillance operation. In the wake of Snowden\u2019s revelations, Taubman told Sullivan, the public editor, that his views had changed: \u201cI would have made a different decision had I known that Jim and Eric were tugging on a thread that led to a whole tapestry.\u201d (Ironically, the fact that the Times held the NSA story for more than a year convinced Edward Snowden not to come to the paper with his trove of documents when he became a whistleblower.) Today, Keller defends his handling of the NSA story \u2014 both in 2004 and in 2005 \u2014 and reflects that one factor was the changing climate in the country, which had soured on Bush, the war in Iraq, and the war on terror. \u201cI\u2019m pretty comfortable both with the decision not to publish and the decision to publish,\u201d he told me recently. Keller decided to give the story only a one-column headline. As Lichtblau wrote in his book, \u201cKeller had decided he didn\u2019t want it to look like we were poking the White House in the eye with a big, screaming headline about NSA spying; we wanted to be discreet, he said, and the story would speak for itself.\u201d I didn\u2019t care about the lack of a banner headline. My game of chicken with the Times was over, and I felt like I had won. Outgoing Attorney General John Ashcroft watches an honor guard in the Great Hall of the Justice Department in Washington\u00a0on Dec. 10, 2004. Photo: Pablo Martinez Monsivais\/AP The impact of the story was immediate and explosive. George W. Bush was forced to confirm the existence of the program, even as he called the leak of information about it a \u201cshameful act.\u201d The administration quickly ordered an investigation, which was handed over to a grand jury. Teams of FBI agents were soon trying to hunt down our sources. Congress was outraged that the Bush administration had hidden the NSA program from all but a select handful of senior congressional leaders. The story ran the day the Senate was scheduled to vote on the reauthorization of the Patriot Act. Saying the NSA program made a mockery of the Patriot Act, lawmakers forced the vote\u2019s delay. Both Republicans and Democrats vowed congressional investigations into the NSA program. Lichtblau and I scrambled to follow up with more stories, including one based on the strange tip I had received to check out what had happened when Ashcroft was sick. We learned that it was a reference to a secret rebellion against the NSA program by Comey and other top Justice Department officials, which had been triggered during a showdown with the White House in Ashcroft\u2019s hospital room in March 2004. After an unusually tight pre-publication embargo, \u201cState of War\u201d was published in the first week of January 2006, but not before the Bush administration tried to intervene. In his 2014 book, \u201cCompany Man: Thirty Years of Controversy and Crisis in the CIA,\u201d former CIA lawyer John Rizzo describes how he got a panicked call from a National Security Council staffer at the White House on New Year\u2019s Eve saying that it might be necessary to try to stop the publication of my book. That night, then-White House Counsel Harriet Miers called Rizzo, suggesting that he call Sumner Redstone, chairman of Viacom, to get him to stop the publication of the book by Simon and Schuster. Rizzo says he had decided not to make the call. Jack Romanos, chief executive of Simon and Schuster at the time, told me that other current and former government officials had also called, wanting to see the book before it was published. Simon and Schuster had refused. After it came out, the CIA was intensely angry about the book; one former CIA officer recalls that managers in his unit warned employees not to read \u201cState of War\u201d; doing so, they were told, would be like committing treason. I did a series of TV interviews to publicize the book. Thanks to the line in the Times\u2019s NSA story saying that the article had been held for a year at the Bush administration\u2019s request, the story behind the story was naturally a hot topic. But each time I was asked about it, I simply said the Times had performed a public service by publishing the story, adding that I wouldn\u2019t go into details about the paper\u2019s internal deliberations. I wanted to keep the focus on the substance of the story itself. Interviewers weren\u2019t always pleased with this. After a conversation with Katie Couric on the \u201cToday\u201d show, I quietly told her I was sorry I couldn\u2019t answer her question. \u201cYeah, bullshit,\u201d she replied. The Times also refused to explain the decision to hold the story, stonewalling media reporters and even the paper\u2019s own public editor. \u201cThe New York Times\u2019s explanation of its decision to report, after what it said was a one-year delay, that the National Security Agency is eavesdropping domestically without court-approved warrants was woefully inadequate,\u201d wrote Times Public Editor Byron Calame in early 2006. \u201cAnd I have had unusual difficulty getting a better explanation for readers, despite the paper\u2019s repeated pledges of greater transparency. For the first time since I became public editor, the executive editor and the publisher have declined to respond to my requests for information about news-related decision-making.\u201d Several weeks later, I discovered that the Times hierarchy had been paying close attention to what I had been saying on my book tour about the paper\u2019s decision-making. Lichtblau, Taubman, and I were asked to make a special presentation to the Times board of directors about the NSA story. Over lunch, one of the board members leaned over to me and quietly told me they were very grateful for the way I had been handling myself on television. Lichtblau and I won a Pulitzer Prize for our NSA stories. When the Times wins a Pulitzer, work in the main newsroom in New York stops at 3 p.m. on the day the prizes are officially announced, and the winners give brief speeches to the entire staff. When it was my turn, I got up and looked around at the crowd. I felt awkward, unsure what to say. For months, I had secretly lived with the fear of being fired for insubordination; now I was being honored for the same thing, by the same people. I decided to leave that issue alone for the day. I looked over at Keller and Sulzberger and said, \u201cWell, thanks. You guys know what happened, how tough this was.\u201d Meanwhile, the political and legal reverberations from the NSA story were unlike those from any story I had ever done, and they continued to build. The story quickly led to protests and congressional hearings, lawsuits against the government and telecommunications companies, and calls for the creation of a new Church Committee. Fearful that the investigative floodgates were about to open, the Bush administration launched an aggressive campaign to counter the mounting criticism. In January 2006, it issued a \u201cwhite paper\u201d laying out its arguments for why the program was legal and, behind the scenes, began lobbying leading members of Congress to stem calls for a major congressional inquiry. But the White House knew the NSA program was on thin ice, particularly after a federal judge sided with the American Civil Liberties Union in a lawsuit and declared the program unconstitutional. White House anger at the New York Times for publishing the story grew. In May, Gonzales told ABC that the government could prosecute journalists for publishing classified information. He clearly had us in mind when he said it. \u201cThere are some statutes on the book which, if you read the language carefully, would seem to indicate that that is a possibility,\u201d Gonzales said.  Senate Intelligence Committee member Sen. Tom Cotton, R-Ark., arrives for a closed-door committee meeting in the Hart Senate Office Building on Capitol Hill in 2017. Photo: Chip Somodevilla\/Getty Images Lichtblau had nicknamed one source \u201cDeath Wish\u201d because during each interview he would say that he couldn\u2019t talk about the SWIFT operation, and then would proceed to talk about it. This time, Lichtblau took the lead and was far more aggressive on the story than I was. I was burned out from the fight over the NSA story, and I wasn\u2019t sure I was ready for another battle with the government over publication. I started to lose my nerve, privately suggesting to Lichtblau that we should delay the SWIFT story. Briefly panicking, I even thought about pulling my byline from the story. Fortunately, Lichtblau called me on it. I quickly recovered from my malaise, and we finished the story. The editors at the Times, no doubt embarrassed by the way they had handled the NSA story, were now aggressively pushing us to do the SWIFT story and others. The Bush administration made only a halfhearted effort to block the SWIFT story. Treasury Secretary John Snow asked Keller not to run it, and a few other officials weighed in, but that was about it. Yet it was after the SWIFT story\u2019s publication in June 2006 that the real onslaught against us began, led by conservatives in Congress and around the country who accused us of being repeat offenders harming national security. Lichtblau and I had faced a storm of criticism after the NSA story; now the outcry grew far more intense. Right-wing groups organized hate mail campaigns against us and staged small but noisy protests outside the Washington bureau and the Times building in New York. Conservative pundits and members of Congress went on television calling for Keller, Lichtblau, and me to be punished. Tom Cotton, then an Army officer in Iraq, wrote a letter to the Times saying that Lichtblau, Keller, and I should be jailed for harming national security. The Times didn\u2019t publish the letter, but it was picked up in the right-wing online universe of the time, and Cotton shot to fame in conservative circles as a result. He was later elected to the Senate as a Republican from Arkansas and soon may be named director of the CIA. Our notoriety also brought out conspiracy theorists and people who claimed the government was after them. Returning to the Washington bureau one day after lunch, I saw a lone man standing on the sidewalk in front of the building\u2019s entrance, holding a sign claiming government harassment. \u201cHello, James,\u201d he said as I approached. \u201cI\u2019ve been waiting for you.\u201d More ominously, the Bush administration now had two major leak investigations underway \u2014 one into the NSA story in the New York Times, and the other into the Iran story I had included in \u201cState of War.\u201d Working with federal grand juries in Alexandria, FBI agents were starting to interview people I knew. Then there was a long lull, lasting more than a year. I thought the administration had decided not to take any action. But in August 2007, I found out that the government hadn\u2019t forgotten about me. Penny called to tell me that a FedEx envelope had arrived from the Justice Department. It was a letter saying the DOJ was conducting a criminal investigation into \u201cthe unauthorized disclosure of classified information\u201d in \u201cState of War.\u201d The letter was apparently sent to satisfy the requirements of the Justice Department\u2019s internal guidelines that lay out how prosecutors should proceed before issuing subpoenas to journalists to testify in criminal cases. The Bush administration had evidently made a strategic decision not to go after the New York Times for our NSA stories. They apparently didn\u2019t want a constitutional showdown with the newspaper. So instead, they were coming after me for what I\u00a0had written in my book. I realized they were trying to isolate me from the Times. I later learned that the Justice Department and FBI had investigated a wide range of information included in several different chapters in my book before settling on the chapter that included the CIA-Iran story. Simon and Schuster agreed to pay for my legal defense until 2011, when my lawyers, Joel Kurtzberg and David Kelley of Cahill Gordon & Reindel, agreed to continue to handle the case pro bono. The New York Times did not pay any of my legal bills. When my lawyers called the Justice Department about the letter I had received, prosecutors refused to assure them that I was not a \u201csubject\u201d of their investigation. That was bad news. If I were considered a \u201csubject,\u201d rather than simply a witness, it meant the government hadn\u2019t ruled out prosecuting me for publishing classified information or other alleged offenses. In January 2008, the Justice Department subpoenaed me to testify before a federal grand jury. I refused to comply, and my lawyers moved to quash the subpoena.  Former CIA officer Jeffrey Sterling leaves the Alexandria Federal Courthouse on\u00a0Jan. 26, 2015, in Alexandria, Virginia. Photo: Kevin Wolf\/AP One of the most egregious government plans to target me was not directly related to the leak investigation into \u201cState of War.\u201d But it apparently was under consideration at the same time I was fighting the government\u2019s efforts to force me to testify. I have obtained evidence of an FBI agent discussing plans to ambush a meeting that the FBI thought was about to occur between a source and me in 2014. The evidence shows that the ambush plan was reviewed by senior Justice Department officials, who insisted the FBI make certain I would not be present when they arrested my source at the meeting. The evidence also reveals that the FBI was planning ways I could be delayed or diverted from the meeting at the last minute so they could arrest the source without me there. No such meeting occurred, so the FBI\u2019s planned ambush was never conducted. But when I was later told about the FBI plan, it made me realize the degree to which I had become a focus of government investigators. The FBI declined to comment. In another recent incident that gave me chilling insight into the power of government surveillance, I met with a sensitive and well-placed source through an intermediary. After the meeting, which occurred a few years ago in Europe, I began to do research on the source. About an hour later, I got a call from the intermediary, who said, \u201cStop Googling his name.\u201d In January 2008, after I received the first subpoena related to the CIA-Iran story in \u201cState of War,\u201d a series of procedural motions prolonged the fight over whether I would be forced to testify before the grand jury until after the 2008 presidential election. I thought Barack Obama\u2019s election would end the case. U.S. District Judge Leonie Brinkema seemed to think so, too. In July 2009, she issued a brief ruling noting that the grand jury in the case had expired, meaning my subpoena was no longer valid. I was surprised when Obama\u2019s Justice Department quickly told Brinkema they wanted to renew the subpoena. In hindsight, this was one of the earliest signals that Obama was determined to extend and even expand many of Bush\u2019s national security policies, including a crackdown on whistleblowers and the press. Ignoring the possible consequences to American democracy, the Obama administration began aggressively conducting surveillance of the digital communications of journalists and potential sources, leading to more leak prosecutions than all previous administrations combined. My case ground on for the next few years. It moved slowly because each time the administration came after me through some procedural motion or new subpoena, Brinkema sided with me. Her rulings in my favor meant that I was never actually ordered to testify before the grand jury. I kept thinking the administration would get the message from Brinkema and abandon the case. Instead, the Obama administration charged Jeffrey Sterling, the former CIA officer, for allegedly leaking information used in the CIA-Iran story.  U.S. Attorney General Eric Holder on\u00a0Dec. 4, 2014, in Cleveland, Ohio. Photo: Angelo Merendino\/Getty Images Brinkema quashed that subpoena, too; once again, I thought I was off the hook. But just days before the trial was to begin, the Justice Department appealed. Obama administration prosecutors told the appeals court that Brinkema\u2019s ruling should be overturned because there was no such thing as a reporter\u2019s privilege in a criminal case. The appeals court accepted that argument, reversing Brinkema and ordering me to testify. The government\u2019s rationale transformed my case into a showdown over press freedom in the United States. I felt that I had no choice but to appeal to the Supreme Court. Some outside media lawyers made it clear that they didn\u2019t want me to do that because it might lead to a bad ruling from a conservative majority. That debate became moot in 2014, when the Supreme Court refused to take up the case. That allowed the appeals court ruling to stand, leaving the legal destruction of a reporter\u2019s privilege in the 4th Circuit as Obama\u2019s First Amendment legacy. But the government\u2019s landmark legal victory came at a cost to the administration and particularly to Obama\u2019s attorney general, Eric Holder. For years, my lawyers and I had waged our legal campaign mostly alone with little fanfare, but as the case barreled toward a climax, the news coverage and publicity reached a fever pitch. With the surge in media attention came added pressure on Holder, and he finally began to back down. He said that as long as he was attorney general, no reporter would go to jail for doing their job. He also modified the Justice Department guidelines that defined when the government would seek to compel the testimony of journalists in leak investigations. (Donald Trump\u2019s Justice Department is now widely expected to weaken those guidelines, making it easier to\u00a0go after reporters.) But even though Holder was making conciliatory public statements, the federal prosecutors directly involved in my case kept fighting hard. At one point, Holder hinted that the Justice Department and I were about to strike a deal, when in fact the prosecutors and my lawyers hadn\u2019t negotiated any deal at all. Behind the scenes, there seemed to be a war going on between Holder and the prosecutors, who were angry at what they perceived as Holder undercutting them. The prosecutors had repeatedly told the court that they needed my testimony to make their case against Sterling. Holder, after supporting their aggressive approach for years, had suddenly reversed direction under public pressure. I was caught in the middle. Finally, in late 2014, we saw the first signs that the prosecutors were softening their stance. They demanded my appearance at the January 2015 pretrial hearing in Alexandria to determine the scope of my possible testimony in Sterling\u2019s trial. But unlike the previous subpoenas, this new one sought my limited testimony and did not demand that I identify confidential sources. Then, as I sat in the conference room with my lawyers waiting for the hearing to start, the prosecutors launched a last-minute offensive. They said they wanted me to get on the witness stand and point out which passages in my book were based on classified information and confidential sources. I refused.  James Risen, center, leaves the federal courthouse in Alexandria, Virginia,\u00a0on Jan. 5, 2015. Photo: Cliff Owen\/AP The lead prosecutor, James Trump (no apparent relation to Donald), did not ask who my sources were or what they\u2019d told me. Instead, he asked again whether I would refuse to identify my sources, even if it meant going to jail. I told him I would. Trump then turned to questions I had already answered in court filings, asking whether I had indeed relied on confidential sources and trying to confirm that I had spoken to Sterling for an unrelated 2002 New York Times article. He wanted me to answer them aloud, in open court. I was defiant and refused to answer basic questions. Brinkema began to lose patience with me. I asked for a break to talk with my lawyers. I returned to the witness stand and answered a few more routine questions. Trump soon announced that he had nothing more for me. In the end, the prosecutors had backed down and followed Holder\u2019s instructions. It was all over, almost before I realized it. I walked out of the court and drove straight home. I believe my willingness to fight the government for seven years may make prosecutors less eager to force other reporters to testify about their sources. At the same time, the Obama administration used my case to destroy the legal underpinnings of the reporter\u2019s privilege in the 4th Circuit, which means that if the government does decide to go after more reporters, those reporters will have fewer legal protections in Virginia and Maryland, home to the Pentagon, the CIA, and the NSA, and thus the jurisdiction where many national security leak investigations will be conducted. That will make it easier for Donald Trump and the presidents who come after him to conduct an even more draconian assault on press freedom in the United States. The battles over national security reporting in the years after 9\/11 have yielded mixed results. In my view, the mainstream media has missed some key lessons from the debacle over WMD reporting before the war in Iraq. Times reporter Judy Miller became an easy scapegoat, perhaps because she was a woman in the male-dominated field of national security reporting. Focusing on her made it easier for everyone to forget how widespread the flawed pre-war reporting really was at almost every major media outlet. \u201cThey wanted a convenient target, someone to blame,\u201d Miller told me recently. The anti-female bias \u201cwas part of it.\u201d She notes that one chapter in her 2015 memoir, \u201cThe Story: A Reporter\u2019s Journey,\u201d is titled \u201cScapegoat.\u201d Since then, I believe the Times, the Washington Post and other national news organizations have sometimes hyped threats from terrorism and weapons of mass destruction. The exaggerated reporting on terrorism, in particular, has had a major political impact in the United States and helped close off debate in Washington over whether to significantly roll back some of the most draconian counterterrorism programs, like NSA spying. But overall, I do believe that the fight inside the Times over the NSA story helped usher in a new era of more aggressive national security reporting at the paper. Since then, the Times has been much more willing to stand up to the government and refuse to go along with White House demands to hold or kill stories. The greatest shame of all is that Jeffrey Sterling was convicted and sentenced to 42 months in prison. Update: April 2, 2018\nThis story has been updated to add the name of a New York Times researcher who worked on the NSA story. James Risenjim.risen@\u200bfirstlook.org Email list managed by MailChimp","time":1525811000,"title":"My Life as a New York Times Reporter in the Shadow of the War on Terror","type":"story","url":"https:\/\/theintercept.com\/2018\/01\/03\/my-life-as-a-new-york-times-reporter-in-the-shadow-of-the-war-on-terror","label":7,"label_name":"random"},{"by":"willsinclair","descendants":0,"id":17024637,"kids":"None","score":3,"text":"For the longest times, developers have taken Chrome OS machines and run tools like Crouton to turn them into Linux-based developer machines. That was a bit of a hassle, but it worked. But things are getting easier. Soon, if you want to run Linux apps on your Chrome OS machine, all you\u2019ll have to do is switch a toggle in the Settings menu. That\u2019s because Google is going to start shipping Chrome OS with a custom virtual machine that runs Debian Stretch, the current stable version of the operating system. It\u2019s worth stressing that we\u2019re not just talking about a shell here, but full support for graphical apps, too. That means you could now, for example, run Microsoft\u2019s Linux version of Visual Studio Code right on your Chrome OS machine. Or build your Android app in Android Studio and test it right on your laptop, thanks to the built-in support for Android apps that came to Chrome OS last year.  The first preview of Linux on Chrome OS is now available on the Pixelbook, with support for more devices coming soon. Google\u2019s  PM director for Chrome OS Kan Liu told me the company was obviously aware that people were using Crouton to do this before. But doing this also meant doing away with all of the security features that come with Google\u2019s operating system. And as more powerful Chrome OS machines hit the market in recent years, the demand for a feature like this also grew. To enable support for graphical apps, the team opted to integrate the Wayland display server; from the user\u2019s perspective, the actual window dressing will look the same as any other Android or web app on Chrome OS. Most regular users won\u2019t necessarily benefit from built-in Linux support, but this will make Chrome OS machines even more attractive to developers \u2014 especially the more high-end ones like Google\u2019s own Pixelbook. Liu stressed that his team spent quite a bit of work optimizing the virtual machine, too, so there isn\u2019t a lot of overhead when you run Linux apps \u2014 meaning that even less powerful machines should be able to handle a code editor without issues. Now, it\u2019s probably only a matter of hours before somebody starts running Windows apps in Chrome OS with the help of the Wine emulator. ","time":1525810984,"title":"You can now run Linux apps on Chrome OS","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/you-can-now-run-linux-apps-on-chrome-os\/","label":9,"label_name":"tech"},{"by":"andruby","descendants":0,"id":17024614,"kids":"None","score":1,"text":"Topics: \n\n\n\u00a0Blog\n In February 2017, we launched Load Balancers, our highly available and managed load balancing service. Thousands of users rely on them to distribute traffic across Web and application servers.  Today, we\u2019re announcing significant upgrades to Load Balancers, including Let's Encrypt integration and HTTP\/2 support. All users now have access to these features at no additional cost and with no action required. In fact, all existing Load Balancers already have been upgraded. Let\u2019s Encrypt Integration Load Balancers now support a simple method to generate, manage, and maintain SSL certificates using Let\u2019s Encrypt. With a couple of clicks, you can add a free Let\u2019s Encrypt SSL certificate to your Load Balancer to secure your traffic and offload SSL processing. Certificates will automatically renew, so you don't have to worry about a thing.  HTTP\/2 Load Balancers now also support the HTTP\/2 protocol, which is a major update to HTTP\/1.x designed primarily to reduce page load time and resource usage. You can find this under the Forwarding Rules dropdown in your Load Balancer settings. Load Balancers can additionally terminate HTTP\/2 client connections to act as a gateway to HTTP\/1.x applications, allowing you to take advantage of HTTP\/2's performance and security improvements without upgrading your backend servers. Keep a look out for more performance-focused announcements in the coming months.  Our improved Load Balancers are available in all regions for the same price of $20\/month. For more information about Load Balancers, please check out our website and these community articles: An Introduction to DigitalOcean Load Balancers How to Use Let\u2019s Encrypt with DigitalOcean Load Balancers Best Practices for Performance on DigitalOcean Load Balancers Happy coding, \nTyler Crandall \nProduct Manager \n      Copyright \u00a9 2018 DigitalOcean \u2122 Inc.\n    ","time":1525810868,"title":"DigitalOcean Introduces Load Balancer Support for Let\u2019s Encrypt and HTTP\/2","type":"story","url":"https:\/\/blog.digitalocean.com\/introducing-load-balancer-upgrades\/","label":3,"label_name":"dev"},{"by":"pauldprice","descendants":1,"id":17024575,"kids":"[17024608]","score":2,"text":"\n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\n        Working...\n    \n \n\n\nLoading...\n    \n \n\n\n        Working...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\n        Working...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n In our first livestream, Alan Noon from our Learning Resources team and Brian Schwab from our Interaction Lab talk us through best practices for designing for spatial computing.Become a Magic Leap creator at: https:\/\/www.magicleap.com\/creator. \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\nLoading...\n    \n \n\n\n        Working...\n    \n \n\n\n        Loading playlists...\n    \n","time":1525810690,"title":"Magic Leap First Livestream","type":"story","url":"https:\/\/www.youtube.com\/watch?v=gHf9CMRLVbk&index=0&t=39m34s","label":7,"label_name":"random"},{"by":"kevitivity","descendants":0,"id":17024572,"kids":"None","score":1,"text":"Advanced Search Edited by Lisa Tauxe, University of California, San Diego, La Jolla, CA, and approved April 6, 2018 (received for review January 16, 2018) This article requires a subscription to view the full text. If you have a subscription you may use the login form below to view the article. Access to this article can also be purchased. Rhythmic climate cycles of various assumed frequencies recorded in sedimentary archives are increasingly used to construct a continuous geologic timescale. However, the age range of valid theoretical orbital solutions is limited to only the past 50 million years. New U\u2013Pb zircon dates from the Chinle Formation tied using magnetostratigraphy to the Newark\u2013Hartford astrochronostratigraphic polarity timescale provide empirical confirmation that the unimodal 405-kiloyear orbital eccentricity cycle reliably paces Earth\u2019s climate back to at least 215 million years ago, well back in the Late Triassic Period. The Newark\u2013Hartford astrochronostratigraphic polarity timescale (APTS) was developed using a theoretically constant 405-kiloyear eccentricity cycle linked to gravitational interactions with Jupiter\u2013Venus as a tuning target and provides a major timing calibration for about 30 million years of Late Triassic and earliest Jurassic time. While the 405-ky cycle is both unimodal and the most metronomic of the major orbital cycles thought to pace Earth\u2019s climate in numerical solutions, there has been little empirical confirmation of that behavior, especially back before the limits of orbital solutions at about 50 million years before present. Moreover, the APTS is anchored only at its younger end by U\u2013Pb zircon dates at 201.6 million years before present and could even be missing a number of 405-ky cycles. To test the validity of the dangling APTS and orbital periodicities, we recovered a diagnostic magnetic polarity sequence in the volcaniclastic-bearing Chinle Formation in a scientific drill core from Petrified Forest National Park (Arizona) that provides an unambiguous correlation to the APTS. New high precision U\u2013Pb detrital zircon dates from the core are indistinguishable from ages predicted by the APTS back to 215 million years before present. The agreement shows that the APTS is continuous and supports a stable 405-kiloyear cycle well beyond theoretical solutions. The validated Newark\u2013Hartford APTS can be used as a robust framework to help differentiate provinciality from global temporal patterns in the ecological rise of early dinosaurs in the Late Triassic, amongst other problems. Author contributions: D.V.K. and P.E.O. designed research; D.V.K., P.E.O., C.R., C.L., R.M., R.B.I., J.W.G., and W.G.P. performed research; R.B.I. and J.W.G. assisted in recovery and description of core; R.B.I. and W.G.P. provided regional geologic context; W.G.P. assisted in providing access to the Petrified Forest National Park; D.V.K., C.R., R.M., G.E.G., and D.G. analyzed data; and D.V.K., P.E.O., and R.M. wrote the paper. The authors declare no conflict of interest. This article is a PNAS Direct Submission. This article contains supporting information online at www.pnas.org\/lookup\/suppl\/doi:10.1073\/pnas.1800891115\/-\/DCSupplemental. Published under the PNAS license. Subscribers, for more details, please visit our Subscriptions FAQ. Thank you for your interest in spreading the word on PNAS. NOTE: We only request your email address so that the person you are recommending the page to knows that you wanted them to see it, and that it is not junk mail. We do not capture any email address. Submit  Feedback\u00a0\u00a0\u00a0\u00a0Privacy\/Legal Copyright \u00a9 2018 National Academy of Sciences.","time":1525810681,"title":"Study Suggests Jupiter and Venus Affect Earth's Climate","type":"story","url":"http:\/\/www.pnas.org\/content\/early\/2018\/05\/01\/1800891115","label":5,"label_name":"ml"},{"by":"buttscicles","descendants":0,"id":17024522,"kids":"None","score":2,"text":"In two lab experiments, nearly 800 people completed tasks designed to measure their cognitive capacity. Before completing these tasks,\u00a0the researchers asked participants to either: place their phones in front of them (face-down on their desks); keep them in their pockets or bags; or leave them in another room. The results were striking: the closer the phone to the participant, the worse she fared on the task.\u00a0The mere presence of our smartphones is like the sound of our names or a crying baby \u2014 something that\u00a0automatically\u00a0exerts a gravitational pull on our attention. Resisting that pull takes a cognitive toll. \u201cPut your phone away\u201d has become a commonplace phrase that is just as often dismissed. Despite wanting to be in the moment, we often do everything within our power to the contrary. We take out our phones to take pictures in the middle of festive family meals, and send text messages or update our social media profiles in the middle of a date or while watching a movie. At the same time, we are often interrupted passively by notifications of emails or phone calls. Clearly, interacting with our smartphones affects our experiences. But can our smartphones affect us even when we aren\u2019t interacting with them \u2014 when they are simply nearby? In recent research, we investigated whether merely having one\u2019s own smartphone nearby could influence cognitive abilities. In two lab experiments, nearly 800 people completed tasks designed to measure their cognitive capacity. In one task, participants simultaneously completed math problems and memorized random letters. This tests how well they can keep track of task-relevant information while engaging in a complex cognitive task. In the second task, participants saw a set of images that formed an incomplete pattern, and chose the image that best completed the pattern. This task measures \u201cfluid intelligence,\u201d or people\u2019s ability to reason and solve novel problems. Performance on both of these tasks is affected by individuals\u2019 available mental resources. Our intervention was simple: before completing these tasks, we asked participants to either place their phones in front of them (face-down on their desks), keep them in their pockets or bags, or leave them in another room. Importantly, all phones had sound alerts and vibration turned off, so the participants couldn\u2019t be interrupted by notifications. The results were striking: individuals who completed these tasks while their phones were in another room performed the best, followed by those who left their phones in their pockets. In last place were those whose phones were on their desks. We saw similar results when participants\u2019 phones were turned off: people performed worst when their phones were nearby, and best when they were away in a separate room. Thus, merely having their smartphones out on the desk led to a small but statistically significant impairment of individuals\u2019 cognitive capacity \u2014 on par with effects of lacking sleep. This cognitive capacity is critical for helping us learn, reason, and develop creative ideas. In this way, even a small effect on cognitive capacity can have a big impact, considering the billions of smartphone owners who have their devices present at countless moments of their lives. This means that in these moments, the mere presence of our smartphones can adversely affect our ability to think and problem-solve \u2014 even when we aren\u2019t using them. Even when we aren\u2019t looking at them. Even when they are face-down. And even when they are powered off altogether. Why are smart phones so distracting, even when they\u2019re not buzzing or chirping at us? The costs of smartphones are inextricably linked to their benefits. The immense value smartphones provide, as personal hubs connecting us to each other and to virtually all of the world\u2019s collective knowledge, necessarily positions them as important and relevant to myriad aspects of our everyday lives. Research in cognitive psychology shows that humans learn to automatically pay attention to things that are habitually relevant to them, even when they are focused on a different task. For example, even if we are actively engaged in a conversation, we will turn our heads when someone says our name across the room. Similarly, parents automatically attend to the sight or sound of a baby\u2019s cry. Our research suggests that, in a way, the mere presence of our smartphones is like the sound of our names \u2014 they are constantly calling to us, exerting a gravitational pull on our attention. If you have ever felt a \u201cphantom buzz\u201d you inherently know this. Attempts to block or resist this pull takes a toll by impairing our cognitive abilities. In a poignant twist, then, this means that when we are successful at resisting the urge to attend to our smartphones, we may actually be undermining our own cognitive performance. Are you affected? Most likely. Consider the most recent meeting or lecture you attended: did anyone have their smartphone out on the table? Think about the last time you went to the movies, or went out with friends, read a book, or played a game: was your smartphone close by? In all of these cases, merely having your smartphone present may have impaired your cognitive functioning. Our data also show that the negative impact of smartphone presence is most pronounced for individuals who rank high on a measure capturing the strength of their connection to their phones \u2014 that is, those who strongly agree with statements such as \u201cI would have trouble getting through a normal day without my cell phone\u201d and \u201cIt would be painful for me to give up my cell phone for a day.\u201d In a world where people continue to increasingly rely on their phones, it is only logical to expect this effect to become stronger and more universal. We are clearly not the first to take note of the potential costs of smartphones. Think about the number of fatalities associated with driving while talking on the phone or texting, or of texting while walking. Even hearing your phone ring while you\u2019re busy doing something else can boost your anxiety. Knowing we have missed a text message or call leads our minds to wander, which can impair performance on tasks that require sustained attention and undermine our enjoyment. Beyond these cognitive and health-related consequences, smartphones may impair our social functioning: having your smartphone out can distract you during social experiences and make them less enjoyable. With all these costs in mind, however, we must consider the immense value that smartphones provide. In the course of a day, you may use your smartphone to get in touch with friends, family, and coworkers; order products online; check the weather; trade stocks; read HBR; navigate your way to a new address, and more. Evidently, smartphones increase our efficiency, allowing us to save time and money, connect with others, become more productive, and remain entertained. So how do we resolve this tension between the costs and benefits of our smartphones? Smartphones have distinct uses. There are situations in which our smartphones provide a key value, such as when they help us get in touch with someone we\u2019re trying to meet, or when we use them to search for information that can help us make better decisions. Those are great moments to have our phones nearby. But, rather than smartphones taking over our lives, we should take back the reins: when our smartphones aren\u2019t directly necessary, and when being fully cognitively available is important, setting aside a period of time to put them away \u2014 in another room \u2014 can be quite valuable. With these findings in mind, students, employees, and CEOs alike may wish to maximize their productivity by defining windows of time during which they plan to be separated from their phones, allowing them to accomplish tasks requiring deeper thought. Moreover, asking employees not to use their phones during meetings may not be enough. Our work suggests that having meetings without phones present can be more effective, boosting focus, function, and the ability to come up with creative solutions. More broadly, we can all become more engaged and cognitively adept in our everyday lives simply by putting our smartphones (far) away. Kristen Duke is a PhD candidate in Marketing at the Rady School of Management, University of California, San Diego. She studies how uncertainty, emotional complexity, and contextual factors affect decision-making and consumer experiences. Adrian Ward is an Assistant Professor of Marketing in the McCombs School of Business, University of Texas at Austin. His research focuses on technology and cognition, consumer financial decision-making, and morality. Ayelet Gneezy is an Associate Professor of Behavioral Sciences and Marketing at the Rady School of Management, University of California, San Diego. Her research focuses on consumers\u2019 judgment and decision-making, prosocial and charitable behavior, and behavior change. Maarten Bos is a visiting scholar at the Department of Social & Decision Sciences, Carnegie Mellon University. His research interests include decision science, persuasion, and behavioral economics.","time":1525810473,"title":"Having your smartphone nearby takes a toll on your thinking","type":"story","url":"https:\/\/hbr.org\/2018\/03\/having-your-smartphone-nearby-takes-a-toll-on-your-thinking","label":7,"label_name":"random"},{"by":"pmoriarty","descendants":0,"id":17024499,"kids":"None","score":1,"text":"There have been many toilet-related injuries and deaths throughout history and in urban legends.   Infants and toddlers have fallen into toilets and drowned. Safety devices exist to help prevent such accidents.[1][2] Young boys may experience a genital injury if the toilet seat falls down while they are standing at a toilet.[3] Injuries to adults include bruised buttocks, tail bones, and dislocated hips from unsuspectingly sitting on the toilet bowl rim because the seat is up or loose. Injuries can also be caused by pinching due to splits in plastic seats or by splinters from wooden seats, or if the toilet itself collapses under the weight of the user. Older high tank cast iron cisterns have been known to detach from the wall when the chain is pulled to flush, causing injuries to the user. The 2000 Ig Nobel Prize in Public Health was awarded to three physicians from the Glasgow Western Infirmary for a 1993 case report on wounds sustained to the buttocks due to collapsing toilets.[4] Furthermore, injuries are frequently sustained by people who stand on toilet seats to reach a height, and slip. There are also instances of people slipping on a wet bathroom floor or from a bath and concussing themselves on the fixture. Toilet related injuries are also surprisingly common, with some estimates ranging up to 40,000 injuries in the US every year.[5] In the past, this number would have been much higher, due to the material from which toilet paper was made. This was shown in a 1935 Northern Tissue advertisement which depicted splinter-free toilet paper.[6] In 2012, 2.3 million toilets in the United States, and about 9,400 in Canada, were recalled due to faulty pressure-assist flush mechanisms which put users at risk of the fixture exploding.[7] There are also injuries caused by animals. Some black widow spiders like to spin their web below the toilet seat because insects abound in and around it. Therefore, several persons have been bitten while using a toilet, particularly an outhouse toilet. Although there is immediate pain at the bite site, these bites are rarely fatal.[8] The danger of spiders living beneath toilet seats is the subject of Slim Newton's comic 1972 country song The Redback on the Toilet Seat. It has been reported that in some cases rats crawl up through toilet sewer pipes and emerge in the toilet bowl, so that toilet users may be at risk of being bitten by a rat.[9] However, many rat exterminators do not believe this, as pipes, at generally six inches (15 centimeters) wide, are too large for rats to climb and are also very slippery. Reports by janitors are always on the top floor, and could involve the rats on the roof, entering the soil pipe through the roof vent, lowering themselves into the pipe and then into the toilet.[10] In May 2016, an 11\u00a0ft snake, a reticulated python, emerged from a squat toilet and bit the man using it on his penis at his home in Chachoengsao Province, Thailand. Both victim and python survived.[11][12] Some instances of toilet-related deaths are attributed to the drop in blood pressure due to the parasympathetic nervous system during bowel movements. This effect may be magnified by existing circulatory issues. It is further possible that people succumb on the toilet to chronic constipation, because the Valsalva maneuver is often dangerously used to aid in the expulsion of feces from the rectum during a bowel movement. According to Sharon Mantik Lewis, Margaret McLean Heitkemper and Shannon Ruff Dirksen, the \"Valsalva maneuver \u2026 occurs during straining to pass a hardened stool. If defecation is suppressed over long periods, problems can occur, such as constipation or stool impaction. Defecation can be facilitated by the Valsalva maneuver. This maneuver involves contraction of the chest muscles on a closed glottis with simultaneous contraction of the abdominal muscles.\"[13] This means that people can die while \"straining at stool.\" In chapter 8 of their Abdominal Emergencies, David Cline and Latha Stead wrote that \"autopsy studies continue to reveal missed bowel obstruction as an unexpected cause of death\".[14] A 2001 Sopranos episode \"He is Risen\" shows a fictional depiction of the risk, when the character Gigi Cestone has a heart attack on the toilet of his social club while straining to defecate.[15] In the Victorian era, there was a perceived risk of toilets exploding. These scenarios typically include a flammable substance either accidentally or deliberately being introduced into the toilet water, and a lit match or cigarette igniting and exploding the toilet.[16] In 2014, Sloan's Flushmate pressure-assisted flushing system which uses compressed air to force waste down the drain was recalled after the company received reports of the air tank failing under pressure and shattering the porcelain.[17] In 1945, the German submarine U-1206 was sunk after a toilet malfunctioned, resulting in seawater flooding into the hull, which when coming into contact with a battery, created chlorine gas, forcing the submarine to resurface. At the surface, it was discovered and sunk by Allied Forces. This case may not be due to toilet malfunction, due to the possibility that the pressurized flushing system in the U-Boats, which was incredibly complex and required a training course to operate, may not have been properly operated.[18] Edmund Ironside was possibly stabbed from under a toilet seat while defecating. Godfrey the Hunchback, Duke of Lower Lorraine (an area roughly coinciding with the Netherlands and Belgium) was murdered in 1069 when staying in the Dutch city of Vlaardingen. Supposedly, the assassin made sure which of the latrines, which were built and drained on the outer side of the wall, according to medieval building style, belonged to the duke\u2019s sleeping room, and took a position underneath. Some sources say that a sword was used for the assassination; others mention a sharp iron weapon, which could have been a sword but also a spear or a dagger, but a spear seems to be the most practical choice. After being stabbed in the bottom it took him several days to die. The assassination was ordered by Dirk V Count of Holland and his ally Robrecht the Frisian, Count of Flanders.[19] King Wenceslaus III of Bohemia was murdered with a spear while sitting in the garderobe on August 4, 1306.[20] George II of Great Britain died on the toilet on October 25, 1760 from an aortic dissection. According to Horace Walpole's memoirs, King George \"rose as usual at six, and drank his chocolate; for all his actions were invariably methodic. A quarter after seven he went into a little closet. His German valet de chambre in waiting heard a noise, and running in, found the King dead on the floor.\" In falling he had cut his face.[21] Ioan P. Culianu was shot dead while on the toilet in the third-floor men's room of Swift Hall on the campus of the University of Chicago on 21 May 1991, in a possibly politically-motivated assassination. His killer has never been caught. [22] Main Article: List of people who died on the toilet Albert Faille found dead in outhouse. Famed gold prospector in Fort Simpson NWT, Canada. Urban legends have been reported regarding the dangers of using a toilet in a variety of situations. Several of them have been shown to be questionable. These include some cases of the presence of venomous spiders[33] Except for the Australian redback spider who has a reputation for hiding under toilet seats.[34] These recent fears have emerged from a series of hoax emails originating in the Blush Spider hoax, which began circulating the internet in 1999.[35] Spiders have also been reported to live under seats of airplanes, however, the cleaning chemicals used in the toilets would result in an incompatibility with spider's survival.[36] In large cities like New York City, sewer rats often have mythical status regarding size and ferocity, resulting in tales involving the rodents crawling up sewer pipes to attack an unwitting occupant. Of late, stories about terrorists booby trapping the seat to castrate their targets have begun appearing.[37] Another myth is the risk of being sucked into an aircraft lavatory as a result of vacuum pressure during a flight.[38]","time":1525810343,"title":"Toilet-related injuries and deaths","type":"story","url":"https:\/\/en.wikipedia.org\/wiki\/Toilet-related_injuries_and_deaths","label":7,"label_name":"random"},{"by":"demillennial","descendants":0,"id":17024490,"kids":"None","score":1,"text":"","time":1525810300,"title":"This Android app allows you to set multiple alarms at once","type":"story","url":"https:\/\/play.google.com\/store\/apps\/details?id=be.demillennial.oneclockfree","label":7,"label_name":"random"},{"by":"MilnerRoute","descendants":0,"id":17024480,"kids":"None","score":1,"text":"Walmart is the largest company in the world and the single largest employer, with 2.3 million workers serving 200 million customers every week. So it\u2019s a big deal when they roll a blockchain initiative into production, especially one designed to offer a new solution to what may be the ultimate challenge: sharing data between different companies. \u201cLet me be crystal clear, because I can promise you that Walmart is not chasing the new shiny coin,\u201d emphasizes Walmart\u2019s vice president of food safety and health, Frank Yiannis. \u201cI know blockchain is in vogue and in fashion, but what we\u2019re trying to do is solve business challenges.\u201d Before blockchain, it took Walmart six days to track where its produce came from. Now it takes two seconds. Last month Yiannis described their approach at a \u201cBusiness of Blockchain\u201d conference sponsored by MIT Technology Review, explaining that \u201cWe\u2019re now beyond proof-of-concept\u2026 \u201cServing safe food in 12,000 stores around the world, with tens of thousands of suppliers, is a pretty daunting challenge, and it\u2019s a very important responsibility. So we\u2019re always looking for better ways to do that.\u201d In the 1980s a typical grocery store\u00a0had just 15,000 food items \u2014 but today it\u2019s over 50,000. In an IBM video, Frank Yiannas notes there\u2019s an \u201cunspoken expectation\u201d that food will, of course, be safe, and that when there\u2019s a food recall, \u201cthat food product is guilty until proven innocent.\u201d Walmart will pull all the carrots until they\u2019ve identified which ones are affected by the recall, only then returning the ones which aren\u2019t affected. Today\u2019s food system involves many players, each with its own way of tracking where the food has been. \u201cMost actually do it on paper, or on systems that don\u2019t speak to each other,\u201d Yiannis said.  Regulators only require \u201cone step up, one step back\u201d traceability, so even when records are digitized it\u2019s being done with different systems. Blockchain offers a ready-made system for collecting data not only from different companies but from different resources. \u201cIf we\u2019re linking that data with other data points, the internet of things \u2014 all that information will yield to insights that will allow us to have a safer, more affordable and sustainable food system.\u201d Walmart began testing in 2016 with the IBM Blockchain Platform. Qwartz paid a visit to their global food safety collaboration center in Beijing, where Walmart first launched its blockchain pilot program. (\u201cInstead of a lab filled with scientists testing meat or vegetables for toxins, it\u2019s really just a series of meeting rooms, much like any co-working space across the Chinese capital.\u201d) Walmart had suffered some bad press in China in 2011 \u2014 and again in 2014 \u2014 each time apparently due to an issue with their suppliers. Walmart\u2019s process now collects and digitizes data about pork \u2014 farm inspection reports, livestock quarantine certificates \u2014 and adds it to the blockchain. A second pilot program involved mangos, which take a very long journey from farm to table. First, they\u2019re shipped to a packing house, delivered to the U.S., then processed again and packaged for shipping and distribution by truck to thousands of Walmart (and Sam\u2019s Club) stores. Yiannis remembers bringing a package of mangos to a team meeting and challenging his staff to tell him where it came from. \u201cIt took us six days, 18 hours and 26 minutes\u2026 Almost seven days\u2026 Ad we do better than most.\u201d But after implementing a blockchain-based system, a trace back to source took just 2.2 seconds. \u201cThat\u2019s food traceability at the speed of thought. As fast as you could think it, we could know it.\u201d \u201cThis isn\u2019t a lab. This isn\u2019t theory,\u201d he told Qwartz. \u201cThis is the real world.\u201d  Why It Matters The blockchain-stored data also contained a wealth of other information about mangos, Yiannis told his audience at MIT. \u201cDid they receive the hot-water treatment? Were the farms inspected? If they made an organic claim, we could know if it was truly organic.\u201d But Yiannis also reminded his audience that foodborne illnesses are currently estimated to cost between $55 billion and $93 billion just in the U.S., pointing to the CDC\u2019s ongoing response to E. coli infections in romaine lettuce, with over 121 cases reported in 25 different states, including 52 hospitalizations and one death in California. While today\u2019s food system is safer than it\u2019s ever been, \u201cone food-borne illness is one food-borne illness too many.\u201d Yiannis also remembers a health scare in 2006 when health officials pulled all spinach for two weeks to trace a contamination. \u201cWhen it was all said and done, it was one producer, one day\u2019s production, one lot number\u2026 Spinach suppliers will tell you it took them six or seven years to regain sales and customer trust.\u201d  \u201cEverybody gets hurt in a recall,\u201d agrees Brigid McDermott, a former software developer who\u2019s now vice president of business development for blockchain at IBM. On Walmart\u2019s podcast \u2014 \u201cOutside the Box\u201d \u2014 she shares her conviction that \u201cwill transform transactions the way the internet has changed communications\u2026 It\u2019s the thing that gets me up every morning because I think it\u2019s true.\u201d McDermott says it\u2019s like using a pen when doing a crossword puzzle instead of a pencil \u2014 so there\u2019s no going back later and changing what was written. \u201cYou say, \u2018Hey there is a lot of information that I need to share across the ecosystem. And I want to make sure that people know that I didn\u2019t erase that information.'\u201d This solves a trust issue, which makes the whole food ecosystem more comfortable with sharing data. \u201cAll of a sudden, the 80 percent of the world\u2019s data that is sitting in silos in companies around the world is now accessible.\u201d \u201cAnd now we really get to big data,\u201d McDermott said. \u201cThis is what big data\u2019s promise is. The missing link which blockchain has solved is how do we get people to share the information so that we can use the information and get the insight out of the information. \u201cThe benefit to the consumer is better health. Not necessarily for every consumer but if you\u2019re the one who has the bad salsa, you want to know\u201d Of course, blockchain solves other issues too. At MIT Yiannis drew a slow laugh from his audience with this example of the importance of traceability. \u201cDid you know that there\u2019s more organic food sold in the world than there is produced in the world?\u201d \u201cThat\u2019s a miracle,\u201d he joked, after putting up a slide says that each year food fraud costs the industry between $10 billion and $15 billion. \u201cWe envision a world where blockchain, through this shared concept of trust, will allow you to track and really know \u2014 \u201cWhen I buy an organic product, is it truly organic, or is somebody just charging me a premium?\u201d There are other advantages too. \u201cEvery day you take out of distribution is a day you give back to the customer, and potentially reduce food waste.\u201d Yiannis describes himself as a blockchain skeptic who had an almost religious conversion. \u201cI\u2019ve been working on trying to digitize and create a digital transparent food system with better traceability for 30 years. Never been able to do that.\u201d But the complex stops along the food-supply chain offer another example of a system that\u2019s both decentralized and distributed. \u201cIf you think about it, it\u2019s almost as though blockchain was made for the food system.\u201d He suggests that Walmart\u2019s experience proves the old saying that blockchain is a team sport. \u201cOur CEO got so excited, he reached out and started talking to other suppliers, and now we have 10 large brands working together.\u201d Companies like Dole, Nestle and Tyson Foods all got involved and \u201cwe\u2019ve tracked millions of food packages, and we\u2019ve tracked over 100,000 traceability events.\u201d Walmart\u2019s involvement assured other participants that the blockchain idea had some \u201clegitimacy,\u201d according to Howard Popoola, Kroger\u2019s VP of corporate food technology and regulatory compliance. Kroger ultimately became one of the 10 major brands participating, according to Fortune, and Popoola says he\u2019d concluded that \u201cthe food industry is ripe for a solution like that.\u201d In another article, Fortune notes that Yiannis has demonstrated the technique at Walmart\u2019s annual shareholder\u2019s meeting.  This could be only the beginning. Walmart already has several blockchain-related patents, and has recently filed for two more, according to finance site Investopedia. One patent covers a system that automates both package tracking and encrypted payments to vendors for same-day deliveries \u2014 using as its example fresh produce. And the second patent covers a related courier shopping system. It\u2019s a sign that they may be planning an ambitious response to their battle with Amazon\u00a0since an earlier patent also describes a drone-ready smart package with data about its contents and delivery history. The same article notes that blockchain applications are also being explored by Target. Yiannis told Qwartz he envisions a day when packing pallets will even transmit the temperature of food cargos as they\u2019re being shipped, with that data also stored on the blockchain for both regulators and consumers. And back on YouTube, one commenter joked that food buzzwords could soon be transformed, with \u201cfarm to table\u201d becoming \u201ccloud to client.\u201d","time":1525810217,"title":"Walmart's blockchain program may transform the way we use data","type":"story","url":"https:\/\/thenewstack.io\/walmarts-blockchain-program-may-transform-the-way-we-use-data\/","label":7,"label_name":"random"},{"by":"gurgeous","descendants":0,"id":17024472,"kids":"None","score":1,"text":"","time":1525810170,"title":"2018 O'Reilly AI Keynote, Thomas Reardon, CEO, CTRL-Labs","type":"story","url":"https:\/\/www.youtube.com\/watch?v=5Z5aZK2C3ew","label":7,"label_name":"random"},{"by":"mbgaxyz","descendants":0,"id":17024454,"kids":"None","score":1,"text":" The Zcash Foundation has announced that it will make maintaining ASIC resistance an \u201cimmediate technical priority\u201d in response to Chinese mining hardware manufacturer Bitmain\u2019s claim that is has developed an application-specific integrated circuit (ASIC) miner that is compatible with the Equihash mining algorithm. In a statement penned by executive director Josh Cincinnati, the Zcash Foundation said that it will immediately begin devoting resources to investigating the \u201cpresence and power of ASICs on the Zcash network,\u201d as well as convene a technical advisory board to provide \u201cscientifically grounded inputs\u201d on the matter. As CCN reported, Bitmain announced last week that it would begin shipping the first Equihash ASIC miner \u2014 the Antminer Z9 mini \u2014 in June. ASICs are significantly more efficient than miners powered by general-purpose GPU chips, and most people believe that Bitmain has already been mining with them privately. Once they begin shipping to the general public, the Zcash mining landscape will be permanently altered, because the project must either make peace with the presence of ASICs \u2014 which at present centralize hashpower into the hands of a small number of companies \u2014 or commit to taking drastic measures (likely on a regular basis) to thwart the ability of these devices to operate on the Zcash network. Monero, for instance, has committed to updating its mining algorithm semi-annually to make it more difficult for ASIC manufacturers to disrupt XMR mining, and it adopted an emergency update after Bitmain released the first Cryptonight ASIC miner. However, even the most ardent ASIC critics acknowledge that resistance to these devices cannot be maintained indefinitely. \u201cEven if we manage to neuter a wave of Equihash ASICs, this will not be the end of the discussion. Inevitably, some new ASIC will arise, and we may have to go through this process again,\u201d Cincinnati wrote. Even though it is moving forward with research on how to maintain ASIC resistance, the Foundation still intends to bring the matter before the community through its conventional ballot and election process. Pending community approval, the upgrade will likely be deployed in late 2018, several months after the first Equihash ASIC miners hit the market. But while this course may mitigate the problem in the short-term Cincinnati warned that a long-term solution will require a significant re-framing of the ASIC debate. \u201cI\u2019m all for a Sisyphean effort now and again, but perhaps there is a better solution\u2014one that subverts the entire \u2018ASIC resistance\u2019 debate?,\u201d Cincinnati said. \u201cEventually we will need one, because I\u2019m not sure how sustainable the whack-a-mole strategy will be for the community.\u201d \u201cThere has to be a better way\u2014and I think it starts with reframing the discussion away from \u2018ASIC resistance\u2019 and towards the perceived goals of ASIC resistance\u2014decentralization, less concentrated proof of work, and accessibility to the network,\u201d he concluded. \u201cBut in the meantime, we must act, and act we shall.\u201d Images from Shutterstock   ","time":1525810083,"title":"Zcash Foundation to Make ASIC Resistance \u201cImmediate Technical Priority\u201d","type":"story","url":"https:\/\/www.ccn.com\/zcash-foundation-to-make-asic-resistance-immediate-technical-priority\/","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17024449,"kids":"None","score":1,"text":"The announcement of an online service for the Nintendo Switch yesterday raised more questions than answers, it seems. People have been waiting for the Switch Online service ever since the release of the popular console, and with it comes cloud-based backup saves. The news that the subscription fee ($4 per month, $8 for three months or $20 for a year) would also give subscribers access to 20 NES games spurred discussion of whether Virtual Console would be making a return to the Switch. Sadly, it doesn't look like that's the case, but it doesn't mean retro and classic games won't be available on the Nintendo Switch. Virtual Console was first introduced on the Nintendo Wii as a move to appeal to original Nintendo fans in the wake of an experimental console. It allowed users to play games from older retro consoles like the NES and Nintendo 64. But the project never quite lived up to its promise, with a limited library and SNES games only available on the Nintendo 3DS. Now, Kotaku has learned that Nintendo is not planning on bringing Virtual Console to the Switch. \"There are currently no plans to bring classic games together under the Virtual Console banner as has been done on other Nintendo systems,\" a spokesperson said to Kotaku. However, it's important to note this doesn't mean that Nintendo won't make older games available to play on the Switch. But when it does, they won't be part of a Virtual Console. \"There are a variety of ways in which classic games from Nintendo and other publishers are made available on Nintendo Switch, such as through Nintendo Entertainment System \u2013 Nintendo Switch Online, Nintendo eShop or as packaged collections,\" the spokesperson said to Kotaku. They cut through the winter confusion to see what's actually present. Surprise, surprise: The Pixelbook will get the feature first.  They could make life a lot easier for millions of workers. It's coming alongside support for starting orders inside the app. Not even one of the largest and richest companies can tackle accessibility by itself.","time":1525810044,"title":"Nintendo Switch Won\u2019t Have a Virtual Console","type":"story","url":"https:\/\/www.engadget.com\/2018\/05\/08\/nintendo-switch-no-virtual-console\/","label":7,"label_name":"random"},{"by":"tin7in","descendants":0,"id":17024447,"kids":"None","score":3,"text":"","time":1525810029,"title":"David Marcus Leaves Messenger to Focus on Blockchain Within Facebook","type":"story","url":"https:\/\/www.facebook.com\/davidm\/posts\/10160585531500195","label":7,"label_name":"random"},{"by":"pepys","descendants":0,"id":17024446,"kids":"None","score":1,"text":"Let\nme tell you about my selfie face. I like my head to be at a slight angle, one\ncheek turned to the lens so my eyes are looking at the camera sideways and\nnever straight on. This is best for hiding the fact that my eyes are not\nsymmetrical\u2014a result of being born with ptosis, a drooping eyelid, and many\noperations to correct it. From the side and with a bit of a squint, it\u2019s not\nthat prominent. I also like to shoot the picture from below so it catches my\njaw, which has a sharp angle, the jaw of a serious man. And then there\u2019s my\nsmile. My lips pursed look funny. So I keep them apart a bit, but not too much\nor my big teeth dominate and look goofy. I go for a wry thing, just a small\nlift at the corners of my mouth, slightly bemused, the look of someone who is\nmaybe a little bit better than you. Don\u2019t\njudge. I know I\u2019m not the only one. I walk through Times Square every day to\nwork, and nearly knock into at least a dozen of these selfie faces each\nway\u2014lips unnaturally puckered, eyes wide and chins thrust out. So here\u2019s a\nchicken-and-egg question: Where does this urge to capture and project perfect\nselves come from? Are the supercomputers in our pockets to blame, offering an\nirresistible pull, shaping the desire itself? Or did this narcissistic drive\nexist before the technology, an instinct of the modern self that smartphones\nhave simply allowed us to indulge?\u00a0 Ours is a boom time for technological determinism, of constant worry\nabout how Siri or Instagram or YouTube is changing the way our brains and\nsociety and politics works. A long line of polemical books now form a\ntechno-pessimist canon of sorts: from way back in 1993 when Neil Postman warned\nin Technopoly about how culture was\nbecoming a slave to technology to Nicholas Carr\u2019s The Shallows and its vision of the distracted, dumbed-down human\nbeings the Internet was producing. And on and on. It would be easy to suggest,\nlikewise, that today much self-obsession is also a function of the ever-present\ngadget. But consider this cute counterfactual from W. Keith Campbell, the\nsocial psychologist best known for diagnosing a \u201cnarcissism epidemic\u201d:\u00a0 \u201cWe could have filled the Internet with\n\u2018Mom-ies,\u2019 taking pictures of our mom every day and saying how great our mom\nis. But we didn\u2019t do that.\u201d No, we certainly did not. As soon as phones with\ncameras appeared, almost everyone took the opportunity to turn them on\nthemselves. Campbell\nis among the many experts the British journalist and novelist Will Storr\ninterviewed for Selfie, his free\nranging account of the modern, ego-driven Western self. Despite the digital\nwink of the title, this is a story that arrives at Silicon Valley only in its\nsecond to last chapter. As he sets out to reverse engineer that ridiculous face\nI make into my phone, he finds it to be the result of a long historical process.\nIt is 2,500 years of culture layered on top of biology that have determined this\nneed to selfie. Aristotle, Jesus, Freud, Ayn Rand and the Esalen Institute are\nto blame\u2014much more than Steve Jobs. Storr\u2019s\nstarting point is his own self-loathing. Why does he feel so doomed never to\nmeasure up? He diagnoses himself\u2014and many of the rest of us\u2014with neurotic\nperfectionism, oppressed by an ideal of the self as \u201can extroverted, slim, beautiful,\nindividualistic, optimistic, hard-working, socially aware yet\nhigh-self-esteeming global citizen with entrepreneurial guile and a selfie\ncamera.\u201d This ideal should be familiar from our Facebook feeds\u2014that distant\nfriend who can be seen vacationing on a beautiful beach with his loving family.\nBut this unrealistic self is more than just an annoyance. For Storr, the\ndelusions it inspires of possible, achievable perfection can be fatal. He even\ncites suicide statistics. He\ntraces the origins of this perfectionism to the tribal nature of the earliest\nhuman societies, in which individuals became super attuned to social hierarchy\nand status\u2014all the better to survive. This, Storr believes, encoded humans with\na genetic predisposition to \u201cget along and get ahead.\u201d From there, culture took\nover. The physical landscape of Ancient Greece, jagged mountains, islands and\ninlets, not particularly well suited to agriculture, pushed individuals and\ncommunities there toward market-based economies, competing with one another to excel\nat whatever it is they produced or sold. \u201cPotter resents potter and carpenter\nresents carpenter, and beggar is jealous of beggar and poet of poet,\u201d wrote\nHesiod in the 7th or 8th Century BC. What emerged from all this jealousy and\njostling was an ethos of self-improvement. As the Stanford historian Adrienne\nMayor has written, Aristotle believed that \u201call things in nature\u201d\u2014including\nhuman beings\u2014\u201cmoved towards achieving perfection of their potentials.\u201d\u00a0 What\nhad been, for the Greeks, a desire for ostentatious success became in medieval Europe\na struggle to control the self through prayer and flagellation\u2014to battle inner badness\nand become pure. The focus became the soul, but the notion of an ideal persisted\nand that an individual has the capacity, on their own and through enough\nindividual will and work, to arrive perfection. Storr hopscotches over the\nRenaissance and the Enlightenment to arrive at Freud. But he sees little that\nis new in the theory of a battling id and superego, which he contends is simply\na secular version of the Christian notion of reforming the bad self. The real\nchange\u2014the big one when it comes to the self, and most responsible for the current\ncraziness\u2014comes a little later in the 20th century, according to Storr, in\nCalifornia, to which his whole book shifts in its second half. On\nthe Pacific Coast, where Storr arrives like a Brit out of water, he makes his\nway to the Esalen Institute, which in the 1960s and 70s \u201chelped rewrite our\nsense of who we are,\u201d he grandly claims. In the encounter groups that took\nplace here (and still do, in a different form, an experience to which Storr\nsubjects himself), people were provoked, often harshly, into screaming at\nimaginary parents and usually ended up berating each other. Sobbing breakdowns\nwere common. It turned the work of psychotherapy, of dredging up desires out of\nthe muck of social expectation, from the practice of gaining mastery over these\nsuppressed feelings into one in which they were given freedom to roam\u2014as many a\nliberated former Freudian analyst did at Esalen, wandering the grounds naked\nand erect. The assumption underlying all of it was that, unfettered, each one\nof us is actually perfect, lovable, and god-like, and that all we want is\nwithin our grasp if we just reach out, ignore others, and take it. This\nis where the idea of \u201cself-esteem\u201d exploded. Storr tells the story (in a long,\nslightly digressive section) of John \u201cVasco\u201d Vasconcellos, a California\nassemblyman, who soaked in Esalen\u2019s famous springs and managed to\nsecure funding for a state task force on the cure-all benefits of self-esteem\nas a \u201cvaccine\u201d for any social disease. It\u2019s in the controversy around the\ncreation of this task force and its positive findings, endorsed by Oprah, that\nStorr locates a patient zero in the spread of this epidemic. It wasn\u2019t long\nbefore children were being given participation trophies: One Massachusetts\nschool district ordered gym classes to allow students to jump rope without\nactual rope\u2014the better to avoid the damage to their self-esteem that tripping\nmight cause. All\nthis me-first-ism coincided with the apotheosis of a no-holds-barred version\nof capitalism\u2014a world of competitive individuals, judged by their\nachievement of wealth and fame, and economies (the new \u201cterroir of the self,\u201d\nStorr writes) that privileged the narcissist. Margaret Thatcher said of her free\nmarket policies in 1981 that \u201cEconomics are the method, but the object is to\nchange the soul.\u201d Storr places Nathaniel Branden at the intersection of\nEsalen\u2019s cult of self-esteem and a \u201cgreed is good\u201d cult of success. A psychotherapist\nand Ayn Rand\u2019s lover (and junior by 25 years), he became a mentor to\nVasconcellos, the California assemblyman. The child, Branden wrote in his notes\nfor Vasconcellos\u2019s task force, should be \u201cin love with his\/her own existence,\u201d\nable to \u201cpractice selfishness in the highest, noblest and least understood\nsense of this word.\u201d After\nall this, the rise of social media, offering everyone an Instagram account and\nan iPhone, only exacerbated this pathological self-love. That the idea of an\neminently elastic and ultimately optimal self is a fiction\u2014a \u201ccultural lie,\u201d\nStorr angrily calls it\u2014is not exactly news. Far more interesting is the way Storr\nframes social media as an enabler of this fiction, but not as its creator. In\nthe dead eyes of CJ, a 20-year-old who takes selfies almost every hour of the\nday (\u201cI\u2019ve genuinely taken selfies at a funeral\u201d), Storr sees an entrenched culture\nof self-esteem and an economy that has taught her that she should always be selling herself.\nThe phone is secondary.\u00a0 Storr\nwants to tell a clean story. His claims can, as a result, often feel overblown.\nIt\u2019s just as reductionist to state that individualism can be traced back to the\ncraggy geography of Greece or that the notion of self-esteem originated from\nwacky happenings at an institute sitting on the cliffs of Big Sur as it is to\nblame Facebook for a chronic lack of empathy in young people. His historical\ntour is pretty loosey goosey, skipping over, for example, the Enlightenment, the\nscientific and industrial revolutions, the rise of democracy, and, yes, the\nimpact of technology on communication from the telegraph until today. All this\ntoo helped chisel the contours of the modern self. But for Storr the story\ncomes down to a few cherry-picked facts from biology, culture and economics. It\nseems possible to say on every page, \u201cyes, but.\u201d\u00a0 Selfie is best approached as a corrective, and a much needed one, to a moment\nfixated on its own particularity. I\u2019m not as concerned as Storr about the state\nof the modern self. There are even good arguments for the selfie\u2014like the\nfeminist one made by Rachel Syme that turning the camera on one\u2019s self,\nbecoming subject and object, is an empowering act. I think that most people, when\nthey\u2019ve shut off their phones at night, know their own finitude and, even if they\nare a little too enthralled by the idea of endless possibility and the prospect\nof perfection, find this fiction more comforting than harmful. Until Google or\nAmazon invents an app for solving the problem of mortality, I think we\u2019ll all\nremain humbled enough.\u00a0 But\nwe should let go of the idea that our technologies are us, that we are somehow\nthe sum total of the platforms we use. Storr is helpful here, if only to point\nout that the modern self is, as he nicely puts it, a \u201csack of noisy ghosts.\u201d\nAnd there are many more rattling inside that sack alongside Aristotle and Esalen.\nHis book is, in this way, a prod toward the study of the humanities\u2014toward\nhistory, literature, philosophy, religion\u2014the very subjects that have dimmed\nnext to the bright light of the screen. Just maybe, if more people can\nbe convinced that this wealth of culture offers them a mirror to themselves, they\nmight be willing to put down the phone for a few minutes and gaze inside. Gal Beckerman is an editor at The New York Times Book Review and the author of When They Come for Us, We\u2019ll Be Gone: The Epic Struggle to Save Soviet Jewry. Copyright 2018 \u00a9 The New Republic. All rights reserved.","time":1525810027,"title":"Don\u2019t Blame Phones for Narcissism","type":"story","url":"https:\/\/newrepublic.com\/article\/148311\/dont-blame-phones-narcissism","label":7,"label_name":"random"},{"by":"el_duderino","descendants":0,"id":17024442,"kids":"None","score":1,"text":"\n          The latest Android and Google Play news for app and game\u00a0developers.\n         \nToday, we launch an early preview of the new Android extension libraries (AndroidX) which represents a new era for the Support Library.  Please preview the change and give us your feedback. Since this is an early preview, we do not recommend trying this on any production projects as there are some known issues.\n \nThe Support Library started over 7+ years ago to provide backwards compatibility to framework APIs. Over the years, the library has grown to include device-specific UX, debugging, testing and other utilities. The adoption of the Support Library has been phenomenal; most Android apps use the Support Library today. We want to increase our investment in this area, and it is critical that we lay a solid foundation.\n \nIn that vein, we took a step back and chatted with many of you. The feedback has been consistent and unanimous; the organic growth of the library has become confusing. There are components and packages named \"v7\" when the minimal SDK level we support is 14! We want to make it clear for you to understand the division between APIs that are bundled with the platform and which are static libraries for app developers that work across different versions of Android.\n \nWith that in mind, say \"Hello World\" to \"AndroidX\". As previously noted in the Android KTX announcement, we are adding new features under this package, and updating some existing ones. \n \nandroid.* vs androidx.* namespaces\n \nWriting Android apps means depending on two kinds of classes:\n \nMany times, unbundled libraries can be a better choice, since they provide a single API surface across different Android versions. This refactor moves the unbundled libraries - including all of the Support Library and Architecture Components - into the AndroidX package, to make it clear to know which dependencies to include.\n \nRevised naming for packages and Maven artifacts\n \nWe redesigned the package structure to encourage smaller, more focused libraries that relieve pressure on apps and tests that aren't using Proguard or Multidex. Maven groupIds and artifactIds have been updated to better reflect library contents, and we have moved to prefixing library packages with their groupId to create an obvious link between the class that you are using and the Maven artifact from which it came.\n \nGenerally, you can expect the following mapping from old to new packages:\n \nThe Architecture Components libraries have also been moved under androidx and their package names simplified to reflect their integration with core libraries. A sample of changes to these libraries:\n \nAdditionally, following the introduction in 28.0.0-alpha1 of Material Components for Android as a drop-in replacement for Design Library, we have refactored the design package to reflect its new direction.\n \nFor a complete listing of mappings from 28.0.0-alpha1 (android.support) to 1.0.0-alpha1 (androidx), please see the full AndroidX refactoring map. Please note that there may be minor changes to this map during the alpha phase.\n \nPer-artifact strict semantic versioning\n \nStarting with the AndroidX refactor, library versions have been reset from 28.0.0 to 1.0.0. Future updates will be versioned on a per-library basis, following strict semantic versioning rules where the major version indicates binary compatibility. This means, for example, that a feature may be added to RecyclerView and used in your app without requiring an update to every other library used by your app. This also means that libraries depending on androidx may provide reasonable guarantees about binary compatibility with future releases of AndroidX -- that a dependency on a 1.5.0 revision will still work when run against 1.7.0 but will likely not work against 2.0.0.\n \nMigration from 28.0.0-alpha1\n \nMoving your app from android.support to androidx-packaged dependencies has two major parts: source refactoring and dependency translation.\n \n \nSource refactoring updates your Java code, XML resources, and Gradle configuration to reference the refactored classes and Maven artifacts. This feature is available in Android Studio Canary 14 for applications targeting Android P.\n \nIf you depend on a library that references the older Support Library, Android Studio will update that library to reference androidx instead via dependency translation. Dependency translation is automatically applied by the Android Gradle Plugin 3.2.0-alpha14, which rewrites bytecode and resources of JAR and AAR dependencies (and transitive dependencies) to reference the new androidx-packaged classes and artifacts. We will also provide a standalone translation tool as a JAR.\n \nWhat's next?\n \nWe understand this is a big change for existing projects and codebases. Our intention is to provide a strong foundation that sets Android library projects up for more sustainable growth, better modularity, and smaller code size.\n \nWe hope that these changes also make it easier for developers to discover features and implement high-quality apps in less time; however, we understand that migration takes time and may not fit into everyone's production schedule. For this reason, we will continue to provide parallel updates to an android.support-packaged set of libraries for the duration of the P preview SDK timeframe. These updates will continue the 28.0.0 versioning scheme that began with 28.0.0-alpha1 in March 2018, and they will continue to be source-compatible with existing projects that depend on the android.support package.\n \nThe stable release of 28.0.0 will be the final feature release packaged as android.support. All subsequent feature releases will only be made available as androidx-packaged artifacts.\n \nWe'd love to hear from you as we iterate on this exciting future. Send us feedback by posting comments below, and please file any bugs you run into on AOSP.\n \nWe look forward to a new era of Android libraries!\n ","time":1525810020,"title":"Hello World, AndroidX","type":"story","url":"https:\/\/android-developers.googleblog.com\/2018\/05\/hello-world-androidx.html","label":3,"label_name":"dev"},{"by":"sandGorgon","descendants":0,"id":17024416,"kids":"None","score":2,"text":"Degrade gracefully on older versions of Android Write more concise, idiomatic Kotlin code Provide support for apps with multiple DEX files An Android testing framework for unit and runtime UI tests Declaratively bind observable data to UI elements Manage your activity and fragment lifecycles Notify views when underlying database changes Handle everything needed for in-app navigation Gradually load information on demand from your data source Fluent SQLite database access Manage UI-related data in a lifecycle-conscious way Manage your Android background jobs Schedule and manage large downloads Backwards compatible API for media playback, routing and Google Cast Provides a backwards-compatible notification API with support for Wear and Auto Compatibility APIs for checking and requesting app permissions Provides a share action suitable for an app\u2019s action bar Create flexible UI elements that can display app data outside the app Move widgets and transition between screens Components to help develop apps for Android Auto. Enable an up-to-date emoji font on older platforms A basic unit of composable UI Lay out widgets using different algorithms Pull useful information out of color palettes Components to help develop apps for Android TV. Components to help develop apps for Wear.  \n  \"With Android Architecture Components, we're re-architecting our\n  entire app. It's great to have a Google-endorsed, opinionated, and\n  clean way to build an Android app that makes it easier to support\n  configuration changes.\"\n \n  Drew Hannay, Staff Software Engineer, LinkedIn \n  \"We see higher agility and turnaround in new feature development\n  with Android Architecture Components. And our overall speed\n  continues to get better as more developers start using it.\"\n \n  Vishwanath Ramarao, CTO, Hike \n  \"We love ViewModel and LiveData! Our code became so much more\n  concise, stable, and readable, and the code architecture was\n  beautifully unified. Stability improved, too!\"\n \n  Zheng Songyin, senior development manager, BeautyPlus \n  \"Room made it really easy for us to create the database table and\n  DAO so we could quickly build our product. And the emphasis on\n  testability is critical for us.\"\n \n  Demian Insung Hwang, KakaoTalk Developer, KakaoTalk ","time":1525809938,"title":"Android Jetpack \u2013 Next Generation Android SDK and Support Library","type":"story","url":"https:\/\/developer.android.com\/jetpack\/","label":3,"label_name":"dev"},{"by":"bpierre","descendants":0,"id":17024401,"kids":"None","score":1,"text":"\n        Instantly share code, notes, and snippets.\n       Note: When this guide is more complete, the plan is to move it into Prepack documentation.\nFor now I put it out as a gist to gather initial feedback. If you're building JavaScript apps, you might already be familiar with some tools that compile JavaScript code to equivalent JavaScript code: Babel lets you use newer JavaScript language features, and outputs equivalent code that targets older JavaScript engines. Uglify lets you write readable JavaScript code, and outputs \"mangled\" JavaScript that does the same thing, but contains fewer bytes. Prepack is yet another tool that aims to compile JavaScript to equivalent JavaScript code. But unlike Babel or Uglify, Prepack isn't focused on new features or code size. Instead, Prepack lets you write normal JavaScript code, and outputs equivalent JavaScript code that runs faster. If this sounds exciting, read on to learn how Prepack works and how you can make it better. Personally, when I finally understood what Prepack can do, I was excited. I thought that in the long term Prepack could solve many problems that I encountered building large JavaScript apps. I wanted to spread the word about it, and get other people excited about it too. However, Prepack can be intimidating to contribute to at first. Its source code contains many terms that I wasn't familiar with, and it took me a while to understand what Prepack does. Compiler codebases tend to use established Computer Science terminology, but it turned out that many of these concepts sounded more difficult than they actually were. I wrote this guide for JavaScript developers who don't necessarily have a Computer Science background, but are excited by the promise of Prepack and want to help make it a reality. This guide provides a high-level overview of how Prepack works, and gives you a starting point for contributing. Many concepts in Prepack directly map to the tools you're relying on every day in JavaScript code: objects, properties, conditions, and loops. Even if you can't use Prepack for your projects just yet, you might discover that working on Prepack enriches your understanding of the JavaScript code you're writing every day. Note that Prepack is not yet ready for mainstream adoption. You cannot just plug it into your build system like you do with Babel or Uglify, and expect it to work. Instead, you can think of Prepack as an ongoing and ambitious experiment that you can participate in, and that will hopefully become useful to you in the future. Because of its vast scope, there are still many opportunities to improve it. Now, this doesn't mean that Prepack doesn't work. But it's currently focused on a very narrow set of scenarios, and it might have more bugs than most people would be comfortable with in production. The good news is you can help Prepack support more use cases, and help fix the bugs in it. This guide will help you get started. Let's recap the Prepack mission statement that I mentioned earlier: Prepack lets you write normal JavaScript code, and outputs equivalent JavaScript code that runs faster. Why don't we just write faster code in the first place? We can certainly try, and we should when we can. However, in many apps, aside from the bottlenecks identified by profiling, there isn't anything obvious to optimize next. Often there's no single place where the program is slow; instead, the program suffers from a \"death by a thousand cuts\". Features that encourage separation of concerns, such as function calls, allocating objects, and various abstractions, eat away at the runtime performance. However, getting rid of them in the source code would make it unmaintainable, and there is no easy \"fix\" we could apply to microoptimize them either. Even JavaScript engines with years of optimization work are limited in what they can do, especially for initialization code that only runs once. The surest way to improve performance is to do less work. Prepack takes this principle to its logical conclusion: it executes the program at the build time to learn what the code would do, and then generates the equivalent code that does the same with the least computations. This sounds pretty magical so let's consider a few examples and see Prepack's strengths and limitations. We will use Prepack REPL which lets us run Prepack on a piece of code online. Let's start by opening this example: The output is: Indeed, running both snippets produces the same observable effect: the value 4 is assigned to a global variable called answer. However, the Prepacked version doesn't contain the code that calculates 2 + 2. Instead, Prepack ran 2 + 2 during compilation, and \"serialized\" (a fancy way of saying \"wrote\" or \"emitted\") the final assignment operation. This isn't very impressive by itself: for example, Google Closure Compiler can also turn 2 + 2 into 4. This optimization is called \"constant folding\". What sets Prepack apart is that it can execute arbitrary JavaScript code, not just constant folding or similar limited optimizations. Prepack also has its own limitations which we'll get to a bit later. Consider this intentionally obtuse and incredibly convoluted way to calculate 2 + 2: While we don't recommend writing code for summing up two numbers this way, you can see that Prepack produces the same exact output for it: In both cases, Prepack has executed the code above at the build time to calculate the observable \"effects\" (changes) it had on the environment (e.g. setting a global called answer to 4), and then \"serialized\" (wrote) the code that produces the same effects with minimal runtime overhead. The same higher-level picture is true for any code that runs through Prepack. \"Executing\" code at the build time sounds scary. You wouldn't want Prepack to remove a file from your system just because your input code contains an fs.unlink() call. We need to clarify that Prepack doesn't just eval the input code in a Node environment. Instead, Prepack includes an implementation of a complete JavaScript interpreter so it can execute arbitrary code in an \"empty\" isolated environment. By default, it doesn't have support for Node primitives like require(), module, or browser primitives like document. We'll get back to these limitations later. This doesn't, however, mean that building a bridge between the \"host\" Node environment and the Prepack JS environment is impossible. In fact it may be an interesting idea to explore in the future. Perhaps you'll be the one to do it? You might have heard this philosophical question: If a tree falls in a forest and no one is around to hear it, does it make a sound? It turns out that it is directly relevant to what Prepack can and cannot do. Consider this slight variation of the first example: The output, perhaps surprisingly, contains the definitions for x and y too: This is because Prepack treats the input code as a script (rather than a module). A var declaration outside a function becomes a global variable, so from Prepack's point of view it would be as if we explicitly assigned them to globals: That's why Prepack kept x and y in the output. Don't forget the goal of Prepack is to produce equivalent code, and it doesn't protect you from JavaScript's pitfalls. The easiest way to protect against this mistake is to always wrap the Prepacked code into an IIFE, and explicitly attach the results you need to keep to a global: This produces the expected output: Here is another potentially confusing example: The Prepack REPL outputs a helpful warning for it: Here, the opposite happened: even though we performed some computation, nothing we calculated has any \"effects\" on the environment. If some other script ran later, it would have no way to determine whether our code ran at all. Therefore, there is no need to serialize any of these values. Again, to fix this we'd need to explicitly mark what we want to keep by putting it on a global object, and let Prepack eliminate the rest: Conceptually, this may remind you of garbage collection: objects that are \"reachable\" from a global object need to \"stay alive\" (or, in case of Prepack, get serialized). There are also other kinds of \"effects\" that Prepack supports aside from setting global properties, but we'll look at them later. Now we know enough to roughly describe how Prepack works. As Prepack interprets the input code, it builds up internal representations of all objects used in the program. For every JavaScript value (an object, a function, a number) there is an internal Prepack object that contains information about it. The Prepack codebase contains classes like ObjectValue, FunctionValue, NumberValue, and even UndefinedValue and NullValue. Prepack also keeps track of all the effects (such as writing a global) that the input code could have had on the environment. In order to faithfully reproduce these effects in the output code, Prepack finds all values that are still \"reachable\" from a global object after the code has finished running. In the example above, global.answer is considered \"reachable\" because unlike local variables x and y, external code could read global.answer in the future. This is why it would be unsafe to omit global.answer from the output, but safe to omit x and y. All values reachable from the global objects (and thus potentially affecting the code that runs later) are collectively called \"residual heap\". This name sounds more complex than the idea itself. The \"residual heap\" is a part of the \"heap\" (all objects created by the executed code) that stays as a \"residue\" (i.e. is left in the output) after the code has finished executing. If we take off our Computer Science hats, we could call it the \"leftover stuff\". So how does Prepack produce the output code? After Prepack has marked all \"reachable\" values as being in the residual heap, it then runs a serializer on them. The job of the serializer is to figure out how to turn Prepack's object representations for JavaScript objects, functions, and other values on the residual heap, into the output code. If you're familiar with JSON.stringify(), conceptually you can think of Prepack serializer as doing something similar. However, JSON.stringify() has the luxury of avoiding complex cases like circular references between objects: JavaScript programs have circular references between objects very often so Prepack serializer has to support all of those cases, and emit correct equivalent code to rebuild those objects. So for an input like this: Prepack generates code like this: Note that the assignment order is different (the input code constructed a first, but the output code started with b). This is because in this case the assignment order doesn't matter. However, this illustrates the core principle of how Prepack works: Prepack doesn't transform the input code. Instead it executes the input code, finds all values in the residual heap, and then serializes these values and the effects that use them, into the output JavaScript code. The examples above might make you wonder: isn't it a bad practice to put values onto globals? It generally is in production code, but if you're using an experimental JavaScript abstract interpreter that isn't production-ready in production, you might have bigger issues to worry about. There is a limited support for running Prepack in a CommonJS-like environment with module.exports, but it's currently very ad-hoc (and is implemented via a global anyway). In either case, this is not very important yet because it doesn't fundamentally change how the code executes, and will become more pressing only when Prepack is more ready to integrate with other tools. Let's say we wanted to add some encapsulation to our code, and wrapped the 2 + 2 calculation into a function: If you try to compile this, you might be surprised by the result: It looks like Prepack didn't optimize our calculation! Why is that? By default, Prepack only optimizes the \"initialization path\" (the code that executes immediately). From Prepack's point of view, the program has finished when Prepack has executed all statements in it. The effect of the program is to set a global variable called getAnswer to a function that we wrote, which Prepack did. As far as it's concerned, the work is over. If we called getAnswer() before exiting the program then Prepack would execute it. Whether or not getAnswer() implementation would stay in the output depends on whether or not the function itself is \"reachable\" from the global object (and thus would be unsafe to omit). Functions that are emitted in the output are called \"residual functions\" (they are \"residue\", or leftovers, in the output). By default, Prepack will not attempt to execute or optimize residual functions. It's not generally safe. By the time a residual function gets called from the outside code, both JavaScript runtime globals like Object.prototype and objects created by the input code could have been mutated without Prepack's knowledge. Then Prepack would either have to use potentially stale values captured in its residual heap, differing in behavior from the original code, or always assume that anything can get mutated, making the optimization too difficult. Neither is desirable so residual functions stay intact. There is, however, an experimental mode which lets you opt into optimizing certain functions, and we will cover it in a later section. Consider this example: Prepack emits the following code, keeping getAnswer() as a residual function in the output: Note that getAnswer() did not get optimized because it's a residual function, and isn't executed at initialization time. The + operation is still there. The only reason we see 2 and 2 instead of x and y is because they don't change throughout the program, so Prepack treats them as constants. But what if we generated a function dynamically, and then attached it to a global? For example: Here, we create several objects, and each of those objects contains a getColor() function that closes over a different value given to makeCar(). The Prepack output looks like this: Note how in the output, Prepack didn't keep the makeCar() abstraction. Instead it executed the makeCar() calls, and serialized the functions they returned. This is why we have many getColor() functions in the output, one per a Car object. This example also demonstrates that Prepack optimizes the runtime performance, potentially at the cost of byte size. It is faster for a JavaScript engine to execute the code generated by Prepack because it doesn't have to do the function calls and initialize all the nested closures. But in return, the generated code may be larger than the input code--sometimes, excessively so. While this \"code explosion\" can help find areas of the code that do too much expensive metaprogramming at initialization time, this makes it harder to use Prepack in projects where the bundle size is sensitive (such as on the web). Today, the easiest way to get around a \"code explosion\" is to delay running such code and move it into a residual function itself, thus moving it outside of Prepack execution path. Of course, in that case Prepack won't optimize it. Longer term, Prepack may offer better heuristics and control over the speed vs. size tradeoff. In the previous example, the color values were just inlined into the residual functions because they were constant. But what if the color value in the closure could change over time? Consider this example with a new paint(newColor) method: Now Prepack can't just emit a bunch of getColor() functions with statements like return \"red\" because the external code could change the color over time by calling paint(newColor). Here is the generated code for this scenario: This looks complicated! Let's see what's going on here. Note: it's totally fine if you don't understand this section from the first attempt. I only understood what was happening after I started writing this section. It might be easier to start reading this from the bottom, and work our way upwards. First of all, we can see that Prepack still doesn't leave the makeCar() abstraction in place, and pieces objects together manually to avoid function calls and closure creation overhead. Each function instance is different: Where do these functions come from? Prepack declares them above: We can see that the function being bound ($_0 and $_1) corresponds to the car method (getColor and paint, respectively). Prepack reuses the same implementations of them for all instances. However, those functions need to know which of the three independent mutable colors each of them should read and write. Effectively, Prepack needs to emulate JavaScript closures without creating nested functions. To solve this problem, the bind() argument (0, 1, and 2) provides a hint as to which of the colors is being \"captured\" by a function. In this example, color number 0 is initially 'red', color number 1 starts as 'green', and number 2 is 'blue' at first. The current colors are kept in an array, and it is lazily initialized by this function: In the code above, __scope_0 is an array where Prepack keeps a mapping from the color index to the current color value. And __scope_1 is a function that sets the initial color in the array at a given index. Finally, all the getColor() implementation does is read the current color value from the color array. If that array doesn't exist yet, it lazily initializes it by calling the function we just described: Simliarly, the paint() implementation ensures the array exists, and then writes to it. Why do we have [0] in both places, and why do we write [\"red\"] to the array instead of storing colors directly? Each closure may contain more than one mutable variable, so Prepack uses an additional level of array nesting to refer to them. In our example color was the only mutable variable in that closure, so Prepack used a single-element array to keep it. You may be concerned that the generated code is pretty long. It gets better after minification. Currently, this part of the serializer is focused more on correctness rather than most efficient output. Most likely, the output can be improved on a case-by-case basis so don't hesitate to file issues if you see opportunities for optimizations. In the beginning, Prepack didn't generate code that would allocate closures lazily. Instead, all captured variables were lifted to and initialized in the generated global code. Again, this is a speed \/ code size trade off that can and will be tuned over time. At this point you might be tempted to try copy and pasting some of your existing initialization code into the Prepack REPL. However, you may soon discover that such basic features as window or document in the browser, or require from Node, don't work as you'd expect. For example, the React DOM bundle contains this feature detection code that Prepack can't compile: The error message is: Most Prepack error codes correspond to a Wiki page with a description of this error. For example, here is a page for PP0004. (The other PP0001 error is a from a legacy error system that you can help migrate away from.) So why didn't this code work? To answer this question we need to recall what makes Prepack work in the first place. In order to execute code, Prepack usually needs to know what different values are equal to. But some things are only known at runtime. Prepack can't possibly know what browser the code will run in ahead of time, so it can't be sure whether it's safe to apply the in operator to the document object, or if it's going to throw (and thus take a potentially different code path if there's a try \/ catch above). This sounds pretty grim. After all, it is common for the initialization code to read something from the environment that is not known at the build time. There are two ways around this. One way is to only Prepack the code that doesn't depend on external data, and put any environment checks outside of the Prepacked bundle. This can be a reasonable strategy if such code is easy to isolate. Another way to solve this is to use Prepack's most powerful feature: abstract values. We will look at abstract values  in detail in the next sections, but the gist is that in a limited set of cases, Prepack can execute code even if it doesn't know the exact values of some expressions, and you can give it some further hints for things like Node or browser APIs, or other unknown inputs. We've covered the basics of how Prepack works, but we haven't looked at its most interesting capabilities yet: We will explore those topics in the next articles.","time":1525809879,"title":"A Gentle Introduction to Prepack (Part 1)","type":"story","url":"https:\/\/gist.github.com\/gaearon\/d85dccba72b809f56a9553972e5c33c4","label":7,"label_name":"random"},{"by":"wybiral","descendants":0,"id":17024399,"kids":"None","score":1,"text":"Having a loyal online following is the holy grail for anyone representing a brand these days. Fervently loyal fans who are willing to amplify your promotional efforts and lobby on your behalf through social media\u2026 What\u2019s not to love? But the culture of your fans can also become representative of your brand and will shape the expectations of what followers of your brand should be like. What do you do when your fan base forms an identity that doesn\u2019t represent your brand values? Or worse, when your base begins to troll or harass people in your name? While it may not seem like your responsibility to be concerned with how your base is perceived, you\u2019re spending marketing resources to reach audiences for the benefit of their support. And in doing so you\u2019re effectively forming a partnership with those communities that will be seen as an endorsement if you aren\u2019t careful. One example of someone handling this correctly was when YouTuber PewDiePie found his fans (who he calls \u201cbros\u201d) harassing a girl named Daisy in a Facebook post. He responded by taking to social media to denounce their behavior saying \u201cI would honestly pick 1 Daisy bro, over 43 million like the ones in the post\u201d and blocked accounts that were involved. He would later go on to sabotage his reputation for other reasons but in this case he seemed to handle it well. Another example is when J.K. Rowling found her fans criticizing the fact that a black actress was cast to play Hermione in Harry Potter and the Cursed Child. She was quick to defend the actress and condemn the behavior that she described as \u201ca bunch of racists\u201d even if it meant losing some fans in the process. In both of these examples, if a representative of the brand hadn\u2019t asserted their position at the risk of losing fans it could have easily tarnished the perception of their values as a whole. Don\u2019t let loyal fandom define your brands values for you. By clapping more or less, you can signal to us which stories really stand out. Austin, Texas","time":1525809869,"title":"Fans you don\u2019t want","type":"story","url":"https:\/\/medium.com\/@davywtf\/fans-you-dont-want-72b533cb40d1","label":10,"label_name":"thought"},{"by":"vuln","descendants":0,"id":17024396,"kids":"None","score":1,"text":"\n\nFull Disclosure\nmailing list archives\n\n\n\n \n\u00a0\u00a0By Date\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0By Thread\u00a0\u00a0\n","time":1525809854,"title":"GNU Wget Cookie Injection [CVE-2018-0494]","type":"story","url":"http:\/\/seclists.org\/fulldisclosure\/2018\/May\/20","label":7,"label_name":"random"},{"by":"buovjaga","descendants":0,"id":17024374,"kids":"None","score":2,"text":"brionv reticula, electronica, & oddities I have a silly dream of seeing graphical Linux\/FOSS programs running portably on any browser-based system outside of app store constraints. Two of my weekend side projects are working in this direction: an x86 emulator core to load and run ELF binaries, and better emscripten cross-compilation support for the GTK+ stack. Emulation ideas An x86 emulator written in WebAssembly could run pre-built Linux binaries, meaning in theory you could make an automated packager for anything in a popular software repository. But even if all the hard work of making a process-level emulator work, and hooking up the Linux-level libraries to emulated devices for i\/o and rendering, there are some big performance implications, and you\u2019re probably also bundling lots of library code you don\u2019t need at runtime. Instruction decoding and dispatch will be slow, much slower than native. And it looks pretty complex to do JIT-ing of traces. While I think it could be made to work in principle, I don\u2019t think it\u2019ll ever give a satisfactory user experience. Cross-compilation Since we\u2019ve got the source of Linux\/FOSS programs by definition, cross-compiling them directly to WebAssembly will give far better performance! In theory even something from the GNOME stack would work, given an emscripten-specific gdk backend rendering to a WebGL canvas just like games use SDL2 or similar to wrap i\/o. But long before we can get to that, there are low-level library dependencies. Let\u2019s start with glib, which implements a bunch of runtime functions and the GObject type system, used throughout the stack. Glib needs libffi, a library for calling functions with at-runtime call signatures and creating closure functions which enclose a state variable around a callback. In other words, libffi needs to do things that you cannot do in standard C, because it needs system-specific information about how function arguments and return values are transferred (part of the ABI, application binary interface). And to top it off, in many cases (emscripten included) you still can\u2019t do it in C, because asm.js and WebAssembly provide no way to make a call with an arbitrary argument list. So, like a new binary platform, libffi must be ported\u2026 It seems to be doable by bumping up to JavaScript, where you can construct an array of mixed-type arguments and use Function.prototype.apply to call the target function. Using an EM_ASM_ block in my shiny new wasm32\/ffi.c I was able to write a JavaScript implementation of the guts of ffi_call which works for int, float, and double parameters (still have to implement 64-bit ints and structs). The second part of libffi is the closure creation API, which I think requires creating a closure function in the JavaScript side, inserting it into the module\u2019s function tables, and then returning the appropriate index as its address. This should be doable, but I haven\u2019t started yet. Emscripten function pointers There are two output targets for the emscripten compiler: asm.js JavaScript and WebAssembly. They have similar capabilities and the wrapper JS is much the same in both, but there are some differences in implementation and internals as well as the code format. One is in function tables for indirect calls. In both cases, the low-level asm.js\/WASM code can\u2019t store the actual pointer address of a function, so they use an index into a table of functions. Any function whose address is taken at compile time is added to the table, and its index used as the address. Then, when an indirect call through a \u201cfunction pointer\u201d is made, the pointer is used as the index into the function table, and an actual function call is made on it. Voila! In asm.js, there are lots of Weird Things done about type consistency to make the JavaScript compilers fast. One is that the JS compiler gets confused if you have an array of functions that _contain different signatures_, making indirect calls run through slower paths in final compiled code. So for each distinct function signature (\u201creturns void\u201d or \u201creturns int, called with float32\u201d etc) there was a separate array. This also means that function pointers have meaning only with their exact function signature \u2014 if you take a pointer to a function with a parameter and call it without one, it could end up calling an entirely different function at runtime because that index has a different meaning in that signature! In WebAssembly, this is handled differently. Signatures are encoded at the call site in the call_indirect opcode, so no type inference needs to be done. But. At least currently, the asm.js-style table separation is still being used, with the multiple tables encoded into the single WebAssembly table with a compile-time-known constant offset. In both cases, the JS side can do indirect calls by calculating the signature string (\u201cvf\u201d for \u201cvoid return, float32 arg\u201d etc) and calling the appropriate \u201cdynCall_vf\u201d etc method, passing first the pointer and then the rest of the argument list. On asm.js this will look up in the tables directly; on WASM it\u2019ll apply the index. (etc) It\u2019s possible that emscripten will change the WASM mode to use a single array without the constant offset indirection. This will simplify lookups, and I think make it easier to add more functions at runtime. Because you see, if you want to add a callback at runtime, like libffi\u2019s closure API wants to, then you need to add another entry to that table. And in asm.js the table sizes are fixed for asm.js validation rules, and in WASM current mode the sub-tables are definitely fixed at compile time, since those constant offsets are used throughout. So currently there\u2019s an option you can use at build time to reserve room for runtime function pointers, I think I\u2019ll have to use it, but that only reserves *fixed space* of a given number of pointers. Next Coming up next time: int64 and struct in the emscripten ABI, and does the closure API work as expected? Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n  ","time":1525809737,"title":"Emscripten fun: porting libffi to WebAssembly part 1","type":"story","url":"https:\/\/brionv.com\/log\/2018\/05\/06\/emscripten-fun-porting-libffi-to-webassembly-part-1\/","label":3,"label_name":"dev"},{"by":"senthil_rajasek","descendants":0,"id":17024361,"kids":"None","score":1,"text":"Published: May 8, 2018 8:48 a.m. ET The median sale price for a home in California is more than double that in the rest of the nation By Say goodbye to Hollywood, Billy Joel sang in 1976. Now, in the midst of a deepening housing crisis, thousands of people are following that advice.  Over a million more people moved out of California from 2006 to 2016 than moved in, according to a new report, due mainly to the high cost of housing that hits lower-income people the hardest.  \u201cA strong economy can also be dysfunctional,\u201d noted the report, a project of Next 10 and Beacon Economics. Housing costs are much higher in California than in other states, yet wages for workers in the lower income brackets aren\u2019t. And the state attracts more highly-educated high-earners who can afford pricey homes.  There are many reasons for the housing crunch, but the lack of new construction may be the most significant. According to the report, from 2008 to 2017, an average of 24.7 new housing permits were filed for every 100 new residents in California. That\u2019s well below the national average of 43.1 permits per 100 people.  If this trend persists, the researchers argued, analysts forecast the state will be about 3 million homes short by 2025.  Read: Why aren\u2019t there enough houses to buy? What does it mean?  California homeowners spend an average of 21.9% of their income on housing costs, the 49th worst in the nation, while renters spend 32.8%, the 48th worst. The median rent statewide in 2016 was $1,375, which is 40.2% higher than the national average. And the median home price was \u2014 wait for it \u2014 more than double that of the national average.  One coping strategy: California residents are more likely to double up. Nearly 14% of renter households had more than one person per bedroom, the highest reading for this category in the nation.  Coping can also mean leaving.  In a separate analysis, Realtor.com found that the number of people searching real estate listings in the 16 top California markets compared to people living there and searching elsewhere was more than double that of other areas \u2014 and growing.  And in those areas \u2014 counties including Santa Clara, San Mateo and Los Angeles \u2014 the growth in views of listings on Realtor.com was virtually unchanged compared to a year ago this spring, while views of listings in other U.S. areas were 15% higher.  (News Corp, owner of MarketWatch, also operates Realtor.com under license from the National Association of Realtors.)  Also read: America\u2019s new great migration in search of lower property taxes   The Next 10 and Beacon Economics researchers used Census data to track migration patterns by demographic characteristics. More than 20% of the 1.1 million people who moved in the decade they tracked did so in 2006, at the height of the housing bubble, when prices were, as they write, \u201csky-high.\u201d As the housing market imploded and prices came back to earth, migration out of the state slowed. But as prices recovered, \u201cout-migration\u201d has not only picked up steam, it\u2019s accelerated. Those migration patterns are shaped by socioeconomics. Most people leaving the state earn less than $30,000 per year, even as those who can afford higher housing costs are still arriving. As the report noted, California was also a net importer of highly skilled professionals from the information, professional and technical services, and arts and entertainment industries. On the other hand, California saw the largest exodus of workers in accommodation, construction, manufacturing and retail trade industries. (In a note about what this statewide trend might mean for the national economy, the report also calls the housing crunch \u201cmost dire\u201d in agricultural areas, particularly the Central Valley and Imperial County.)  And where those refugees head may say a lot about why they\u2019re going. The top five destinations for California migrants between 2014 and 2016 were the nearby, but generally cheaper, states of Texas, Arizona, Nevada, Oregon and Washington.  It\u2019s worth noting that many housing analysts and economists believe that the 2017 tax law changes may push residents of higher-priced properties out of high-tax states like California. But that isn\u2019t happening yet.   Read: Home prices won\u2019t slow down, stumping the experts and shutting out buyers  \r\n                            Andrea Riquier reports on housing and banking from MarketWatch's New York newsroom. Follow her on Twitter @ARiquier.\r\n                         \r\n                            Andrea Riquier reports on housing and banking from MarketWatch's New York newsroom. Follow her on Twitter @ARiquier.\r\n                         Join the conversation Copyright \u00a9 2018 MarketWatch, Inc.  All rights reserved. \n                By using this site you agree to the Terms of Service,\n                Privacy Policy, and\n                Cookie Policy.\n            ","time":1525809663,"title":"California exodus surges","type":"story","url":"https:\/\/www.marketwatch.com\/story\/with-no-letup-in-home-prices-the-california-exodus-surges-2018-05-03","label":7,"label_name":"random"},{"by":"itsjefftong","descendants":0,"id":17024351,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Create a list of important tasks and gotchas to check for before launching a\nsmart contract. This is not meant to be a security guide.\nThis is a generic guide and does not cover cases specific to each contract\u2019s\nuse case. Feel free to make PRs for things I may have missed.","time":1525809635,"title":"Show HN: A checklist of smart contract deployment","type":"story","url":"https:\/\/github.com\/trigun0x2\/ethereum-deployment-checklist","label":4,"label_name":"github"},{"by":"bbimbop","descendants":0,"id":17024326,"kids":"None","score":1,"text":"\n        \tWe reveal the amazing Microsoft Excel\u2122 shortcut strategies used by professional asset managers, investors, and analysts.\n         Excel\u2122 is designed to be used without a mouse. Using shortcuts can increase your speed by over 100%. Professionals using Excel\u2122 with a mouse take a longer to perform the same actions.  Works anytime anywhere on Windows\u2122, OSX, and mobile browsers.  Using Excel\u2122 shortcuts saves time and money. Shortcuts are essential for investment and analysis professionals. New hires and advanced users who are wasting time clicking the mouse will benefit from this course. Our comprehensive course builds skills and eliminates bad habits. Free Trial Video \u00bb Learn how to control data and protect your most valuable assets. You will learn industry best practices for version control and data security used by the top hedge funds and investment banks. Learn techniques for  Free Trial Video \u00bb You are tired of clicking the mouse. Work smarter, not harder. Increase ergonomic comfort and reduce the repetitive strain of from using a mouse. We teach to think in terms of finding efficient solutions, which we believe is the most valuable problem solving approach. Free Trial Video \u00bb Copyright 2018. All Rights Reserved. ","time":1525809494,"title":"New Excel Training Platform for SME's","type":"story","url":"http:\/\/www.shortcutinsider.com","label":7,"label_name":"random"},{"by":"DmenshunlAnlsis","descendants":0,"id":17024324,"kids":"None","score":2,"text":"\u201cMarine protected areas\u201d have been an environmental success story. But a new study finds that most won\u2019t withstand global warming. \u201cWe should\u2019ve known,\u201d said John Bruno, \u201cbut we really didn\u2019t.\u201d Bruno is a professor of marine biology at the University of North Carolina. Recently, he and his colleagues asked a simple question: If scientists know that climate change will alter national parks on land, how will it affect the thousands of national parks and conservation areas around the world that are underwater? The answer, published Monday in Nature Climate Change, shocked him. But first: Yes, there are thousands of underwater national parks. In the last few decades, conservationists have rushed to protect biodiversity in the oceans. Governments have established more than 8,000 marine protected areas, or MPAs, worldwide. These protected areas include world-famous wonders\u2014there are MPAs for Australia\u2019s Great Barrier Reef and the Gal\u00e1pagos Islands\u2014but many of them also surround the United States. During his second term, President Barack Obama protected more than 750,000 square miles of ocean, expanding several national monuments created by his predecessor, President George W. Bush. These conservation areas are supposed to act as refuges, shielding animals and plants from human exploitation of the environment. But Bruno and his colleagues found that they cannot protect underwater life from environmental problems that are truly global. The world\u2019s 8,236 marine protected areas remain intensely vulnerable to climate change, their research found. \u201cThe results predict five times more warming than we\u2019ve seen in the last 15, 20 years,\u201d Bruno told me. In some areas, up to 10 times more warming is forecast. \u201cGiven the effects we\u2019ve already seen of 0.7 [degrees Celsius] of warming\u2014corals being wiped out, species dying, seabirds being affected\u2014it\u2019s staggering to imagine what will happen in the places that will see 10 times that much warming. It\u2019s frightening,\u201d he said. Bruno and his colleagues find that most tropical MPAs will exceed their \u201ccommunity thermal-safety margin\u201d\u2014a scientific term for the amount of heat that makes an entire ecosystem unsustainable\u2014just three decades from now. MPAs in more temperate areas will not exceed that margin, on average, until 2150. The study examined a specific climate scenario called RCP 8.5, which assumes humanity will continue to emit more and more heat-trapping carbon pollution through the end of the century. It\u2019s unclear if that projection will come to pass\u2014and, of course, it all depends on how the world\u2019s richest governments (and their citizens) act. Global carbon emissions rose in 2017, an ominous sign, after remaining flat for several years. Andrew King, a coral researcher at the University of Melbourne who was not involved in the study, said the MPA study was \u201cinteresting and very important\u201d: \u201cIt adds to the evidence that it is difficult to protect marine species in particular\u201d from the ravages of climate change, he told me in an email. \u201cCompared to land-based ecosystems, [marine species] have evolved to adapt to a narrower range of temperatures, so any warming becomes a problem more quickly.\u201d The new paper also raises two problems for conservationists\u2014one new and one old. First, the team\u2019s models suggest that the parts of the oceans that don\u2019t warm will suffer the worst of another environmental problem. Scientists predict that vast swaths of the ocean will lose their oxygen content as the century wears on, creating denuded \u201cdead zones.\u201d Bruno and his colleagues modeled this effect\u2014and they found many of the areas that are most vulnerable to deoxygenation are the same ones where warming is least likely. \u201cIt\u2019s almost an impossible trade-off,\u201d said Bruno. \u201cThe global pattern for deoxygenation is almost the opposite of the warming pattern. You can put [an MPA] in a place that won\u2019t warm, but then there\u2019s going to be less oxygen there, which is at least as big a problem as climate change.\u201d The second problem is a human problem, not a natural one. It\u2019s also more familiar to conservationists. For the last two decades, scientists have aimed to protect parts of the ocean that will resist global warming\u2014to find so-called refugia from warming. \u201cThere\u2019s a huge effort to figure out where warming isn\u2019t happening as fast and to move MPAs there, or put new MPAs there,\u201d Bruno said. \u201cWe\u2019re clearly not doing that.\u201d Warming refugia, Bruno and his colleagues found, didn\u2019t especially coincide with the placement of MPAs. And only 3.5 percent of the world\u2019s marine protected areas\u2014or fewer than 300 areas total\u2014will be protected from both future warming and deoxygenation. These results stunned even Bruno, who does much of his field research in the Gal\u00e1pagos Islands. The Gal\u00e1pagos, which sport famously unique wildlife, also give rise to an unusual climate. Even though the Gal\u00e1pagos are almost precisely equatorial\u2014they are, in fact, several hundred miles due west of Ecuador\u2014they have just a lukewarm climate. \u201cThere are penguins there. It\u2019s almost a temperate community,\u201d Bruno told me. Perhaps because of this, \u201cI was under the impression that they were not predicted to warm, that warming is not really an issue there,\u201d he said. But the model projected \u201creally severe warming\u201d\u20144 degrees Celsius of warming, or more than 7 degrees Fahrenheit. \u201cThat really surprised us,\u201d he told me. It\u2019s not the last surprise that climate change has in store\u2014for scientists, for wildlife, or for the millions of humans that rely on the oceans every day. \n    Jeff VanderMeer discusses how writing fiction about environmental crises may jolt readers out of complacency.\n Kanye West wants freedom\u2014white freedom. I could only have seen it there, on the waxed hardwood floor of my elementary-school auditorium, because I was young then, barely 7 years old, and cable had not yet come to the city, and if it had, my father would not have believed in it. Yes, it had to have happened like this, like folk wisdom, because when I think of that era, I do not think of MTV, but of the futile attempt to stay awake and navigate the yawning whiteness of Friday Night Videos, and I remember that there were no VCRs among us then, and so it would have had to have been there that I saw it, in the auditorium that adjoined the cafeteria, where after the daily serving of tater tots and chocolate milk, a curtain divider was pulled back and all the kids stormed the stage. And I would have been there among them, awkwardly uprocking, or worming in place, or stiffly snaking, or back-spinning like a broken rotor, and I would have looked up and seen a kid, slightly older, facing me, smiling to himself, then moving across the floor by popping up alternating heels, gliding in reverse, walking on the moon. The new music video from Childish Gambino weaponizes the viewer\u2019s instinctive bodily empathy. \u201cThis Is America\u201d isn\u2019t the first time that Donald Glover, as his musical alter-ego Childish Gambino, has harnessed dance in service of surrealism. But the art form has a conspicuous symbolic significance in the artist\u2019s latest single, which Glover debuted on Saturday Night Live: The song\u2019s emphasis on dance was apparent in his live performance on the show, in the cover art for the track, and in the remarkable music video itself, which has more than 36 million views on YouTube as of publication. In the video, a grinning, shirtless Glover dances through a giant warehouse, occasionally accompanied by black school children in uniform, as chaotic scenes of violence unfold behind him\u2014and are sometimes enacted by him. The question is less whether a dress or an idea is borrowed, than the uses to which it\u2019s then put. Meet the Death Metal Cowboys of Botswana. In black leather decorated with metal studs, they play a pounding style of music that people who know more than me trace to the British band \u201cVenom\u201d and its 1981 album Welcome to Hell. Question: Is this cultural appropriation? Why or why not? The question is inspired by a spasm of social-media cruelty that caught wide attention last week. A young woman in Utah bought a Chinese-style dress to wear to her high school formal. She posted some photographs of herself on her personal Instagram page\u2014and suddenly found herself the target of virulent online abuse. For once, the story has a happy ending. Good sense and kindness prevailed, and instead of her prom being ruined, the young woman exited the dance buoyed by worldwide support and affirmation, most of all from within China. President Trump said he'll begin reinstating nuclear sanctions on the Iranian regime, effectively marking the beginning of the end of the agreement. Updated at 3:49 p.m. ET President Trump announced Tuesday that he would reimpose nuclear-related sanctions on Iran, setting the stage for a long-expected dismantling of the Obama-era nuclear deal with the Islamic Republic. \u201cI am announcing today that the United States will withdraw from the Iran nuclear deal,\u201d he said. \u201cIn a few moments, I will sign a presidential memorandum to begin reinstating U.S. nuclear sanctions on the Iranian regime.\u201d On the face of it, the announcement goes much further than had been expected. Observers had expected Trump to decline to waive only the most immediate set of sanctions related to Iran\u2019s oil trade\u2014the sanctions that were specifically at play with the May 12 deadline, giving America\u2019s European allies time to find a fix that would placate the president. Childish Gambino\u2019s sensational \u201cThis Is America\u201d video implicates the viewer in the misuse of black art. If you search for \u201cThis Is America\u201d on Twitter, you find not only a gushing river of well-deserved praise for Donald Glover\u2019s new work, which has quickly become the most talked-about music video of recent memory. You also find Trump supporters using the moment to spread their messages. The hashtag #ThisIsAmerica sits next to a rant about the deep state. It sits next to a sneering meme about Hillary Clinton. It sits next to a picture of white pioneers, shared by a \u201cEuropean rights activist,\u201d who says, \u201cMost of the people who built America looked like this.\u201d Trending hashtags get hijacked by unsympathetic causes as a matter of course, but Glover knew what he was getting into with the name \u201cThis Is America.\u201d The defining of a nation is the essential task of politics, and Glover\u2019s definition has now been made clear. America is a place where black people are chased and gunned down, and it is a place where black people dance and sing to distract\u2014themselves, maybe, but also the country at large\u2014from that carnage. America is a room in which violence and celebration happen together, and the question of which one draws the eye is one of framing, and of what the viewer wants to see. What predicts whether a politician accused of sexual harassment or abuse will try to stay in office\u2014or quickly fold? The fact of a powerful man being accused of sexual misconduct is not, at this stage, all that unusual\u2014though the details of the accusations against New York Attorney General Eric Schneiderman were nauseating. What was unusual was the speed with which he resigned. Just three hours elapsed from the moment The New Yorker published an article detailing allegations of abuse by former romantic partners until Schneiderman, a Democrat, announced he was leaving office. He offered a terse statement denying the claims, but added, \u201cWhile these allegations are unrelated to my professional conduct or the operations of the office, they will effectively prevent me from leading the office\u2019s work at this critical time. I therefore resign my office, effective at the close of business on May 8, 2018.\u201d Many of the assumptions that guided America\u2019s march to conflict in 2003 still dominate American foreign policy today.\u00a0 \u00a0 Last week, while watching Benjamin Netanyahu unveil secret information that supposedly proved that Iran is deceiving the world about its nuclear-weapons program, I had a flashback. It was to February 5, 2003, when then-Secretary of State Colin Powell unveiled secret information that supposedly proved that Iraq was deceiving the world about its nuclear, chemical, and biological weapons programs. Like Netanyahu\u2019s, Powell\u2019s presentation was dramatic. He informed the United Nations Security Council that some of the material he was about to present came from \u201cpeople who have risked their lives to let the world know what Saddam Hussein is really up to.\u201d He went on to play a secretly recorded conversation of two Iraqi officials supposedly plotting to mislead weapons inspectors. He later presented a photo of bunkers that allegedly held \u201cactive chemical munitions\u201d but were \u201cclean when the inspectors get there.\u201d Saddam, Powell insisted, wants \u201cto give those [of] us on this Council the false impression that the inspection process was working.\u201d Powell\u2019s presentation was designed to prove that it was not. Secretary of Education Betsy DeVos's address at Ave Maria University last weekend reignited a debate about the proper role of a Catholic school in a sharply divided time. AVE MARIA, Florida\u2014In this enclave in Southwest Florida, the lush, pruned golf course and ritzy subdivisions are eclipsed only by the magnificent church that marks the town\u2019s distinctive Catholic character. The town is also home to a similarly named religious institution, Ave Maria University, which was founded in 2003. The institution\u2014and the master-planned community in which it is now located\u2014is the brainchild of Tom Monaghan, the billionaire founder of Domino\u2019s, who is originally from Michigan and known for his Catholic philanthropy. Ave Maria is still in its infancy compared with the big names in Catholic higher education. But Monaghan has a vision. He wants to build a campus that is more religious than the Notre Dames of the world, and this patch of Florida, he decided, was the right location for that. The secluded community feels less like the rest of the state and more like a paradise for those who want to live faithfully. One student, Anne Marie Schlueter, a sophomore, told me the institution\u2019s mission of faith-driven education drew her here from Ohio; it was a university where she could build a strong Catholic foundation with which to answer critical questions about the world. But the young institution still has important questions to answer of its own\u2014many about itself\u2014as it grows and navigates debates within its central Catholic identity. With the 2018 and 2020 elections on the horizon, race and racism are becoming ever-larger issues among the most marginalized communities in America, making the Democratic coalition harder and harder to hold. In 2018, black voters are finding out just what the hell they had to lose. Nazis and Klansmen march openly and proudly, and hate crimes appear to be on the rise. Police killings of people\u2014especially black people\u2014remain largely the same year to year, and this iteration of the Justice Department has largely abdicated any federal responsibility in reducing brutality. An infant-mortality crisis is tightening its grip on the most marginalized communities, and across many economic metrics\u2014from evictions, to generational wealth, to segregation\u2014disparities are either stagnating or trending in the wrong direction. Fifty years after the Kerner Commission\u2019s report said the country was \u201cmoving toward two societies, one black, one white\u2014separate and unequal,\u201d the prophecy has been all but fully realized. Rather than enforcing a top-down mandate, the school trains teachers in the science behind trauma and leaves the rest up to them. In education, initiatives tend to roll down from above. A district buys a new curriculum, or gets funding for a new program, and principals receive their marching orders, which they in turn hand down to teachers below. That\u2019s not the case at Ohio Avenue Elementary School in Columbus, Ohio. The 19th-century corniced brick building is perhaps an unlikely home for experimental methods of nurturing children\u2019s developing brains. The surrounding streets are lined with abandoned buildings, pawn shops, cash-advance outlets, and dollar stores. A large house with a boarded-up door sits directly across from the school\u2019s playground. In Ohio Avenue\u2019s zip code, half of the families with children under 18 live in poverty, as compared with 25 percent across Columbus and 17 percent nationally, according to census data. Two young girls escape Syria in an intimate short film, told largely through home movies. After a lifetime of intestinal problems, biohacker Josiah Zayner declares war on his own body's microbes. An enslaved woman who jumped from a building in 1815 is later revealed to be the plaintiff in a successful lawsuit for her freedom. Support 160 years of independent journalism.   TheAtlantic.com Copyright (c) 2018 by The Atlantic Monthly Group. All Rights Reserved.","time":1525809489,"title":"Climate Change Could Destroy Even the Ocean's Most Pristine Parks","type":"story","url":"https:\/\/www.theatlantic.com\/science\/archive\/2018\/05\/conservation-projects-cant-stop-climate-change\/559757\/?single_page=true","label":7,"label_name":"random"},{"by":"DmenshunlAnlsis","descendants":0,"id":17024322,"kids":"None","score":1,"text":"How the fatty green fruit went from near-extinction to a worldwide boom  Avocados are on a roll. More precisely, they\u2019re on toast\u2014a lot of toast. Last summer, British Vogue reported that more than 3 million new photos of whole, sliced, and toast-topping avocados are uploaded to Instagram every day. But how did this humble fruit, originally named after testicles, get from its Mexican forest home to a tattoo on Miley Cyrus\u2019s upper arm? This episode, we unravel the avocado\u2019s amazing journey, a story that involves not only conquistadors and cartel violence, but also a Southern California postman and the actress Angie Dickinson lounging in a white leotard. And we discover where the avocado is headed next\u2014a place where it\u2019s known as the butter fruit, and often consumed in shake form. Listen in now for all this creamy green goodness and more. Nobody is sure exactly where the avocado first came from, but the earliest evidence for its consumption dates back nearly 10,000 years, from the remains of settlements in central Mexico. The avocado tree itself is, of course, much more ancient, so ancient that it had already been an evolutionary ghost for 3,000 years by that point. Its partners in evolution\u2014the giant, elephant-like gomphotheres and three-ton ground sloths that dined on its fruit in return for transporting and then pooping out its giant seed\u2014went extinct soon after the first bipedal apes arrived in the region. Rodents, jaguars, and eventually humans stepped in as dispersal mechanisms, albeit significantly less effective ones. The flourishing avocado forests that carpeted much of Mesoamerica dwindled and died out. And, as Mary Lu Arpaia, who runs the avocado breeding program at the University of California at Riverside, explained, the avocado became a backyard fruit, enjoyed by first the indigenous peoples and later the conquistadors, but rarely cultivated intensively\u2014until recent decades. The story of this anachronistic fruit\u2019s astonishing resurgence hinges on a trade agreement. With the help of Brook Larmer, a financial columnist, we explore the machinations that turned the avocado into green gold. But the avocado\u2019s rise is more than just a business story: Smashed up on a piece of toasted bread, avocado has become a signifier of a certain lifestyle, popularized by none other than Gwyneth Paltrow. Although the journalist Lauren Oyler warned us that trying to pinpoint the dish\u2019s origins is \u201ca fool\u2019s errand,\u201d she nonetheless guides us through the celebrity-strewn story, dissecting avocado toast\u2019s allure\u2014and expense. Today, avocado is everywhere: It\u2019s worshipped for its heart-healthy fats, and blamed for bankrupting a generation. But, according to Larmer, we\u2019re nowhere near peak avocado yet. Listen in now for the next chapter in the avocado\u2019s astonishing history. This post appears courtesy of Gastropod. \n    Jeff VanderMeer discusses how writing fiction about environmental crises may jolt readers out of complacency.\n Kanye West wants freedom\u2014white freedom. I could only have seen it there, on the waxed hardwood floor of my elementary-school auditorium, because I was young then, barely 7 years old, and cable had not yet come to the city, and if it had, my father would not have believed in it. Yes, it had to have happened like this, like folk wisdom, because when I think of that era, I do not think of MTV, but of the futile attempt to stay awake and navigate the yawning whiteness of Friday Night Videos, and I remember that there were no VCRs among us then, and so it would have had to have been there that I saw it, in the auditorium that adjoined the cafeteria, where after the daily serving of tater tots and chocolate milk, a curtain divider was pulled back and all the kids stormed the stage. And I would have been there among them, awkwardly uprocking, or worming in place, or stiffly snaking, or back-spinning like a broken rotor, and I would have looked up and seen a kid, slightly older, facing me, smiling to himself, then moving across the floor by popping up alternating heels, gliding in reverse, walking on the moon. The new music video from Childish Gambino weaponizes the viewer\u2019s instinctive bodily empathy. \u201cThis Is America\u201d isn\u2019t the first time that Donald Glover, as his musical alter-ego Childish Gambino, has harnessed dance in service of surrealism. But the art form has a conspicuous symbolic significance in the artist\u2019s latest single, which Glover debuted on Saturday Night Live: The song\u2019s emphasis on dance was apparent in his live performance on the show, in the cover art for the track, and in the remarkable music video itself, which has more than 36 million views on YouTube as of publication. In the video, a grinning, shirtless Glover dances through a giant warehouse, occasionally accompanied by black school children in uniform, as chaotic scenes of violence unfold behind him\u2014and are sometimes enacted by him. The question is less whether a dress or an idea is borrowed, than the uses to which it\u2019s then put. Meet the Death Metal Cowboys of Botswana. In black leather decorated with metal studs, they play a pounding style of music that people who know more than me trace to the British band \u201cVenom\u201d and its 1981 album Welcome to Hell. Question: Is this cultural appropriation? Why or why not? The question is inspired by a spasm of social-media cruelty that caught wide attention last week. A young woman in Utah bought a Chinese-style dress to wear to her high school formal. She posted some photographs of herself on her personal Instagram page\u2014and suddenly found herself the target of virulent online abuse. For once, the story has a happy ending. Good sense and kindness prevailed, and instead of her prom being ruined, the young woman exited the dance buoyed by worldwide support and affirmation, most of all from within China. President Trump said he'll begin reinstating nuclear sanctions on the Iranian regime, effectively marking the beginning of the end of the agreement. Updated at 3:49 p.m. ET President Trump announced Tuesday that he would reimpose nuclear-related sanctions on Iran, setting the stage for a long-expected dismantling of the Obama-era nuclear deal with the Islamic Republic. \u201cI am announcing today that the United States will withdraw from the Iran nuclear deal,\u201d he said. \u201cIn a few moments, I will sign a presidential memorandum to begin reinstating U.S. nuclear sanctions on the Iranian regime.\u201d On the face of it, the announcement goes much further than had been expected. Observers had expected Trump to decline to waive only the most immediate set of sanctions related to Iran\u2019s oil trade\u2014the sanctions that were specifically at play with the May 12 deadline, giving America\u2019s European allies time to find a fix that would placate the president. Childish Gambino\u2019s sensational \u201cThis Is America\u201d video implicates the viewer in the misuse of black art. If you search for \u201cThis Is America\u201d on Twitter, you find not only a gushing river of well-deserved praise for Donald Glover\u2019s new work, which has quickly become the most talked-about music video of recent memory. You also find Trump supporters using the moment to spread their messages. The hashtag #ThisIsAmerica sits next to a rant about the deep state. It sits next to a sneering meme about Hillary Clinton. It sits next to a picture of white pioneers, shared by a \u201cEuropean rights activist,\u201d who says, \u201cMost of the people who built America looked like this.\u201d Trending hashtags get hijacked by unsympathetic causes as a matter of course, but Glover knew what he was getting into with the name \u201cThis Is America.\u201d The defining of a nation is the essential task of politics, and Glover\u2019s definition has now been made clear. America is a place where black people are chased and gunned down, and it is a place where black people dance and sing to distract\u2014themselves, maybe, but also the country at large\u2014from that carnage. America is a room in which violence and celebration happen together, and the question of which one draws the eye is one of framing, and of what the viewer wants to see. What predicts whether a politician accused of sexual harassment or abuse will try to stay in office\u2014or quickly fold? The fact of a powerful man being accused of sexual misconduct is not, at this stage, all that unusual\u2014though the details of the accusations against New York Attorney General Eric Schneiderman were nauseating. What was unusual was the speed with which he resigned. Just three hours elapsed from the moment The New Yorker published an article detailing allegations of abuse by former romantic partners until Schneiderman, a Democrat, announced he was leaving office. He offered a terse statement denying the claims, but added, \u201cWhile these allegations are unrelated to my professional conduct or the operations of the office, they will effectively prevent me from leading the office\u2019s work at this critical time. I therefore resign my office, effective at the close of business on May 8, 2018.\u201d Many of the assumptions that guided America\u2019s march to conflict in 2003 still dominate American foreign policy today.\u00a0 \u00a0 Last week, while watching Benjamin Netanyahu unveil secret information that supposedly proved that Iran is deceiving the world about its nuclear-weapons program, I had a flashback. It was to February 5, 2003, when then-Secretary of State Colin Powell unveiled secret information that supposedly proved that Iraq was deceiving the world about its nuclear, chemical, and biological weapons programs. Like Netanyahu\u2019s, Powell\u2019s presentation was dramatic. He informed the United Nations Security Council that some of the material he was about to present came from \u201cpeople who have risked their lives to let the world know what Saddam Hussein is really up to.\u201d He went on to play a secretly recorded conversation of two Iraqi officials supposedly plotting to mislead weapons inspectors. He later presented a photo of bunkers that allegedly held \u201cactive chemical munitions\u201d but were \u201cclean when the inspectors get there.\u201d Saddam, Powell insisted, wants \u201cto give those [of] us on this Council the false impression that the inspection process was working.\u201d Powell\u2019s presentation was designed to prove that it was not. Secretary of Education Betsy DeVos's address at Ave Maria University last weekend reignited a debate about the proper role of a Catholic school in a sharply divided time. AVE MARIA, Florida\u2014In this enclave in Southwest Florida, the lush, pruned golf course and ritzy subdivisions are eclipsed only by the magnificent church that marks the town\u2019s distinctive Catholic character. The town is also home to a similarly named religious institution, Ave Maria University, which was founded in 2003. The institution\u2014and the master-planned community in which it is now located\u2014is the brainchild of Tom Monaghan, the billionaire founder of Domino\u2019s, who is originally from Michigan and known for his Catholic philanthropy. Ave Maria is still in its infancy compared with the big names in Catholic higher education. But Monaghan has a vision. He wants to build a campus that is more religious than the Notre Dames of the world, and this patch of Florida, he decided, was the right location for that. The secluded community feels less like the rest of the state and more like a paradise for those who want to live faithfully. One student, Anne Marie Schlueter, a sophomore, told me the institution\u2019s mission of faith-driven education drew her here from Ohio; it was a university where she could build a strong Catholic foundation with which to answer critical questions about the world. But the young institution still has important questions to answer of its own\u2014many about itself\u2014as it grows and navigates debates within its central Catholic identity. With the 2018 and 2020 elections on the horizon, race and racism are becoming ever-larger issues among the most marginalized communities in America, making the Democratic coalition harder and harder to hold. In 2018, black voters are finding out just what the hell they had to lose. Nazis and Klansmen march openly and proudly, and hate crimes appear to be on the rise. Police killings of people\u2014especially black people\u2014remain largely the same year to year, and this iteration of the Justice Department has largely abdicated any federal responsibility in reducing brutality. An infant-mortality crisis is tightening its grip on the most marginalized communities, and across many economic metrics\u2014from evictions, to generational wealth, to segregation\u2014disparities are either stagnating or trending in the wrong direction. Fifty years after the Kerner Commission\u2019s report said the country was \u201cmoving toward two societies, one black, one white\u2014separate and unequal,\u201d the prophecy has been all but fully realized. Rather than enforcing a top-down mandate, the school trains teachers in the science behind trauma and leaves the rest up to them. In education, initiatives tend to roll down from above. A district buys a new curriculum, or gets funding for a new program, and principals receive their marching orders, which they in turn hand down to teachers below. That\u2019s not the case at Ohio Avenue Elementary School in Columbus, Ohio. The 19th-century corniced brick building is perhaps an unlikely home for experimental methods of nurturing children\u2019s developing brains. The surrounding streets are lined with abandoned buildings, pawn shops, cash-advance outlets, and dollar stores. A large house with a boarded-up door sits directly across from the school\u2019s playground. In Ohio Avenue\u2019s zip code, half of the families with children under 18 live in poverty, as compared with 25 percent across Columbus and 17 percent nationally, according to census data. Two young girls escape Syria in an intimate short film, told largely through home movies. After a lifetime of intestinal problems, biohacker Josiah Zayner declares war on his own body's microbes. An enslaved woman who jumped from a building in 1815 is later revealed to be the plaintiff in a successful lawsuit for her freedom. Support 160 years of independent journalism.   TheAtlantic.com Copyright (c) 2018 by The Atlantic Monthly Group. All Rights Reserved.","time":1525809468,"title":"Peak Avocado Is yet to Come","type":"story","url":"https:\/\/www.theatlantic.com\/science\/archive\/2018\/05\/peak-avocado-is-yet-to-come\/559883\/?single_page=true","label":7,"label_name":"random"},{"by":"wjossey","descendants":1,"id":17024290,"kids":"[17024323]","score":1,"text":"\n                By Jon Schleuss and Lorena I\u00f1iguez Elebee\n \n\n                    Oct. 28, 2016\n                \n The 2016 count of the homeless population, taken in January, revealed that our county has more than 43,000 people living on the street or in shelters. (Long Beach, Glendale and Pasadena were not included.) It\u2019s a diverse group of individuals. They\u2019re not defined by one characteristic, but by many. Some homeless subpopulations grew in 2016, while others decreased. And they\u2019re disproportionately represented. Only 8% of the county\u2019s population is black. More homeless people (sheltered and unsheltered) live downtown than in any other neighborhood. This Los Angeles neighborhood saw about a 23% drop in its homeless population from 2015. Once a haven for struggling artists, Venice is a stark example of the nation\u2019s gulf between the haves and have-nots. More than 36,000 homeless people in the county are working-age adults. And about 500 are transgender, according to L.A. County's estimates. Los Angeles city officials pledged to end veteran homelessness last year. They didn\u2019t. But the number of homeless veterans did drop in the city by 41%. The county also saw a drop in veterans, families and those estimated to have HIV\/AIDS. These groups still represent small portions of the population. More than xxx live in .... Note: Numbers are rounded. Dots are randomly positioned inside census tracts and do not reflect the actual location of individual people. Sources: Los Angeles Homeless Services Authority, Times reporting, Mapbox, OpenStreetMap Leaflet | Map tiles by CartoDB, under CC BY 3.0. Data by OpenStreetMap, under ODbL., CartoDB attribution","time":1525809345,"title":"LA Homeless Population Map (2016)","type":"story","url":"http:\/\/www.latimes.com\/projects\/la-me-homeless-los-angeles-2016\/","label":7,"label_name":"random"},{"by":"ejstronge","descendants":0,"id":17024277,"kids":"None","score":2,"text":"There over 7 million Google and 13,000 Giphy search results for \u201chate meetings\u201d. There are almost as many for \u201clove meetings\u201d but that is just irony. Beyond that there are countless posts on time and money wasted in meetings\u200a\u2014\u200a\u201dThis Company Spent 300,000 Hours a Year in Pointless Meetings\u201d was a headline recently. The Muse reported on study results that 35\u201350% of work time is in meetings and 67% of meetings are \u201cfailures\u201d. We\u2019ve all computed the financial cost to the company of a meeting we hated. Ugh. I\u2019ve endured a lot of bad meetings in my time. I\u2019ve led bad meetings and attended bad meetings. I\u2019ve tried to fix meetings. I\u2019ve broken meetings. This post is about why meetings really are important to getting things done, even with incredibly divergent views on that fact, and why meetings so often go sideways or worse. By and large meetings come and go and most of us just accept them as part of doing collaborative work in a company. Like AutoCorrect, once in a while a meeting backfires to such a degree that it just sets off a stream of emotions about how horrible meetings can be and what a huge waste of time all meetings have become. Some react by attempting to define when\/how\/if to have meetings as if there is a secret that has eluded most everyone for 100 years. In the course of building a company the most important tool you have to create a culture of shared values is communication and meetings are critical to communication. When you bring together a team of talented and diverse individuals, the only way they will come to operate as a team is by spending time talking, listening, and understanding the perspective individuals bring to contribute to a larger whole. Unless everyone hired shares the same background and experiences, there\u2019s no way a group of people can converge to a high-performance team without meeting, sharing, and learning together. No amount of ping-pong, email, or shared docs can substitute for meeting. This essay does not contain the magical PowerPoint template for how to run an effective meeting, nor does it espouse a system for deciding how, when, or why to meet. I\u2019ve seen every type of agenda, preparation, tracking, issue-list, decision-making tool, template (whether using Word, Excel, Powerpoint, or Outlook). Call me skeptical. In my experience the best tool for meetings is scheduling time to have them in the first place and then to be present. The rest are just distractions to the real goals of sharing, building, deciding. Meetings in this post refers to internal to a company or focused on the internal workings of a company, primarily the most common meetings of small groups. External meetings with (potential) customers, partners, investors, press, and so on are rather different. The last section of the post discusses 1:1s and broader team meetings as well. Before I come to the defense of meetings, we should be honest with ourselves about meetings and some nearly universal truths. I really like @pg\u2019s 2009 post on Maker\u2019s Schedule, Manager\u2019s Schedule. It is absolutely correct and a great way to view time management in general. Read it and internalize it. I believe, however, the post might be interpreted too literally and used in a way that could cause difficulty in a growing company rather than to clarify reality. \u201cMaker time\u201d, aka engineer time, is the most valuable resource in any product effort and everything should always be done to treat engineer time as the precious resource it is and to make it as effective as it can be, especially in an early stage [tech] company when the company is effectively one collective coding brain. Amen. That said, meetings are just as important for engineers as they are for \u201cmanagers\u201d or \u201cmarketing\u201d or \u201csales\u201d or \u201cexecs\u201d or any other functional part (function means job function like eng, marketing, product, sales) of a company. Often engineers can miss the importance because striving for efficiency causes a certain blind spot relative to the larger opportunity of meeting. The shortest way I can say this is that the most important thing in growing a company or team is to of course focus on getting things done, but the only way to get the right things done is by having meetings, by talking. There is no doubt that a group of people not meeting will get a lot of stuff done. It can be said with equal confidence, however, that by not meeting the stuff that will get done will lack cohesiveness, quality, and a shared set of values\u200a\u2014\u200athe wrong stuff. The most expensive thing a growing company can do is get the wrong stuff done. This risk is magnified the larger the team, but clearly starts with just a couple of people. The question is how to get the right stuff done. The answer is by talking, listening, and discussing. Those together are the ingredients for a shared understanding and with a shared understanding the micro-decisions that everyone makes every day whether writing code, creating positioning, deploying a build, designing an experience, and so on. Talking, listening, discussing should not be thought of as \u201csoft skills\u201d or worse a \u201cwaste of time\u201d. Call it talking or call it meeting, but no matter how good each member of the team is at the \u201chard skills\u201d of their discipline, I am confident even the best are not psychic. That\u2019s why teams need to have meetings. Where meetings are generally misunderstood is that the effort to make them efficient, goal-oriented, and conclusive is exactly what shuts down discussion, causes people to think about what to say next rather than listen, and generally prevents collaboration. We tend to think of meetings themselves as the main event, when in reality meetings should be the practice sessions. Instead of thinking about meetings as the regular season, think of meetings as practice drills or warm-ups. The real main event comes when you\u2019re actually committing work to the screen. If you have done enough drills with your teammates then there\u2019s a really good chance you know how they will react, how they can help, and how the work you\u2019re committing will impact the overall \u201cgame\u201d. And since no one thinks they are above practice, we can agree no one is above meetings. Since no one likes to redo work or revisit plans unnecessarily, we can agree that the best tool to avoid that is to use talking and listing\u2014meetings\u2014to get to and remain on the same page. The reality of a company beyond the seed stage is that failing to communicate and collaborate result in massive inefficiencies and rework. And nothing upsets anyone more than having to redo work unnecessarily. In fact, people will spend countless hours debating whether to redo something rather than just a short time redoing just to make that point (reworking shipped code also introduces bugs, I get that). While it is common to view this as an engineering problem because code is expensive, to someone in marketing redoing messaging or redirecting a vendor for an event are each equally costly. Bringing this back to manager schedule versus maker schedule, one important consideration is that everyone at every level in a company is a maker in something. Every single job function has work products that are created. Even CEOs and execs need to have specific work products that they create on their own and do not pass along or \u201cvend out\u201d to others. For a fun description of hiring out people to do your own work, see Michael Kinsley\u2019s Ode to Managers which is the best post on what managers do I have read. Some meeting or time management systems attempt to force a classification of meetings before having a meeting. You decide what a meeting is for and from that decision the length of meeting, the format, the preparation, attendees, and more just follow. Given that I think most meeting should be lightly unstructured, it is no surprise I put little faith in meeting \u201csystems\u201d. There are, however, ways to classify meetings. The most useful reason to classify meetings is so that you have some idea as an attendee or as an organizer as to why everyone is sitting at a table burning hours. Once you recognize why a meeting is happening, it is much easier to participate as well as to reach a Zen state of meeting attendance. Each of the following meetings types is offered along with an approach that focuses on sharing, calibrating, and informing rather than trying to reach approval or decide something. Meetings suck for a lot of reasons, but the least of which is because they take time. Meetings primarily suck because of a fairly common set of human behaviors that are especially dysfunctional in a meeting context. Below are some of the most common dysfunctions. These are listed because the best suggestion for good meetings is to counter dysfunction while it is happening and see if collaboration can be put back on track in real-time. The general rule of providing feedback that is actionable at the time it is happening applies to meetings too. It is worth noting that some of these dysfunctions can become \u201cweaponized\u201d in large organizations where these anti-collaboration skills are used to prevent moving in a direction one does not agree with. Leaders need to recognize and act when someone is derailing a meeting, though in a strong culture peers should be able to call out such behavior as well. This is a long list, and at the same time it is hardly complete! I might add more based on discussions and learning. The thing is that most people say they don\u2019t want to have meetings. Most say \u201coh that meeting could have been handled with an email or slack message\u201d. They might be right. But that is only if you take the meeting literally. Meetings mean so much more to a company than conveying information or deciding. Meetings are literally how cultures are formed, values expressed, and companies made. Different people have different ways of learning, creating, and ultimately deciding. For a team to function such that the output is greater than the individual efforts summed, people need to align, communicate, and collaborate. People need to be on the same page and share the same values. For most everyone that means they need to have forums to talk, and if they don\u2019t want to talk at the very least they need forums to hear others. The most important things I know that make meetings effective is a very short list. The reason this list is not about how to have a meeting, how to decide things, or how to be accountable is because meetings do not happen in a vacuum or some academic bubble. Meetings happen to get work done. Meetings are about collaborating. Collaborating is about communicating. To communicate you need a baseline upon which that communication takes place. The very best foundation I\u2019ve found is to have a plan. A plan is four things: Together these represent a plan. That\u2019s it. Everything else are details that can be left to the people actually doing the work. The reason meetings can be effective when you have these is because of two factors. First, everything you are meeting about is relative to these factors and you\u2019re not revisiting them every time you talk. Everyone knows concretely why you are there. A plan is the actionable level of detail that explains what everyone is doing over the next month, quarter, years. Second, when something new comes along or a problem arises it will be relative to this foundation and so any changes are not one-off decisions but changes to a plan that can\u2019t be viewed in isolation. A change in goals, schedule, processes or values can only be made as a trade-off against what exists not as something to do in addition or in contradiction to the plan, but as a deliberate trade-off against a plan. Making decisions outside the context of a plan is a waste of time\u2026it is just talking. Establishing the above three are combinations of, yes, meetings and decisions by fiat. Let\u2019s go through them. I would add that there are two very concrete tactics about meetings that I have found to be the most important. The best meetings I remember are the ones where our team got a little closer and more connected and I remember that \u201cfeeling\u201d more than I remember the specifics of what we talked about. A team making, marketing, or selling a product is as much a team as any sports, military, or performing troupe. These types of teams \u201cmeet\u201d to practice and rehearse. In business too often we think meetings need to be new or decide, but in reality spending time together just makes a better team. The more you know about what and how people think, the more the micro choices you make day in and day out (without meetings) will likely be done in unison. High performance teams don\u2019t really need to meet, but every high-performance team I know seems to have a brain-wave connection across the team. That connection comes from talking, listening, talking, and listening. That can only happen in meetings. As a young programmer I too read The Fountainhead and know that the best work is done by people with principles. Freedom and creativity come from within, your own standards, depending on nothing and no one. \u201cI do not recognize anyone\u2019s right to one minute of my life. Nor to any part of my energy.\u201d And so on\u2026it seems when it comes to meetings people just want to be left alone. What I would say to Roark is that software and products at scale are not really done by one person, even if one person has a vision. To execute that vision requires a team. Teams require collaboration and execution. And the only way to do that is in meetings. I skipped a lot of meetings and at the same time was given a lot of grief for spending too much time meeting with the team. Everyone has their own style and every company its own culture, but to I wanted to share what I found most valuable in executing at scale: The contents or agenda for each of these was far less important than allocating the time to talk, to listen, to discuss. I always knew the best decisions were made simply because we had spent time together to get on the same page in broader goals, team values, and shared understanding of how work should be done. Author\u2019s note. This is a very long post. Why? I wrote this to be a one-stop-shop for meetings figuring the internet is filled with snappy short posts on meeting effectiveness. I thought having an archive of a few decades of meeting experience, dysfunction, and best practices would be a unique contribution. Please you are encouraged to take this in over time and to copy\/paste some tips. \ud83d\ude4f By clapping more or less, you can signal to us which stories really stand out. a16z \u2022 Box \u2022 Tanium \u2022 Product Hunt \u2022 Everlaw \u2022 \u0950 \u2022 \ud83d\ude4f products, development, management","time":1525809239,"title":"Reaching Peak Meeting Efficiency","type":"story","url":"https:\/\/medium.learningbyshipping.com\/reaching-peak-meeting-efficiency-f8e47c93317a","label":7,"label_name":"random"},{"by":"thg","descendants":0,"id":17024275,"kids":"None","score":1,"text":"A new paper by Donna Brazile, Joan Shorenstein Fellow (fall 2017) and former Democratic National Committee interim chair, examines whether political campaigns are up to the task of handling the threat of cyber attacks ahead of the 2018 midterm elections. Brazile provides an overview of hacking that took place during the 2016 election, and what elected officials have done since then to address the issue. She also analyzes a Shorenstein Center survey of Republican and Democratic campaign operatives at the state and congressional levels, and finds that the respondents were not well prepared for cyber attacks. Less than half of those surveyed said they had taken steps to make their data secure and most were unsure if they wanted to spend any money on this protection. \u00a0 \u00a0\u201cThere should be no doubt that Russia perceives its past efforts as successful and views the 2018 U.S. midterm elections as a potential target for Russian influence operations.\u201d  \u2014Dan Coats, Director of National Intelligence, February 13, 2018. The 2016 presidential election was unlike any other.\u00a0 The contest had the look of a circus, with each news cycle driven by outrageous claims and scandalous events rather than an examination of the issues facing the nation.\u00a0 As a result, some voters didn\u2019t know who to believe or trust and stayed home on Election Day.\u00a0 The result was an election where the candidate who won the highest office in the land did not win the popular vote. Since then we have come to know how much our fears and yearnings were manipulated by agents for the Russian government, and how few protections we have in place to prevent them from distorting our electoral process in the coming 2018 midterm elections and the 2020 presidential contest.\u00a0 The hacking of our electoral system poses a significant threat to our democracy by undermining faith in our public institutions such as the mainstream media, political parties, and statewide election systems and databases. There was a time when, if a foreign power interfered in an American election, both major political parties would spring into action to protect the integrity of our election. That is not happening, and a 2017 Shorenstein Center survey of campaign managers and campaign staff members revealed how unprepared our political candidates are for the digital threats they face at election time. The results show that while those surveyed are aware of the cyber threats, many of them do not take them seriously. The survey of nearly forty Republican and Democratic campaign operatives, administered through November and December 2017, revealed that American political campaign staff \u2014 primarily working at the state and congressional levels \u2014 are not only unprepared for possible cyber attacks, but remain generally unconcerned about the threat.\u00a0 The survey sample was relatively small, but nevertheless the survey provides a first look at how campaign managers and staff are responding to the threat. The overwhelming majority of those surveyed do not want to devote campaign resources to cybersecurity or to hire personnel to address cybersecurity issues.\u00a0 Even though campaign managers recognize there is a high probability that campaign and personal emails are at risk of being hacked, they are more concerned about fundraising and press coverage than they are about cybersecurity. Less than half of those surveyed said they had taken steps to make their data secure and most were unsure if they wanted to spend any money on this protection. Campaign officials should understand that their key assets in political campaigns, data and technologies, are at risk. Campaign staff, volunteers, and candidates should receive cyber education training. Our democracy is at risk. Cyber threats are constantly evolving, especially through the use of social media platforms. These \u201cactive measures,\u201d if not countered through training and a public education campaign, will erode confidence in the U.S. political system, destabilize campaigns, discredit candidates, and weaken both campaigns and election systems through deception, intraparty discord, and the spread of false information. On January 16, 2017, U.S. intelligence agencies concluded that \u201cRussian President Vladimir Putin ordered an influence campaign in 2016 aimed at the U.S Presidential election.\u201d The goal, said the agencies, was to \u201cundermine faith in the U.S. democratic process, denigrate Secretary Clinton, and harm her electability and potential presidency.\u201d Nearly a year later, on February 16, 2018, Special Counsel Robert Mueller, who has been investigating Russian interference in the 2016 election, filed an indictment against 13 Russian nationals and 3 Russian organizations, which sought a specific outcome to the 2016 presidential election. The 37-page indictment reads like a spy novel with Russians posing as Americans to scout out our politics, setting up fake identities and shell companies to pay for demonstrations and rallies, and creating websites and bots that promoted then-candidates Donald Trump, Bernie Sanders, and Jill Stein. The Democratic National Committee (DNC), along with the presidential campaign of Hillary Clinton (HFA), the Democratic Congressional Campaign Committee (DCCC), and private individuals \u2014 plus other entities with the responsibility for keeping statewide databases, voter registration, and electronic poll books \u2014 became the targets of a sophisticated cyber hacking operation that sought to sow discord, weaponize hacked emails, create chaos at the ballot box, and undermine faith in the integrity of the election and its outcome. As Americans prepare for the 2018 midterm election, where there are 435 seats in the U.S. House of Representatives, 34 Senate seats, and 36 gubernatorial seats up for grabs, neither President Donald Trump nor Congress has fully acknowledged the assault on American democracy nor taken credible steps to protect, educate, and prepare American voters on Russian threats to future elections. President Trump continues to label each investigation into the Russian meddling as a \u201cwitch hunt\u201d or \u201ca total hoax\u201d created by his opponents. He refuses to accept the findings of the intelligence agencies. Trump appears unbothered by the threat posed by Russia\u2019s attack on our democracy, even as we learn more about its efforts to divide our country and create chaos. In response to the Mueller indictment, Trump issued not a call to action, but a vindication of himself. He tweeted, \u201cRussia started their anti-US campaign in 2014, long before I announced that I would run for President. The results of the election were not impacted. The Trump campaign did nothing wrong \u2014 no collusion!\u201d Both houses of Congress have undertaken investigations into Russian interference in the 2016 presidential election.\u00a0 On April 27, 2018, the House Intelligence Committee concluded its yearlong investigation and issued a report that declared that the Trump campaign did not collude with Russia or aid in Russia\u2019s meddling in the presidential election. The Senate\u2019s investigation remains ongoing. \u00a0To date, legislation to address Russia\u2019s intrusions remain in their formative, initial stages.\u00a0 However, a bill imposing new sanctions on Russia for its behavior in the Ukraine and its meddling in the 2016 election passed the House 419-3 on July 25, 2017.\u00a0 Democrats have introduced the Election Security Act (on February 14, 2018 in the U.S. House) to help states restore the integrity and privacy of our elections. The law provides grants to states to enable them to update and secure their election infrastructure, but so far it has little to no support from Republicans. Meanwhile, several states are taking actions to upgrade their electronic voting systems, electronic poll books, and the like to protect against malicious hardware and software vulnerabilities. During the summer of 2017, the National Council of State Legislators (NCSL) convened a major conference to discuss steps to protect the electoral integrity of their systems. One of their recommendations is the creation of a Cybersecurity Task Force to ensure state lawmakers are acutely aware of the threat posed to future elections. NCSL is working closely with the Department of Homeland Security (DHS) to build cyber literacy among state officials and to train state election officials. With evidence continuing to mount that Russia did indeed attempt to influence the 2016 election, coordination continues to lag at both the federal and state levels where election administration is managed. During his testimony to Congress in June 2017, former FBI Director James Comey warned, \u201cit\u2019s not a Republican thing or a Democratic thing. It really is an American thing.\u201d Comey added, \u201cThey\u2019re going to come for whatever party they choose to try and work on behalf of, and they\u2019re not devoted to either, in my experience. They\u2019re just about their own advantage and they will be back.\u201d The United States is not alone in being under attack. The tactics used by the Russians in the United States were perfected after earlier efforts to meddle in the elections of other Western democracies. Within the European Union (EU), Russia has been accused of directly and indirectly engaging in a surreptitious, continent-wide effort to undermine pro-European groups, particularly in the Ukraine and France. Russia\u2019s state-owned media have constantly lauded the efforts of far right, anti-EU political parties \u00a0including those in Holland, France, Germany, and most recently in Austria. The Russian disinformation campaign continues to amplify stories harmful to political leaders and parties that are strongly in favor of the European Union, NATO, and other pro-Western groups. The U.S. has seen \u201cinitial signs\u201d of Russian \u201csubversion and disinformation and propaganda\u201d in the Mexican presidential campaign. In this context, then-National Security Adviser H.R. McMaster said: \u201c\u2026with Russia we are concerned, increasingly concerned, with these sophisticated campaigns of subversion and disinformation and propaganda, the use of cyber tools to do that.\u201d According to a clip of his speech obtained by Jos\u00e9 D\u00edaz Brise\u00f1o, Reforma\u2019s U.S. correspondent, McMaster said:\u00a0 \u201cAs you\u2019ve seen this is a really sophisticated effort to polarize democratic societies and pit communities within those societies against each other and create crises of confidence and to undermine the strength within Europe.\u00a0 You see this most recently with the Catalonia independence referendum in Spain, for example. You see actually initial signs of it in the Mexican presidential campaign already.\u201d Former President George W. Bush has repeatedly spoken out on the dangers of Russian interference. According to a USA Today\u00a0report, Bush stated, \u201cIt\u2019s problematic that a foreign nation is involved in our election system. Our democracy is only as good as people trust the results.\u201d In 2018, American voters will go back to the polls to elect 36 governors, 34 U.S. Senators and 435 members of the U.S. House of Representatives. The integrity of our electoral system remains vulnerable to attack. \u00a0The stakes are high. Jeh Johnson, former director of the Department of Homeland Security, stated that the U.S. was \u201con alert on Election Day and in the days leading up to it, along with the FBI.\u201d\u00a0 Johnson commented that \u201c\u202633 states and 36 cities and counties came to the Department before the 2016 election to seek their cybersecurity assistance.\u00a0 In working with those states, DHS helped to address a number of vulnerabilities in election infrastructure.\u201d He added, \u201cI\u2019m concerned that we are almost as vulnerable perhaps now as we were six, nine months ago.\u201d One way to prepare for the upcoming elections is to provide state and local election officials with resources to address threats to IT systems and voting technology.\u00a0 In 2017, the House Democrats launched an Election Security Task Force headed by Representatives Bennie Thompson (D-MS) and Robert Brady (D-PA). They issued a\u00a0report\u00a0that identified\u00a0ten specific recommendations on what the federal government and states can and should be doing to secure our nation\u2019s elections. The Secure Elections Act was introduced on December 21, 2017, in the U.S. Senate by a bipartisan group of lawmakers led by Senators James Lankford (R-OK), Amy Klobuchar (D-MN), Lindsey Graham (R-SC), Kamala Harris (D-CA), Susan Collins (R-ME) and Martin Heinrich (D-NM).\u00a0 The Secure Elections Act would mandate DHS to share more information with state and local election officials about threats to their IT systems or voting machines. The bill would also\u00a0set up an expert panel to draft voluntary risk management guidelines and best practices that state and local agencies can use. \u00a0Finally, it would authorize a $386 million grant program to help states implement these guidelines and replace outdated electronic voting machines. In addition, The Securing America\u2019s Voting Equipment (SAVE) Act was introduced on October 31, 2017, by Senators Susan Collins (R-ME) and Martin Heinrich (D-NM).\u00a0In a press release accompanying introduction of the legislation, the Senators noted that \u00a0\u201c[i]ntelligence assessments that Russian actors targeted state election voting centers and state-level voter registration databases as part of Russia\u2019s larger hostile effort to interfere in last year\u2019s election demonstrate a vulnerability to future cyber-attacks and manipulations by foreign hackers in our democratic process. The SAVE Act would facilitate information sharing with states, provide guidelines for how best to secure election systems, and allow states to access funds to develop their own solutions to the threats posed to elections.\u201d In January 2018, all Democratic members of the House Committee on Oversight and Government Reform sent a\u00a0letter\u00a0asking Chairman Trey Gowdy to issue a subpoena to finally compel the Department of Homeland Security (DHS) to produce documents it has been withholding from Congress for months related to Russian government-backed efforts to monitor, penetrate, or otherwise hack at least 21 state election systems in the 2016 election.\u00a0 DHS has failed to provide the requested information, but the agency has confirmed that Russia was behind the attacks It\u2019s clear from recent revelations that Russian meddling on social media platforms like Google, Facebook, and Twitter was extensive during the 2016 election.\u00a0 Whether it swayed the election is a hard question to answer, but what we do know is that millions \u2014 and potentially tens of millions \u2014 of American voters were exposed to content pushed by Russia in an election that was decided by just tens of thousands of votes. And it hasn\u2019t stopped, meaning its effect on 2018 and 2020 could be just as pernicious. Social media platforms have been slow to acknowledge the situation and, until recently, reluctant to do anything about it. In some cases, their policies and actions may have caused valuable information to be lost about the Russian attacks, which often aimed to divide America on race, guns, immigration, religion, and other issues. As detailed in Politico, Twitter was one of the most effectively exploited weapons by the Russian government to undermine Hillary Clinton\u2019s campaign and to help Donald Trump in the 2016 race. Kremlin-backed operatives used targeted ad buys, fake users, and automated bots to spread disinformation and false stories. Even so, Twitter\u2019s privacy policies for consumers may have resulted in the loss of tweets and data that would be invaluable to investigators trying to see how the Russian operation was carried out. Twitter failed to crack down on suspicious activity, and then allowed the data about that activity to be lost. Senator Mark Warner, the ranking Democrat on the Senate Intelligence Committee that is investigating Russian interference, said Twitter\u2019s response had been \u201cinadequate.\u201d According to Senator Warner, Twitter did only the bare minimum of investigating when it came to looking into the activity of the Russians on their platform. As reported in The New York Times, members of the Congressional Black Caucus are just as angry about Facebook\u2019s response. Caucus Chair Cedric L. Richmond (D-LA) stated that \u201c[the Caucus] needed Facebook to understand that they [Facebook] play a role in the perception of African-Americans.\u201d Russian-backed operatives made substantial ad buys on Facebook that were aimed at inflaming racial and political divisions at a time when members of the African-American community were trying to highlight problems of systemic racism in the nation\u2019s justice system. Meanwhile, the foreign actors were using targeted Facebook ads to make white voters hostile to African-Americans\u2019 message and to exploit racial tension and mistrust. The Senate Intelligence Committee summoned attorneys for Twitter, Facebook, and Google for a public hearing on November 1, 2017, to discuss how Russia may have used their sites to influence the 2016 election. \u00a0The issue became more pressing for legislators in March 2018 after it was revealed that the personal data of millions of Facebook users was improperly shared with the political data firm Cambridge Analytica, which, in turn, used that data to advance President Trump\u2019s campaign. Mark Zuckerberg, CEO and Chairman of Facebook, was called to testify before the Senate Committee on the Judiciary and the Senate Committee on Commerce, Science and Transportation. Zuckerberg appeared before the joint Senate Committee on April 10, 2018, and testified that Facebook \u201cdidn\u2019t do enough\u201d to prevent misuse of its platform during the 2016 election.\u00a0 Zuckerberg elaborated: \u201cThat goes for fake news, foreign interference in elections, and hate speech, as well as developers and data privacy. We didn\u2019t take a broad enough view of our responsibility, and that was a big mistake. It was my mistake, and I\u2019m sorry. I started Facebook, I run it, and I\u2019m responsible for what happens here.\u201d \u00a0 Zuckerberg further testified that Facebook \u201cshould have spotted Russian interference earlier,\u201d and that it is taking steps to prevent future interference by hostile foreign actors. Zuckerberg\u2019s contrition and the fact that Facebook appears serious about making changes to its security and advertising policies are critical first steps to preventing misuse of the social media site in future U.S. elections. Prior to Zuckerberg\u2019s hearing, Senators Warner (D-VA), McCain (R-AZ), and Klobuchar (D-MN) introduced the Honest Ads Act that aims to \u201cprevent foreign interference in future elections and improve the transparency of online political advertisements.\u201d\u00a0 Days before his Senate hearing, Zuckberberg endorsed the Act and Twitter soon followed. This legislation is important for the preservation of our democracy because it will inhibit foreign powers from polluting the internet with fabricated stories and disinformation. Faced with the president\u2019s denial and the sluggish response by the tech community, it\u2019s more vital than ever that Congress step into the void to raise the alarm and to apply pressure to social media companies through the current Russia investigations and possible legislative remedies. Over the past year, the world has become increasingly aware of the importance of cybersecurity for political systems and governments. From Russian infiltration of the 2016 presidential elections in the U.S., to emails from top political leaders leaked to the public, political operatives must be better prepared to protect the integrity of online data and information After the hacking of the 2016 election, it\u2019s not surprising that nearly all the campaign staff surveyed (92%) were familiar with news about cyber meddling in political campaigns and party offices during the 2016 election cycle.\u00a0 Nearly two-thirds are aware of the practice of fraudulent emails sent under the guise of a trusted colleague to gather confidential information from targeted individuals (known as spear phishing). Perhaps the most famous example of spear phishing was the hacking of Hillary Clinton campaign chairman John Podesta\u2019s emails. Figure 1. Familiarity with 2016 Campaign Cyber Meddling  \u00a0 Figure 2. Aware of Spear Phishing?  \u00a0The concern about hacking and interference is quite high among the campaign staffers surveyed. Indeed, 81 percent said that they are either \u201csomewhat\u201d or \u201cvery concerned\u201d about stories reporting the hacking of campaigns and party offices. Fewer than 1 in 5 (18%) reported they are not concerned. Figure 3. Concerns: Hacking and Interference Note: Numbers may not add up to 100% due to rounding. Nevertheless, these same campaign staffers are much less concerned about foreign actors meddling in their own campaigns. Two-thirds (65%) reported they are not \u201cvery concerned\u201d or \u201cnot concerned at all\u201d about foreign threats to campaign cybersecurity. When asked about concerns and obstacles on the campaign trail, the hacking of campaign files and emails ranked second to last, only above the risk of not receiving key endorsements. Campaign staffers are mostly concerned about losing the election or not raising enough money. Other areas of concern included an inability to hire enough qualified staffers, the candidate making a gaffe, and receiving bad press.\u00a0 High-level staffers generally ranked these issues as a much bigger concern to their campaigns than cyber interference. Figure 4. Campaign Concerns Ranked on a 0 to 10 scale. These same campaign staffers are generally confident in the security of their campaign files and databases (voter files, donor records, etc.). Half of respondents reported it would be \u201csomewhat\u201d (37%) or \u201cvery\u201d (14%) difficult for someone to hack their campaign files, and even more believed it would be \u201csomewhat\u201d (43%) or \u201cvery\u201d (20%) difficult for someone to hack their campaign databases. However, the campaign operatives\u2019 perspectives shifted when asked about email; half of the campaign operatives studied think it would be \u201csomewhat\u201d (46%) or \u201cvery\u201d (9%) easy for someone to hack their campaign emails. Figure 5. Difficulty to Hack Note: Numbers may not add up to 100% due to rounding. When it comes to their preparedness compared to other campaigns, staffers generally feel they are about as well-equipped as most campaigns (64%), while only 19 percent think that their current campaign is better equipped than other campaigns. Eleven percent of those surveyed believe they are less equipped compared to other campaigns and 6 percent are unsure. Figure 6. Campaign Preparedness Compared to Others  The lack of seriousness with which these staffers regard campaigns\u2019 cyber defense becomes clearer as campaigns report their willingness to spend money on added protections. Nearly half said their campaigns are willing to spend less than $5,000 on added protections against hacking, and 34 percent were not certain how much they would be willing to spend. Figure 7. Money Willing to Spend on Cyber Protections  When asked whether their campaign has taken steps to prevent their network and emails from being hacked, nearly half (40%) shared they \u201chave taken steps to protect their campaigns,\u201d while 31 percent say they have \u201cnot yet but plan on doing so in the future.\u201d Just 11 percent say they \u201cdo not plan\u201d on taking these steps. Figure 8. Campaign Taking Steps to Prevent Hacking  A majority of staffers surveyed have taken concrete steps toward more effective campaign security. The most common best practices include requiring passwords for staff, installing antivirus software on all campaign devices, setting up firewalls, requiring two-step verification, requiring password standards for consultants and volunteers, and not allowing campaign staff, volunteers, or consultants to use thumb drives \u2014 especially important since nearly half of campaign staffers (46%) believe that \u201cat least\u201d some volunteers on their campaigns have access to data. Figure 9. Campaign Volunteers\u2019 Access to Data  Despite this, staffers are still relatively ill-prepared. The campaign operatives surveyed have not generally set up firewalls, required passwords for staff, or hired a designated staff person for information or data security. Just over two-thirds (69%) have neither hired a Chief Information Security Officer, nor an equivalent staffer. The same amount do not actively use tabletop exercises (a simulated emergency situation) to train their IT staff how to assess and respond to network vulnerabilities. Figure 10. Protections against Hacking Note: Numbers may not add up to 100% due to rounding. While Congress completes its investigations into the 2016 presidential election, it is vital that we prepare state election officials, candidates, and their campaigns to protect themselves from cyber threats. The White House recently announced that President Trump convened a meeting with top intelligence and legal officials to discuss the administration\u2019s plans to secure state and local election systems and to protect them from malign foreign influence.\u00a0 The administration\u2019s acknowledgment that election security is directly related to our national security is a critical first step in securing our elections and, more importantly, it may raise cybersecurity awareness among state and local campaigns. The Shorenstein Center survey shows that while campaign staffs are well aware of the threat posed by cyber attacks, most are more concerned with other things, such as raising enough money or receiving bad press.\u00a0 They fail to recognize, as the 2016 election revealed, that a cyber attack that exposes campaign data can depress fundraising, inhibit endorsements, and create weeks of bad press. Despite this, few campaigns have invested in cybersecurity efforts like hiring a chief information security officer or using tabletop exercises to train staff. The vital tools of modern campaigns \u2014 internet-enabled devices, software, and connectivity \u2014\u00a0need as much protection as the voting box, yet staffs and party leaders show that digital protection is not a high priority. The threat of cyber attacks cannot be eliminated. Nevertheless, campaigns must practice vigilance and view IT and cybersecurity across the campaign organization as a necessity. Campaigns will need to adopt the necessary tools to protect against these threats and expand their budgets and staff. If not, campaigns and elections will continue to be highly vulnerable to hacking and interference. For the second time in my political career, I was honored to return to Harvard\u2019s Kennedy School of Government.\u00a0 As I often tell my friends, every time the winner of the Electoral College loses the popular vote, I get to go to Harvard for a semester. Like 2000, the 2016 presidential election was one of the most disruptive campaigns in recent memory. Special thanks to my colleagues at the Shorenstein Center on Media, Politics and Public Policy. Nicco Mele, Director of the Shorenstein Center, along with Executive Director Nancy Palmer, Communications Director Nilagia McCoy, and Professor Thomas Patterson gave insights and recommendations to look beyond the 2016 presidential election and to focus on how cyber security should be a major focus of political campaigns in the future. I also had the assistance of Michael Auslen who helped me monitor and track congressional and statewide legislation to restore the integrity to our electoral process, along with Matthew Spector who helped draft and analyze the campaign survey. Thank you to my bipartisan team of advisors who not only helped draft and review the final survey and results, but also helped to disseminate the survey to campaign officials.\u00a0 They include Stefan Hankin and Anne Hazlett of Lincoln Park Strategies, former Virginia gubernatorial candidate Ed Gillespie, the Democratic Governor\u2019s Association, the National Association of State Legislators, Celinda Lake of Lake Research Partners, Whit Ayres of North Star Opinion Research, and Neil Newhouse of Public Opinion Strategies. Finally, I must thank the graduate students at the Kennedy School for checking in and spending time reminiscing about the election, the undergraduate students, the Institute of Politics, The Belfer Center for Science and International Affairs, and the faculty at the Kennedy School. Thank you very much for taking the time to complete this survey, which covers a handful of different issues regarding the campaign you work on. Your opinions are incredibly valuable to this study and we really appreciate your time and attention. We are planning to make the results of this survey public, but your identity and individual responses will be kept confidential and will only be shared in the aggregate. 1. Starting out, we want to make sure we\u2019re talking to a lot of different kinds of people. What is your position in the campaign? 2. And to whom do you directly report? 3. Is this your first political campaign, or have you worked on previous campaigns? 4. In which state is your campaign? 5. For what elected office is the candidate running? 6. What is the candidate\u2019s political party? 7. Thank you. Now, how concerned are you that each of the following may occur during your campaign? Please rate each of the following on a scale of 0 to 10, with 10 representing extreme concern and 0 representing no concern. You may use any number between 0 and 10. 8. How familiar are you with the recent news focusing on hacking into political campaigns and party offices during the 2016 election cycle? 9. And on a scale from 0 to 10 where a ten means very concerned and a zero means not concerned at all, how concerned are you about these stories about hacking of campaigns and party offices? You can use any number from 0 to 10. 10. How easy do you think it would be for someone to hack into your campaign emails and\/or senior campaign officials\u2019 personal email accounts? 11. How easy do you think it would be for someone to hack your campaign files? 12. How easy do you think it would be for someone to hack your campaign databases (e.g. voter files, donor records, etc.)? 13. And do you feel your campaign is better equipped to deal with hacking than most campaigns, about the same as most campaigns, or less equipped than most campaigns? 14. Compared to previous campaigns you have worked on, do you believe this campaign is better prepared, not as well prepared, or about as well prepared to deal with hacking? 15. How much money has your campaign raised? 16. How much is your campaign currently spending on cybersecurity protections? 17. How much is your campaign willing to spend for additional protections against hacking? 18. In general, how concerned are you about foreign actors meddling in your current race by hacking? 19. On a scale from 0 to 10 where a ten means very concerned and a zero means not concerned at all, please rate how concerned you would be about the following possible outcomes if your campaign were hacked. You can use any number from 0 to 10. 20. Are there any other concerns you would have if your campaign were hacked? 21. Has your campaign taken steps to prevent your network and emails from being hacked? 22. If answer is no, not planning on it: What is the main reason your campaign is not planning on taking steps to prevent your network and emails from being hacked? 23. If answer is yes or not yet: What steps has your campaign taken to prevent your campaign from being hacked? Please mark all that apply. 24. What other steps has your campaign taken to prevent being hacked? 25. And what is the position\/title of the designated staff person the campaign hired for information or data security? 26. Does the campaign currently allow volunteers to have access to the campaign\u2019s network? 27. What kind of log in credentials are required to access your network system? 28. Has your campaign hired a Chief Information Security Officer or equivalent role? 29. Are you actively using tabletop exercises (a simulated emergency situation) to train your IT staff and determine network vulnerabilities? 30. Are you aware of what spear phishing is? 31. If you suspected that your data was breached, what actions would you take? Please select all that apply. 32. Do you have any other thoughts on this topic that you would like to share that have not been discussed in this survey? \u00a0 Download as a PDF Copyright \u00a9 2018 Shorenstein Center","time":1525809235,"title":"Improving Cyber Literacy in Political Campaigns","type":"story","url":"https:\/\/shorensteincenter.org\/campaign-2018-cyber-literacy\/","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17024274,"kids":"None","score":1,"text":"The Future Private space companies no longer have to follow the law The Space Commerce Free Enterprise Bill says private companies don't have to abide by a foundational half-century-old space treaty. Caroline Haskins It just got a whole lot easier for private companies to launch satellites, rovers, and spacecrafts, and pursue future industries like asteroid mining. The catch? The U.S. is completely ignoring what\u2019s outlined in a 51-year-old treaty designed to keep space peaceful and war-free. The Space Commerce Free Enterprise Bill, which passed the House of Representatives yesterday, works off the Outer Space Treaty, which the United States and dozens of other countries signed in 1967 and serves as a basic framework for keeping space safe and accessible for every country. Countries can\u2019t own property on behalf of their own nation, and they\u2019re liable for any private activity from their country.  But the U.S.\u2019s new bill won\u2019t apply every part of the Outer Space Treaty to private companies. In other words, the U.S. doesn\u2019t believe that it\u2019s liable for activities of private space companies like SpaceX or Blue Origin. The bill also bundles almost all space mission approvals under one roof, the Office of Space Commerce, to try and encourage as many companies as possible to launch objects into space. The office would be in charge of everything from a theoretical asteroid mining industry to private space stations, which have been proposed as tourist attractions by companies like Blue Origin.\nSo it\u2019s likely that other countries, um, won\u2019t exactly be thrilled about the U.S. disregarding the first major peacemaking treaty for activity in outer space. According to an email to The Outline, Mike Listner, the founder of the private space policy consulting firm Space Law & Policy Solutions, other countries may also be tempted to have a similar disregard for the rules. \u201cThe method used by the bill to permit private space activities could create some unfavorable interpretation of international law\u2014and set a bad example for other nations who are enacting private space activities,\u201d Listner said. It\u2019s also not clear that the Office of Commercial Space would have strict guidelines in place for enforcing the Outer Space Treaty for private companies. The treaty also states that countries can\u2019t launch or test \u201cnuclear weapons\u201d or \u201cweapons of mass destruction.\u201d Companies only need to say they don\u2019t plan on bringing or using a nuclear weapon or weapon of mass destruction in space, and there are no guidelines in place for evaluating these claims. Military companies like Boeing are already looking to expand into space, and Trump has expressed interest in a \u201cSpace Force.\u201d It seems less likely than ever that the U.S. respects the idea of space as a war-free commons. \u201cThe main criticism I have of the Bill is that [its regulation] is about as \u2018light touch\u2019 as you could possibly get, almost to the point of being \u2018no touch,\u2019\u201d Brian Weeden, the Director of Program Planning for Secure World Foundation, told The Outline in an email. Weeden said that the State Department should probably be assessing whether a company really has peaceful intentions or not. Instead, the responsibility falls under the Office of Space Commerce, which is under the Department of Commerce\u2014a government agency with a reputation for having a lax stance toward regulation. But Weeden said that the Office of Commercial Space is incredibly small: just 8 people work there. And although the Act proposes a big funding increase\u2014from $2 million annually to $5 million\u2014it\u2019s unclear if the office will have the resources to keep up with the influx of applications that the Trump administration is explicitly encouraging. \u201c[The bill] doesn't really address the resources that will be necessary for Commerce to properly do this new job,\u201d Weeden said. \nScreengrab of a mockup of the Axiom Commercial Space Station.            \n                Axiom Space Still, private companies will probably love this bill. Weeden said that placing most approvals under one roof will make it easier for these companies to figure out how to get their missions approved. And theoretically, the success of private space companies could help the U.S. economy. According to Brendan Cunningham, an assistant professor of economics for Eastern Connecticut State University who has written about commercial space, it\u2019s also important to consider that in order for the U.S. economy to actually benefit from commercial space activity, we\u2019d have to use space efficiently. But Cunningham said in an email that the bill fails to consider efficiency at all. \u201cCommons resources are susceptible to overuse and degradation\u2014one example is overfishing,\u201d Cunningham said in an email to The Outline. \u201cHazardous debris environment and the risk of [space trash collisions] indicate that space is succumbing to this pattern.\u201d It\u2019s not exactly surprising that the U.S. is moving toward deregulating outer space\u2014a de facto arena for soft nationalistic power. Space offers a way to acquire information (like weather, GPS, or national security data) or practice ownership over some small slice of valuable space real estate. Basically, whether it\u2019s military satellites or private space tourism, anything that the U.S. launches into space has value, and the country has made it clear that these corporate interests take priority over the idea that outer space should serve as a commons for all of humanity. And the U.S. is far from alone in this incentive. Australia just created its first space agency, whose explicit goal is to promote private companies. The UK is investing tremendous resources toward growing its domestic space program since the Brexit vote (with limited success). France, Japan, Russia, and China also want in.","time":1525809225,"title":"Private space companies no longer have to follow the law","type":"story","url":"https:\/\/theoutline.com\/post\/4469\/outer-space-treaty-commerce-free-enterprise-bill-spacex-blue-origin-boeing-lockheed-martin","label":7,"label_name":"random"},{"by":"alanfranzoni","descendants":5,"id":17024245,"kids":"[17024656, 17024571, 17024479]","score":24,"text":"\n\n\n\n Android Ten years ago, when we launched the first Android phone\u2014the T-Mobile G1\u2014it was with a simple but bold idea: to build a mobile platform that\u2019s free and open to everyone. Today, that idea is thriving\u2014billions of people around the world rely on their Android phone every day.  To make Android smarter and easier to use than ever, today we\u2019re unveiling a beta version of Android P, the next release of Android. Android P makes your smartphone smarter, helping it learn from and adapt to you. Take battery life, for instance\u2014I think all of us often wish we had more of it. In Android P, we partnered with DeepMind to build Adaptive Battery, which prioritizes battery power only for the apps and services you use the most, to help you squeeze the most out of your battery. We also used machine learning to create Adaptive Brightness, which learns how you like to set the brightness slider given your surroundings. Across the platform, your phone will help you better navigate your day, using context to give you smart suggestions based on what you like to do the most and automatically anticipating your next action. App Actions, for instance, help you get to your next task more quickly by predicting what you want to do next. Say you connect your headphones to your device, Android will surface an action to resume your favorite Spotify playlist. Actions show up throughout Android in places like the Launcher, Smart Text Selection, the Play Store, the Google Search app and the Assistant.  Adaptive Battery App Actions Slices We want the entire device experience to be smarter, not just the OS, so we\u2019re bringing the power of Google\u2019s machine learning to app developers with the launch of ML Kit, a new set of cross-platform APIs available through Firebase. ML Kit offers developers on-device APIs for text recognition, face detection, image labeling and more. So mobile developers building apps like Lose It!, a nutrition tracker, can easily deploy our text recognition model to scan nutritional information and ML Kit\u2019s custom model APIs to automatically classify over 200 different foods with your phone\u2019s camera.  With Android P, we put a special emphasis on simplicity. The look and feel of Android is more approachable with a brand new system navigation. In Android P, we\u2019re extending gestures to enable navigation right from your homescreen. This is especially helpful as phones grow taller and it\u2019s more difficult to get things done on your phone with one hand. With a single, clean home button, you can swipe up to see a newly designed Overview, the spot where at a glance you have full-screen previews of your recently used apps. Simply tap to jump back into one of them. If you find yourself constantly switching between apps, we\u2019ve got good news for you: Smart Text Selection (which recognizes the meaning of the text you\u2019re selecting and suggests relevant actions) now works in Overview, making it easier to perform the action you want. Changing how you navigate your phone is a big deal, but small changes can make a big difference too. Android P also brings a redesigned Quick Settings, a better way to take and edit screenshots (say goodbye to the vulcan grip that was required before), simplified volume controls, an easier way to manage notifications and more. You\u2019ll notice small changes like these across the platform, to help make the things you do all the time easier than ever. \u00a0 Dashboard, App Timer, Wind Down Beyond smarts, simplicity and digital wellbeing, there are hundreds of additional improvements coming in Android P, including security and privacy improvements such as DNS over TLS, encrypted backups, Protected Confirmations and more.\u00a0\u00a0 Android P Beta is available today on Google Pixel. And thanks to work on Project Treble, an effort we introduced last year to make OS upgrades easier for partners, a number of our partners are making Android P Beta available today on their own devices, including Sony Xperia XZ2, Xiaomi Mi Mix 2S, Nokia 7 Plus, Oppo R15 Pro, Vivo X21, OnePlus 6, and Essential PH\u20111.\u00a0\u00a0 Since we first launched Android ten years ago, we\u2019ve been focused on how to build a platform for everyone. Android P is an important step toward bringing machine learning to everyone with an operating system that learns from you, adapts to you and helps you with everyday tasks. \n              Follow Us\n            ","time":1525809091,"title":"Android P: Packed with smarts and simpler than ever","type":"story","url":"https:\/\/www.blog.google\/products\/android\/android-p\/","label":9,"label_name":"tech"},{"by":"Tomte","descendants":0,"id":17024244,"kids":"None","score":1,"text":"When Nintendo first released Animal Crossing: Pocket Camp on mobile last November, I was into it. The game felt somewhat slight by Animal Crossing standards, however. It lacked the open-ended nature of past games, and it didn\u2019t really offer a whole lot to do. But there\u2019s something about the laid-back, leisurely nature of the series that makes it a great fit for smartphones. It\u2019s a mini escape that you can pull out whenever you need it. For me, these kinds of mobile games have historically turned into a short-lived habit, something I play daily for a few weeks before burning out. But that hasn\u2019t happened with Pocket Camp. Instead, the game has continued to grow and improve. Months later, I still can\u2019t put it down. The core of the game was always solid and enjoyable. Pocket Camp gives you a tiny slice of campground to decorate however you want, and it surrounds you with animals that you can befriend and run errands for. You can progress at your own pace, and the game has a charm that makes it incredibly inviting. It\u2019s very relaxing to just spend five minutes fishing or watering plants. Over the last few months, Nintendo has built on that foundation with new features and content. There have been some huge (at least by Animal Crossing standards) changes. You can now craft your own clothes, cultivate a garden, and swap furniture with a confused seagull. These are some of my favorite parts of the game, and none were available at launch. There have also been multiple quality-of-life improvements \u2014 die-hard fans of the game got really excited when you could place two carpets on your campground \u2014 as well as the expected stream of new characters, furniture, and clothing. \n    Related\n   The feature that has me coming back the most, though, is the limited time events. My favorite so far was a fishing tournament, where you could (finally!) get fish tanks in which to display some of your catches. But there have also been gardening contests, where you need to plant certain flowers to attract special bugs, and themed events where you can unlock rare items. Recently, two of these events were going on simultaneously: I found myself playing multiple times a day so that I could gather as much Super Mario and Alice in Wonderland themed items as possible. It's Rosie's pop-star stage\u2014talk about exciting! Fans of Rosie, Francine, and Chrissy shouldn't miss the Scrapbook memory featuring these stars! #PocketCamp pic.twitter.com\/cq7vWAdelg These kinds of updates aren\u2019t unique to Animal Crossing, of course. Mobile games and other service-oriented experiences like Fortnite are constantly updated in order to keep people coming back regularly. What makes Pocket Camp stand out is both the frequency and quality of its updates. There seems to be a new event every other week, and for the most part, I\u2019ve enjoyed every one of them. Likewise, even though I play regularly, it feels like there\u2019s always something next for me to build toward. I only just installed a pool at my campground, after months of work, and it\u2019s probably going to be a while before I manage to build a big shrine to lure longtime Animal Crossing character Red to my place. Pocket Camp still isn\u2019t a full-fledged Animal Crossing game. It\u2019s a streamlined take on the franchise, built for quick bouts of play on a phone. But with each passing day, it becomes closer to that ideal. With these kinds of daily games, I often find myself playing more out of habit than enjoyment; it\u2019s a thing I do, rather than a thing I love. That hasn\u2019t been the case with Pocket Camp. Instead, I look forward to checking it out each day. As I write this, there\u2019s a new fortune cookie shop coming to the game, where eating a cookie can somehow unlock items like a rocket ship or a pop star stage. I\u2019m not even sure where I\u2019ll put a rocket ship \u2014 but I\u2019m going to keep playing to find out. Command Line delivers daily updates from the near-future.","time":1525809089,"title":"Nintendo\u2019s mobile Animal Crossing keeps getting better","type":"story","url":"https:\/\/www.theverge.com\/2018\/4\/15\/17234112\/nintendo-animal-crossing-pocket-camp-updates","label":7,"label_name":"random"},{"by":"Tomte","descendants":0,"id":17024238,"kids":"None","score":1,"text":"Set edition preference: Set edition preference: Set edition preference: By Matias Grez, CNN Updated 1159 GMT (1959 HKT) November 6, 2017 Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds.  (CNN)Bibiana Steinhaus had pictured the moment many times before.       ","time":1525809074,"title":"Bibiana Steinhaus: The Bundesliga referee shattering football's glass ceiling","type":"story","url":"http:\/\/edition.cnn.com\/2017\/11\/06\/football\/bibiana-steinhaus-first-female-referee-bundesliga\/index.html","label":6,"label_name":"news"},{"by":"uptown","descendants":1,"id":17024237,"kids":"[17024261]","score":2,"text":"Facebook instituted its biggest executive shakeup in its 15-year history this week, appointing new leaders for WhatsApp, Messenger and Facebook\u2019s core app while giving other longtime Facebook executives new responsibilities, including a new effort to tackle blockchain technology. The moves, which were announced internally to employees today, are meant to improve executive communication and user privacy, but the changes also come as Facebook contends with the backlash from the U.S. presidential election, revelations of manipulation by the Russian government and the recent Cambridge Analytica scandal. \n    Related\n   CEO Mark Zuckerberg has reorganized the social giant\u2019s product and engineering organizations into three main divisions, including a new \u201cFamily of apps\u201d group run by Chief Product Officer Chris Cox, the executive previously in charge of the core Facebook app. Cox will now oversee Facebook, Instagram, WhatsApp and Messenger, according to multiple sources, four social apps with a combined reach of more than five billion monthly users.  Facebook is also building a new team dedicated to blockchain technology. David Marcus, the executive in charge of Facebook\u2019s standalone messaging app, Messenger, is leaving that post to run the blockchain group, these sources said. That new team will fall under one of the other three divisions, referred to as \u201cNew platforms and infra,\u201d which will be managed by CTO Mike Schroepfer. Facebook\u2019s AR, VR and artificial intelligence efforts will also live under Schroepfer\u2019s division.  Longtime Facebook exec Javier Olivan, the company\u2019s VP of growth, will oversee the third division, called \u201cCentral product services,\u201d which includes all of the shared features that operate across multiple products or apps such as ads, security and growth.  Surprisingly, no one appears to be leaving Facebook. Just a lot of old faces in new places.  You may have noticed from the diagram that almost all of Facebook\u2019s top product and engineering execs are men. That\u2019s true, though Facebook does have a number of high ranking and influential female product executives that aren\u2019t directly involved in these changes. For example: Fidji Simo, who runs video; Deb Liu, who runs Marketplace; and Julie Zhuo, who runs design. Then, of course, there\u2019s Sheryl Sandberg on the business side of things.  The changes all come at an interesting time for Facebook and Zuckerberg, who has been openly discussing his need to take more responsibility for Facebook\u2019s impact on the world.  Zuckerberg\u2019s New Year\u2019s resolution was to fix Facebook, and restructuring the team is clearly part of that fix. The hope is that these new roles will keep more open lines of communication among executives without hurting the speed Facebook is known for. (\u201cMove fast and break things,\u201d remember?) The new product and engineering orgs have been divided into three key groups.  This is the group Cox will oversee, which includes WhatsApp, Messenger, Instagram and the core Facebook app. Instagram CEO Kevin Systrom will continue to run Instagram, but the other three apps are getting new leaders: Having all four product leaders roll up to Cox is meant to improve communication among the products. Previously, the leaders of all of these teams had different bosses. Koum rolled up to Zuckerberg, Systrom rolled up to Schroepfer and Marcus reported to Olivan. As Facebook increasingly builds more features that live inside all of the apps (e.g. Stories), it makes sense to have their leaders working closer together.  This group will be under the direction of Mike Schroepfer, Facebook\u2019s CTO. As you can probably guess from the name, this group will incorporate all of Facebook\u2019s longterm product and business efforts, like virtual reality, augmented reality and the newly formed blockchain group.  All of the other product and engineering functions \u2014 ads, security, growth \u2014 will fall to another longtime Facebooker, Javier Olivan, who has been with the company more than a decade. Olivan has run Facebook\u2019s growth team for years and is credited for helping Facebook achieve the massive scale it\u2019s now known for. Olivan will also oversee a lot of important parts of the Facebook business.  Here are a few more changes taking place at Facebook this week.  Adam Mosseri, the Facebook product executive who runs News Feed, is headed over to Instagram to become the company\u2019s new VP of product. In his old job, Mosseri was tasked with building and then explaining Facebook\u2019s ever-changing News Feed algorithm to journalists and media companies, and he became a savvy Twitter user in the process. The company didn\u2019t share specifics about his new role, but it seems fair to assume his experience running one feed at Facebook will help running another feed at Instagram.  You might now be thinking: What about Instagram\u2019s existing VP of product, Kevin Weil? Good question. Weil is leaving Instagram and headed over to Facebook\u2019s newly formed blockchain team, the one that David Marcus is running. So a few new places for a couple of familiar faces.  Facebook is shuffling the top of its communication team, too. Caryn Marooney, who has been running the day-to-day operations for all Facebook communications over the past two years, is handing over some of her responsibilities to PR veteran Rachel Whetstone, who joined Facebook last summer. Marooney will handle product communications and Whetstone is taking over corporate comms.  Whetstone is very well known in Silicon Valley. She joined Facebook after running communications at Uber during the company\u2019s Travis Kalanick-inflicted PR nightmare. Before that, she worked at Google for 10 years running communication around public policy. She\u2019s climbed up Facebook\u2019s internal ranks very quickly and is already an influential voice in the room when it comes to policy decisions, sources say.  Sign up for our Recode Daily newsletter to get the top tech and business news stories delivered to your inbox. Sign up for our Recode Daily newsletter to get the top tech and business news stories delivered to your inbox. The group\u2019s first product: \"Clear History,\" a newly announced feature so people can opt out of Facebook using their browsing history. Facebook, WhatsApp, Instagram and Messenger are now all rolling up to Cox. What is the blockchain? Facebook wants to find out, too. Jan Koum left WhatsApp last week. Now we know who\u2019s taking over. A Verge affiliate site","time":1525809066,"title":"Facebook is making its biggest executive shuffle in company history","type":"story","url":"https:\/\/www.recode.net\/2018\/5\/8\/17330226\/facebook-reorg-mark-zuckerberg-whatsapp-messenger-ceo-blockchain","label":7,"label_name":"random"},{"by":"pedrorijo91","descendants":0,"id":17024234,"kids":"None","score":1,"text":"\nGet Started\n vmlens enables you to test multithreaded java \n           vmlens analyses the execution trace of your test run. \nVmlens detects all java race conditions \nwithout false positives.All classes are analyzed. Directly analyze and fix all detected race conditions inside eclipse.\n\n\t    \n           vmlens analyses the order in which monitors are acquired by your application. vmlens detects all potential deadlocks\n       without false positives. Directly analyze and fix all detected deadlocks inside eclipse.\n\t    GET STARTED Use vmlens to detect race conditions and deadlocks during your manual tests Use vmlens to systematically test your multithreaded software Simply add vmlens to the VM arguments of your application Directly analyze and fix all detected bugs inside eclipse \u00a9 2018 vmlens","time":1525809034,"title":"Has anyone tried vmlens (java concurrency bugs detector)?","type":"story","url":"http:\/\/vmlens.com\/","label":7,"label_name":"random"},{"by":"zippoxer","descendants":0,"id":17024203,"kids":"None","score":2,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Bow is a minimal embedded database powered by Badger. The mission of Bow is to provide a simple, fast and reliable way to persist structured data for projects that don't need an external database server such as PostgreSQL or MongoDB. Bow is powered by BadgerDB, implementing buckets and serialization on top of it. Badger is more actively maintained than bbolt, allows for key-only iteration and has some very interesting performance characteristics. With options: Each record in the database has an explicit or implicit unique key. If a structure doesn't define a key or has a zero-value key, Bow stores it with a randomly generated key. Structures must define a key if they wish to manipulate it. Keys must be a string, a byte slice, any built-in integer or a type that implements Marshaler and Unmarshaler. Id is a convenient placeholder for Bow's randomly generated keys. Id.String() returns a user-friendly representation of Id. ParseId(string) parses the user-friendly representation into an Id. NewId() generates a random Id. Only necessary when you need to know the inserted Id. Put persists a structure into the bucket. If a record with the same key already exists, then it will be updated. Get retrieves a structure by key from a bucket, returning ErrNotFound if it doesn't exist. Iterate over records whose key starts with a given prefix. For example, let's define Page with URL as the key: Finally, let's iterate over HTTPS pages: By default, Bow serializes structures with encoding\/json. You can change that behaviour by passing a type that implements codec.Codec via the bow.SetCodec option. msgp is a code generation tool and serialization library for MessagePack, and it's nearly as fast as protocol buffers and about an order of magnitude faster than encoding\/json (see benchmarks). Bow provides a codec.Codec implementation for msgp under the codec\/msgp package. Here's how to use it: Replace any use of bow.Id in your structures with msgp.Id. Import github.com\/zippoxer\/bow\/codec\/msgp and open a database with msgp.Codec: Read more about msgp and it's code generation settings at https:\/\/github.com\/tinylib\/msgp Since Badger separates keys from values, key-only iteration should be orders of magnitude faster, at least in some cases, than it's equivalent with Bolt. Bow's key-only iterator is a work in progress. Cross-bucket transactions are a work in progress. See branch tx. Bow doesn't feature a querying mechanism yet. Instead, you must iterate records to query or filter them. For example, let's say I want to perform the equivalent of in Bow, I could iterate the pages bucket and filter pages with URLs containing '\/home': Meanwhile, you can try Storm if you want convenient querying. Bow is nearly as fast as Badger, and in most cases faster than Storm. See Go Database Benchmarks. I welcome any feedback and contribution. My priorities right now are tests, documentation and polish. Bow lacks decent tests and documentation for types, functions and methods. Most of Bow's code was written before I had any plan to release it, and I think it needs polish.","time":1525808883,"title":"Bow \u2013 Minimal embedded database for Go powered by Badger","type":"story","url":"https:\/\/github.com\/zippoxer\/bow","label":4,"label_name":"github"},{"by":"blakehaggerty","descendants":0,"id":17024181,"kids":"None","score":2,"text":"In the last twenty years, the internet applications that improve our lives and drive our economy have become far more powerful. As a necessary side-effect, these applications have become far more complex, and that makes it much harder for us to measure and explain their performance\u2014especially in real-time. Despite that, the way that we both reason about and actually measure performance has barely changed. I\u2019m not here to argue about the importance of understanding real-time performance in the face of rising complexity\u200a\u2014\u200aby now, we all realize it\u2019s vital\u200a\u2014\u200abut for the need to improve our mental model as we recognize and diagnose anomalies. When assessing \u201cright now,\u201d our industry relies almost entirely on averages and percentile estimates: these are not enough to efficiently diagnose performance problems in modern systems. Performance is a shape, not a number, and effective tools and workflows should present and explore that shape, as we illustrate below. We\u2019ll divide the evolution of application performance measurement into three \u201cphases.\u201d Each phase had its own deployment model, its own predominant software architecture, and its own way of measuring performance. Without further ado, let\u2019s go back to the olden days: before AWS, before the smartphone, and before Facebook (though perhaps not Friendster)\u2026 If you measured application performance at all in 2002, you probably did it with average request latency. Simple averages work well for simple things: namely, normally-distributed things with low variance. They are less appropriate when there\u2019s high variance, and they are particularly bad when the sample values are not normally distributed. Unfortunately, latency distributions today are rarely normally distributed, can have high variance, and are often multimodal to boot. (More on that later) To make this more concrete, here\u2019s a chart of average latency for one of the many microservice handlers in LightStep\u2019s SaaS: It holds steady at around 5ms, essentially all of the time. Looks good! 5ms is fast. Unfortunately it\u2019s not so simple: average latency is a poor leading indicator of reliability woes, especially for scaled-out internet applications. We\u2019ll need something better\u2026 Even if average latency looks good, we still don\u2019t know a thing about the outliers. Per this great Jeff Dean talk, in a microservices world with lots of fanout, an end-to-end transaction is only as fast as its slowest dependency. As our applications transitioned to the cloud, we learned that high-percentile latency was an important leading indicator of systemic performance problems. Of course, this is even more true today: when ordinary user requests touch dozens or hundreds of service instances, high-percentile latency in backends translates to average-case user-visible latency in frontends. To emphasize the importance of looking (very) far from the mean, let\u2019s look at recent p95 for that nice, flat, 5ms average latency graph from above: The latency for p95 is higher than p50, of course, but it\u2019s still pretty boring. That said, when we plot recent measurements for p99.9, we notice meaningful instability and variance over time: Now we\u2019re getting somewhere! With a p99.9 like that, we suspect that the shape of our latency distribution is not a nice, clean bell curve, after all\u2026 But what does it look like? When we reason about a latency distribution, we\u2019re trying to understand the distinct behaviors of our software application. What is the shape of the distribution? Where are the \u201cbumps\u201d (i.e., the modes of the distribution) and why are they there? Each mode in a latency distribution is a different behavior in the distributed system, and before we can explain these behaviors we must be able to see them. In order to understand performance \u201cright now\u201d, our workflow ought to look like this: Too often we just panic and start clicking around in hopes that we stumble upon a plausible explanation. Other times we are more disciplined, but our tools only expose bare statistics without context or relevant example transactions. This article is meant to be about ideas (rather than a particular product), but the only real-world example I can reference is the recently released Live View functionality in LightStep [x]PM. Live View is built around an unsampled, filterable, real-time histogram representation of performance that\u2019s tied directly to distributed tracing for root-cause analysis. To get back to our example, below is the live latency distribution corresponding to the percentile measurements above: The histogram makes it easy to identify the distinct modes of behavior (the \u201cbumps\u201d in the histogram) and to triage them. In this situation, we care most about the high-latency outliers on the right side. Compare this data with the simple statistics from \u201cPhase 1\u201d and \u201cPhase 2\u201d where the modes are indecipherable. Having identified and triaged the modes in our latency distribution, we now need to explain the concerning high-latency behavior. Since [x]PM has access to all (unsampled) trace data, we can isolate and zoom in on any feature regardless of its size. We filter interactively to hone in on an explanation: first by restricting to a narrow latency band, and then further by adding key:value tag restrictions. Here we see how the live latency distribution varies from one project_id to the next (project_id being a high-cardinality tag for this dataset): Here we are surprised to learn that project_id 36 experienced consistently slower performance than the aggregate. Again: Why? We restrict our view to project_id=36, filter to examine the latency outliers, and open a trace. Since [x]PM can assemble these traces retroactively, we always find an example, even for rare behavior: The (rare) trace we isolated shows us the smoking gun: that contention around mutex acquisition dominates the critical path (and explains why this particular project\u200a\u2014\u200awith its own highly-contended mutex\u200a\u2014\u200ahas inferior performance relative to others). Again, compare against a bare percentile: simply measuring p99 latency is a far cry from effective performance analysis. As practitioners, we must recognize that countless disconnected timeseries statistics are not enough to explain the behavior of modern applications. While p99 latency can still be a useful statistic, the complexity of today\u2019s microservice architectures warrants a richer and more flexible approach. Our tools must identify, triage, and explain latency issues, even as organizations adopt microservices. If you made it this far, I hope you\u2019ve learned some new ways to think about latency measurements and how they play a part in diagnostic workflows. LightStep continues to invest heavily in this area: to that end, please share your stories and points of view in the comment section, or reach out to me directly (Twitter, Medium, LinkedIn), either to provide feedback or to nudge us in a particular direction. I love to nerd out along these lines and welcome outside perspectives. Want to work on this with me and my colleagues? It\u2019s fun! LightStep is hiring. Want to make your own complex software more comprehensible? We can show you exactly how LightStep [x]PM works. By clapping more or less, you can signal to us which stories really stand out. Co-founder and CEO at LightStep, Co-creator of OpenTracing, built Dapper (Google\u2019s tracing system). Distributed thoughts\u200a\u2014\u200aA blog by the LightStep team discussing distributed systems, microservices, and how to achieve true visibility in production with minimal overhead.","time":1525808696,"title":"Performance is a shape not a number","type":"story","url":"https:\/\/medium.com\/lightstephq\/performance-is-a-shape-not-a-number-a3c1a9ae19cc","label":3,"label_name":"dev"},{"by":"DCH2","descendants":0,"id":17024175,"kids":"None","score":1,"text":"\nAll or nothing.\nThis project will only be funded if it reaches its goal by Fri, Jun 8 2018 7:34 am EDT.\n \nAn interactive book game. Solve the riddles and puzzles, submit your answers online, collect the keys and complete the adventure.\n \nAn interactive book game. Solve the riddles and puzzles, submit your answers online, collect the keys and complete the adventure.\n\nRead more\n\n \nAll or nothing.\nThis project will only be funded if it reaches its goal by Fri, Jun 8 2018 7:34 am EDT.\n Journal 29 Revelation is a unique book game where you can solve puzzles and submit your answers online to get the keys and move forward in the story. To solve the puzzles, you need to think out of the box.\n\nYou can write, draw, search,\u00a0 fold pages, combine different methods and try to get those puzzles right. Journal 29 Revelation is a 152 pages physical  book with over 52 puzzles to solve. The story of Journal 29 Revelation takes place after the events of Journal 29. It is not necessary to have solved Journal 29\u00a0 to enjoy Journal 29 Revelation. \u00a0A top secret excavation did not bring any result for 28 weeks.\nIt was on the 29th week that something unexpected happened.\nThe team disappeared and the only thing that was left behind was their Journal.\nYou must solve the puzzles in order to solve the mystery. Journal 29 Revelation will conclude the story and you will\u00a0discover what happened to the team.\n \u00a0Journal 29 Revelation is a book game of riddles and puzzles.\nIn order to play, you will need:\nA copy of Journal 29\n RevelationA pencil\nAn internet connected device (preferably a smartphone)\n\nEach two-page spread in Journal 29 Revelation contains two elements:The puzzle page and the key page. \u00a0\n\nTo solve the puzzles you will need to think out of the box:Write, draw, search,\u00a0 fold pages, combine and more.\nYou don't need any special app to play the game. Just a browser will do (preferably on your smartphone). \u00a0Solve the puzzle on the right page (check the hint). \u00a0Visit Journal29.com\/2\/ to submit your answer. \u00a0The idea behind Journal 29 Revelation is to create a game of puzzles and riddles that you can play by using your imagination and enjoy the experience of interacting with a real book at the same time.\n\nYou can write, draw, fold and use different techniques to make the Journal 29 Revelation a unique experience.\n\nBy submitting your answer online and collecting keys, we wish to solve 2 basic issues that often come up with an interactive book. First, we make sure that your answer is correct before you continue in the game.\nSecond the player cannot skip pages of the game due to the requirement of the online keys in order to solve the next riddles. Journal 29 Revelation is a standalone sequel to Journal 29, the first interactive book we published. It has already been translated into two more languages, and has been extensively reviewed.\n\nNote: It is not necessary to have solved Journal 29 in order to enjoy Journal 29 Revelation.\n\nTake a look at some of the reviews about Journal 29: \u00a0Journal 29 is available on Journal29.com \u00a0Dimitris Chassapakis is the creator of several puzzle games. Besides\u00a0 Journal 29 and Journal 29 Revelation is\u00a0 the creator of none* game a very succesfull mobile game. Journal 29 Revelation book specs:\n\nDimensions: 5.5\" x 8.5\" (13.97 x 21.59 cm)\nPages: 152\nPuzzles 52+\nKeys: 52+And extra pages with the story of the team. We expect to fulfill the rewards bound for the USA from within the USA, and the rewards bound for EU and other countries from Germany. EU backers do not have to worry about customs or VAT. We will be using Backerkit as our post-campaign fulfillment suite and pre-order management platform, through which additional copies may be obtained.\n\n Shipping will be charged after Kickstarter campaign finishes.\n \n \n\nThe shipping cost: IMPORTANT: Do not add this cost to your pledge! We will collect the shipping cost after the campaign, via Backerkit. Is Journal 29 Revelation a real book?\n\nYes. Journal 29 Revelation is a real, physical book.\n\n \n\u00a0 Do I need a special app to play?\n\nNo, You will only need a device connected to the internet and a web browser.\n\n\u00a0 Do I have to solve Journal 29 first book in order to play Journal 29 Revelation?\n\nNo, you can play Journal 29 Revelation without solving the first book.\n\n\u00a0 \n What can I do if I cannot solve a puzzle?\n\nThere is a puzzles discussion and hints on Journal 29 forum.\n\n \n\n\u00a0 Will there be an ebook version?\n\nNo. Many of the puzzles of Journal 29 Revelation need a physical paper in order to be solved.\n\n \n\n\u00a0 Can I skip a puzzle page? Play, for example, the last puzzle first?\n\nNo. The keys you receive online from solving the puzzles are essential to play the next puzzles.\n\n \n\n\u00a0 Do I need to buy anything else to play?\n\nNo. You will only need a copy of Journal 29 Revelation.\n\n \n\n\u00a0 In what language is the Journal 29 Revelation written?\n\nEnglish.More at FAQ tab of the campaign page. The puzzles for Journal 29 Revelation are ready.\nHaving the experience from making Journal 29 the book will be ready for shipping at the end of September.\nThank you for your support. Questions about this project? Check out the FAQ \nIt's a way to bring creative projects to life.\n Select this reward A copy of Journal 29 Revelation book. SHIPPING:\nShipping will be charged after the campaign, please do *not* add any shipping cost to your pledge. Please check the Shipping Information section of the campaign page for rates and policies. \nIt's a way to bring creative projects to life.\n Select this reward A copy of Journal 29 Revelation book and your name on Journal 29 website credits page. SHIPPING:\nShipping will be charged after the campaign, please do *not* add any shipping cost to your pledge. Please check the Shipping Information section of the campaign page for rates and policies. \nIt's a way to bring creative projects to life.\n Select this reward A copy of Journal 29 Revelation book + a copy of Journal 29 book and your name on Journal 29 website credits page. SHIPPING:\nShipping will be charged after the campaign, please do *not* add any shipping cost to your pledge. Please check the Shipping Information section of the campaign page for rates and policies. \nIt's a way to bring creative projects to life.\n Select this reward A copy of Journal 29 Revelation book and your name on Journal 29 website credits page + your name on the Silver Credits page of the book. SHIPPING:\nShipping will be charged after the campaign, please do *not* add any shipping cost to your pledge. Please check the Shipping Information section of the campaign page for rates and policies. \nIt's a way to bring creative projects to life.\n Select this reward A copy of Journal 29 Revelation book and your name on Journal 29 website credits page + your name on the Golden Credits page of the book. SHIPPING:\nShipping will be charged after the campaign, please do *not* add any shipping cost to your pledge. Please check the Shipping Information section of the campaign page for rates and policies. \nIt's a way to bring creative projects to life.\n","time":1525808669,"title":"Escape room in a book","type":"story","url":"https:\/\/www.kickstarter.com\/projects\/rain-ludibooster\/journal-29-revelation-interactive-book-game","label":7,"label_name":"random"},{"by":"harry-radicle","descendants":0,"id":17024173,"kids":"None","score":1,"text":"Today, we\u2019re releasing the alpha version of the Rad 30 Crypto Composite. Accordingly, we thought we\u2019d sketch out the what, the why, and the how. The What The Rad 30 Crypto Composite (RAD 30) tracks thirty crypto assets, similar to how the S&P 500 and the NASDAQ Composite track stocks. In a growing and snowballing crypto landscape, the Rad 30 will help anyone interested in the decentralized economy to broaden their perspective, prioritize their attention, and access a third benchmark for gauging the performance of the crypto market. While correlation to Bitcoin and Ethereum across the crypto market is relatively high, in Q1 2018, the Rad 30 (which excludes BTC and ETH) moved in the opposite direction of BTC and ETH on 45% of days. (You can find our Quarterly Weighting Report here for more analysis of Q1.) The Why Radicle is a disruption research company. We study and provide insights on the startups and technologies that are creating the future. It\u2019s impossible to be in that business\u200a\u2014\u200aor any business for that matter\u200a\u2014\u200aand not have confronted the mega-themes of decentralization, blockchains, and crypto assets. Especially over the past ~24 months. When contending with the topic of decentralization and crypto, in conversation with ourselves and our customers, we found ourselves asking many familiar questions: One challenge, in particular, felt like something that we could solve: Show me a handful of real crypto assets that together and individually will help me make sense of decentralization and \u201ccrypto\u201d. How is it progressing? What markets will it disrupt? How big of an opportunity is there for disruption? The Rad 30 Crypto Composite will help address these questions. The Rad 30 consists of thirty crypto assets that are representative of the broader decentralized economy. These are not just the crypto assets with the largest market caps. These are crypto assets and platforms that are addressing opportunities for decentralization beyond digital money and store-of-value. Why in the world should I care? There are a number of reasons to care about the RAD 30. But we\u2019ll keep it to three: 1. Broaden your perspective. There\u2019s more to pay attention to than Bitcoin, Bitcoin copycats, and Ethereum. The top crypto assets by market cap present a very specific view of decentralization. They do not offer an understanding of how crypto is impacting the diverse set of industries in which it is being deployed and explored. 2. Prioritize your attention. Once you\u2019ve decided to broaden your perspective, finding the crypto assets that matter will allow you to track substantive progress on a weekly, monthly, quarterly basis. Understanding what crypto assets are included (and why they\u2019re included) in the RAD 30 will help business people of all types prioritize the attention they dedicate to decentralization and crypto. 3. A third benchmark. Use the Composite, its component sectors, and constituent crypto assets as benchmarks not only for each other but also for ETH and BTC. The Composite will be a lens for analyzing questions such as: These questions are only the tip of the iceberg. We see the potential for analysts to use the RAD 30 to build new formulas offering new analysis that benefits the crypto community. We believe that informed business people should understand how the decentralized economy is progressing, what markets are being disrupted by it, and where opportunity for further disruption lies. To that end, just as the stock market has the S&P 500, the Dow Jones Industrial Average, and the NASDAQ Composite, we want the decentralized economy to have the Rad 30 Crypto Composite. The How There are more than 30 crypto assets out there. So, how then did we go about choosing the RAD 30? In a space where there were 159 ICOs in Q1 2018 alone, it\u2019s impractical to analyze every emergent crypto platform and asses its viability and potential. So, instead of being reactive to every new crypto asset, one ought to be proactive and seek to identify the best opportunities. To solve this, rather than starting with specific crypto assets, we take a broader approach, asking questions from a sector lens. Which sectors are the most susceptible to decentralization? Of those sectors, which present the largest market opportunities? Then we ask ourselves: how do the crypto assets in these sector compare in terms of incentive structures and current trading levels? Our combined bottoms-up and top-down approach enables us to prioritize certain crypto assets and sectors over others. Each crypto asset is then weighted according to a combined fundamental and quantitative analysis. Similar to the S&P 500, the RAD 30 is finally calculated as the aggregate score of the weighted thirty crypto assets. We believe the RAD 30 has the potential to bring a little bit of structure to a structureless market. We\u2019re excited to see what can come out of the RAD 30! Find out more about the Rad 30 Crypto Composite here! Update: April\u2019s Performance Over the course of April, the RAD 30 was up 93.7% while BTC and ETH were up 32.5% and ~69%, respectively. Decentralized social media platforms and decentralized cloud computing were the two best performing sectors, both up 146% and 139%, respectively. If you\u2019d like to access our RAD 30 data, please email analyst@rad.report. Radicle is a category-defining research and information company built on understanding startups and new technologies. Rad.Crypto is Radicle\u2019s crypto research arm. Disclaimer: Radicle is not an investment advisor, and Radicle makes no representation regarding the advisability of investing in any security, asset, token, fund or other investment vehicle. Radicle is not a tax advisor. Inclusion of a security, asset or token within a Radicle composite or any Radicle analysis is not a recommendation by Radicle to buy, sell, or hold such security, nor is it considered to be investment advice. Past performance of a composite is not an indication or guarantee of future results. All Radicle materials have been prepared solely for informational purposes based upon information generally available to the public and from sources believed to be reliable. The composite data and analysis shown do not represent the results of actual trading of investable assets\/securities. Radicle maintains the composite(s) and calculates the composite levels and performance shown or discussed, but does not manage actual assets. By clapping more or less, you can signal to us which stories really stand out. Crypto @ Radicle Labs (www.rad.report\/crypto) how hackers start their afternoons.","time":1525808651,"title":"Introducing the Rad 30 Crypto Composite","type":"story","url":"https:\/\/hackernoon.com\/introducing-the-rad-30-crypto-composite-64cda215e891","label":7,"label_name":"random"},{"by":"CaliforniaKarl","descendants":0,"id":17024159,"kids":"None","score":1,"text":"We talk with jack Conte about how Patreon came about, and the challenges it faces as it grows. MP3  For all DTNS shows, please SUBSCRIBE HERE. Follow us on Soundcloud. A special thanks to all our supporters\u2013without you, none of this would be possible. If you are willing to support the show or give as little as 5 cents a day on Patreon. Thank you! Big thanks to Mustafa A. from thepolarcat.com for the DTNS logo and Ryan Officer for the DTNS Labs take! Amazing interview! I\u2019ve been a listener for awhile, every since another daily tech news show went weekly, but hadn\u2019t pulled the trigger on becoming a patron, until now. As a product manager, Jack dished out TONS of great knowledge that every product manager must know and understand. Keep up the great work! Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n  ","time":1525808550,"title":"Daily Tech News Show Interview with Jack Conte, CEO of Patreon","type":"story","url":"http:\/\/www.dailytechnewsshow.com\/dtns-labs-interview-jack-conte-ceo-of-patreon\/","label":7,"label_name":"random"},{"by":"jamesblonde","descendants":1,"id":17024152,"kids":"[17024167]","score":1,"text":"Today \n\nKyle Bradshaw\n\n \n\t\t\t\t\t\t\t- May. 8th 2018 10:43 am PT\n\t\t\t\t\t\t\t\t @SkylledDev During the I\/O 2018 Keynote, Google announced the release of its new TPU 3.0 chip. \n\n(adsbygoogle = window.adsbygoogle || []).push({});\n Following a string of demonstrations of the power of machine learning, including Smart Compose for Gmail and intelligent editing suggestions in Google Photos, Sundar Pichai took a moment to put the spotlight on the new chip that brings these possibilities to life. \u2018Tensor processing units\u2019 or TPUs are the hardware components that enable most of Google\u2019s AI and machine learning capabilities, including AlphaGo. These TPUs can be leased to developers though Google Cloud. The third-generation Tensor processing unit has 8 times the performance of the previous generation, the new chips are so powerful that Google had to add liquid cooling to their data centers to compensate for the additional heat. Updating\u2026  \n@SkylledDev\n Watch the Google I\/O 2018 Keynote live! Android P DP2 leaks ahead of I\/O Review: Lenovo\u2019s Mirage Solo VR headset Giveaway: Win a Samsung Galaxy S9!","time":1525808504,"title":"TPU V3","type":"story","url":"https:\/\/9to5google.com\/2018\/05\/08\/google-introduces-tpu-v3-0-the-next-generation-of-machine-learning-processors\/","label":5,"label_name":"ml"},{"by":"Ihmahr","descendants":0,"id":17024143,"kids":"None","score":2,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 The resurgence in high-flying technology stocks is far from just a U.S. phenomenon. In fact, the Guggenheim China Tech exchange-traded fund, ticker CQQQ, has outpaced the Nasdaq 100 Index\u2019s rally since the release of Facebook\u2019s earnings after the close on April 25, results which helped reignite the sector\u2019s advance. Both the FAANG constituents stateside and China\u2019s BAT trio (Baidu, Alibaba, and Tencent, which are the three top holdings of the Guggenheim fund) had come under acute pressure in mid-April as crowded trades cracked. Tencent has continued to trade sideways in recent weeks but the other two Chinese components have more than picked up the slack, bolstered by stellar financial performance. Internet search-engine operator Baidu and e-commerce giant Alibaba each posted results that exceeded analysts\u2019 expectations along with a brighter outlook for top-line growth. Like their U.S. peers, pessimism about these Chinese behemoths had been mounting ahead of quarterly results. A potentially fraying trading relationship between the world\u2019s two largest economies compounded concerns. In March, the Chinese tech ETF plunged more than 5 percent, its biggest daily drop since 2015, amid the Trump administration\u2019s pursuit of tariffs and Tencent\u2019s company-specific travails. On Tuesday morning, Trump tweeted that \u201cgood things will happen\u201d on trade as dialogue with his \u201cfriend, President Xi of China\u201d continues. This sign of public rapprochement belies the lack of concrete progress in trade talks thus far, with major disagreements remaining between the two sides following Treasury Secretary Steven Mnuchin\u2019s recent trip to Beijing.","time":1525808474,"title":"China\u2019s Tech Comeback Outpaces the U.S","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-08\/china-tech-comeback-outpaces-u-s-despite-trade-war-overhang","label":0,"label_name":"biz-news"},{"by":"guofangli","descendants":0,"id":17024142,"kids":"None","score":1,"text":"Front page layout Site theme Sign up or login to join the discussions! \nSean Gallagher\n    -  May 8, 2018 3:13 pm UTC\n On May 7, executives of Equifax submitted a \"statement for the record\" to the Securities and Exchange Commission detailing the extent of the consumer data breach the company first reported on September 7, 2017. The data in the statement, which has also been shared with congressional committees investigating the breach, reveals to a fuller extent how much personal data was exposed in the breach. Millions of driver's license numbers, phone numbers, and email addresses were also exposed in connection with names, dates of birth, and Social Security numbers\u2014offering a gold mine of data for identity thieves and fraudsters. Equifax had already reported that the names, Social Security numbers, and dates of birth of 143 million US consumers had been exposed, along with driver's license numbers \"in some instances,\" in addition to the credit card numbers of 209,000 individuals. The company's management had also reported \"certain dispute documents\" submitted by about 182,000 consumers contesting credit reports had been exposed as well, in addition to some information about British and Canadian consumers. But the exact details of the nature of these documents and information had not been revealed, in part because Equifax felt it did not have a legal obligation to disclose those details. \"With respect to the data elements of gender, phone number, and email addresses, US state data breach notification laws generally do not require notification to consumers when these data elements are compromised, particularly when an email address is not stolen in combination with further credentials that would permit access,\" Equifax's management asserted in the SEC letter. Of the 146.6 million individuals affected by the breach: In addition, Equifax provided more detail about the \"dispute documents\" that were stolen in the breach. These were personal identity documents uploaded as images to Equifax: The stolen data did not come from a single, centralized database but from a collection of disparate databases associated with Equifax's\u00a0Web applications and payment systems. \"As earlier statements made clear,\" Equifax's letter stated, \"the company\u2019s forensics experts found no evidence that Equifax\u2019s US and international core consumer, employment and income, or commercial credit reporting databases were accessed as part of the cyber attack.\" Because the databases stolen did not have a consistent schema, Equifax's forensic investigation team (with the assistance of a team from Mandiant) had to map the database fields to standard data elements in order to \"determine the impacted consumers and Equifax\u2019s notification obligations.\" Further ReadingEquifax website borked again, this time to redirect to fake Flash updateEquifax did not share information in the letter about the correlation between the data elements exposed, so there's no way to tell how many individuals had multiple types of personal data stolen. But name, address, Social Security number, date of birth, and driver's license numbers are enough on their own to do significant damage through identity theft. And the combination of that data with email addresses and phone numbers exposes millions to potential \"spear phishing\" and phone scams. Equifax has offered credit protection services to individuals affected by the breach. But the company badly bungled its early communications with victims, including sending communications that directed consumers to a fraudulent website. Then they did it again, sending consumers to a site with a fake \"Adobe Flash update\" malware downloader. You must login or create an account to comment. Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox.","time":1525808472,"title":"Equifax breach exposed millions of driver\u2019s licenses, phone numbers, emails","type":"story","url":"https:\/\/arstechnica.com\/information-technology\/2018\/05\/equifax-breach-exposed-millions-of-drivers-licenses-phone-numbers-emails\/","label":6,"label_name":"news"},{"by":"aaossa","descendants":0,"id":17024132,"kids":"None","score":1,"text":"\r\n\tScientists, engineers and reporters gathered at NASA's Glenn Research Center in Cleveland on May 2 for a news conference announcing the latest results of the Kilopower\u00a0nuclear power plant project: It has finished all of its major ground tests and met or surpassed the development team's expectations.\u00a0 \r\n\tNASA is developing the experimental reactor to provide reliable energy for long-duration crewed missions to the moon, Mars and destinations beyond. \r\n\tFor decades, spacecraft have relied on nuclear power as a compact, reliable source of electricity, especially on missions for which solar power is not feasible, like expeditions to the moon's polar regions. The Voyager 1 and 2 spacecraft, which are now billions of miles from the sun, still have enough nuclear energy after more than 40 years to transmit signals back to Earth. Meanwhile, the Curiosity rover has been driving around the Red Planet for nearly six years courtesy of a trunk full of plutonium. [Nuclear Generators for NASA Deep Space Probes (Infographic)] \r\n\tIn spacecraft like the Voyagers and Curiosity, a device called a radioisotope thermoelectric generator (RTG) converts heat energy from passive radioactive decay directly into electricity. The decay causes a temperature difference across plates of two kinds of metal \u2014 one connected to the reactor, and the other attached to a radiator, thereby producing a voltage. This component, called a thermocouple, is commonly used in thermometers and temperature sensors. Although RTGs are not particularly efficient, they are simple and have no moving parts, making them perfect for applications in which repair is not an option.\u00a0 \n \r\n\tBut many future missions, especially those involving human crews, will require much more power than the RTGs can produce. That's why NASA and the U.S. Department of Energy (DOE) are collaborating to create a space-ready nuclear reactor, which harvests energy from active nuclear fission, or atom splitting.\u00a0 \r\n\tAt the news conference, NASA and DOE officials announced the completion of successful ground tests of the experimental reactor, called the Kilopower Reactor Using Stirling Technology (KRUSTY). The team tested the reactor at the DOE Nevada National Security Site in four phases. The first two were conducted without power, to ensure that the components reacted as expected. During the third phase, the team gradually increased power to heat the core. The final phase consisted of a 28-hour, full-power test that simulated an actual mission.\u00a0 \r\n\tThe team evaluated the reactor's startup sequence, steady state performance, efficiency and more. In each category, the test reactor met or exceeded the team's benchmarks, running at full power under vacuum conditions, according to Kilopower lead engineer Marc Gibson.\u00a0 \r\n\t\"We threw everything we could at this reactor, in terms of nominal and off-normal operating scenarios, and KRUSTY passed with flying colors,\" said David Poston, chief reactor designer at the National Nuclear Security Administration's Los Alamos National Laboratory. \r\n\tThe significance of these results is hard to overstate, said Gibson. Research on space-ready fission reactors has been mired by high costs and lengthy time frames from the late 1970s to the early 2000s, which resulted in many canceled projects. \"This is the first nuclear-powered operation of a new fission reactor concept in the U.S. in 40 years,\" Gibson said.\u00a0 \r\n\tFission reactors have many practical advantages over RTGs. For instance, RTGs generally produce only a few hundred watts, but the reactor is scalable to 10,000 watts. Four units could provide enough power to establish an extraterrestrial outpost, according to a statement NASA released after the event.\u00a0 \n \r\n\tAnother advantage is that, unlike an RTG, which runs continuously, a reactor's output can be tailored to current demands, Gibson said. That also means it can remain dormant during launch and travel and turn on once it reaches its destination. This ability, combined with the increased efficiency of fission reactors over RTGs, means that a Kilopower generator could maintain its 1-kilowatt output for at least 10 years, NASA officials said in the statement.\u00a0 \r\n\tSelf-regulation has been a crucial requirement in designing the reactor, according to Poston. This feature not only increases the safety of the reactor but also frees astronauts from having to monitor the controls. \"We're not going to have operators there,\" Poston said. \"And even if there are astronauts around, they're not going to be wanting to sit at a reactor control center the whole time.\" [NASA's Human Mars Mission Will Require Living Off the Land] \r\n\tThe system works like a thermostat, where feedback keeps the device at a preset temperature, Poston said. If the reactor overheats, the Stirling engines that produce electricity draw more heat from the uranium core. If it is too cool, the core naturally contracts, trapping more free neutrons, which then increases the rate of fission. \r\n\tResponding to concerns over Russia's recently launched, floating nuclear power plant, Poston explained that the reactor will pose little or no risk to the public. NASA follows all relevant protocols, including those set out by the United Nations, he said. Additionally, the reactor will not be turned on until it is far away from Earth. \"We've done calculations to show that, under all worst-case conditions, we don't believe that there's any chance the reactor would come on accidentally, [even] during a launch accident,\" Poston said. \r\n\tThe team has also considered safety at the reactor site. Engineers at NASA's Johnson Space Center are designing containers to safely store spent fuel on-site, since returning it to Earth would not be practical, said Patrick Cahalane, the principal deputy associate administrator for safety, infrastructure and operations at the National Nuclear Security Administration. The reactors have no radioactive coolant that would pose a contamination risk, and the development team is researching mechanisms to shield astronauts from radiation that the reactor may emit, including building in protection and burying part of the reactor under the surface. \r\n\tAlthough the prototype isn't the same as units that will be deployed into space, it was designed with the flight-unit in mind, Gibson said. Flight tests are the next major step in development, though NASA has yet to plan them.\u00a0 \r\n\tThe Kilopower generator is optimized for use on surface missions, but officials at the news conference said it could also be used to power ion propulsion systems or be included on the Lunar Orbital Platform-Gateway, a proposed outpost for astronauts positioned in the space near Earth's moon. \nFollow Harrison Tasoff @harrisontasoff. Follow us @Spacedotcom, Facebook and Google+. Original article on Space.com. Hawaii's Kilauea Volcano Eruption Spotted from Space (Photos) Artificial Intelligence Arms Race Accelerating in Space This Alien World Is the 1st Cloudless Exoplanet Ever Discovered United Launch Alliance Machinists Go On Strike Humans to Mars Summit 2018 Launches This Week: Watch It Live Copyright \u00a9\n\t\t\t\t\t\t\t\t\tdocument.write(new Date().getFullYear());\n\t\t\t\t\t\t\t\t\tAll Rights Reserved.\n\t\t\t\t\t\t\t","time":1525808435,"title":"A Nuclear Reactor for Space Missions Passes Final Major Ground Tests","type":"story","url":"https:\/\/www.space.com\/40479-space-nuclear-reactor-kilopower-passes-big-test.html","label":8,"label_name":"science"},{"by":"hackyio","descendants":0,"id":17024127,"kids":"None","score":4,"text":"In late 2016, whilst leading the Developer Platform team at Intercom, I published a blog post titled \u201cBrowsers, not apps, are the future of mobile.\u201d It quickly became the most-read post published on Inside Intercom that year, and generated a lot of debate\u200a\u2014\u200aexternally on Hacker News and Medium, and internally within Intercom. At one point I was summonsed to the Inside Intercom podcast to explain myself. For some people, I had hit a nerve. So, 18 months later, what\u2019s changed? Are native apps dead yet? Are browsers and the web still the future of mobile? What is a browser, anyway? Let\u2019s dig in. The main argument of the post was this: browsers and the web are fast becoming the mobile operating system of the future, and native mobile apps are dying. There were three theories core to the argument, each of which i\u2019ll revisit and reexamine here. Recent mobile infrastructure, standards, and product development trends are fast making this theory a reality for most internet businesses. Granted, native apps are still useful for services like messengers and social networks\u200a\u2014\u200aall of which demand hours of user time daily (more on those types of apps later). For everything else, though, native apps are bloated and unnecessary. Networks like Verizon have confirmed their ambition to launch 5G broadband before the end of 2018. A battle between Samsung, Ericsson, Nokia and Huawei to become the 5G infrastructure provider of choice is also hotting up. Native apps were useful at a time when spotty 2G connections meant our smartphones were in offline mode most of the day. 5G is slated to be about 10 times faster than 4G, with less latency to boot\u200a\u2014\u200athe \u2018pipes\u2019 are in place to enable fast and immediate streaming of immersive and responsive experiences through the browser. No need for native apps. Standards have evolved quickly too, in favor of the mobile web. Fast-following Chrome and ChromeOS, Microsoft launched Progressive Web Apps to Microsoft Edge and the half a billion devices running Windows 10. This is a huge leap towards adoption of browser based progressive mobile web apps, particularly for enterprise users. And, hidden in the release notes of Safari 11.1 on iOS 11.3, was the addition of Service Workers and Payments APIs, key elements of progressive web apps. Apple still rely on Native apps sold through their App Store to bolster revenues, hence the lack of fanfare about this update. But developers now have a viable web based alternative to native apps. Realising that native apps are good for some things, but not all things, businesses are allocating resources to progressive web apps over iOS or Android apps. Platforms like Mobify are helping retailers like Debenhams, Crocs, and Lancome to make the transition. The frictionless mobile web experience (no native app download required) gave Lancome a 17% lift in mobile revenue, whilst Debenhams reported double digit increases in mobile revenues within weeks of launching their progressive web app. Money talks, and there\u2019s more of it to be made on the mobile web if you\u2019re an ecommerce business. Key to this theory is an evolving definition of what a \u2018browser\u2019 is. We\u2019re still spending increasing amounts of time inside messaging apps and social networks, themselves wrappers for the mobile web. They\u2019re new types of browsers, bringing social context and connections into the experience, something traditional browsers lack. And, in markets like India and China, these social networks and messengers are main way new internet users discover content on the web. The lines that divide traditional browsers, social networks, and messengers are blurring quickly. Mary Meeker\u2019s excellent KPCP 2017 Internet Trends report highlights just how blurry those lines have become. The slide below on China tells some interesting stories. Aside from Tencent now dominating the market over incumbents Alibaba and Baidu, messaging apps like WeChat (Tencent) are directly compared to \u2018traditional\u2019 browsers like UC Browser (Alibaba). In China, legacy categorisations like \u2018social network\u2019 or \u2018browser\u2019 don\u2019t matter. What matters are the tools that enable users to create, consume, and share information with each other, and the usage of those tools. On mobile, traditional browsers are but one of these tools. These slides on India tell a similar story, even if it\u2019s not immediately apparent. One one hand, \u2018Mobile Browser Usage Market Share\u2019 only includes traditional browsers like UC Browser, Opera, and Chrome. Messengers and social networks are nowhere to be seen. The full story becomes clear one slide later. Of the Top 10 Downloaded Android Apps, UC Browser appears just 6th in the list, far behind WhatApp and Facebook Messenger in 1st and 2nd place\u200a\u2014\u200aboth comparable products to WeChat in China. Let\u2019s not forget that UC Browser was mentioned as the market leading mobile browser in the previous slide. This is not accurate. Users in India are primarily creating, consuming and sharing information through messengers, not through traditional browsers. It\u2019s time to rethink our definition of what a browser is. Why do we use social networks and messengers? In part, to discover relevant information based on friend recommendations and to ask businesses for product recommendations. The information shared back to us includes links to pages on the internet. This is browsing the web, just a more modern and personalised way of doing it. This is the idea that bots are new type of intelligent, dynamic bookmark for mobile web browsers. Based on previous messenger interactions, bots push us relevant content, with improved relevancy over time. Like a social network feed, but based on your interactions only\u200a\u2014\u200anot those of your friends. For the most part, this has come to pass. Pinterest acknowledged the value of this new way to browse the internet through their bot, which recommends new pins based on users\u2019 previous requests and interactions. AI curation bots like Neil take this idea one step further, by using like, dislike, and bookmark additions as signals to improve the relevancy of recommendations. In Mexico, airline Aeromexico leveraged the power of Facebook Messenger bots to provide destination inspiration to customers, help them book flights, and guide them to knowledge base content to address frequently asked questions. And Tommy Hilfiger took the ecommerce browsing experience to Messenger through their \u201cSee now, buy now\u201d bot, which allows customers to purchase items featured on a virtual runway. Just two of the 300,000 active bots available on Facebook\u2019s Messenger platform. The evolution of the internet has come full circle. Whereas the world wide web started out as a collection of connected sources of information, native mobile apps siloed that information again, adding friction to our experience. Progressive web apps, and improved 5G connectivity are set to break those silos, reinstating the web as the most popular mobile operating system in the world\u200a\u2014\u200anot iOS or Android. For developers and start ups, this means a dedicating more focus to mobile web technologies, from progressive web apps, to new ways of delivering those apps closer to users and their mobile devices. New tools like Netlify have made this easier and cheaper than ever. It also means valuable time is spent creating new value\u200a\u2014\u200anot reinventing the wheel across multiple operating systems. And where do distribution opportunities exist\u200a\u2014\u200anot just in China and India, but globally? The biggest opportunities are still in consumer social and messaging apps, the new mobile browsers for an increasingly bot-enabled world. Originally published at www.developerecosystem.com. By clapping more or less, you can signal to us which stories really stand out. AI Platform Product Management & DX at @ZalandoTech. Previous: Platform at @Facebook & @Intercom. Blog: https:\/\/www.developerecosystem.com\/ Strategies and tools to build, grow and participate in developer ecosystems","time":1525808379,"title":"Browsers, not apps, are still the future of mobile","type":"story","url":"https:\/\/medium.com\/developer-ecosystem\/browsers-not-apps-are-still-the-future-of-mobile-e42f661f12eb","label":3,"label_name":"dev"},{"by":"walterbell","descendants":0,"id":17024124,"kids":"None","score":1,"text":"Every time you visit a website, you leave behind a trail of information, including seemingly innocuous data, like whether you use an Android or Apple device. And while that might feel like a mere personal preference, it turns out that lenders can use that type of passive signal to help predict whether you'll default. In fact, new research suggests that those signals can predict consumer behavior as accurately as traditional credit scores. That could disrupt the traditional credit bureau industry that's dominated since the 1980s\u2014and have serious ramifications for privacy. In a new working paper from The National Bureau of Economic Research, a team of researchers analyzed over 270,000 purchases from October 2015 to December 2016 on a German e-commerce website that allows customers to buy furniture and pay for it later. (Think of it as Germany's version of Wayfair.) The store was of particular interest because it already uses a digital footprint, in conjunction with a user's German credit score, to decide whether buyers qualify for a loan. At least a handful of European retailers have been using similar systems for several years. The use of a largely outdated email service\u2014like Hotmail or Yahoo\u2014was also an indicator of a higher default rate. The researchers looked at 10 different types of information customers passively provide, including things like what type of device they used, their operating system, how they got to the site (like whether they clicked on an ad), the time of day they made the purchase, and what kind of email provider they use.  The researchers didn't take into account some factors the retailer normally does, like whether the person has paid a loan back from the same company in the past. Still, they found that those simple variables could be used to estimate whether someone might default, just like a FICO score does. The difference in default rates between iOS and Android users, for instance, was equivalent to the difference between a median FICO score and the 80th percentile of FICO scores. On one level, these types of insights are intuitive: The average iPhone is much more expensive than the average Android device, and previous research has shown whether someone owns an iOS device is one of the best predictors of whether they're in the top 25 percent of earners. The study's other findings, though, are more subtle. For example, customers who placed orders through cell phones rather than desktop computers were also more likely to default. The use of a largely outdated email service\u2014like Hotmail or Yahoo\u2014was also an indicator of a higher default rate. Customers who incorrectly entered their email address defaulted 5.09 percent of the time; those who didn't were at .94 percent. In this case, \"defaulting\" means the loan was sold to a collections agency, usually several months after the purchase and after the customer had been notified three times about their outstanding bill. Even how you arrive at an e-commerce website can be used to predict whether you'll default. Those coming in from a price-comparison website were half as likely to default as those who clicked on a targeted ad. That makes sense; savvy, careful consumers browse different retailers' prices before making a purchase. But even seemingly irrelevant information can say more about your spending behavior than you might expect. For example, customers who have their first or last names in their email addresses were 30 percent less likely to default than those who used something like \"cutie367.\" The researchers ultimately found that digital footprints equaled or exceeded the predictive power of traditional FICO-like credit scores, and could even be used to predict how a person's FICO score might change in future. The authors say digital data could also potentially be used to assess customers outside of the traditional banking system, who often don't have FICO scores. But they also acknowledge that wide use of digital footprints for creditworthiness would likely have serious implications for user behavior and freedom online. Imagine buying an iPhone to qualify for a mortgage, or thinking about car loans when signing up for an email account. Customers fudging their digital footprints could also cause lenders to issue loans to customers that can't actually pay them back. \"My personal opinion is that among most people, if you have someone who thinks about these types of issues, you're already talking about people who are financially quite sophisticated,\" says Tobias Berg, the study's lead author and an associate professor at Frankfurt School of Finance & Management. He also points out that most consumers in Germany aren't aware that information like what type of device they use is sometimes factored into loan approvals, even though it's explained in retailers' terms of service agreements. \"Almost no one reads that, and no one really understands what it literally means,\" he says. Another concern is that digital footprints might serve as proxies for variables lenders are prohibited by law from taking into account, like race. There are clearly people who \"are going to be disadvantaged by these digital footprints, no doubt about it,\" says Berg. That includes individuals inadvertently categorized as risky, even when they're not; plenty of people can afford iPhones but go with Android instead. And there are perfectly valid privacy reasons to leave your name out of your email address, for example. Berg and his coauthors found some correlation between FICO scores and digital footprints but not much; someone's FICO score might indicate that they're qualified for a loan, while their digital footprint says otherwise. Berg accounts for the difference by pointing out that credit scores are fairly crude, and only account for extreme situations like when a customer misses a payment. Digital footprints can reveal more psychologically oriented traits, like how someone thinks about making a purchase or what time of the day they shop. That's why the researchers suggest the strongest signal comes from combining the two. But for an unbanked person with only a digital footprint, that disparity might result in being denied a loan they would otherwise get. The good news is that in the United States, digital-footprint loans are likely a long ways off, in part because companies have found in the past that online information may not be as useful as it seems. \"We've heard this before. The last iteration was social media; companies saying that they're going to use your Facebook posts to judge how creditworthy you are,\" says Liz Weston, a columnist at NerdWallet and the author of five books, including Your Credit Score. \"This stuff sounds scary, but a lot of things don't affect your credit score now and they're not likely to in the future.\" That's partly because the lending industry moves incredibly slowly, and is reluctant to change its methods. \"The basic scoring formula has worked pretty well and continues to work pretty well,\" says Weston. \"I just can't see it being displaced, and certainly not overnight.\" That's not to say it works perfectly; Weston notes FICO puts minority groups that depend more on cash or informal lending at a disadvantage. Digital footprints, meanwhile, do come into play somewhat in the US; online retailers have used some of that info to manipulate prices for years. For now, the general behaviors you need to create good credit aren't based on whims, like whether you kept your goofy email address from high school. But you can't predict how websites will analyze and use passive data in the future, especially given how hard it is to avoid disclosing information like what kind of phone you have to a retailer. At the very least, though, you can understand how that information is analyzed\u2014and what conclusions companies draw from it. Heads up, iPhone owners. iOS 11 comes with a batch of security features that merit your attention. CNMN Collection Use of this site constitutes acceptance of our user agreement (effective 3\/21\/12) and privacy policy (effective 3\/21\/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast.","time":1525808371,"title":"Smartphone Choice Is a Better Predictor to Lenders Than Credit Score","type":"story","url":"https:\/\/www.wired.com\/story\/your-smartphone-could-decide-whether-youll-get-a-loan\/","label":7,"label_name":"random"},{"by":"prostoalex","descendants":0,"id":17024103,"kids":"None","score":3,"text":"Download File May 8, 2018 America\u2019s vacation behavior is changing\u2014and for the better. State of American Vacation 2018 shows improvement in America\u2019s work culture. While a transformation is not at hand, the beginnings of change may be. Employers are showing signs of having more encouraging vacation cultures, and employees are feeling more confident about using the time they earn. There is still a long way to go, with more than half of Americans leaving vacation time on the table. In analyzing how vacation time was spent, the data shows an unmistakably strong correlation between travel and happiness that forces the question, does the function of the time or the time itself have a greater impact? Fewer Americans left vacation time on the table in 2017. While still a majority, 52 percent of employees reported having unused vacation days at the end of the year, compared to 54 percent in 2016 and 55 percent in 2015. Though a two percent change might seem small, the impact is mighty. Americans used nearly a half-day (.4 days) more of vacation than the previous year. The increase marks the third straight year of increased vacation usage and brings the national average up to 17.2 vacation days taken per employee. It also marks more than a full-day increase since the lowest point in America\u2019s vacation usage in 2014, when that number stood at 16.0 days. The increase in vacation usage from 16.8 to 17.2 days delivered a $30.7 billion impact to the U.S. economy. It also produced an estimated 217,200 direct and indirect jobs and generated $8.9 billion in additional income for Americans.While the numbers are moving in a positive direction, more than half of Americans are still not using all the vacation time they earn. The 52 percent who left vacation on the table accumulated 705 million unused days last year, up from 662 million days the year before. Though this increase may seem counterintuitive to Americans using more vacation time, it is a function of employees earning more time. The average employee reported earning 23.2 paid time off days, an increase of more than half a day (.6 days) over the previous year. Of these days, Americans forfeited 212 million days, which is equivalent to $62.2 billion in lost benefits. That means employees effectively donated an individual average of $561 in work time to their employer in 2017. The more than 700 million days that go unused represent a $255 billion opportunity that the American economy is not capturing. Had Americans used that vacation time, the activity could have generated 1.9 million jobs. The turning trend line is a clear sign that Americans are increasingly realizing the value of their vacation time. Yet challenges remain\u2014particularly when it comes to workplace optics. Work-related challenges had the most influence on Americans\u2019 ability to vacation. Employees who were concerned that they would appear less dedicated or even replaceable if they took a vacation were dramatically less likely to use all their vacation time (61% leave time unused, compared to 52% overall). Those who felt their workload was too heavy to get away were also more likely than average to have unused vacation time (57% to 52%), as were employees who felt there was a lack of coverage or that no one else could do their job (56% to 52%). Alleviating the barriers in the workplace rests on creating a positive vacation culture. With nearly four-in-ten (38%) employees saying their company culture encouraged vacation, there has been improvement in employees\u2019 perception of their company\u2019s level of support for vacation (compared to 33% last year). The other barriers had far less influence on vacation behavior. When asked about the barriers to taking time off to travel, unsurprisingly, cost is at the top. However, the respondents who agreed that cost was a barrier take about the same amount of vacation time as average (53% leave time unused, compared to 52% overall). Everyone has a budget\u2014the variable is how large. Where 71 percent of overall respondents said cost was a challenge, that number drops just slightly to 68 percent for Americans with household incomes over $100,000 annually. Other top challenges also had little impact on vacation behavior. While children, pets, and the logistical hassles are felt by many Americans, the experience of travel makes overcoming those challenges worth it, as evidenced by their vacation usage being the same or close to average. America\u2019s demanding work culture has given rise to the idea of a \u201cworkcation.\u201d A workcation is not simply spending time working during a vacation. Rather, it is a proactive decision to travel somewhere with the intent to work a regular schedule remotely. For some employees, it is an option that allows them to enjoy a destination without having it count against their vacation time. For others, though, it may be further evidence of work martyrdom. Debate aside, are Americans actually taking workcations? And, more importantly, do they want to? At first glance, the data says no: only 10 percent of Americans have taken a workcation. Twenty-nine percent of all workers describe the idea as appealing, where 70 percent called the concept unappealing. But the proof may be in the pudding. The 10 percent of employees who reported taking a workcation report a much stronger affinity for the idea, with 55 percent of this experienced group calling it appealing. Digging deeper into the numbers shows there may be more to workcations as workplace demographics change. Millennials\u2014and to a lesser extent Generation X\u2014have a much keener interest in the idea of a workcation. Nearly four-in-ten (39%) Millennials say they find the idea of a workcation appealing. From there, interest declines as age rises: 28 percent of Generation X and 18 percent of Boomers feel that a workcation is appealing. Time will tell if Millennials interest signals the start of a trend or just the result of being a hyper-connected generation with fewer vacation days earned. The tenuous appeal of a workcation may be the result of Americans not satisfying their wanderlust. Where the average employee is taking 17.2 days of vacation, less than half of that time\u2014just eight days\u2014is used for travel. Nearly a quarter (23%) said they used none of their time off to travel. It is not for lack of caring. The majority (84%) still say it is important to them that they use their time off to travel. \u00a0 Millennials use a greater proportion of their days for travel than Generation X or Boomers, but take fewer days than older generations as a result of earning fewer days. The average Millennial takes 14.5 days, 7.1 of which are used to travel. Generation X uses 17.9 days and dedicates 8.2 of those for travel. Boomers take the most days off at 19.8, and spend 9.0 days on travel vacations. It follows that a staggering 86 percent of Americans say they have not seen enough of their own country. With more than 400 sites, one of the best and most time-honored ways to see the U.S. is its robust system of national parks. But of the 59 national parks in the U.S., only 19% of Americans have seen more than five. More than a quarter (26%) say they have never been to a national park. The majority (54%) say they have visited two or fewer. The nearly half (48%) of Americans who are not using a majority of their vacation for travel are missing out on more than sun and fun. Americans taking all or most of their vacation days to travel\u2014or mega-travelers\u2014report dramatically higher rates of happiness than those using little to none of their time for travel\u2014or homebodies. While it may not be surprising that mega-travelers are happier with how they spend their paid time off compared to homebodies (76% to 48%), there are some significant differences that show that travel\u2014not simply taking time off for any purpose\u2014is how to get the greatest benefit out of vacation time. The employees spending more of their vacation time traveling may also be more successful when they are in the office. More than half (52%) of mega-travelers reported receiving a promotion in the last two years compared to Americans who use some (44%) or little to none (44%) of their time to travel. Mega-travelers also report a higher likelihood of receiving a raise, bonus, or both than homebodies. Employees who shared they use little to none of their vacation time for travel were five percentage points less likely than those who use all or most of their vacation time for travel to report a raise or bonus in the last three years (81% to 86%). All vacation days are not equal. Travel is the clearest way to achieve the benefits of time off. Yet far too many Americans are using precious few of their vacation days for getting away, leaving an unexplored nation that defies its very founding. The ripple effects of America\u2019s travel deficit are about far more than unrealized days and dollars. They are about missing out on the experiences, moments, and memories that define us. That is an unacceptable outcome\u2014but only if it is allowed to continue. \n      Press Releases     The State of American Vacation 2018 results show an increase in vacation and evidence there may be a right way to spend your vacation days.\u00a0 \n      Blog     Are workcations the next big trend in travel or substitute for an under-vacationed nation?\u00a0 \n      Infographics     The State of American Vacation 2018 by the numbers.\u00a0  \u00a9 U.S. Travel Association","time":1525808292,"title":"State of American Vacation 2018","type":"story","url":"https:\/\/projecttimeoff.com\/reports\/state-of-american-vacation-2018\/","label":7,"label_name":"random"},{"by":"jashkenas","descendants":0,"id":17024101,"kids":"None","score":1,"text":"The open source movement has fundamentally changed how software gets written. But in the process, corporations have been given a vast gift of free labor. May 7, 2018 \n Open source is a movement within software where source code is freely shared among a community that takes a more collective responsibility for improving a software project. Open source has become pervasive throughout the tech industry because of the sheer number of high-quality components to use as a starting point for any new project, free (as in beer) of charge. Unlike a prior era when many of those building blocks (when they existed at all) would have been offered as a proprietary, commercial offering, where companies were locked into onerous terms and couldn\u2019t generally do as they pleased with the software they licensed. But \u201copen source\u201d is only a more recent business-friendly spin given to a much older and more radical tradition, the Free Software Movement. Free Software rests on the idea that software users have \u201cthe freedom to run, copy, distribute, study, change and improve the software\u201d. This definition rests solely on what users (which includes other software developers) may do with a computer program, and what privileges they ought to enjoy. These conditions, emphasized in the four essential freedoms baked into every Free Software license, allow computer programmers to create a commons and protect it from enclosure by private companies who would not otherwise contribute back their improvements and enhancements in-kind. Part of why this works is that Free Software comes with a critique of power: With proprietary software, the program controls the users, and some other entity (the developer or \u201cowner\u201d) controls the program. So the proprietary program gives its developer power over its users. But there is a striking absence, in traditional discussion of the philosophy behind free software, of a much more salient power asymmetry: that between software developers and the private firms that generate wealth on top of ecosystems of free and open source software. Companies do sometimes pay people to work on open source projects that they derive benefit from. And there are some opportunities from academia, the public sector, and grants. But open source software development depends upon large amounts of unpaid labor. One of the most important reasons is that software projects in their infancy are much less interesting to companies which care more about established, proven solutions that have had their bugs squashed and features built up. Larger companies with more resources tend to be even more conservative about when to adopt new technology. The experimentation necessary to refine a project into something production-ready is more than a single project, as every new piece of software builds on what came before. Most projects do not become popular enough to attract attention from funding sources, even if many projects depend on them upstream. Even when money does arrive for open source projects, it tends to arrive too late in the process, after the technology is already proven. This means that successful open source projects will tend to be started by people who can dedicate large amounts of time to unpaid work, sustaining a situation where open source contributors are overwhelmingly white and male. Employers increasingly expect the software developers they hire to have a large portfolio of published open source software. These very same employers rarely permit their own workers the autonomy to publish open source projects in the course of their duties with the company, meaning that workers will have to put in extra hours outside of work to remain competitive in the market. Some people are able to fund their work on open source by running a consulting business on the side, after having built a name for themselves through their open source work. Some others are able to live off donations. But these are not models that scale, and these sources of income can be precarious. You can be fairly well known in programming circles but still struggle to find gigs. So why do we give away our software for free to companies that exploit us and our work? It can be fun to participate in open source ecosystems\u2014you can make friends, and build software you are proud of. But free software, by itself, is not an emancipatory political movement. How can we build up this digital peer-production commons while surviving the ravages of late capitalism? One idea is to alter the terms of open source licensing in order to preserve the parts of open source that we like, such as commons-based peer-production and free association, while also addressing how to get paid for our labor. Free and open source software licenses form a continuum between \u201cpermissive\u201d and \u201ccopyleft\u201d. Permissive licenses largely permit use in proprietary, closed-source codebases common to private companies. Copyleft licenses, meanwhile, mandate that any codebase that uses a copyleft software library be licensed under the copyleft license or another compatible license. Permissive licenses are overwhelmingly favored by private companies that produce proprietary software because they are under no contractual obligation to contribute their changes back upstream to the commons. Some businesses figured out that because idealistic copyleft licenses are considered toxic to many companies, they could sell permissive licenses as a form of market segmentation. New initiatives such as license zero1 create a streamlined way for companies to purchase licenses for proprietary or commercial use. For individual software developers or those working in coops or other groups, this may provide a way to build a sustainable business around an open source ecosystem. Software developers who are currently employed can pressure management at their companies to produce clear policies for contributing to existing open source projects and creating new ones on company time. And please consider the totality of someone\u2019s work experience when hiring, not only what shows up on GitHub. There are many tactics at our disposal, and we should be clear about the changes we want to see. In the long term, this means the abolition of copyright, building technology for people over profit, and providing for everyone\u2019s needs. But until then, we can ensure that everyone who works to create and maintain the open source ecosystems is paid for their work. We can also work toward welcoming more people from more diverse backgrounds into the computer software profession and our open source ecosystems without letting private companies turn our communities into a gatekeeping exercise. Disclaimer: the creator of license zero is my coop\u2019s lawyer.\u00a0\u21a9 James Halliday is a programmer with hundreds of open source modules and a member of bits.coop. Twitter: @substack \n                    no politics without inquiry\n                ","time":1525808258,"title":"Open Source Is Not Enough","type":"story","url":"http:\/\/notesfrombelow.org\/article\/open-source-is-not-enough","label":7,"label_name":"random"},{"by":"skybrian","descendants":0,"id":17024097,"kids":"None","score":4,"text":"The other day, I ran a photograph of my penis potato through an AI to see what the technology would call it. Would it know my potato was a potato despite the penis-like protuberance that had earned the tuber its nickname? I asked the system to look at several different shots of my penis potato\u200a\u2014\u200awith flash and without, indoor and outdoor and at a variety of angles\u200a\u2014\u200aand obtained answers that ranged from \u2018butternut squash\u2019 to \u2018dough,\u2019 even when I shot the potato in such a way that its private parts were demurely hidden. What I didn\u2019t know at the time was that the system would never identify my potato as a potato, not because of its penis, but because it did not know about potatoes at all. The AI I was using is famous in certain circles and performed remarkably well in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) a few years back. For those unfamiliar with this contest, I\u2019d compare it to the Kentucky Derby, well known and equally exciting to those interested in horses and racing. The AI is called VGG-19, and ImageNet, the image data set used to train and test it, is famous in its own right, providing thousands of labeled images that researchers can use. As of now, according the site, ImageNet contains over fourteen million images for over 21,000 categories of things; however, only 1000 of these categories are used in the competition\u200a\u2014\u200a1000 synsets, as they are called, of which \u2018potato\u2019 is not one. In other words, because the AI had been trained on the ILSVRC contest data set, no matter how often I showed the system a potato or how perfect that potato\u2019s form, the system would never come up with the word \u2018potato\u2019 because it had not been taught to identify it. Potato is not one of the 1000. I took a look at the 1000 categories and learned that the AI ought to be able to identify a brassiere, a rifle, a feather boa, lotion, a manhole cover, a computer mouse, a pinwheel, a prayer rug, a whiskey jug. Mostly, as I looked through the list of the chosen, I wondered, why these? A paper on the contest explains that the 1000 contest categories were chosen randomly in 2010, the first year of the contest, and then curated to ensure they were not too obscure and that there was no overlap between any two categories in the hierarchy (if \u2018hound\u2019 is included, then \u2018hunting dog\u2019 (superclass) or \u2018beagle\u2019 (subclass) is not. Over the years, the set has changed, but 639 of the categories have been used in every challenge (through 2015, at least, the year the paper was published). In other words, I had randomness to thank for why the model could identify a brassiere, but not a painting, a pomegranate, but not a kiwi, a mashed potato, not my potato, a bookshop, but not a book. Wait\u2026 no books? I showed the AI a couple photographs of a book and I was told that it was an \u2018eraser\u2019 or \u2018envelope\u2019, or sometimes, more eerily, something that was in the photograph but that I hadn\u2019t really noticed at all. When I showed the AI a woman holding a stack of books, for example, it classified the photograph as a \u2018jersey\u2019. In another photograph, the AI looked right past the man reading, but identified the shelves of books in the background as a bookshop. The thing the AI predicts most confidently is not incorrect, it\u2019s just not necessarily where my eye is drawn when I look at the photo. To be clear, I do not expect the AI to see something it does not know about, and yet it is interesting to think about what we are teaching our technologies and what they are blind to because we have not shown them examples. I was able to spy on the AI to see where it looked thanks to the work of researchers at Virginia Tech and Georgia Institute of Technology, who built Guided Grad-CAM, which I used to generate the illustrations above. The Guided Grad-CAM technology has a number of interesting applications, but the one that struck me most was identifying bias based on the direction of the AI\u2019s gaze. In the example below, the biased model notices a female face and identifies her as a nurse. The biased model learned its bias from the data it was trained on, the researchers write, but using \u201cthe insights gained from the Grad-CAM visualizations, we balanced the dataset and retrained the model.\u201d The unbiased model looks at the stethoscope instead of the face when it determines the profession. In another thought-provoking study, researchers at Facebook looked at how well our technologies are doing at classifying objects, and where they are failing, with a special look at basketballs and the images used to illustrate them in ImageNet. Though the percentage of images they looked at in which at least one white or one black person appear is very similar (55% for the former, 53% for the latter), a model trained on this data behaves in an unexpected way. When asked to make predictions on a set of images selected so that \u2018the primary apparent difference between the two images is the skin color of the persons\u2019, the model demonstrates this pattern: \u201cAll images containing a black person are classified as basketball while similar photos with persons of different skin color are labeled differently\u201d (as a Volleyball, ping-pong ball, or baseball player, for example). In other words, the model has learned a bias. The researchers note: The reasons why the model learns these biases are unclear. One hypothesis is that despite the balanced distribution of races in pictures labeled basketball, blacks persons are more represented in this class in comparison to the other classes. I read the above passage a few times and thought about the implications. Who exactly is represented in ImageNet and who is not, I wondered. And what patterns are our technologies picking up simply because of who and what is included (or omitted) in the data? Our models are great at identifying patterns, only some of these patterns are not intended, or worse, harmful. Unsettling decisions by algorithms have appeared in the world of beauty contests, facial recognition, and natural language processing, among other areas. Last year, researchers found significant gender bias in several of our large image data sets. AIs trained on these images have not only picked up on but amplified these biases, associating women with things like kitchens and men with sporting goods (see the story in Wired). I appreciate the tremendous efforts that goes into creating and maintaining the image data sets that we rely on to train and test our AIs. Without ImageNet and the competition around it, I do not think our technologies would have evolved as rapidly. Still, I have not come across a study relating to skin color and representation in this data set, and I am concerned about what it means to associate human skin tones with some types of things and not others. Repeatedly, I hear image data called \u2018real world data\u2019. But do the data reflect reality? And how can we make sure that the data are not simply reinforcing the biases that we as a society must strive to conquer as well? * * * Further reading: \u201cGrad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization,\u201d Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra https:\/\/arxiv.org\/abs\/1610.02391 \u201cConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism,\u201d Pierre Stock, Moustapha Cisse https:\/\/arxiv.org\/abs\/1711.11443 \u201cImageNet Large Scale Visual Recognition Challenge,\u201d Olga Russakovsky* \u00b7 Jia Deng* \u00b7 Hao Su \u00b7 Jonathan Krause \u00b7 Sanjeev Satheesh \u00b7 Sean Ma \u00b7 Zhiheng Huang \u00b7 Andrej Karpathy \u00b7 Aditya Khosla \u00b7 Michael Bernstein \u00b7 Alexander C. Berg \u00b7 Li Fei-Fei https:\/\/arxiv.org\/pdf\/1409.0575.pdf By clapping more or less, you can signal to us which stories really stand out. Writer of fiction and technical docs Sharing concepts, ideas, and codes.","time":1525808247,"title":"What Does It See? The troubled gaze of an artificial intelligence","type":"story","url":"https:\/\/towardsdatascience.com\/what-does-it-see-f2dcd9dff9af","label":5,"label_name":"ml"},{"by":"ChicagoDave","descendants":1,"id":17024091,"kids":"[17024128]","score":3,"text":"Durable Functions is an extension of Azure Functions and Azure WebJobs that lets you write stateful functions in a serverless environment. The extension manages state, checkpoints, and restarts for you. The extension lets you define stateful workflows in a new type of function called an orchestrator function. Here are some of the advantages of orchestrator functions: Note Durable Functions is an advanced extension for Azure Functions that is not appropriate for all applications. The rest of this article assumes that you have a strong familiarity with Azure Functions concepts and the challenges involved in serverless application development. The primary use case for Durable Functions is simplifying complex, stateful coordination problems in serverless applications. The following sections describe some typical application patterns that can benefit from Durable Functions. Function chaining refers to the pattern of executing a sequence of functions in a particular order. Often the output of one function needs to be applied to the input of another function.  Durable Functions allows you to implement this pattern concisely in code. The values \"F1\", \"F2\", \"F3\", and \"F4\" are the names of other functions in the function app. Control flow is implemented using normal imperative coding constructs. That is, code executes top-down and can involve existing language control flow semantics, like conditionals, and loops.  Error handling logic can be included in try\/catch\/finally blocks. The ctx parameter (DurableOrchestrationContext) provides methods for invoking other functions by name, passing parameters, and returning function output. Each time the code calls await, the Durable Functions framework checkpoints the progress of the current function instance. If the process or VM recycles midway through the execution, the function instance resumes from the previous await call. More on this restart behavior later. Fan-out\/fan-in refers to the pattern of executing multiple functions in parallel, and then waiting for all to finish.  Often some aggregation work is done on results returned from the functions.  With normal functions, fanning out can be done by having the function send multiple messages to a queue. However, fanning back in is much more challenging. You'd have to write code to track when the queue-triggered functions end and store function outputs. The Durable Functions extension handles this pattern with relatively simple code. The fan-out work is distributed to multiple instances of function F2, and the work is tracked by using a dynamic list of tasks. The .NET Task.WhenAll API is called to wait for all of the called functions to finish. Then the F2function outputs are aggregated from the dynamic task list and passed on to the F3 function. The automatic checkpointing that happens at the await call on Task.WhenAll ensures that any crash or reboot midway through does not require a restart of any already completed tasks. The third pattern is all about the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having the long-running action triggered by an HTTP call, and then redirecting the client to a status endpoint that they can poll to learn when the operation completes.  Durable Functions provides built-in APIs that simplify the code you write for interacting with long-running function executions. The samples show a simple REST command that can be used to start new orchestrator function instances. Once an instance is started, the extension exposes webhook HTTP APIs that query the orchestrator function status. The following example shows the REST commands to start an orchestrator and to query its status. For clarity, some details are omitted from the example. Because the state is managed by the Durable Functions runtime, you don't have to implement your own status tracking mechanism. Even though the Durable Functions extension has built-in webhooks for managing long-running orchestrations, you can implement this pattern yourself using your own function triggers (such as HTTP, queue, or Event Hub) and the orchestrationClient binding. For example, you could use a queue message to trigger termination.  Or you could use an HTTP trigger protected by an Azure Active Directory authentication policy instead of the built-in webhooks that use a generated key for authentication.  The DurableOrchestrationClient starter parameter is a value from the orchestrationClient output binding, which is part of the Durable Functions extension. It provides methods for starting, sending events to, terminating, and querying for new or existing orchestrator function instances. In the previous example, an HTTP triggered-function takes in a functionName value from the incoming URL and passes that value to StartNewAsync. This binding API then returns a response that contains a Location header and additional information about the instance that can later be used to look up the status of the started instance or terminate it. The monitor pattern refers to a flexible recurring process in a workflow - for example, polling until certain conditions are met. A regular timer-trigger can address a simple scenario, such as a periodic cleanup job, but its interval is static and managing instance lifetimes becomes complex. Durable Functions enables flexible recurrence intervals, task lifetime management, and the ability to create multiple monitor processes from a single orchestration. An example would be reversing the earlier async HTTP API scenario. Instead of exposing an endpoint for an external client to monitor a long-running operation, the long-running monitor consumes an external endpoint, waiting for some state change.  Using Durable Functions, multiple monitors that observe arbitrary endpoints can be created in a few lines of code. The monitors can end execution when some condition is met, or be terminated by the DurableOrchestrationClient, and their wait interval can be changed based on some condition (i.e. exponential backoff.) The following code implements a basic monitor. When a request is received, a new orchestration instance is created for that job ID. The instance polls a status until a condition is met and the loop is exited. A durable timer is used to control the polling interval. Further work can then be performed, or the orchestration can end. When the ctx.CurrentUtcDateTime exceeds the expiryTime, the monitor ends. Many processes involve some kind of human interaction. The tricky thing about involving humans in an automated process is that people are not always as highly available and responsive as cloud services. Automated processes must allow for this, and they often do so by using timeouts and compensation logic. One example of a business process that involves human interaction is an approval process. For example, approval from a manager might be required for an expense report that exceeds a certain amount. If the manager does not approve within 72 hours (maybe they went on vacation), an escalation process kicks in to get the approval from someone else (perhaps the manager's manager).  This pattern can be implemented using an orchestrator function. The orchestrator would use a durable timer to request approval and escalate in case of timeout. It would wait for an external event, which would be the notification generated by some human interaction. The durable timer is created by calling ctx.CreateTimer. The notification is received by ctx.WaitForExternalEvent. And Task.WhenAny is called to decide whether to escalate (timeout happens first) or process approval (approval is received before timeout). An external client can deliver the event notification to a waiting orchestrator function using either the built-in HTTP APIs or by using DurableOrchestrationClient.RaiseEventAsync API from another function: Behind the scenes, the Durable Functions extension is built on top of the Durable Task Framework, an open-source library on GitHub for building durable task orchestrations. Much like how Azure Functions is the serverless evolution of Azure WebJobs, Durable Functions is the serverless evolution of the Durable Task Framework. The Durable Task Framework is used heavily within Microsoft and outside as well to automate mission-critical processes. It's a natural fit for the serverless Azure Functions environment. Orchestrator functions reliably maintain their execution state using a cloud design pattern known as Event Sourcing. Instead of directly storing the current state of an orchestration, the durable extension uses an append-only store to record the full series of actions taken by the function orchestration. This has many benefits, including improving performance, scalability, and responsiveness compared to \"dumping\" the full runtime state. Other benefits include providing eventual consistency for transactional data and maintaining full audit trails and history. The audit trails themselves enable reliable compensating actions. The use of Event Sourcing by this extension is transparent. Under the covers, the await operator in an orchestrator function yields control of the orchestrator thread back to the Durable Task Framework dispatcher. The dispatcher then commits any new actions that the orchestrator function scheduled (such as calling one or more child functions or scheduling a durable timer) to storage. This transparent commit action appends to the execution history of the orchestration instance. The history is stored in a storage table. The commit action then adds messages to a queue to schedule the actual work. At this point, the orchestrator function can be unloaded from memory. Billing for it stops if you're using the Azure Functions Consumption Plan.  When there is more work to do, the function is restarted and its state is reconstructed. Once an orchestration function is given more work to do (for example, a response message is received or a durable timer expires), the orchestrator wakes up again and re-executes the entire function from the start in order to rebuild the local state. If during this replay the code tries to call a function (or do any other async work), the Durable Task Framework consults with the execution history of the current orchestration. If it finds that the activity function has already executed and yielded some result, it replays that function's result, and the orchestrator code continues running. This continues happening until the function code gets to a point where either it is finished or it has scheduled new async work. The replay behavior creates constraints on the type of code that can be written in an orchestrator function. For example, orchestrator code must be deterministic, as it will be replayed multiple times and must produce the same result each time. The complete list of constraints can be found in the Orchestrator code constraints section of the Checkpointing and restart article. Currently C# (Functions v1 and v2) and JavaScript (Functions v2 only) are the only supported languages for Durable Functions. This includes orchestrator functions and activity functions. In the future, we will add support for all languages that Azure Functions supports. See the Azure Functions GitHub repository issues list to see the latest status of our additional language support work. The Durable Functions extension automatically emits structured tracking data to Application Insights when the function app is configured with an Application Insights instrumentation key. This tracking data can be used to monitor the behavior and progress of your orchestrations. Here is an example of what the Durable Functions tracking events look like in the Application Insights portal using Application Insights Analytics:  There is a lot of useful structured data packed into the customDimensions field in each log entry. Here is an example of one such entry fully expanded.  Because of the replay behavior of the Durable Task Framework dispatcher, you can expect to see redundant log entries for replayed actions. This can be useful to understand the replay behavior of the core engine. The Diagnostics article shows sample queries that filter out replay logs so you can see just the \"real-time\" logs. The Durable Functions extension uses Azure Storage queues, tables, and blobs to persist execution history state and trigger function execution. The default storage account for the function app can be used, or you can configure a separate storage account. You might want a separate account due to storage throughput limits. The orchestrator code you write does not need to (and should not) interact with the entities in these storage accounts. The entities are managed directly by the Durable Task Framework as an implementation detail. Orchestrator functions schedule activity functions and receive their responses via internal queue messages. When a function app runs in the Azure Functions Consumption plan, these queues are monitored by the Azure Functions Scale Controller and new compute instances are added as needed. When scaled out to multiple VMs, an orchestrator function may run on one VM while activity functions it calls run on several different VMs. You can find more details on the scale behavior of Durable Functions in Performance and scale. Table storage is used to store the execution history for orchestrator accounts. Whenever an instance rehydrates on a particular VM, it fetches its execution history from table storage so that it can rebuild its local state. One of the convenient things about having the history available in Table storage is that you can take a look and see the history of your orchestrations using tools such as Microsoft Azure Storage Explorer.  Warning While it's easy and convenient to see execution history in table storage, avoid taking any dependency on this table. It may change as the Durable Functions extension evolves. All known issues should be tracked in the GitHub issues list. If you run into a problem and can't find the issue in GitHub, open a new issue and include a detailed description of the problem. Continue reading Durable Functions documentation Install the Durable Functions extension and samples \r\nWhat type of feedback would you like to provide?\t\t Our new feedback system is built on GitHub Issues. For more information on this change, please read our blog post. Loading feedback...","time":1525808230,"title":"Azure Durable Functions","type":"story","url":"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/durable-functions-overview","label":3,"label_name":"dev"},{"by":"thetall0ne","descendants":0,"id":17024083,"kids":"None","score":3,"text":"Whether you\u2019re a new startup or an existing business, here\u2019s one way you can get an AI-enabled product or service into production in 1 week or less. And you certainly don\u2019t have to just take my word for it, I\u2019ll share with you some tools you can use to really speed things up. Pick a problem that can be solved with machine learning. This sounds obvious, but believe me it isn\u2019t always. I\u2019ve written a lot about good and bad use cases for machine learning, so have a look at these or do some research and make sure you\u2019re tackling something that is possible. Think tagging product images, recommending content, making data more searchable or classifying Tweets, not predicting the stock market or building a self-driving car. Let\u2019s take the idea of a news aggregation website that lets your users search by topics, people, and content. First, sit down and decide what categories of news you want your users to be able to browse through. Perhaps they are science, sports, politics, health, technology, and crime. You could spend the rest of your first day gathering data. Go out and grab as many articles as you can in those categories and organize them into folders with the names of the categories. Make sure you have 100\u2013500 (the more the better) examples of articles in each category. Now comes the fun part. Visit https:\/\/machinebox.io and download Classificationbox. You\u2019ll have to sign up with your e-mail, install Docker, and run a Terminal command. Shouldn\u2019t take more than a few minutes. Once Classificationbox is up and running, clone this open source tool that lets you create a machine learning model from text files on your computer. Run textclass on your folders with the sample articles in them. After a few seconds, you\u2019ll have a machine learning model that can automatically classify news articles. Assuming the data set was good, your accuracy should be above 90%. If it isn\u2019t, consider cleaning the data set a bit or simply adding another 100 samples or so to each category. It\u2019s not even 10am and you\u2019ve already got a machine learning model ready to go into production. But let\u2019s keep going. The next thing you\u2019re going to want to do is make your articles searchable, but as you start aggregating millions of articles, you\u2019ll quickly hit a wall if you\u2019re trying to index every word. Further more, if you search for the word Radcliffe you want to be able to return results that are relevant, whether that person is searching for the place or news about Daniel Radcliffe. Once again, visit Machine Box and download Textbox. Textbox is a keyword and named entity extraction tool that will give you a nice bit of metadata for every news article that will work well in Elastic Search. There\u2019s some great detail on how to do this here, but the highlights are: Articles sometimes come with photos of people and places. You might want to be able to tag people in articles, and tag things found in images to further improve search and categorization. Machine Box has two tools that make this really simple; Facebox and Tagbox. Go ahead and download both. Although Tagbox is teachable with custom labels, it also comes pretrained with tens of thousands of labels so it is really easy to start with without having to put in any work. When you start writing the web services part of the app, you can easily just pass every photo into Tagbox and store the response in Elastic Search. You might even just decide to pick the top two tags, or tags above a certain confidence. You might also want to start recognizing people in photos inside the articles. The first step is to pass every photo into Facebox, and then store the Faceprint in your database. You can read more about how the Faceprint works here. Once you\u2019ve built up a nice dataset of Faceprints, you can, at any point, go through and teach Facebox who these people are. It will not only recognize these people you teach it in new photos you give to it, but you can apply the learning back through time to the photos you\u2019ve already processed by simply updated the relationship of the Faceprint to name in your database. Now that you\u2019ve got the machine learning models ready to go, you could start to build the web app in the afternoon of Day 3. Spend these two days building the web app, aggregating news stories, deciding how to display the information to your customers, integrating Elastic Search, and maybe handling customer logins. Then, build a flow for every new article to pass through the various Machine Box boxes, and handle the output. Deploy to your favorite hosting site. Now you\u2019ve got a product that has multiple machine learning models integrated, doing everything from auto-categorizing news articles based on how they\u2019re written, to improving search using people, places, things, and more, both in photos and in the content of the article itself. And there is a lot more you can do. You could start recommending articles that people are more likely to click on based on who they are, detect and weed out fake news, or build your own language detection model to engage auto-translation. The point is, you should be able to launch a product or service with AI in less than a week, even if you\u2019re starting from scratch. Machine Box puts state of the art machine learning capabilities into Docker containers so developers like you can easily incorporate natural language processing, facial detection, object recognition, etc. into your own apps very quickly. The boxes are built for scale, so when your app really takes off just add more boxes By clapping more or less, you can signal to us which stories really stand out. Co-founder and CEO Machine Box, Inc. | Machine Learning Superfan | Agile Product Owner | Author | Father | Amateur Programmer Sharing concepts, ideas, and codes.","time":1525808206,"title":"Show HN: Launch with AI in a week or less","type":"story","url":"https:\/\/towardsdatascience.com\/launch-with-ai-in-1-week-a4f4f45cc177","label":5,"label_name":"ml"},{"by":"jduyan","descendants":0,"id":17024067,"kids":"None","score":1,"text":"","time":1525808110,"title":"Video: CTRL-labs Announces Developer Kits at O'Reilly AI Keynote","type":"story","url":"https:\/\/www.youtube.com\/watch?v=5Z5aZK2C3ew&feature=youtu.be","label":7,"label_name":"random"},{"by":"joak","descendants":0,"id":17024066,"kids":"None","score":1,"text":"Uber has teamed up with Karem Aircraft to develop electric vertical takeoff and landing (eVTOL) vehicles for the ride-hailing company\u2019s upcoming flying taxi service, the companies announced today at Uber Elevate. Karem Aircraft, which has patented Optimum Speed Tiltroter technology for military and commercial applications, has been working with Uber  for about a year to create the Butterfly concept. This type of vehicle is supposed to be a passenger-friendly adaptation of Karem\u2019s core technology. \u201cWe were always dreaming of doing things commercially, but all of our funding came from the military,\u201d Karem Aircraft founder Abe Karem told TechCrunch ahead of the announcement. \u201cWhat we were doing was advanced and labeled \u2018risky.'\u201d Now, Karem is able to do what people previously thought was impossible, Karem said. The Butterfly (rendered above) is a quad tiltrotor with four large rotors mounted on the wings and tail. The idea is to combine the vertical lift capability of a helicopter with the speed and range of a fixed-wing aircraft. The Butterfly is also designed to be more efficient as a result of its rotors with variable RPM. \u201cVariable RPM allows us to maintain good efficiency across a wide range of rotor thrust,\u201d Karem Aircraft CEO Ben Tigner told me. This partnership comes a little over one year after Uber announced a number of vehicle partnerships with established aeronautics and VTOL manufacturers at last year\u2019s Elevate event. Other partners include Aurora Flight Sciences, Embraer, Bell Helicopter, Pistrel Aircraft, Mooney and ChargePoint. That\u2019s because Uber itself isn\u2019t building any vehicles \u2014 it\u2019s relying on its partners to do that. Earlier today, Uber unveiled its common reference model design concepts, with the goal to encourage companies and eVTOL manufacturers to design prototypes with uberAIR in mind. For example, the design model requires the propeller blades to be as high as possible in order to ensure people don\u2019t have to duck while they\u2019re boarding and exiting the aircraft. As long as vehicle manufacturers can adhere to Uber\u2019s common reference designs, they will be eligible to participate in Uber Elevate. By 2020, Uber envisions having multiple vehicle partners ready, Uber Head of Aviation Eric Allison told me, \u201cbut we\u2019re not going to launch them if they\u2019re not ready.\u201d The idea with Uber Elevate is to create an ecosystem with partners across the entire spectrum \u2014 batteries, skyports, vehicles and so forth, Allison said. \u201cWe believe that this is a potentially huge market and it\u2019s not just about the ecosystem,\u201d he said. \u201cYou need the right ground infrastructure, as well as vehicles to make the overall system be much more useful on a larger scale than small plane aviation is today.\u201d Uber Elevate\u2019s ultimate goal is to launch and operate a ridesharing network of small, electric aircrafts worldwide that can carry four people at any given time.  Other fun facts: Given that the airspace is much more regulated, Uber is prepared to create core systems that enable the entire ecosystem to operate. That means developing an airspace management system that is a more complex version of what we know today as air traffic control. \u201cAir transport is much more regulated,\u201d Allison said, \u201cand needs to be much more highly coordinated. It can\u2019t be a total free-for-all in the sky.\u201d In order to achieve all of this, Uber will need skyports. Uber has also teamed up with real estate companies like Hillwood Properties and Sandstone Properties to create skyports for the uberAIR network. Earlier today, Allison showed off some early design concepts. Here\u2019s a quick look at one that could handle 1,000 landings per hour.  The below skyport, which Uber envisions sitting on top of a parking garage, could handle about 100 landings per hour.  I\u2019ll be at Uber Elevate today and tomorrow, so be on the lookout for more news. This is Uber\u2019s plan to deliver on flying \u2018cars\u2019 ","time":1525808108,"title":"Uber flying taxis","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/uberair-adds-another-flying-taxi-partner\/","label":7,"label_name":"random"},{"by":"jstanley","descendants":0,"id":17024065,"kids":"None","score":1,"text":"\n\n \n","time":1525808098,"title":"OpenRouteService is libre routefinding for OpenStreetMap","type":"story","url":"https:\/\/maps.openrouteservice.org\/","label":7,"label_name":"random"},{"by":"d2p","descendants":0,"id":17024057,"kids":"None","score":4,"text":"This week marks the release of the third beta for Flutter, our toolkit for building beautiful mobile UI for iOS and Android. In our announcement over at the Google Developer site, we provide a broader overview of the Flutter project and showcase some examples of how customers are using it to create amazing applications. In this blog post, we\u2019ll take a deeper look at the specific improvements we\u2019ve made in Beta 3 itself. As mentioned when we shipped our first beta at Mobile World Congress in February, our intent is to continue to ship releases that are at least of beta quality on approximately a monthly cadence. This release (v0.3.2) is the third release in that lineage, and demonstrates continued progress towards completing the 1.0 release. Our work in this release has focused on three primary areas: fundamentals, ecosystem and tooling. Let\u2019s start with the fundamentals, where we\u2019ve improved the built-in UI widgets, completed the remaining feature work for Dart 2, and introduced a new embedding API. In Flutter Beta 3, we\u2019ve made a number of improvements to the Material Design widgets to enable greater flexibility and customization. For example: Additionally, we\u2019ve added support for animated resources in formats like GIF and WebP, ready for your best meme apps to be published to the app stores! We also made a number of updates to the Flutter Gallery application to demonstrate these and other changes. For example: We\u2019ve completed the work to enable Dart 2, our reboot of the Dart language that is optimized for client development. As of this release, Dart 2 is feature complete and enabled by default. In this release, Dart 2 adds some syntactic sugar to help with instantiating widgets in Flutter. The new keyword is now entirely optional: there should be no need to use it any longer, and it\u2019s a bug if the compiler complains. A build() method like this is easier than ever to write: The const keyword becomes optional for any child constructors within an existing const scope. During the first two betas, we identified scenarios where it was difficult to infer from the context whether an object\u2019s children are immutable, so we require the const keyword at the top level to make the constant nature of the code explicit. Once you\u2019ve declared a section as const, however, the children are automatically const without requiring further declaration, so now a statement like the following is valid: In other areas, we\u2019ve made improvements to the accessibility support for apps that use Flutter, including improving support for screen readers, large text and contrast capabilities, as well as starting to document our accessibility support. We\u2019re also ready for apps that are offered in languages with right-to-left scripts. In addition to supporting right-to-left text, controls mirror where appropriate (for example the left \u2018back\u2019 button has inverted direction and justification in languages like Arabic). We\u2019ve also rewritten the Flutter engine\u2019s threading model to make it possible to host multiple FlutterViews within a single application. This is part of our larger focus on making it easier to add Flutter to your existing app\u200a\u2014\u200awork that continues in progress. We launched an initial suite of Firebase plugins at Google I\/O last year. Several of those plugins are reaching their 1.0 milestone this week: Realtime Database, Firebase Analytics, Firebase Messaging, and Firebase Core. In addition, we have added new, fully-featured plugins for Remote Config, Cloud Firestore, and Performance Monitoring. For an overview of our Firebase support, please see the FlutterFire page. Our support for ads powered by AdMob by Google is graduating to beta, enabling you to monetize your Flutter-based applications. The AdMob plugin supports loading and displaying banner, interstitial (full-screen), and rewarded video ads using the AdMob API. There are many other packages that have recently been made available for Flutter, some contributed by the Flutter team directly, others by community members. As mentioned in our I\/O announcement post, Flutter is a first-class toolkit for Material, which means the Material and Flutter teams have partnered to deliver even more support for Material Design. We continue to release regular updates to the Flutter plugin for Android Studio and IntelliJ to improve the development experience. In particular, we\u2019ve redesigned the UI Inspector with a new \u201cJust My Widgets\u201d feature that filters out auto-generated widgets. You can also run your Flutter app in profile mode, which adds frames-per-second and memory usage displays. Visual Studio Code is now considered a fully-supported development environment for Flutter, in addition to the Android Studio support mentioned above. Flutter support is enabled through the Flutter extension, available through the Visual Studio Marketplace. The latest version of our Visual Studio Code extension supports Flutter Beta 3 features including Dart 2, and the changelog is here. Other notable features in this release for both Android Studio and Visual Studio Code include a broader set of refactorings. This includes an Extract Widget refactor that creates a new Widget class and inserts a call to its constructor in the original position: There are a few ways to get in touch with us and find out what\u2019s going on with Flutter. By clapping more or less, you can signal to us which stories really stand out. Flutter is Google\u2019s mobile UI framework for crafting high-quality native interfaces on iOS and Android in record time. Flutter works with existing code, is used by developers and organizations around the world, and is free and open source. Learn more at https:\/\/flutter.io","time":1525808065,"title":"What\u2019s New in Flutter Beta 3?","type":"story","url":"https:\/\/medium.com\/flutter-io\/flutter-beta-3-7d88125245dc","label":3,"label_name":"dev"},{"by":"devkulkarni","descendants":0,"id":17024054,"kids":"None","score":2,"text":"This post analyzes potential value of the Operator Framework to the Kubernetes community. Operator Framework is an open source project which was released last week at KubeCon. CoreOS introduced the term Kubernetes Operator in Nov 2016. An Operator is an application-specific controller that extends Kubernetes to create, configure, and manage instances of complex applications. \u2018Kubernetes Operator\u2019 is not a native Kubernetes term; it primarily represents combination of two native Kubernetes terms: 1) Kubernetes CRD (Custom Resource Definition) and 2) Kubernetes Controller that manages CRD\/s. The Operator term has been well accepted by the community and a simple Github search on \u2018Kubernetes Operator\u2019 gives 186 repository results. Operators have been written for various platform elements such as etcd, Prometheus, Postgres, Elasticsearch, Kafka, Redis, Spark, etc. The Operator Framework aims to simplify building and managing Operators. It currently includes two components: The announcement mentions that soon an additional component for Operator Metering will be added to enable usage reporting. We see following two benefits of the Operator SDK. 1) Less code is required to get started with when developing an Operator. We did a quick comparison of the number of steps required when using manual method of creating CRD\/Operator code and when using the Operator SDK. The common steps between the two approaches are creating CRD type definitions and writing the Operator\u2019s reconciliation logic (the control loop). However, other steps from the manual method, such as creating the required directory structure, defining the registration and doc files, are automated by the SDK. 2) Abstracting details of the client-go library\u200a\u2014\u200aThe relationship between your Operator code, the Operator SDK, and the client-go library is shown in the following figure. The Operator SDK with its abstracted interface\u200a\u2014\u200aa call-back function with context and event as provided parameters\u200a\u2014\u200ahides the details which you otherwise have to know such as work queue, Indexer, Informer, Object key. We think that the main concern with the SDK is whether it will be sufficient for Operator developers to work only with the abstracted interface; or it will be beneficial for them to still have basic understanding of the client-go library and its semantics when developing and debugging their Operator code. Today, lifecycle management of an Operator can be done using kubectl or Helm. The lifecycle management of the application itself is part of the Operator\u2019s controller logic. Operator framework introduces two meta-operators (OLM Operator and Catalog Operator) and a custom resource (ClusterServiceVersion) towards lifecycle management of custom Operators. ClusterServiceVersion is essentially a manifest definition which is supposed to be used by Kubernetes administrators to define the composition of an Operator. It includes things such as\u200a\u2014\u200ahow an Operator should be deployed, what Custom Resource types the Operator is managing, what native Kubernetes resources will the Operator be using, etc. For a given Operator, OLM requires that the Custom Resource types managed by that Operator be already registered in the Cluster. Registering these types can be done using the Catalog Operator or by the Kubernetes administrator manually. OLM will instantiate your Operator only after this requirement is satisfied. These steps are shown in the following figure. It seems that the main advantage of using the Operator framework is that OLM can perform seamless upgrades of your Operator to newer versions. One of the challenges when it comes to Operator upgrades is, how to handle custom resource instances that were managed by the previous version of the Operator. Potentially, you can write your new Operator such that it is aware of the previous version and takes ownership of those instances. But this won\u2019t be a scalable approach as custom logic would need to be written for every upgrade (v\u2192v+1). As per the documentation, OLM solves this problem. It knows about the two versions and can update the ownership information for each custom resource instance to point to the new version of your Operator. Overall, the Operator Framework is a welcome contribution as it seems to be solving immediate problems of Operator developers and Kubernetes cluster administrators. Operator Framework seems to target two types of personas. SDK is targeting Operator developers and OLM is targeting Kubernetes cluster administrators. But what about the end users though? We believe that what is really missing in this space is how to improve the consumability of Operators by the end users. They are application developers who are building their application platforms on Kubernetes using Operators\/CRDs as their platform elements. A typical application platform these days can consist of more than one Operator, such as Prometheus, Postgres, Nginx, Fluentd, etc. In such situations, there is a genuine need to bring more consistency in the end user experience of the Operators by answering questions like\u200a\u2014\u200ahow can end users know about functions offered by new CRDs, what knobs are offered to them to customize the applications managed by Operators, how can they diagnose any issues in applications managed by Operators. www.cloudark.io By clapping more or less, you can signal to us which stories really stand out. Platform as Code","time":1525808043,"title":"Analyzing Value of Operator Framework for Kubernetes Community","type":"story","url":"https:\/\/medium.com\/@cloudark\/analyzing-value-of-operator-framework-for-kubernetes-community-5a65abc259ec","label":3,"label_name":"dev"},{"by":"alexandernst","descendants":3,"id":17024050,"kids":"[17024684, 17024423, 17024408]","score":8,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n     By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n    privacy statement. We\u2019ll occasionally send you account related emails. Already on GitHub? Sign in to your account Hi, I contribute to https:\/\/parceljs.org We noticed that the @parcel\/, @parceljs\/, and @parcel-bundler\/ npm scopes\/usernames are taken by members of the Webpack team. I sent a message the other day to the team member (@rynclark) who owns @parcel\/ requesting it for ourselves and they refused and then blocked me and other Parcel team members. Squatting on names like this isn't okay. It's not how we should behave in the open source ecosystem. I didn't expect this kind of hostility from the Webpack team. Could we please fix this? \/cc @webpack\/core-team @webpack\/contributor-team Looks like shame, sorry \/cc @webpack\/core-team @webpack\/contributor-team Looks like shame, sorry https:\/\/www.npmjs.com\/settings\/parcel\/members\nhttps:\/\/www.npmjs.com\/settings\/parceljs\/members\nhttps:\/\/www.npmjs.com\/settings\/parcel-bundler\/members Someone with admin access add thejameskyle and\/or devongovett and everything will be 200 OK. https:\/\/www.npmjs.com\/settings\/parcel\/members\nhttps:\/\/www.npmjs.com\/settings\/parceljs\/members\nhttps:\/\/www.npmjs.com\/settings\/parcel-bundler\/members Someone with admin access add thejameskyle and\/or devongovett and everything will be 200 OK. I would like to apologize for some members of webpack team. It is not possible to learn about what is happening in the minds of some people. I hope for an early solution of the problem. I would like to apologize for some members of webpack team. It is not possible to learn about what is happening in the minds of some people. I hope for an early solution of the problem. I second @evilebottnawi, as part of the webpack documentation team, hope this gets fixed soon. I second @evilebottnawi, as part of the webpack documentation team, hope this gets fixed soon. I'm locking this issue as we don't know, nor can we presume to the know the reasoning or motivation behind the conflict. Posting an issue instead of going through a myriad of other available channels to resolve this seems an awful lot like trying to shame the project into action. You could have chosen a better path, and your issue isn't going to do much to lessen the silly polarization and side-taking between the two projects. We'll address this internally, find out why it happened, what transpired during the requests, and go from there. I'd also like to note that if an individual chooses to act on their own, even if they've contributed to the project in the past, it does not constitute policy of the rest of contributors, past or present, nor of the actual core or contrib teams. And the project as an entity cannot compel an individual to act if they don't want to act. Neither webpack nor webpack-contrib orgs orchestrated this situation. cc: @TheLarkInn I'm locking this issue as we don't know, nor can we presume to the know the reasoning or motivation behind the conflict. Posting an issue instead of going through a myriad of other available channels to resolve this seems an awful lot like trying to shame the project into action. You could have chosen a better path, and your issue isn't going to do much to lessen the silly polarization and side-taking between the two projects. We'll address this internally, find out why it happened, what transpired during the requests, and go from there. I'd also like to note that if an individual chooses to act on their own, even if they've contributed to the project in the past, it does not constitute policy of the rest of contributors, past or present, nor of the actual core or contrib teams. And the project as an entity cannot compel an individual to act if they don't want to act. Neither webpack nor webpack-contrib orgs orchestrated this situation. cc: @TheLarkInn We do not own this package. I have already spoken with @devongovett about this. We can do our best at the very least and just as we have in the same way in the past, will not make any exceptions and help if we can. We do not own this package. I have already spoken with @devongovett about this. We can do our best at the very least and just as we have in the same way in the past, will not make any exceptions and help if we can.","time":1525808021,"title":"Webpack squatting on parcel scopes on npm","type":"story","url":"https:\/\/github.com\/webpack\/webpack\/issues\/7240","label":4,"label_name":"github"},{"by":"molecule","descendants":0,"id":17024045,"kids":"None","score":1,"text":"Two studies published in the past week have troubling implications for the effects hurricanes have on society\u00a0because of climate change, now and in the future. One directly links Hurricane Harvey\u2019s disastrous rains to the amount of heat stored in the ocean, which was record-setting before the storm plowed into Texas last year. The other shows an increasing trend in storms that are becoming really strong, really fast. Storms that unload more rain and explosively intensify\u00a0cause more destruction and suffering, as the 2017 Atlantic hurricane season painfully made clear. Harvey, Irma and Maria each\u00a0ranked among the five costliest hurricanes on record. These studies suggest that future storms\u00a0will carry even greater damage potential. Hurricanes are fueled by heat, and in late August, before Harvey struck, ocean heat content reached record high levels in the western Gulf of Mexico. The study determined that the energy released into the atmosphere from Harvey\u2019s rainfall matched the amount of energy which was removed from the ocean in the storm\u2019s wake. In other words, the study found that the amount of heat stored in the ocean is directly related to how much rain a storm can unload. The implication is that if\u00a0climate change, driven by increasing greenhouse gases from human activity, increases the heat content of the ocean, storms passing over it will be able to draw ever more moisture that they can unload as rain. Harvey dumped more than 60 inches of\u00a0rain in parts of Southeast Texas, the most ever recorded from a single storm in the United States in recorded history. \u201c[R]ecord high ocean heat values not only increased the fuel available to sustain and intensify Harvey, but also increased its flooding rains on land,\u201d the study said. \u201cHarvey could not have produced so much rain without human-induced climate change.\u201d The study, published in the journal Earth\u2019s Future, said the added ocean heat content not only increases a storm\u2019s rainfall but also \u201cinvigorates and enlarges the storm,\u201d turning it into an even greater rain-producer. Two independent studies found climate warming boosted Harvey\u2019s rainfall by about 20\u00a0to 35 percent. Kevin Trenberth, lead author of this latest study, called \u201cHurricane Harvey links to Ocean Heat Content and Climate Change Adaptation,\u201d is a climate scientist at the National Center for Atmospheric Research who has published other studies\u00a0that connect human activities and increasing hurricane intensity. During the 2017 hurricane season, Harvey, Irma, Jose\u00a0 and Maria all underwent what is known as \u201crapid intensification.\u201d This means their peak wind speed rose at a torrid pace, by at least 29 mph in 24 hours, according to one definition (alternative definitions vary). Maria, for example, strengthened from a Category 1 to Category 4\u00a0 storm on the Saffir-Simpson wind scale in 12 hours. The storm would go on to decimate the island of Dominica and, ultimately, St. Croix and Puerto Rico. A study in the journal Geophysical Research Letters found that the magnitude of these rapid intensification events increased from 1986 to 2015 in the central and eastern tropical Atlantic Ocean. From 1986 to 2000, the average storm that rapidly intensified saw its peak winds increase by 32 mph in 24 hours, but the increase was 36 mph in 24 hours from 2001 to 2015. The study analysis did not detect a trend in these events in the western tropical Atlantic, which includes the Caribbean Sea and Gulf of Mexico. Even so, the study said its results \u201chave substantial implications\u201d for the eastern Caribbean Islands, because they\u00a0face hurricanes that form over the central and eastern Atlantic and head west. And it is in this region where the study detected an uptick in rapid strengthening events and also where Irma, Jose and Maria became such behemoths. The increase in these events occurred during a period when upper ocean temperatures heated up. The study concluded that it could not \u201crule out\u201d that the buildup of greenhouse gases from human activity was to blame. But it mostly attributed the change to a cycle known as the Atlantic Multidecadal Oscillation, which is in its warm phase. Scientists differ on whether this cycle or human activity is driving trends in tropical Atlantic Ocean temperatures. Michael Mann, a climate scientist at Penn State\u00a0who was not involved in the study, has published work that demonstrates a dominant man-made role in the temperature increase. In an email, he said the study\u2019s attribution to this cycle, rather than human activity, was \u201cnot justified.\u201d The study on the connection between Harvey\u2019s rains and the high ocean heat content devotes an entire section on the importance of planning for even more \u201csupercharged\u201d hurricanes. \u201c[T]he risk is clear, and preparations for expected effects of climate change on hurricanes and more generally are woefully inadequate,\u201d the study says. It calls for vulnerable zones to \u201cbuild resilience and plan for inevitable impacts\u201d or \u201csuffer the consequences.\u201d It suggests: \u201cGiven the price tag with units of hundreds of billions of dollars for the recent hurricanes, a modest (two orders of magnitude less) investment in building resiliency may well have saved billions and a lot of grief,\u201d the study concluded.","time":1525807995,"title":"Due to climate change, hurricanes raining harder, may be growing stronger faster","type":"story","url":"https:\/\/www.washingtonpost.com\/news\/capital-weather-gang\/wp\/2018\/05\/08\/due-to-climate-change-hurricanes-are-raining-harder-and-may-be-growing-stronger-faster\/","label":7,"label_name":"random"},{"by":"lusob","descendants":0,"id":17024020,"kids":"None","score":1,"text":"Year after year, every time Bill Gates published his list of favorite books, I\u2019m feeling a certain envy, not only of his bank account but also because of his reading ability (he reads about 50 books a year) Fascinated by the ability of this kind of people to read so many books in such a short time, I barely read a couple a year, I became interested in techniques of speed reading, but after a while testing them I realized that I did not enjoy reading like this\u00a0, sometimes it generated me some stress and sometimes I lost the focus very easily, besides my sight getting tired faster than with normal reading.\u00a0I also tried summary applications like Blinklist, but they try to synthesize so much that the context is completely lost, you can not really summarize a book of 300 pages in 10 pages without losing part of the essence of the book, although I think they are really interesting to review concepts of books that you have already read. One day trying a pdf reader for Android, I discovered that it incorporated an option that allowed reading the text using Google\u2019s text to speech engine and after trying it with different texts I saw that it had improved a lot compared to past versions where the voice was too much robotized and without intonations, so I tried several applications that used that engine to read some articles until I found Reedy. There are several text2speech apps, but Reedy audio settings are really easy to manage, it also allows you to send to it any web content and to open several formats of ebooks. The results were satisfactory and every time I read more web articles and news with this system on the way to the office.\u00a0After a while testing and adapting the settings (type of voice, speed, tone) everything was ready to test the reading of books with this system. I started my experiment with a list of 5 books as main goal, some of https:\/\/www.gatesnotes.com\/About-Bill-Gates\/Best-Books-2017 and the results were amazing, just listening them during my way to work and when I was running I finished the first book in less than 2 weeks, little by little I was increasing the speed of reading up to 300% without hardly noticing it and in less than two months I had read the 5 books from the list. Although there are other alternatives to this method like listen audiobooks, I think these are less versatile because they do not allow you to listen web articles or news, neither you have the option to read some parts normally and listen others.) Moreover, most of its content is only in English whereas google\u2019s text2speech engine supports multiple languages \u200b\u200b(I read a lot in Spanish) Here is the list of books that I have read in the first 3 months of this year: As downside I would say that at the beginning it is a bit strange, when there are annotations or figures you have to look at your phone (although the app warns you that there is an image in the content), so I usually read the parts of books showing many graphics and relevant figures instead of listen them. But the benefits far outweigh the disadvantages: Since I wrote the draft of this article in early April until its publication today I have 3 more books read plus two in process, the idea is to continue testing it throughout the year and see if it really works, so at the end of the year I will publish an update with the results. By clapping more or less, you can signal to us which stories really stand out.","time":1525807867,"title":"How I manage to read a book per week","type":"story","url":"https:\/\/medium.com\/@lusob\/how-i-manage-to-read-a-book-per-week-1b33cad0f058","label":10,"label_name":"thought"},{"by":"spacemanspiffy","descendants":0,"id":17024018,"kids":"None","score":1,"text":"CNET tambi\u00e9n est\u00e1 disponible en espa\u00f1ol. Don't show this again The company's mobile operating system update will include App Actions, which applies AI to the actions you'll take within the apps on your phone or tablet. App Actions is a new feature in development for Google's Android P. The feature will do things like prompt you to resume listening to music when you plug in to your headphone jack. Google's mobile operating system already tries to predict which app you'll want to use next. It's next update, Android P, will move on to \"predicting the next action you want to take,\" said Dave Burke, Google's VP of engineering for Android. You'll experience this in a variety of ways as you're using a phone that runs Android P, Burke said. For example, plug in your headphones, and you'll see an \"action\" on your screen that lets you press play on music you were listening to earlier.\u00a0 Actions will eventually appear as you're interacting with the launcher, Google's Play Store and Assistant. It'll also appear as part of smart text selection and Google search. \"The phone is adapting to me and trying to help me get to my next task more quickly,\" Burke said. It's one way Google is applying artificial intelligence to phones, he added, saying the move will make \"Android smart by teaching the operating system to adapt to the user.\" Android P will also show you what it calls \"slices,\" or a small section of an app's interface, when you might need it. For example, Burke said, if you look up Lyft in Google search, a slice of the Lyft app will appear on the screen, giving you the option to start a ride request.\u00a0 Slices will eventually show up in a variety of places on your Android phone, but they'll first appear in search, Burke said. The goal is to \"enable a dynamic two-way experience where the app's UI can intelligently show up in context.\" Early access to the feature will begin in June. Android P will give Android gestures like the iPhone X: Google's vision of Android P is now a lot less hazy. But the company still won't tell us what the \"P\" stands for. Google's Duplex could make Assistant the most lifelike AI yet: Experimental technology called Duplex, rolling out soon in a limited release, makes you think you're talking to a real person. Be respectful, keep it clean and stay on topic. We delete comments that violate our policy, which we encourage you to read. Discussion threads can be closed at any time at our discretion.","time":1525807858,"title":"Google's Android P will know what you want to do before you do it","type":"story","url":"https:\/\/www.cnet.com\/news\/google-android-p-will-know-what-you-want-to-do-before-you-do-it-io-ai-app-actions\/","label":9,"label_name":"tech"},{"by":"danso","descendants":0,"id":17024012,"kids":"None","score":1,"text":"You might be seeing a lot of red on the internet Wednesday. Many sites, including Etsy, Reddit, and OKCupid will adorn their pages with \u201cred alerts\u201d asking readers to tell their representatives to save net neutrality. Last December, the Federal Communications Commission voted to jettison its Obama-era rules forbidding broadband providers from blocking, throttling, or otherwise discriminating against legal content. The change has not taken effect yet. But Wednesday, Democratic Senator Ed Markey of Massachusetts will try to force the Senate to schedule a vote on his proposal to reverse the December decision. He has the support to do so, but it\u2019s not clear when the vote would take place. Markey\u2019s maneuver is an attempt to employ the Congressional Review Act, which allows Congress to overturn decisions made by federal agencies. So far, 50 senators have agreed to back the legislation\u2014all of the chamber\u2019s Democrats and independents, plus Maine Republican Susan Collins. With Senator John McCain (R-Arizona) ill and absent, the legislation could pass 50 to 49 if all of its supporters vote in favor. The measure would still face long odds, however. Republicans, who tend to support the FCC\u2019s move to repeal net neutrality, hold a solid majority in the House of Representatives. If it were to pass the House, the measure would also need the signature of President Trump or a two-thirds vote in both houses of Congress to override a veto. Organizers of the \u201cRed Alert\u201d campaign want sites to carry banners or other notifications urging lawmakers to vote for Markey\u2019s legislation until the vote. Etsy manager of public policy Ilyssa Meyer says the e-commerce site will do just that. Even if Markey\u2019s legislation fails, net neutrality won\u2019t be entirely dead. Several states have passed their own rules to protect net neutrality. In March, Washington state enacted a law banning broadband providers in the state from blocking or throttling legal content, and from offering paid fast lanes. Oregon followed shortly after with a law that prohibits state agencies from doing business with broadband providers that don\u2019t follow net neutrality. Governors of Hawaii, Montana, New Jersey, New York, and Vermont have signed executive orders similar to the Oregon law. Washington Governor Jay Inslee tells WIRED that net neutrality is now a mainstream issue in the state. \u201cIt\u2019s one of the top three things that I\u2019m thanked for of what I\u2019ve done to try to stand up to this administration,\u201d he says. \u201cI would tell other governors that if you do this, you will get a lot of grateful people in your state. People understand that this a real concern and it requires action.\u201d His state\u2019s effort to maintain net neutrality could face its own challenges. The FCC\u2019s order repealing net neutrality prohibits states from passing their own net neutrality rules, and the broadband industry complains that state laws will lead to a \u201cpatchwork\u201d of different regulations across the country. The telecommunications industry group USTelecom has promised to \"aggressively challenge\" state and municipal net neutrality rules. Experts are unsure whether state rules will withstand legal challenges. Inslee counters that state laws are the only way to protect consumers and small businesses from being exploited by large broadband providers. \u201cWe have to act in absence of federal action,\u201d he says. He also argues that because the principles of net neutrality remain the same, it shouldn\u2019t be hard for broadband providers to comply with different state rules. After all, the industry\u2019s largest players frequently claim to support rules against blocking, throttling, and discriminating against content. Elsewhere, strong state-level protections have been proposed in California and New York. Bills in both states would ban not only throttling, and paid \u201cfast lanes,\u201d but also certain types of data cap exemptions, sometimes called \u201czero rating.\u201d For example, AT&T, which owns DirecTV, does not count content from streaming video service DirecTV Now against its customers\u2019 data caps. That might be good for AT&T customers who are also DirecTV Now subscribers, but critics argue it gives AT&T\u2019s service an unfair advantage over competitors like Dish\u2019s Sling TV. The California and New York bill would prohibit this type of exemption. Inslee says it\u2019s possible that Washington could adopt a similar policy in the future. Just about everyone wants Congress to create a more uniform set of rules. But deep divisions remain over what exactly those rules should be. Democrats and internet freedom activists prefer Markey\u2019s plan. Representative Marsha Blackburn (R-Tennessee), meanwhile, has proposed a bill that would ban broadband providers from blocking legal content, but would still allow fast lanes. It would also ban states from making their own net neutrality rules. A world without net neutrality might end up meaning that you have to pay more to access the internet content that you want. But it also might crush innovation. CNMN Collection Use of this site constitutes acceptance of our user agreement (effective 3\/21\/12) and privacy policy (effective 3\/21\/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast.","time":1525807796,"title":"\u201cRed-Alert\u201d Is a Last-Ditch Effort to Save Net Neutrality","type":"story","url":"https:\/\/www.wired.com\/story\/your-favorite-websites-are-rallying-in-a-last-ditch-effort-to-save-net-neutrality\/","label":7,"label_name":"random"},{"by":"graystevens","descendants":0,"id":17024002,"kids":"None","score":2,"text":"Recently (April 2018), I redeployed grh.am using Hugo after running the site with Jekyll for a number of years (which I never really blogged about, the last blog post talks about Pelican! Previously, deployment felt a little clunky, requiring a git push to a remote repository set up on a server, where a git hook picks up on the new commit and runs Jekyll (as per the Jekyll documentation). This time round, I fancied doing it a little differently & automating all of the deployment whilst also removing a lot of the moving parts to limit areas for things to go wrong. I\u2019ve been using GitLab for a while now thanks to their free private repositories & various other wonderful offerings, one of which is CI\/CD which we\u2019ll be using a little later in this post. The process for publishing a new post or making any changes is as follows: And that\u2019s it. There are a few extra details, such as drafts being ignored and the CI\/CD pipeline within GitLab only running on the master branch of the repository, but that is essentially it. Lets take a closer look at those two jobs, and see how they work\u2026 To go from commit and push to seeing the changes live on grh.am, we first need to make hugo build the static site, and this is what we get from Job 1. GitLab CI can be configured from a .gitlab-ci.yml file within your repository. Below is the config for the first job, formatted in YAML: At the top of the file, we have outlined all our jobs. In this instance, we only have two - build (job 1) & deploy (job 2). We will look at the config for deploy shortly. Next, we tell GitLab what stage we are configuring, and set a Docker image to use. I found the best Docker image for the job to be jojomi\/hugo, which is regularly updated & well maintained. We can then tell GitLab CI what commands to run within this Docker container, in this instance I have made it print out the version of hugo that is running (which we can see in the CI logs), update any submodules which in my instance is a theme, and then run hugo -d public_html. We then tell GitLab CI to treat this new folder public_html as an artifact which allows us to reuse this output in other jobs, as well as simply making it available for download from the CI web front-end. Lastly, we tell GitLab CI to only run on the master branch. So we have our static source files as artifacts, but how do we now get these onto our server for nginx to then serve? The answer is of course another Docker image. Here is the config for the deploy stage of our CI pipeline: Here we are using a bog-standard Alpine Linux Docker image, where we then load our SSH private key, add our server key to known_hosts, and fire up rsync! GitLab very kindly outline most of this in their documentation: Using SSH keys with GitLab CI\/CD, plus a very useful example repository with a .gitlab-ci.yml ready to go. From the top, we outline which stage we are configuring, and set the Docker image we which to use. I chose Alpine as it is tiny to reduce build times, as well as being well regarded in the Docker community for a good base OS\/image. We then come across tags, which tells GitLab CI to only run this job on machines that are associated with that tag. Because we are handling SSH private keys here, I have setup a Docker instance of gitlab-runner on my home lab, limiting the possibility of that key going walkabout or being gleamed somehow from other GitLab CI jobs. We run a few commands prior to actually executing some of our commands, this is just to ensure we have the right packages installed & up to date. In this instance, we install or update openssh-client (for SSH), bash, and rsync (for.. well, rsync). And then finally we run a number of commands on our Docker instance: Because public_html is an artifact from our previous job, GitLab CI shares that with our deploy job seamlessly. Clearly there are some variable being used here, hence the ${SSH_PRIVATE_KEY} etc. These are set within GitLab CI\/CD settings, so that I don\u2019t have to hardcode any credentials into our git repository. And once again, this will only run on the master branch of our repository. Finally, we end up with the following .gitlab-ci.yml file (plus some variables set on the GitLab CI\/CD settings page): As soon as that config file is pushed to our repository, GitLab picks it up, and runs it. Assuming everything else is setup correctly (SSH connectivity works, public key authentication works etc.), you\u2019ll see the contents of public_html deployed to the location specific in SSH_USER_HOST_LOCATION.","time":1525807753,"title":"Deploying a Hugo Static Site Using GitLab, CI\/CD, and SSH","type":"story","url":"https:\/\/grh.am\/2018\/deploying-a-hugo-static-site-using-gitlab-ci-cd-and-ssh\/","label":3,"label_name":"dev"},{"by":"prostoalex","descendants":0,"id":17023986,"kids":"None","score":1,"text":"\n\n\n\n            var postLoadFunctions = {};\n            var foresee_enabled = 1\n            var dynamic_yield_enabled = 1\n                    \n\n\n\n\tCNBC_Comscore = 'Finance';\n\n\n\n\t\tvar mpscall = {\n\t\t\t\t\t\t'node_brand' : '2'  ,  \n\n\t\t\t\t\t'site' : 'cnbc.com-relaunch'  ,  \n\n\t\t\t\t\t'content_id' : '105189129'  ,  \n\n\t\t\t\t\t'path' : '\/id\/105189129'  ,  \n\n\t\t\t\t\t'is_content' : '1'  ,  \n\n\t\t\t\t\t'is_sponsored' : '0'  ,  \n\n\t\t\t\t\t'adunits' : 'Top Banner|Badge A|Badge B|Badge C|Badge D|Flex Ad First|Box Ad 1|Non Iframe Custom|Inline Custom|Movable Box Ad|Responsive Rectangle'  ,  \n\n\t\t\t\t\t'keywords' : '~'  ,  \n\n\t\t\t\t\t'cat' : 'Finance|Investing|Warren Buffett Watch'  ,  \n\n\t\t\t\t\t'cag[configuration_franchise]' : 'Warren Buffett Watch'  ,  \n\n\t\t\t\t\t'cag[attribution_author]' : 'Evelyn Cheng'  ,  \n\n\t\t\t\t\t'cag[related_primary]' : 'Investment strategy|Bitcoin|Autos|Elon Musk|Microsoft Corp|Amazon.com Inc|Alphabet Class A|Apple Inc|Bill Gates|Charlie Munger|Squawk Box U.S.|Markets|Economy|Warren Buffett|Berkshire Hathaway Inc|Investing|Consumer Goods|Technology'  ,  \n\n\t\t\t\t\t'cag[related_related]' : 'Bitcoin\/USD Coinbase|Tesla Inc'  ,  \n\n\t\t\t\t\t'cag[attribution_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[type_franchise]' : 'Warren Buffett Watch|Investing|Bitcoin|Consumer Goods|Technology|Markets|Economy|Bitcoin|Autos|Squawk Box U.S.'  ,  \n\n\t\t\t\t\t'cag[type_creator]' : 'Evelyn Cheng'  ,  \n\n\t\t\t\t\t'cag[type_tag]' : 'Investment strategy|Bitcoin|Autos|Markets|Economy'  ,  \n\n\t\t\t\t\t'cag[type_person]' : 'Elon Musk|Bill Gates|Charlie Munger|Warren Buffett'  ,  \n\n\t\t\t\t\t'cag[type_company]' : 'Microsoft Corp|Amazon.com Inc|Alphabet Class A|Apple Inc|Tesla Inc|Berkshire Hathaway Inc'  ,  \n\n\t\t\t\t\t'cag[type_security]' : 'Bitcoin\/USD Coinbase'  ,  \n\n\t\t\t\t\t'cag[type_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[brand]' : 'none'  ,  \n\n\t\t\t\t\t'cag[template]' : 'story_simple'  ,  \n\n\t\t\t\t\t'cag[device]' : 'web'  ,  \n\n\t\t\t\t\t'hline' : 'Warren Buffett Watch'  ,  \n\n\t\t\t\t\t'type' : 'blogpost'  ,  \n\n\t\t\t\t\t'template' : 'story_simple'  ,  \n\n\t\t\t\t\t'title' : 'Bill Gates: I would short bitcoin if I could'  ,  \n\n\t\t\t\t\t'pubdate' : '1525697620'  ,  \n\n\t\t\t\t\t'stitle' : 'Gates bitcoin short CHENG 180507'  ,  \n\n\t\t\t\t\t'byline' : 'Evelyn Cheng'  ,  \n\n\t\t\t\t\t'subtype' : 'special_report'  ,  \n\n\t\t\t\t\t'id' : '105189129'  ,  \n\n\t\t\t\t\t'nid' : '105189129'  \n\n\t\t\n\t\t}, mpsopts = {\n\t\t  \n\t\t\"host\" : 'mps.cnbc.com',\n\t\t\"updatecorrelator\" : true\n\n\t\t};\n\n\t\tvar mps = mps || {};\n\t\tmps._ext = mps._ext || {};\n\t\tmps._adsheld = [];\n\t\tmps._queue = mps._queue || {};\n\t\tmps._queue.mpsloaded = mps._queue.mpsloaded || [];\n\t\tmps._queue.mpsinit = mps._queue.mpsinit || [];\n\t\tmps._queue.gptloaded = mps._queue.gptloaded || [];\n\t\tmps._queue.adload = mps._queue.adload || [];\n\t\tmps._queue.adclone = mps._queue.adclone || [];\n\t\tmps._queue.adview = mps._queue.adview || [];\n\t\tmps._queue.refreshads = mps._queue.refreshads || [];\n\t\tmps.__timer = Date.now ? Date.now() : (function() { return +new Date })();\n\t\tmps.__intcode = \"v2\";\n\t\tif (typeof mps.getAd != \"function\") mps.getAd = function(adunit) {\n\t\t    if (typeof adunit != \"string\") return false;\n\t\t    var slotid = \"mps-getad-\" + adunit.replace(\/\\W\/g, \"\");\n\t\t    if (!mps._ext || !mps._ext.loaded) {\n\t\t        mps._queue.gptloaded.push(function() {\n\t\t            typeof mps._gptfirst == \"function\" && mps._gptfirst(adunit, slotid);\n\t\t            mps.insertAd(\"#\" + slotid, adunit)\n\t\t        });\n\t\t        mps._adsheld.push(adunit)\n\t\t    }\n\t\t    return '<div id=\"' + slotid + '\" class=\"mps-wrapper\" data-mps-fill-slot=\"' + adunit + '\"><\/div>'\n\t\t};\n\t\t(function() {\n\t\t    head = document.head || document.getElementsByTagName(\"head\")[0], mpsload = document.createElement(\"script\");\n\t\t    mpsload.src = \"\/\/\" + mpsopts.host + \"\/fetch\/ext\/load-\" + mpscall.site + \".js?nowrite=2\";\n\t\t    mpsload.id = \"mps-load\";\n\t\t    head.insertBefore(mpsload, head.firstChild)\n\t\t})();\n\n\t\n\n\n\n\n  \n\n\n\nwindow.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var o=e[n]={exports:{}};t[n][0].call(o.exports,function(e){var o=t[n][1][e];return r(o||e)},o,o.exports)}return e[n].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<n.length;o++)r(n[o]);return r}({1:[function(t,e,n){function r(t){try{c.console&&console.log(t)}catch(e){}}var o,i=t(\"ee\"),a=t(19),c={};try{o=localStorage.getItem(\"__nr_flags\").split(\",\"),console&&\"function\"==typeof console.log&&(c.console=!0,o.indexOf(\"dev\")!==-1&&(c.dev=!0),o.indexOf(\"nr_dev\")!==-1&&(c.nrDev=!0))}catch(s){}c.nrDev&&i.on(\"internal-error\",function(t){r(t.stack)}),c.dev&&i.on(\"fn-err\",function(t,e,n){r(n.stack)}),c.dev&&(r(\"NR AGENT IN DEVELOPMENT MODE\"),r(\"flags: \"+a(c,function(t,e){return t}).join(\", \")))},{}],2:[function(t,e,n){function r(t,e,n,r,o){try{d?d-=1:i(\"err\",[o||new UncaughtException(t,e,n)])}catch(c){try{i(\"ierr\",[c,s.now(),!0])}catch(u){}}return\"function\"==typeof f&&f.apply(this,a(arguments))}function UncaughtException(t,e,n){this.message=t||\"Uncaught error with no additional information\",this.sourceURL=e,this.line=n}function o(t){i(\"err\",[t,s.now()])}var i=t(\"handle\"),a=t(20),c=t(\"ee\"),s=t(\"loader\"),f=window.onerror,u=!1,d=0;s.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(p){\"stack\"in p&&(t(12),t(11),\"addEventListener\"in window&&t(6),s.xhrWrappable&&t(13),u=!0)}c.on(\"fn-start\",function(t,e,n){u&&(d+=1)}),c.on(\"fn-err\",function(t,e,n){u&&(this.thrown=!0,o(n))}),c.on(\"fn-end\",function(){u&&!this.thrown&&d>0&&(d-=1)}),c.on(\"internal-error\",function(t){i(\"ierr\",[t,s.now(),!0])})},{}],3:[function(t,e,n){t(\"loader\").features.ins=!0},{}],4:[function(t,e,n){function r(){C++,M=y.hash,this[u]=b.now()}function o(){C--,y.hash!==M&&i(0,!0);var t=b.now();this[l]=~~this[l]+t-this[u],this[d]=t}function i(t,e){E.emit(\"newURL\",[\"\"+y,e])}function a(t,e){t.on(e,function(){this[e]=b.now()})}var c=\"-start\",s=\"-end\",f=\"-body\",u=\"fn\"+c,d=\"fn\"+s,p=\"cb\"+c,h=\"cb\"+s,l=\"jsTime\",m=\"fetch\",v=\"addEventListener\",w=window,y=w.location,b=t(\"loader\");if(w[v]&&b.xhrWrappable){var g=t(9),x=t(10),E=t(8),O=t(6),R=t(12),P=t(7),T=t(13),S=t(\"ee\"),N=S.get(\"tracer\");t(14),b.features.spa=!0;var M,j=w[v],C=0;S.on(u,r),S.on(p,r),S.on(d,o),S.on(h,o),S.buffer([u,d,\"xhr-done\",\"xhr-resolved\"]),O.buffer([u]),R.buffer([\"setTimeout\"+s,\"clearTimeout\"+c,u]),T.buffer([u,\"new-xhr\",\"send-xhr\"+c]),P.buffer([m+c,m+\"-done\",m+f+c,m+f+s]),E.buffer([\"newURL\"]),g.buffer([u]),x.buffer([\"propagate\",p,h,\"executor-err\",\"resolve\"+c]),N.buffer([u,\"no-\"+u]),a(T,\"send-xhr\"+c),a(S,\"xhr-resolved\"),a(S,\"xhr-done\"),a(P,m+c),a(P,m+\"-done\"),E.on(\"pushState-end\",i),E.on(\"replaceState-end\",i),j(\"hashchange\",i,!0),j(\"load\",i,!0),j(\"popstate\",function(){i(0,C>1)},!0)}},{}],5:[function(t,e,n){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t(\"ee\"),i=t(\"handle\"),a=t(12),c=t(11),s=\"learResourceTimings\",f=\"addEventListener\",u=\"resourcetimingbufferfull\",d=\"bstResource\",p=\"resource\",h=\"-start\",l=\"-end\",m=\"fn\"+h,v=\"fn\"+l,w=\"bstTimer\",y=\"pushState\",b=t(\"loader\");b.features.stn=!0,t(8);var g=NREUM.o.EV;o.on(m,function(t,e){var n=t[0];n instanceof g&&(this.bstStart=b.now())}),o.on(v,function(t,e){var n=t[0];n instanceof g&&i(\"bst\",[n,e,this.bstStart,b.now()])}),a.on(m,function(t,e,n){this.bstStart=b.now(),this.bstType=n}),a.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),this.bstType])}),c.on(m,function(){this.bstStart=b.now()}),c.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),\"requestAnimationFrame\"])}),o.on(y+h,function(t){this.time=b.now(),this.startPath=location.pathname+location.hash}),o.on(y+l,function(t){i(\"bstHist\",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance[\"c\"+s]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"c\"+s]()},!1):window.performance[f](\"webkit\"+u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"webkitC\"+s]()},!1)),document[f](\"scroll\",r,{passive:!0}),document[f](\"keypress\",r,!1),document[f](\"click\",r,!1)}},{}],6:[function(t,e,n){function r(t){for(var e=t;e&&!e.hasOwnProperty(u);)e=Object.getPrototypeOf(e);e&&o(e)}function o(t){c.inPlace(t,[u,d],\"-\",i)}function i(t,e){return t[1]}var a=t(\"ee\").get(\"events\"),c=t(22)(a,!0),s=t(\"gos\"),f=XMLHttpRequest,u=\"addEventListener\",d=\"removeEventListener\";e.exports=a,\"getPrototypeOf\"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+\"-start\",function(t,e){var n=t[1],r=s(n,\"nr@wrapped\",function(){function t(){if(\"function\"==typeof n.handleEvent)return n.handleEvent.apply(n,arguments)}var e={object:t,\"function\":n}[typeof n];return e?c(e,\"fn-\",null,e.name||\"anonymous\"):n});this.wrapped=t[1]=r}),a.on(d+\"-start\",function(t){t[1]=this.wrapped||t[1]})},{}],7:[function(t,e,n){function r(t,e,n){var r=t[e];\"function\"==typeof r&&(t[e]=function(){var t=r.apply(this,arguments);return o.emit(n+\"start\",arguments,t),t.then(function(e){return o.emit(n+\"end\",[null,e],t),e},function(e){throw o.emit(n+\"end\",[e],t),e})})}var o=t(\"ee\").get(\"fetch\"),i=t(19);e.exports=o;var a=window,c=\"fetch-\",s=c+\"body-\",f=[\"arrayBuffer\",\"blob\",\"json\",\"text\",\"formData\"],u=a.Request,d=a.Response,p=a.fetch,h=\"prototype\";u&&d&&p&&(i(f,function(t,e){r(u[h],e,s),r(d[h],e,s)}),r(a,\"fetch\",c),o.on(c+\"end\",function(t,e){var n=this;e?e.clone().arrayBuffer().then(function(t){n.rxSize=t.byteLength,o.emit(c+\"done\",[null,e],n)}):o.emit(c+\"done\",[t],n)}))},{}],8:[function(t,e,n){var r=t(\"ee\").get(\"history\"),o=t(22)(r);e.exports=r,o.inPlace(window.history,[\"pushState\",\"replaceState\"],\"-\")},{}],9:[function(t,e,n){var r=t(\"ee\").get(\"mutation\"),o=t(22)(r),i=NREUM.o.MO;e.exports=r,i&&(window.MutationObserver=function(t){return this instanceof i?new i(o(t,\"fn-\")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype)},{}],10:[function(t,e,n){function r(t){var e=a.context(),n=c(t,\"executor-\",e),r=new f(n);return a.context(r).getCtx=function(){return e},a.emit(\"new-promise\",[r,e],e),r}function o(t,e){return e}var i=t(22),a=t(\"ee\").get(\"promise\"),c=i(a),s=t(19),f=NREUM.o.PR;e.exports=a,f&&(window.Promise=r,[\"all\",\"race\"].forEach(function(t){var e=f[t];f[t]=function(n){function r(t){return function(){a.emit(\"propagate\",[null,!o],i),o=o||!t}}var o=!1;s(n,function(e,n){Promise.resolve(n).then(r(\"all\"===t),r(!1))});var i=e.apply(f,arguments),c=f.resolve(i);return c}}),[\"resolve\",\"reject\"].forEach(function(t){var e=f[t];f[t]=function(t){var n=e.apply(f,arguments);return t!==n&&a.emit(\"propagate\",[t,!0],n),n}}),f.prototype[\"catch\"]=function(t){return this.then(null,t)},f.prototype=Object.create(f.prototype,{constructor:{value:r}}),s(Object.getOwnPropertyNames(f),function(t,e){try{r[e]=f[e]}catch(n){}}),a.on(\"executor-start\",function(t){t[0]=c(t[0],\"resolve-\",this),t[1]=c(t[1],\"resolve-\",this)}),a.on(\"executor-err\",function(t,e,n){t[1](n)}),c.inPlace(f.prototype,[\"then\"],\"then-\",o),a.on(\"then-start\",function(t,e){this.promise=e,t[0]=c(t[0],\"cb-\",this),t[1]=c(t[1],\"cb-\",this)}),a.on(\"then-end\",function(t,e,n){this.nextPromise=n;var r=this.promise;a.emit(\"propagate\",[r,!0],n)}),a.on(\"cb-end\",function(t,e,n){a.emit(\"propagate\",[n,!0],this.nextPromise)}),a.on(\"propagate\",function(t,e,n){this.getCtx&&!e||(this.getCtx=function(){if(t instanceof Promise)var e=a.context(t);return e&&e.getCtx?e.getCtx():this})}),r.toString=function(){return\"\"+f})},{}],11:[function(t,e,n){var r=t(\"ee\").get(\"raf\"),o=t(22)(r),i=\"equestAnimationFrame\";e.exports=r,o.inPlace(window,[\"r\"+i,\"mozR\"+i,\"webkitR\"+i,\"msR\"+i],\"raf-\"),r.on(\"raf-start\",function(t){t[0]=o(t[0],\"fn-\")})},{}],12:[function(t,e,n){function r(t,e,n){t[0]=a(t[0],\"fn-\",null,n)}function o(t,e,n){this.method=n,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],\"fn-\",this,n)}var i=t(\"ee\").get(\"timer\"),a=t(22)(i),c=\"setTimeout\",s=\"setInterval\",f=\"clearTimeout\",u=\"-start\",d=\"-\";e.exports=i,a.inPlace(window,[c,\"setImmediate\"],c+d),a.inPlace(window,[s],s+d),a.inPlace(window,[f,\"clearImmediate\"],f+d),i.on(s+u,r),i.on(c+u,o)},{}],13:[function(t,e,n){function r(t,e){d.inPlace(e,[\"onreadystatechange\"],\"fn-\",c)}function o(){var t=this,e=u.context(t);t.readyState>3&&!e.resolved&&(e.resolved=!0,u.emit(\"xhr-resolved\",[],t)),d.inPlace(t,y,\"fn-\",c)}function i(t){b.push(t),l&&(x?x.then(a):v?v(a):(E=-E,O.data=E))}function a(){for(var t=0;t<b.length;t++)r([],b[t]);b.length&&(b=[])}function c(t,e){return e}function s(t,e){for(var n in t)e[n]=t[n];return e}t(6);var f=t(\"ee\"),u=f.get(\"xhr\"),d=t(22)(u),p=NREUM.o,h=p.XHR,l=p.MO,m=p.PR,v=p.SI,w=\"readystatechange\",y=[\"onload\",\"onerror\",\"onabort\",\"onloadstart\",\"onloadend\",\"onprogress\",\"ontimeout\"],b=[];e.exports=u;var g=window.XMLHttpRequest=function(t){var e=new h(t);try{u.emit(\"new-xhr\",[e],e),e.addEventListener(w,o,!1)}catch(n){try{u.emit(\"internal-error\",[n])}catch(r){}}return e};if(s(h,g),g.prototype=h.prototype,d.inPlace(g.prototype,[\"open\",\"send\"],\"-xhr-\",c),u.on(\"send-xhr-start\",function(t,e){r(t,e),i(e)}),u.on(\"open-xhr-start\",r),l){var x=m&&m.resolve();if(!v&&!m){var E=1,O=document.createTextNode(E);new l(a).observe(O,{characterData:!0})}}else f.on(\"fn-end\",function(t){t[0]&&t[0].type===w||a()})},{}],14:[function(t,e,n){function r(t){var e=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<d;r++)t.removeEventListener(u[r],this.listener,!1);if(!e.aborted){if(n.duration=a.now()-this.startTime,4===t.readyState){e.status=t.status;var i=o(t,this.lastSize);if(i&&(n.rxSize=i),this.sameOrigin){var s=t.getResponseHeader(\"X-NewRelic-App-Data\");s&&(e.cat=s.split(\", \").pop())}}else e.status=0;n.cbTime=this.cbTime,f.emit(\"xhr-done\",[t],t),c(\"xhr\",[e,n,this.startTime])}}}function o(t,e){var n=t.responseType;if(\"json\"===n&&null!==e)return e;var r=\"arraybuffer\"===n||\"blob\"===n||\"json\"===n?t.response:t.responseText;return l(r)}function i(t,e){var n=s(e),r=t.params;r.host=n.hostname+\":\"+n.port,r.pathname=n.pathname,t.sameOrigin=n.sameOrigin}var a=t(\"loader\");if(a.xhrWrappable){var c=t(\"handle\"),s=t(15),f=t(\"ee\"),u=[\"load\",\"error\",\"abort\",\"timeout\"],d=u.length,p=t(\"id\"),h=t(18),l=t(17),m=window.XMLHttpRequest;a.features.xhr=!0,t(13),f.on(\"new-xhr\",function(t){var e=this;e.totalCbs=0,e.called=0,e.cbTime=0,e.end=r,e.ended=!1,e.xhrGuids={},e.lastSize=null,h&&(h>34||h<10)||window.opera||t.addEventListener(\"progress\",function(t){e.lastSize=t.loaded},!1)}),f.on(\"open-xhr-start\",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),f.on(\"open-xhr-end\",function(t,e){\"loader_config\"in NREUM&&\"xpid\"in NREUM.loader_config&&this.sameOrigin&&e.setRequestHeader(\"X-NewRelic-ID\",NREUM.loader_config.xpid)}),f.on(\"send-xhr-start\",function(t,e){var n=this.metrics,r=t[0],o=this;if(n&&r){var i=l(r);i&&(n.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{\"abort\"===t.type&&(o.params.aborted=!0),(\"load\"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||\"function\"!=typeof e.onload))&&o.end(e)}catch(n){try{f.emit(\"internal-error\",[n])}catch(r){}}};for(var c=0;c<d;c++)e.addEventListener(u[c],this.listener,!1)}),f.on(\"xhr-cb-time\",function(t,e,n){this.cbTime+=t,e?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&\"function\"==typeof n.onload||this.end(n)}),f.on(\"xhr-load-added\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&!this.xhrGuids[n]&&(this.xhrGuids[n]=!0,this.totalCbs+=1)}),f.on(\"xhr-load-removed\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&this.xhrGuids[n]&&(delete this.xhrGuids[n],this.totalCbs-=1)}),f.on(\"addEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-added\",[t[1],t[2]],e)}),f.on(\"removeEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-removed\",[t[1],t[2]],e)}),f.on(\"fn-start\",function(t,e,n){e instanceof m&&(\"onload\"===n&&(this.onload=!0),(\"load\"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),f.on(\"fn-end\",function(t,e){this.xhrCbStart&&f.emit(\"xhr-cb-time\",[a.now()-this.xhrCbStart,this.onload,e],e)})}},{}],15:[function(t,e,n){e.exports=function(t){var e=document.createElement(\"a\"),n=window.location,r={};e.href=t,r.port=e.port;var o=e.href.split(\":\/\/\");!r.port&&o[1]&&(r.port=o[1].split(\"\/\")[0].split(\"@\").pop().split(\":\")[1]),r.port&&\"0\"!==r.port||(r.port=\"https\"===o[0]?\"443\":\"80\"),r.hostname=e.hostname||n.hostname,r.pathname=e.pathname,r.protocol=o[0],\"\/\"!==r.pathname.charAt(0)&&(r.pathname=\"\/\"+r.pathname);var i=!e.protocol||\":\"===e.protocol||e.protocol===n.protocol,a=e.hostname===document.domain&&e.port===n.port;return r.sameOrigin=i&&(!e.hostname||a),r}},{}],16:[function(t,e,n){function r(){}function o(t,e,n){return function(){return i(t,[f.now()].concat(c(arguments)),e?null:this,n),e?void 0:this}}var i=t(\"handle\"),a=t(19),c=t(20),s=t(\"ee\").get(\"tracer\"),f=t(\"loader\"),u=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=u);var d=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],p=\"api-\",h=p+\"ixn-\";a(d,function(t,e){u[e]=o(p+e,!0,\"api\")}),u.addPageAction=o(p+\"addPageAction\",!0),u.setCurrentRouteName=o(p+\"routeName\",!0),e.exports=newrelic,u.interaction=function(){return(new r).get()};var l=r.prototype={createTracer:function(t,e){var n={},r=this,o=\"function\"==typeof e;return i(h+\"tracer\",[f.now(),t,n],r),function(){if(s.emit((o?\"\":\"no-\")+\"fn-start\",[f.now(),r,o],n),o)try{return e.apply(this,arguments)}finally{s.emit(\"fn-end\",[f.now()],n)}}}};a(\"setName,setAttribute,save,ignore,onEnd,getContext,end,get\".split(\",\"),function(t,e){l[e]=o(h+e)}),newrelic.noticeError=function(t){\"string\"==typeof t&&(t=new Error(t)),i(\"err\",[t,f.now()])}},{}],17:[function(t,e,n){e.exports=function(t){if(\"string\"==typeof t&&t.length)return t.length;if(\"object\"==typeof t){if(\"undefined\"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if(\"undefined\"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!(\"undefined\"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(e){return}}}},{}],18:[function(t,e,n){var r=0,o=navigator.userAgent.match(\/Firefox[\/s](d+.d+)\/);o&&(r=+o[1]),e.exports=r},{}],19:[function(t,e,n){function r(t,e){var n=[],r=\"\",i=0;for(r in t)o.call(t,r)&&(n[i]=e(r,t[r]),i+=1);return n}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],20:[function(t,e,n){function r(t,e,n){e||(e=0),\"undefined\"==typeof n&&(n=t?t.length:0);for(var r=-1,o=n-e||0,i=Array(o<0?0:o);++r<o;)i[r]=t[e+r];return i}e.exports=r},{}],21:[function(t,e,n){e.exports={exists:\"undefined\"!=typeof window.performance&&window.performance.timing&&\"undefined\"!=typeof window.performance.timing.navigationStart}},{}],22:[function(t,e,n){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t(\"ee\"),i=t(20),a=\"nr@original\",c=Object.prototype.hasOwnProperty,s=!1;e.exports=function(t,e){function n(t,e,n,o){function nrWrapper(){var r,a,c,s;try{a=this,r=i(arguments),c=\"function\"==typeof n?n(r,a):n||{}}catch(f){p([f,\"\",[r,a,o],c])}u(e+\"start\",[r,a,o],c);try{return s=t.apply(a,r)}catch(d){throw u(e+\"err\",[r,a,d],c),d}finally{u(e+\"end\",[r,a,s],c)}}return r(t)?t:(e||(e=\"\"),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,e,o,i){o||(o=\"\");var a,c,s,f=\"-\"===o.charAt(0);for(s=0;s<e.length;s++)c=e[s],a=t[c],r(a)||(t[c]=n(a,f?c+o:o,i,c))}function u(n,r,o){if(!s||e){var i=s;s=!0;try{t.emit(n,r,o,e)}catch(a){p([a,n,r,o])}s=i}}function d(t,e){if(Object.defineProperty&&Object.keys)try{var n=Object.keys(t);return n.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(r){p([r])}for(var o in t)c.call(t,o)&&(e[o]=t[o]);return e}function p(e){try{t.emit(\"internal-error\",e)}catch(n){}}return t||(t=o),n.inPlace=f,n.flag=a,n}},{}],ee:[function(t,e,n){function r(){}function o(t){function e(t){return t&&t instanceof r?t:t?s(t,c,i):i()}function n(n,r,o,i){if(!p.aborted||i){t&&t(n,r,o);for(var a=e(o),c=l(n),s=c.length,f=0;f<s;f++)c[f].apply(a,r);var d=u[y[n]];return d&&d.push([b,n,r,a]),a}}function h(t,e){w[t]=l(t).concat(e)}function l(t){return w[t]||[]}function m(t){return d[t]=d[t]||o(n)}function v(t,e){f(t,function(t,n){e=e||\"feature\",y[n]=e,e in u||(u[e]=[])})}var w={},y={},b={on:h,emit:n,get:m,listeners:l,context:e,buffer:v,abort:a,aborted:!1};return b}function i(){return new r}function a(){(u.api||u.feature)&&(p.aborted=!0,u=p.backlog={})}var c=\"nr@context\",s=t(\"gos\"),f=t(19),u={},d={},p=e.exports=o();p.backlog=u},{}],gos:[function(t,e,n){function r(t,e,n){if(o.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[e]=r,r}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){o.buffer([t],r),o.emit(t,e,n)}var o=t(\"ee\").get(\"handle\");e.exports=r,r.ee=o},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||\"object\"!==e&&\"function\"!==e?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i=\"nr@id\",a=t(\"gos\");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!x++){var t=g.info=NREUM.info,e=p.getElementsByTagName(\"script\")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return u.abort();f(y,function(e,n){t[e]||(t[e]=n)}),s(\"mark\",[\"onload\",a()+g.offset],null,\"api\");var n=p.createElement(\"script\");n.src=\"https:\/\/\"+t.agent,e.parentNode.insertBefore(n,e)}}function o(){\"complete\"===p.readyState&&i()}function i(){s(\"mark\",[\"domContent\",a()+g.offset],null,\"api\")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(c=Math.max((new Date).getTime(),c))-g.offset}var c=(new Date).getTime(),s=t(\"handle\"),f=t(19),u=t(\"ee\"),d=window,p=d.document,h=\"addEventListener\",l=\"attachEvent\",m=d.XMLHttpRequest,v=m&&m.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:m,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var w=\"\"+location,y={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",agent:\"js-agent.newrelic.com\/nr-spa-1039.min.js\"},b=m&&v&&v[h]&&!\/CriOS\/.test(navigator.userAgent),g=e.exports={offset:c,now:a,origin:w,features:{},xhrWrappable:b};t(16),p[h]?(p[h](\"DOMContentLoaded\",i,!1),d[h](\"load\",r,!1)):(p[l](\"onreadystatechange\",o),d[l](\"onload\",r)),s(\"mark\",[\"firstbyte\",c],null,\"api\");var x=0,E=t(21)},{}]},{},[\"loader\",2,14,5,3,4]);\n;NREUM.info={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",licenseKey:\"356631dc7f\",applicationID:\"22258555\",sa:1}\n\n\nvar admantx_url =\ndocument.addEventListener(\"DOMContentLoaded\", function(event) {\n  mps._queue.mpsloaded.push(function(){\n    mps._log('**** LOADED: cnbc-cms-header-insert');\n    if (window.mps) {\n        if (window.CNBC_Premium && CNBC_Premium.isPremium && document.cookie.indexOf('ispro=true') == -1 && (mps.pagevars.type!=\"franchise\")) {\n          mps.nlformtypes = mps.nlformtypes || [];\n          mps.nlformtypes.push('paywall');\n        }\n\n        \/\/<!-- Omniture s_code path -->\n        mps.scodePath=\"\/\/fm.cnbc.com\/applications\/cnbc.com\/staticcontent\/scripts\/omniture\/s_code.js?v=1.6.4.1\";\n        \/\/<!-- end: Omniture s_code path -->\n\n        \/\/<!-- Google PII Fix BEGIN -->\n        mps._queue.mpsinit.push(function() {\n          (function(){\n            mps._urlContainsEmail = function() {\n              var _qs = window.location.href;\n              if (!_qs) {\n                return false;\n              }\n              var _regex = \/([^=&\/<>()[].,;:s@\"]+(.[^=&\/<>()[].,;:s@\"]+)*)@(([[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}])|(([a-zA-Z-0-9]+.)+[a-zA-Z]{2,}))\/;\n              return _regex.test(_qs);\n            };\n            if (mps._urlContainsEmail()) {\n              mps._debug('[MPS]: email address detected in url, bypass gpt.');\n              if (mps.response && mps.response.dart && typeof(mps.response.dart.adunits) === 'object') {\n                if (typeof(window._mpspixZ) != 'string') {\n                  window._mpspixZ = (function(a){var b=\"abcdefghiklmnopqrstuvwxyz\".split(\"\");a||(a=Math.floor(Math.random()*b.length));for(var c=\"\",d=0;d<a;d++)c+=b[Math.floor(Math.random()*b.length)];return c})(12)\n                }\n                for (var i in mps.response.dart.adunits) {\n                  var pixelurl = ((document.location.protocol === 'https') ? 'https' : 'http') + ':\/\/pix.nbcuni.com\/a-pii.gif?X=piiblock&S=' + mps.pagevars.instance + '&P=' + mps.pagevars.mpsid + '&A=' + i + '&U=' + encodeURIComponent(window.location.href) + '&_=' + window._mpspixZ;\n                  mps.response.dart.adunits[i].data = '<img id=\"div-gpt-x-0\" class=\"mps-slot\" data-mps-slot=\"x\" data-mps-loadset=\"0\" style=\"width:0;height:0;margin:0;padding:0;border:0;display:none;\" src=\"' + pixelurl + '\"\/>';\n                }\n              }\n              mps.cloneAd = function() { return false; }\n              return true;\n            } else {\n              return false;\n            }\n          })();\n        });\n        \/\/<!-- Google PII Fix END -->\n   }\n });\n});\n\n\n\n    var setAdblockerCookie = function(adblocker) {\n        var d = new Date();\n        d.setTime(d.getTime() + 60 * 60 * 24 * 30 * 1000);\n        document.cookie = \"__adblocker=\" + (adblocker ? \"true\" : \"false\") + \"; expires=\" + d.toUTCString() + \"; path=\/\";\n    }\n    var script = document.createElement(\"script\");\n    script.setAttribute(\"async\", true);\n    script.setAttribute(\"src\", \"\/\/www.npttech.com\/advertising.js\");\n    script.setAttribute(\"onerror\", \"setAdblockerCookie(true);\");\n    script.setAttribute(\"onload\", \"setAdblockerCookie(false);\");\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n\n\n\n\n    var admantx_url = ('https:'==document.location.protocol?'https:':'http:')+'\/\/usasync01.admantx.com\/admantx\/service?request=' + escape('{\"decorator\":\"template.nbc_template\",\"key\":\"62263fff3cc1d07f85c7f8261a0c8f7dc096b35f59c82a713f20a9db8d562ff2\",\"method\":\"descriptor\",\"filter\":\"default\",\"mode\":\"async\",\"type\":\"URL\",\"body\":\"' + escape(document.location.href) + '\"}');\n\n    function admantx_callback(data) {\n        if (top.__nbcudigitaladops_inject && top.__nbcudigitaladops_inject.dtprm) {\n            if (data && data.admants) {\n                if (data.status == \"OK\") {\n                    for (var x = 0; x < data.admants.length; x++) {\n                        var kv = 'adg=' + data.admants[x] + ';';\n                        top.__nbcudigitaladops_inject.dtprm(kv)\n                    }\n                } else {\n                    top.__nbcudigitaladops_inject.dtprm(\"asg=othererror;\")\n                }\n            }\n        }\n    }\n    (function() {\n        var sct = document.createElement('script');\n        sct.src = admantx_url;\n        sct.type = 'text\/javascript';\n        sct.async = 'true';\n        var domel = document.getElementsByTagName('script')[0];\n        domel.parentNode.insertBefore(sct, domel);\n    })();\n\n\n\n\n  if(typeof window.MediaSource !== 'function') {\n    if(typeof document.getElementsByTagName('meta')['tp:PreferredRuntimes'] === 'object') {\n        document.getElementsByTagName('meta')['tp:PreferredRuntimes'].setAttribute(\"content\", \"flash,html5\");\n    }\n}\n\n\n\n\n\n\n Microsoft co-founder Bill Gates said Monday he would bet against bitcoin if he could.  \"As an asset class, you're not producing anything and so you shouldn't expect it to go up. It's kind of a pure 'greater fool theory' type of investment,\" Gates said on CNBC's \"Squawk Box.\" \"I agree I would short it if there was an easy way to do it,\" he said. The price of one bitcoin briefly soared $2,000 last year to above $19,000 in mid-December. The cryptocurrency has since lost more than half its value and was trading near $9,300 on Monday morning. Roughly $9.8 billion has been raised through sales of new coins, or ICOs, since 2016, according to financial research firm Autonomous Next. \"Bitcoin and ICOs, I believe completely [they're some] of the crazier, speculative things,\" Gates said. He added someone once gave him some bitcoin for his birthday, but he sold it a few years afterward. Cryptocurrencies can in fact be shorted \u2014 futures and put options for bitcoin have been made available through some exchanges, for instance \u2014 but making bets against virtual currencies is not as easy as it is for more established assets such as corporate shares. Like many critics of the cryptocurrency price surge, Gates did say the underlying blockchain technology had its merits. Blockchain eliminates the need for a third-party intermediary, such as a bank, by quickly creating a secure, permanent record of a transaction between two parties. Bitcoin is the first application of blockchain technology, and companies are exploring ways to apply blockchain to supply chain management, trading and other areas.  Gates is on the board of Warren Buffett's Berkshire Hathaway and was speaking from Omaha, Nebraska, where the conglomerate held its annual shareholders meeting over the weekend. Billionaire investor Buffett and his longtime investing partner and vice chairman, Charlie Munger, spoke to the tens of thousands attendees on a wide range of topics, including bashing bitcoin as \"rat poison.\" With Berkshire's 2018 annual meeting in the books, users can revisit the highlights in CNBC's Warren Buffett Archive, which houses searchable video from 25 full annual meetings, going back to 1994, synchronized to 2,600 pages of transcripts. The Warren Buffett Archive also includes 500 shorter-form videos arranged by topic, CNBC interviews, a Buffett Timeline, and a Berkshire Portfolio Tracker. Disclaimer  Playing Share this video... Watch Next...","time":1525807673,"title":"Bill Gates: I would short Bitcoin if I could","type":"story","url":"https:\/\/www.cnbc.com\/2018\/05\/07\/bill-gates-i-would-short-bitcoin-if-i-could.html","label":0,"label_name":"biz-news"},{"by":"robterthaddeus","descendants":5,"id":17023981,"kids":"[17024063, 17024137, 17024400, 17024550]","score":82,"text":"All 2018 Sprint Projects with complete documentation for participants \ud83e\udd47","time":1525807643,"title":"This May 10-11, Mozilla is hosting a worldwide sprint on open-source projects","type":"story","url":"https:\/\/github.com\/mozilla\/global-sprint\/milestone\/1","label":7,"label_name":"random"},{"by":"JSeymourATL","descendants":0,"id":17023979,"kids":"None","score":5,"text":"People are increasingly relying on mobile apps, rather than physical banks by THE DATA TEAM IN 2017 the number of people without access to banking services fell to 1.7bn, down from 2.5bn in 2011, thanks largely to the rise of mobile-payment apps. Physical banks and ATMs can be expensive to setup, especially in rural areas, but the rise of mobile banking has meant that for the first time, hundreds of millions of people living in the developing world now have access to finance. According to the Findex, an index compiled by the World Bank, only 69% of adults around the world have bank or mobile-money accounts. But 78% of wage-earners without bank accounts have mobile phones. So mobile-payment services should have plenty of room for further growth. Read more in our special report on financial inclusion by THE DATA TEAM Tell us what you think of Economist.com Need assistance with your subscription? Get 3 free articles per week, daily \n newsletters and more. \n  Published since September 1843 to take part in\n  \u201ca severe contest between intelligence, which presses forward,\n  and an unworthy, timid ignorance obstructing our progress.\u201d\n Copyright \u00a9 The Economist Newspaper Limited 2018. All rights reserved.","time":1525807613,"title":"Access to banking services is spreading throughout the developing world","type":"story","url":"https:\/\/www.economist.com\/blogs\/graphicdetail\/2018\/05\/daily-chart-3","label":7,"label_name":"random"},{"by":"jonbaer","descendants":0,"id":17023970,"kids":"None","score":2,"text":"Google is reportedly helping young tech entrepreneur Michael Sayman build and launch a social gaming startup. Bloomberg reports that the project \u2014 called Arcade \u2014 won\u2019t be tied to any existing social networks; instead, users will create accounts based off their phone number. The games might involve some trivia elements. Google confirmed Arcade\u2019s existence to Bloomberg but provided no details other than the project being \u201cfocused on mobile gaming with friends.\u201d So while this sounds like an extremely nascent endeavor, it appears Google is taking cues from live trivia game HQ. Those live trivia sessions can reach millions of users and have created real-world social gatherings where colleagues, friends, and families get together for a game. Bloomberg says Google won\u2019t tie Arcade to a specific social platform because the game will generate its own place for users to hang out, which HQ demonstrates could work out just fine. Command Line delivers daily updates from the near-future.","time":1525807557,"title":"Google is secretly building a social-gaming startup called Arcade","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/2\/17312318\/google-alphabet-arcade-gaming-startup","label":7,"label_name":"random"},{"by":"driverlessnick","descendants":0,"id":17023946,"kids":"None","score":1,"text":"With warm weather and limited regulation, Arizona has become the center of the driverless world. With this, there has been a lot of media coverage and overall interest in driverless technology from the general public.  We recently came across a video of a driver who was asked by Fox10 Phoenix to share his experience with Tesla Autopilot.\u00a0 The video is a quick watch and the driver, Peter Lafford, does a great job explaining the functionality in a way that is easy for everyone to understand.\u00a0 Mr. Lafford utilizes his Autopilot nearly everyday from Phoenix to East Mesa, over a 20 mile stretch of mostly highway driving. He states in the video that 95% of the time the Autopilot is fine but that he is very cautious about having his hands on the wheel and staying alert at all times. He mentions that the best part about Autopilot is that it frees you from the pedal and the brake in stop and go traffic. This may seem like a minor benefit but for an everyday driver like Mr. Lafford, this could be the difference between an enjoyable trip and a stressful commute.\u00a0 The reporter asks Mr. Lafford how confident are you in Autopilot, and he responded, \"I'm really confident, I'm quite happy with it.\"\u00a0 Here is the segment: We enjoyed the video and were interested in hearing more from Mr. Lafford about his experience.\u00a0 He responded and gave us a full review of Autopilot on his 2018 Tesla Model 3. It was no surprise, but he gave the vehicle a perfect rating: He continued with additional information:  Thanks for the information Mr. Lafford and continue to enjoy your assisted daily commute!  Image credit: Peter Lafford No Comments Enter your email below to receive a weekly recap. This privacy policy has been compiled to better serve those who are concerned with how their 'Personally Identifiable Information' (PII) is being used online. PII, as described in US privacy law and information security, is information that can be used on its own or with other information to identify, contact, or locate a single person, or to identify an individual in context. Please read our privacy policy carefully to get a clear understanding of how we collect, use, protect or otherwise handle your Personally Identifiable Information in accordance with our website.What personal information do we collect from the people that visit our blog, website or app? When registering on our site, as appropriate, you may be asked to enter your name, email address or other details to help you with your experience.When do we collect information? We collect information from you when you register on our site, subscribe to a newsletter, fill out a form or enter information on our site.How do we use your information? We may use the information we collect from you when you register, make a purchase, sign up for our newsletter, respond to a survey or marketing communication, surf the website, or use certain other site features in the following ways:  Reviews are those of the individual poster and are their own subjective opinions and not opinions of Driverless Ratings. We do not endorse nor are we affiliated with any manufacturer or provider of any vehicle listed or reviewed on this site. Driverless Ratings reserves the right to remove any review or rating that fails to meet these guidelines.Reviews must be: Driverless Ratings reserves the right to remove any review for any reason at any time. ","time":1525807409,"title":"\u201cMore Like a Copilot Than Self Driving\u201d \u2013 A Wise Tesla Driver","type":"story","url":"https:\/\/driverlessratings.com\/news\/review-more-like-a-copilot-than-self-driving-a-wise-tesla-driver","label":7,"label_name":"random"},{"by":"727374","descendants":0,"id":17023943,"kids":"None","score":2,"text":"","time":1525807399,"title":"Redis Client side caching: initial design ideas","type":"story","url":"https:\/\/groups.google.com\/forum\/#!topic\/redis-db\/xfcnYkbutDw","label":7,"label_name":"random"},{"by":"deesix","descendants":22,"id":17023936,"kids":"[17024162, 17024661, 17024390, 17024612, 17024289, 17024062, 17024160, 17024096, 17024155, 17024414]","score":50,"text":"As the war for creating customized AI hardware heats up, Google  announced at Google I\/O 2018 that is rolling out out its third generation of silicon, the Tensor Processor Unit 3.0. Google CEO Sundar Pichai said the new TPU is eight times more powerful than last year, with up to 100 petaflops in performance.\u00a0Google joins pretty much every other major company in looking to create custom silicon in order to handle its machine operations. And while multiple frameworks for developing machine learning tools have emerged, including PyTorch  and Caffe2, this one is optimized for Google\u2019s TensorFlow. Google is looking to make Google Cloud an omnipresent platform at the scale of Amazon, and offering better machine learning tools is quickly becoming table stakes.\u00a0 Amazon and Facebook  are both working on their own kind of custom silicon. Facebook\u2019s hardware is optimized for its Caffe2 framework, which is designed to handle the massive information graphs it has on its users. You can think about it as taking everything Facebook knows about you \u2014 your birthday, your friend graph, and everything that goes into the news feed algorithm \u2014 fed into a complex machine learning framework that works best for its own operations. That, in the end, may have ended up requiring a customized approach to hardware. We know less about Amazon\u2019s goals here, but it also wants to own the cloud infrastructure ecosystem with AWS.\u00a0 All this has also spun up an increasingly large and well-funded startup ecosystem looking to create a customized piece of hardware targeted toward machine learning. There are startups like Cerebras Systems, SambaNova Systems, and Mythic, with a half dozen or so beyond that as well (not even including the activity in China). Each is looking to exploit a similar niche, which is find a way to outmaneuver Nvidia on price or performance for machine learning tasks. Most of those startups have raised more than $30 million.\u00a0 Google unveiled its second-generation TPU processor at I\/O last year, so it wasn\u2019t a huge surprise that we\u2019d see another one this year. We\u2019d heard from sources for weeks that it was coming, and that the company is already hard at work figuring out what comes next. Google at the time touted performance, though the point of all these tools is to make it a little easier and more palatable in the first place.\u00a0 Google also said this is the first time the company has had to include liquid cooling in its data centers, CEO Sundar Pichai said. Heat dissipation is increasingly a difficult problem for companies looking to create customized hardware for machine learning. There are a lot of questions around building custom silicon, however. It may be that developers don\u2019t need a super-efficient piece of silicon when an Nvidia card that\u2019s a few years old can do the trick. But data sets are getting increasingly larger, and having the biggest and best data set is what creates a defensibility for any company these days. Just the prospect of making it easier and cheaper as companies scale may be enough to get them to adopt something like GCP.\u00a0 Intel, too, is looking to get in here with its own products. Intel has been beating the drum on FPGA as well, which is designed to be more modular and flexible as the needs for machine learning change over time. But again, the knock there is price and difficulty, as programming for FPGA can be a hard problem in which not many engineers have expertise. Microsoft is also betting on FPGA, and unveiled what it\u2019s calling Brainwave just yesterday at its BUILD conference for its Azure cloud platform \u2014 which is increasingly a significant portion of its future potential. Microsoft launches Project Brainwave, its deep learning acceleration platform  Google more or less seems to want to own the entire stack of how we operate on the internet. It starts at the TPU, with TensorFlow layered on top of that. If it manages to succeed there, it gets more data, makes its tools and services faster and faster, and eventually reaches a point where its AI tools are too far ahead and locks developers and users into its ecosystem. Google is at its heart an advertising business, but it\u2019s gradually expanding into new business segments that all require robust data sets and operations to learn human behavior.\u00a0 Now the challenge will be having the best pitch for developers to not only get them into GCP and other services, but also keep them locked into TensorFlow. But as Facebook increasingly looks to challenge that with alternate frameworks like PyTorch, there may be more difficulty than originally thought. Facebook unveiled a new version of PyTorch at its main annual conference, F8, just last month. We\u2019ll have to see if Google is able to respond adequately to stay ahead, and that starts with a new generation of hardware. Facebook announces PyTorch 1.0, a more unified AI framework  \u00a0 \u00a0 ","time":1525807346,"title":"Tensor Processing Unit (TPU) Version 3","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/google-announces-a-new-generation-for-its-tpu-machine-learning-hardware\/","label":5,"label_name":"ml"},{"by":"dean","descendants":0,"id":17023931,"kids":"None","score":1,"text":"","time":1525807320,"title":"Searching for Positive Returns at the Track","type":"story","url":"https:\/\/www.researchgate.net\/publication\/292145708_Searching_for_Positive_Returns_at_the_Track","label":7,"label_name":"random"},{"by":"joak","descendants":0,"id":17023923,"kids":"None","score":1,"text":"Uber  sees a future where users can request a flying Uber. And it\u2019s what, as CEO Dara Khosrowshahi calls it, a big, bold bet. He says in an interview with CBS This Morning that big bold bets are what built Uber. \u201cWe want to create the network around those vehicles so regular people can take these taxis in the air for longer distances when they want to avoid traffic at affordable prices,\u201d said Khosrowshahi. The goal is for the flying taxis to be driverless and hold four riders per vehicle to keep the cost lower for the passengers. Users would haul the air taxi from an app and then meet it at an Uber rooftop facility. According to the CBS interview, Uber says the taxis will be relatively quiet thanks to multiple props and electric motors. The company plans to launch trials as early as 2020. Uber is set to unveil more details about its air taxi today and tomorrow at its Uber Elevate  conference.","time":1525807272,"title":"First look at Uber's flying taxi models","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/this-is-the-first-look-at-ubers-air-taxi-concept\/","label":7,"label_name":"random"},{"by":"vinnyglennon","descendants":0,"id":17023918,"kids":"None","score":2,"text":"\n\n\n\t\tThis web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please\n\t\tview our cookie policy.\n\t But Eric Schmidt urges DoD to do more AI programs like the one that sparked protests from Google employees. \n                                   By Liam Tung\n            \n                    \n        \n                                        |            April 19, 2018 -- 11:13 GMT (04:13 PDT)\n                \n                                                                                            | Topic: Innovation\n Video: Worried AI will take your job? Google wants to help you out. After Google employees rebelled over the company's AI work for the Pentagon, Eric Schmidt, Google's former CEO and current Alphabet board member, has laid out what will need to happen to break the impasse. But he also thinks the Pentagon should press ahead with more Silicon Valley partnerships. AI would be useful for \"defensive and perhaps offensive purposes\" in warfare, Schmidt told the House Armed Services Committee on Tuesday, The Verge reports.  The comment hit on one of the main concerns thousands of Google staff raised in a letter to Google CEO Sundar Pichai in March over the company's work for the Department of Defense's (DoD) Project Maven, an AI-based video analysis system for object recognition.  The Pentagon hopes that Project Maven, which taps Silicon Valley for skills it lacks, will help it catch up with cutting-edge AI that it believes will be the future of warfare.  See: Special report: How to implement AI and machine learning (free PDF) However, as the Google employee protest highlighted, the tech community has yet to agree on the boundaries of its acceptable use.     While Google has said the TensorFlow-based system is \"for non-offensive uses only\", staff were concerned that once the technology is delivered it could easily be used for offensive purposes.  Elise Stefanik, a Republican member of the House of Representatives, said tech companies contributing to AI had \"expressed reluctance to working with the DoD\".  \"How do we overcome this skepticism?\" she asked Schmidt.      \"The industry is going to reach some sort of agreement on the use of AI what is called principles -- what is appropriate use and what is not,\" said Schmidt. \"My guess is that there will be some kind of consensus among key industry players on that.\"  Stefanik asked the question after expressing grave concerns over China's stated ambitions to lead AI development by 2030 and wondered how the US would keep pace. Schmidt said the department had vast amounts of data that could be used to develop AI algorithms but hadn't organized it appropriately.  \"The DoD broadly has a great deal of data, which isn't stored anywhere or places in which programs are no longer alive. Getting all that data in a place that's useable, discoverable and useful for the mission is crucial.\" Schmidt added that most DoD contractors aren't \"AI-capable\", though they're developing those skills.    In his written testimony, Schmidt said the DoD had an \"innovation adoption problem\" and was hamstrung by a culture that values \"compliance over results\".  Executive Guide: What is AI? Everything you need to know about Artificial Intelligence   The department was also wasting its possession of data by failing to label and structure it in a way that tech companies have for several years now.   \"Though labeled data is the fuel for AI, DoD has yet to fully leverage the value of both unclassified and classified datasets,\" he wrote. Despite the lack of agreement on what is acceptable use of AI, Schmidt said the DoD needs to develop more initiatives like Project Maven.  \"The world's most prominent AI companies focus on gathering the data on which to train AI and the human capital to support and execute AI operations,\" he said. \"If DoD is to become 'AI-ready', it must continue down the pathway that Project Maven paved and create a foundation for similar projects to flourish, in addition to its basic research efforts.\" Google employees protest: 'Stop work on AI for Pentagon drone video analysis'  Google workers call on the company to cease developing AI for warfare. Google Alphabet's Schmidt: Here's why we can't keep fake news out of search results  Google's search algorithms have a problem ranking truthful information when two groups oppose each other. Google Alphabet's Schmidt: Ignore Elon Musk's AI fears - he's no computer scientist  Our artificial intelligence won't kill you. We just want everyone to have their own personal assistant, says Alphabet chairman Eric Schmidt. Mobility Windows 10 PC but pocket-sized? 5-inch Mi Mini PC arrives at an initial $149 price CXO IT jobs: How the role of the CIO is changing - again Security Budget 2018: Government eyes blockchain, funds Parliamentary cyber centre Robotics Robots with soft hands will transform the world. Here's why. Robotics Restaurants and retail snapping up automated kiosks Innovation Australian VR headset sales driven by video games: Telsyte Innovation Reddit plans Ethereum and Litecoin support with crypto relaunch Innovation Amazon opens more monetization tools for Alexa developers \u00a9 2018 CBS Interactive. All rights reserved.\n                    Privacy Policy |\n                    Cookies |\n                    Ad Choice |\n                    Advertise |\n                    Terms of Use |\n                    Mobile User Agreement\n","time":1525807236,"title":"Ex-Google CEO Schmidt's AI Warfare Warning","type":"story","url":"https:\/\/www.zdnet.com\/article\/ex-google-ceo-schmidts-warfare-warning-we-need-ai-ground-rules-for-pentagon-work\/","label":5,"label_name":"ml"},{"by":"panarky","descendants":5,"id":17023917,"kids":"[17024680, 17024647, 17024638, 17024674]","score":23,"text":"We\u2019re dedicated to building technology that is truly helpful for everyone. We\u2019re creating tools and features that help people better understand their tech usage, focus on what matters most, disconnect when needed, and create healthy habits for the whole family. We\u2019re committed to giving everyone the tools they need to develop their own sense of digital wellbeing. So that life, not the technology in it, stays front and center.  The first step toward digital wellbeing is often understanding more about how you interact with technology in the first place. We\u2019re introducing new ways to keep you more informed and proactive. The app dashboard gives you a complete picture of how you use your phone. Get a daily view of the time spent on your phone, how frequently you use different apps, and how many notifications you get. Coming soon Schedule custom breathers as often as you want, pausing what you\u2019re currently watching and encouraging you to step away. Learn more Your time watched profile gives you a closer look at how much time you spend on YouTube, as well as comparisons to previous time periods. Coming soon In addition to helping you find answers quicker and get places sooner, we\u2019re also building tools that help you avoid daily distractions and look at your devices less. Hey Google, it\u2019s bedtime Ok, turning off lights and your alarm is set for tomorrow Now even when your hands are full you can make phone calls and send texts just by talking to the Assistant. Learn more Play all your TV shows, queue up your favorite music, and more just by asking the Assistant. Learn more Control devices like the Nest Thermostat to set the temperature or the alarm, all with your voice. Learn more Do more without reaching for your device. Your voice can activate Custom Routines, automating everything from your TV to your alarm clock all at once. Learn more Turn on high-priority notifications to limit the number of email alerts you receive and get notified only when it\u2019s important. Learn more Priority Inbox hides all your nonessential communications so that your inbox is only unread, starred, and important messages. Learn more Now you can get all of your YouTube push notifications bundled into a single notification each day \u2013 and choose when you get it. Learn more Get automatic suggestions of photos to archive (like receipts or screenshots), so your main photo library stays full of your best shots. Learn more Google Photos can automatically stylize your photos and create themed movies made just for you. Learn more Like hitting snooze on your alerts, now you can temporarily hide specific notifications for as long as you like. Learn more With notification categories you have more control over the alerts you see, what they look like, and how often you see them. Learn more The app timer lets you set limits for how much you use your apps. When you\u2019ve reached that limit, the app icon is grayed out for the rest of the day. Coming soon With one tap in your Quick Settings, Do Not Disturb can hide all notifications so you won\u2019t see them on screen. Coming soon Save time and send contextually relevant responses straight from your notifications with Smart Reply. Coming soon Ok Google, navigate home All right, home. Let\u2019s go Android Auto is your driving companion, so you can keep your eyes on the road while controlling navigation, media, and more with the Google Assistant. Learn more In today\u2019s always-on world, disconnecting can be hard. Across a range of products,\nwe\u2019re creating more opportunities for you to hit pause, so you can strike the right balance. Now notifications sent between 10pm and 8am will be received without sound or vibration, and you can customize your own quiet hours. Learn more Autoplay control lets you decide whether you want the next video to start playing automatically. Learn more Now with voice-powered Do Not Disturb mode, you can tell your phone to silence all notifications and communications. Learn more Now your voice can activate the Bedtime Routine \u2013 set an alarm, dim the lights, lower your music, get tomorrow\u2019s weather, and Zzzz. Learn more Wind Down gets your phone ready for bed by letting you schedule changes to the display. Activating Night Light reduces blue light and Grayscale gets rid of all color, reminding you to switch off for the night. Coming soon Easily turn on Do Not Disturb by flipping your phone face down. Coming soon Kids today are growing up with technology, not growing into it like previous generations. We\u2019re working with families to ensure kids can safely enjoy the best of tech, while forming balanced relationships with all things digital. As your child begins using their first Android device, the Family Link app helps you set certain digital ground rules for them. Learn more Get notifications that let you approve or block the apps your kids want to download from the Google Play Store. Learn more Set screen time limits, schedule device bedtimes, see how often your kids use apps, and remotely lock their devices when it\u2019s time to play, study, or sleep. Learn more Get recommendations from actual teachers and help your kids enjoy quality content you can feel good about. Coming soon YouTube Kids makes it safer and simpler for kids to explore the world through online video, and easier for parents to guide them. Access a suite of parental controls and find the right balance for your family. Decide whether or not your kids can use YouTube Kids search, keep tabs on the videos they\u2019re watching, and even block videos or channels you don\u2019t want them to see. Learn more Set a timer to control how long your kids are able to play with the app. Learn more Decide when your kids have access to the Internet by pausing connectivity on one or all of their devices simultaneously, or help them wind down by scheduling a time out. Learn more Tap into Google\u2019s SafeSearch technology to automatically block content from the devices your kids use. Learn more Be Internet Awesome helps kids become smart, confident explorers of the Internet through a game for kids, info for parents, and custom curriculum for teachers. Kids can explore Interland, an online adventure that teaches them key lessons of digital safety and citizenship. Learn more Parents can tap into resources from the Family Online Safety Institute and ConnectSafely, and sign the Be Internet Awesome pledge along with their kids. Learn more Educators can download lesson plans and classroom activities that have received the ISTE Seal of Alignment to help their students learn about digital safety. Learn more","time":1525807232,"title":"Great technology should improve life, not distract from it","type":"story","url":"https:\/\/wellbeing.google\/","label":7,"label_name":"random"},{"by":"joak","descendants":0,"id":17023916,"kids":"None","score":1,"text":"","time":1525807231,"title":"CBS News: First look at Uber's flying taxi models","type":"story","url":"https:\/\/m.youtube.com\/watch?v=-XFbyWAUwp4","label":7,"label_name":"random"},{"by":"ryan_j_naughton","descendants":0,"id":17023912,"kids":"None","score":1,"text":" This post is adapted from the blog of\u00a0Wikileaf, a Priceonomics Data Studio customer.\u00a0Does your company have interesting data?\u00a0Become a Priceonomics customer. *** A tidal wave of change is hitting the cannabis industry in both the United States and Canada. In the United States, medical marijuana is now legal in 29 states. Not only that, but the recreational use of marijuana is now legal in 10 states. You can walk into a licensed store in places like California, Oregon and Colorado and purchase cannabis nearly as easily as buying beer. Similar dynamics are taking place in Canada. Medical marijuana consumption was first legalized in 2001, and in 2017 legislation paved the way for the legalization of recreational use throughout all of Canada -- a development that\u2019s expected to be implemented in the summer of this year. With the emergence of legal weed in the US and Canada, we were curious, in which country is it more expensive? How does the price of marijuana vary across cities in the United States and Canada? We analyzed data from Priceonomics customer Wikileaf,\u00a0a company that tracks cannabis prices at dispensaries across the US and Canada and aggregated the data at the national level and find out the answers. We discovered that cannabis is 30% less expensive in Canada than the United States. When you look at different cities, the price differential can be even more pronounced. Legal marijuana is 39% cheaper in Vancouver than San Francisco, for example. *** We begin our analysis by looking at the average price of an eighth of an ounce of marijuana in the United States versus Canada at the beginning of April 2018. Throughout this piece, Canadian prices are converted to US dollars to make the price comparison consistent.  Data source: Wikileaf Across dispensaries tracked by Wikileaf in the United States, the price of an eighth of marijuana is $40.0, compared to $27.9 in Canada, where it is 30% cheaper. Part of the reason cannabis is so much cheaper in Canada than the United States is there is a much longer history of legalization in Canada, and thus a larger supply of legal marijuana growers and sellers. While cannabis companies in the United States can\u2019t even have bank accounts, in Canada there are publicly traded cannabis companies on the stock market. In anticipation of nationwide legalization this year, supply of marijuana continues to grow. Next we look at the price difference between the two countries based on the size of the purchase? Is Canadian marijuana still less expensive if you buy a small amount (a gram) versus large amount (an ounce)?  Data source:\u00a0Wikileaf At every quantity purchased, it\u2019s much cheaper to buy in Canada versus the United States. At the bottom of the chart, we calculate the percentage discount of Canadian weed versus American. Interestingly, the largest quantity you purchase, the smaller the price discount in Canada is. It turns out that in the United States, they give you a larger bulk discount on weed. Since the beginning of 2018, the prices of weed in Canada and the United States have been very stable. Below we chart the weekly average price for an eighth of an ounce of cannabis on Wikileaf since the new year:  Data source:\u00a0Wikileaf In the past year, the price of marijuana has been extremely stable in both the United States and Canada. *** Lastly, we conclude by looking at the price of marijuana in the most popular cities among Wikileaf users in the United States and Canada. Is there a large variation in the price of weed in the Canadian cities Vancouver and Toronto? How about in US cities like San Francisco, Seattle, Denver, Portland, and Los Angeles?  Data source:\u00a0Wikileaf. Chart created with Onomics. San Francisco, a city which wins a lot of these \u201cmost expensive awards\u201d (toast, real estate, coffee, etc), also has the most expensive marijuana of all the cities we looked at. In San Francisco, an eighth of an ounce of weed is 12% more expensive than in Seattle and 20% more expensive than in Los Angeles.\u00a0 When compared to its US counterparts, the Canadian cities of Toronto and Vancouver offer a substantial discount. Someone from San Francisco visiting Vancouver can purchase weed at 39% discount compared to at home.\u00a0 *** So, it turns out marijuana is about 30% cheaper in Canada than the United States. Those differences have persisted all year and are pervasive at all purchase sizes. By most accounts, the demand for marijuana in Canada is quite high, so that is unlikely to be the reason for the price discount versus the United States.\u00a0 Instead, cannabis is cheaper in Canada because there has been a longer history of a legal supply of weed. \u00a0With marijuana, as with most things, when the supply is high, the prices are not. *** Note:\u00a0If you\u2019re a company that wants to work with Priceonomics to turn your data into great stories, learn more about the\u00a0Priceonomics Data Studio.\u00a0  Learn More \u00bb  Learn More \u00bb  Read Now \u00bb A dashboard for tracking content marketing performance Learn More A free tool from from  A free content marketing dashboard from Priceonomics. Learn More \u00bb  \n        Turn your company data into content marketing people actually like.\n       \n        Learn More \u00bb\n      ","time":1525807201,"title":"Here\u2019s How Much Marijuana Costs in the United States vs. Canada","type":"story","url":"https:\/\/priceonomics.com\/heres-how-much-marijuana-costs-in-the-united\/","label":7,"label_name":"random"},{"by":"SiempreViernes","descendants":1,"id":17023885,"kids":"[17023919, 17023938]","score":5,"text":" Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp LinkedIn Copy this link These are external links and will open in a new window US President Donald Trump says he will withdraw the US from an Obama-era nuclear agreement with Iran. Calling it \"decaying and rotten\", he said the deal was \"an embarrassment\" to him \"as a citizen\". Going against advice from European allies, he said he would reimpose economic sanctions that were waived when the deal was signed in 2015.  In response, Iran said it was preparing to restart uranium enrichment, key for making both nuclear energy and weapons. Iran's President Hassan Rouhani said: \"The US has announced that it doesn't respect its commitments. \"I have ordered the Atomic Energy Organisation of Iran to be ready to start the enrichment of uranium at industrial levels.\" He said he would \"wait a few weeks\" to speak to allies and the other signatories to the nuclear deal first. \"If we achieve the deal's goals in cooperation with other members of the deal, it will remain in place,\" he said. The so-called Joint Comprehensive Plan of Action (JCPOA) curbed Iran's nuclear activities in return for the lifting of sanctions that had been imposed by the UN, US and EU. Mr Trump had previously complained that the deal only limited Iran's nuclear activities for a fixed period; had failed to stop the development of ballistic missiles; and had handed Iran a $100bn (\u00a374bn) windfall that it used \"as a slush fund for weapons, terror, and oppression\" across the Middle East. Former President Barack Obama, who signed the deal on behalf of the US three years ago, called Mr Trump's announcement \"misguided\". The US Treasury said economic sanctions would not be reimposed on Iran immediately, but would be subject to 90-day and 180-day wind-down periods. In a statement on its website, it said sanctions would be reimposed on the industries mentioned in the 2015 deal, including Iran's oil sector, aircraft exports, precious metals trade, and Iranian government attempts to buy US dollar banknotes. US National Security Advisor John Bolton is reported as saying that European companies doing business with Iran will have to finish within six months or face US sanctions. Analysis by Sebastian Usher, BBC Arab Affairs editor All the signatories of the Iran deal, except for the US, have been in favour of keeping it in some form.   After weeks of trying to dissuade President Trump, they pretty much knew what was coming.   But the announcement may have been even starker than they expected, with Mr Trump making clear that the full weight of sanctions would be reimposed, allowing little or no wriggle room.   This leaves any foreign companies trying to do business with Iran in a difficult position.   And it will make it even harder for leaders like the French President Emmanuel Macron to salvage the deal, despite committing themselves before and after the announcement to persevere with it. France, Germany and the UK have said they \"regret\" the American decision. Former President Obama said on Facebook that the deal was working and was in US interests. \"Walking away from the JCPOA turns our back on America's closest allies, and an agreement that our country's leading diplomats, scientists, and intelligence professionals negotiated. \"At a time when we are all rooting for diplomacy with North Korea to succeed, walking away from the JCPOA risks losing a deal that accomplishes - with Iran - the very outcome that we are pursuing with the North Koreans,\" he said. The European Union's top diplomat, Federica Mogherini, said the EU was \"determined to preserve\" the deal. But Israel's Prime Minister Benjamin Netanyahu says he \"fully supports\" Mr Trump's \"bold\" withdrawal from a \"disastrous\" deal. And Saudi Arabia, Iran's regional rival, says it \"supports and welcomes\" Mr Trump's moves towards pulling out of the deal. The so-called Joint Comprehensive Plan of Action (JCPOA) saw Iran agree to limit the size of its stockpile of enriched uranium - which is used to make reactor fuel, but also nuclear weapons - for 15 years and the number of centrifuges installed to enrich uranium for 10 years.  Iran also agreed to modify a heavy water facility so it could not produce plutonium suitable for a bomb. In return, sanctions imposed by the UN, US and EU that had crippled Iran's economy were lifted. The deal was agreed between Iran and the five permanent members of the UN Security Council - the US, UK, France, China and Russia - plus Germany. Iran insists its nuclear programme is entirely peaceful, and its compliance with the deal has been verified by the International Atomic Energy Agency (IAEA). Iran prepares to restart uranium enrichment while European leaders call on Tehran to uphold the deal.                ","time":1525807067,"title":"Iran nuclear deal: Trump pulls US out","type":"story","url":"http:\/\/www.bbc.com\/news\/world-us-canada-44045957","label":6,"label_name":"news"},{"by":"kikitee","descendants":0,"id":17023875,"kids":"None","score":1,"text":"Posted Tuesday 1st May 2018 \/ Photography &copy Ben Simms \n Megan Hine is a survival expert who consults on TV shows like The Island. She may be the only woman in her field, says longtime friend Bear Grylls, but she\u2019s stronger than 99% of the guys. When she first hears the gunfire, Megan Hine freezes. She\u2019s in a remote part of Kenya, rigging a stunt for a celebrity survival TV show, attaching a rope bridge to one side of a gorge while the rest of her teammates are on the other side.  In between them is a river full of crocodiles, who are reported to have killed and eaten two locals a couple of days before. As the silence is punctured by volleys of shots, with sand exploding near her feet, Hine\u2019s adrenaline spikes. The only way to rejoin her team is to clamber down a rock face and through croc-infested waters. \nLife-or-death decisions like this require a cool head. Hine has faced plenty of them over the years as a survival expert who consults for TV, sometimes co-presenting shows with longtime friend Bear Grylls. She\u2019s also a rafting guide, climbing instructor, bushcraft expert, off-road driving tutor as well as an expedition leader for groups of adults and children alike. To get out of situations like this (an altercation between locals which she eventually escaped unscathed) she pictures a box in her mind \u2014 \u201ca big wooden pirate chest with metal clasps\u201d \u2014 and locks her fears inside.  The vision came to her spontaneously one day when she was blinded by fear during a climb. \u201cSuddenly I was able to see again.\u201d  It\u2019s got her through many scary situations, including being stalked by lions in Namibia, getting caught in an Alpine avalanche, and having a drug cartel tracking her team in Mexico. \nSince she was first headhunted as a safety consultant by Bear Grylls, for his show Man Vs Wild, she has worked with him regularly over 10 years, including on Running Wild, for which celebs like Channing Tatum, Kate Winslet and Barack Obama were brought out into the wilderness. (Grylls has described Hine as his \u201cbest friend\u201d and says she\u2019s \u201cstronger than 99% of the men I know.\u201d) It\u2019s led to some surreal experiences. Camping out with Julia Roberts in Kenya, having sprawling conversations, was \u00a0\u201cincredible,\u201d she says. \u201cYou couldn\u2019t pay for that experience.\u201d She\u2019s quick to point out that whether someone is an Oscar winner or contestant on a survival show like The Island, there\u2019s no special treatment or fakery for the cameras. If anything, she worries about pushing people too hard. \n\u201cWe\u2019re taking away their food, we\u2019re depriving them of sleep, and that messes with people\u2019s heads. When I\u2019m guiding [usually], I wouldn\u2019t let my clients get into the state when they\u2019re an emotional wreck. But it would be boring TV if they didn\u2019t have an emotional response that was visible enough for the viewer to see. \u201cYou have to be very careful that you don\u2019t get Stockholm Syndrome developing,\u201d she adds. The condition, usually associated with kidnapping victims who bond with their captors as a survival strategy, can develop during a shoot. \u201cIf they have been out with us for long enough that they\u2019ve lost their sense of being able to make choices for themselves.\u201d Hine is the only woman in a very small \u2013 very macho \u2013 group of survival experts working in TV. \u201cI\u2019ve spent most of my life being the only girl,\u201d she says, going all the way back to her membership of the military cadets as a teen. \u201cIn all avenues of my career I\u2019ve felt like I had to prove myself, or that guys didn\u2019t treat me the same way. But I decided early on that I wasn\u2019t going to let that bother me.\u201d \nShe recalls the days when she would take stag groups off-roading, and they would greet her with scepticism before bombing off. When they got stranded or stuck, as they inevitably did, it was Hine who came to the rescue. \u201cActions speak louder than words,\u201d she says, with a smile. \u201cBy the end of the week, they had a huge amount of respect.\u201d As a teenager, she drew inspiration from books by pioneering mountain climbers Alison Hargreaves and Lynn Hill. But it\u2019s her grandmother, who went snowshoeing with her in the Alps at 83 and earned a psychology degree a few years before her 90th birthday, that is role model number one. \nHine is still adjusting to the idea of being a role model herself. \u201cGrowing up I was a bit of a rebel,\u201d she says. \u201cI didn\u2019t really fit in. I\u2019d be sent out of lessons, or skip school to go mountain biking, and would be drinking in the park at 14. Because I had all these restrictions around me, I felt so trapped. I\u2019d be sitting in lessons, dreaming about the mountains.\u201d Today, Hine travels for up to 11 months a year, currently alternating between the landscapes of Panama and Nevada for an upcoming Netflix show. She\u2019s passionate about sharing the therapeutic impact the wilderness can have, especially on people who suffer from stress and anxiety. \u201cWhen they\u2019re on an expedition, their symptoms seem to disappear and that happens far too many times for it to be a coincidence.\u201d \nBeing pushed to your limits out in the wild may not be everyone\u2019s idea of therapy \u2013 but Hine wouldn\u2019t call it \u2018stress\u2019. \u201cIt\u2019s more a feeling of hyper-awareness,\u201d she says. \u201cYou can almost see behind you. Your smell and hearing is heightened. You\u2019re aware of any movement. It\u2019s almost like a meditative state. Every cell in your body is fighting for survival.\u201d It\u2019s this \u201cprimal calling\u201d inside us all that makes survival shows so popular, she says. And the resilience and confidence that stems from overcoming challenges makes up for all the pain. \u201cI find it fascinating to see what I\u2019m capable of,\u201d she says. \u201cIt\u2019s the most incredible feeling to discover that you\u2019re stronger than you ever imagined.\u201d She Moves Mountains, stories of women who make it happen, is produced in partnership with The North Face. \u00a0 Posted Tuesday 1st May, 2018Text by Jessica Holland Photography &copy Ben Simms  Read More Read More Read More Read More Read More  Read More  Read More  Read More  Read More  Read More  newsletter sign up\n   \u00a0subscribe","time":1525807017,"title":"The outsider who found her purpose in the wilderness","type":"story","url":"http:\/\/www.huckmagazine.com\/ride\/megan-hine-survival-celebrities-alive-in-the-wild\/","label":7,"label_name":"random"},{"by":"exolymph","descendants":0,"id":17023866,"kids":"None","score":2,"text":"The only thing hotter than the GPUs securing the Zcash network is the debate surrounding ASIC resistance. You don\u2019t need to look further than the most-debated Zcash Forum thread of all time from February, or the debates that routinely pop up in the community chat. The ASIC resistance question has taken on new urgency now that Bitmain has announced the availability of the AntMiner Z9 mini, an Equihash-focused ASIC that seems tailor-made for Zcash, shipping in late June. Another complicating factor is that the Zcash Company\u2019s chief, Zooko, has signalled ambivalence about ASIC resistance. The Foundation is in the process of adopting community feedback as part of our own governance process, and in fact has already planned to represent the community\u2019s interest in this debate via a proposed ballot for our election process. If the world had stood still, I imagine this would have been the only step necessary to take right now, collecting the community\u2019s feedback and iteratively transform the community\u2019s voice into an official Foundation position. But the world doesn\u2019t stand still, as Bitmain has reminded us. We\u2019re still planning on including an ASIC resistance ballot measure in our election process, but we also think that the community expects more from us than waiting until June to act. Consequently, we are currently taking the following steps: Investigation and principled decision making. We are committing funds and effort to investigate the presence and power of ASICs on the Zcash network. We do not know for sure how effective the upcoming AntMiner Z9 mini will be, or the degree to which ASICs already affect the mining process, or whether more powerful ASICs will be developed in the future. All of these questions matter when deciding to change the Equihash parameters, adopt a new PoW type altogether, or welcome ASICs. Board member Andrew Miller is planning to create a proposal through the grants program to convene a Technical Advisory Board to provide scientifically grounded inputs into this decision. Making research and development of a more ASIC resistant strategy an immediate technical priority for the Foundation. We have already outlined a technical roadmap for the next year and are in the process of hiring and project-planning to execute on it. Our roadmap includes development on Bolt payment channels, on alternative wallets, and starting an independent, consensus-compatible implementation of full node software. We are now adding ASIC resistance development as an additional technical priority. Based on continued community approval and the results of our investigation above, we have a rough goal of developing and submitting a mitigation plan through the ZIP process, targeting a deployment in late 2018. Having the ability to carry out a PoW change in the future, especially if it is lead by the Foundation, means we should start now. The Company is signalling they may not do this, but we think there is already a loud and clear interest in the community to at least have this option available. (This is an easy case, the governance experiments are really about harder cases!) Continuing to run the ballot process to gauge community sentiment. Sometimes miner manufacturer claims are hot air. What if the efficiency gains are minimal compared to new GPUs? Would it be worth engaging in a potentially contentious fork, or splitting the community, if GPUs were still competitive? Or, more gravely, perhaps these new ASICs are more advanced than we thought and could handle different parameters of Equihash. One of the \u201ceasy wins\u201d that is often suggested in the community is to update Equihash parameters (which would still take a while to thoroughly test) to maintain ASIC resistance, but if this AntMiner has the ability to adapt to new parameters, then such a fork would be a waste of effort. Simply put, there are too many unknowns to commit to any particular path yet. By putting resources into investigation we can make better decisions and have options available. The Foundation believes it\u2019s important to maintain the power of GPUs in Zcash mining. However\u2014and this nuance is important\u2014we also recognize that ASIC resistance may be a red herring, for the health and decentralization of the protocol in the long term. Perhaps there is another path that we could take, with ample time for community buy-in\u2014and we welcome input on getting there. In the short term, we consider it critical to protect the community members who are building the ecosystem with us. If it\u2019s necessary based on our evaluation of the ASICs on the network, we will hire a developer to construct and submit a ZIP to mitigate its effect on the network. If the Zcash core development team and community approves, it will ideally be deployed by late 2018. While we are committed to engaging in these activities ourselves, we encourage particularly passionate and talented members of the community to apply to our 2018Q2 Grant Program to augment or replace our work here. Yes, we are eager to give money away to help preserve the health of the network. Even if we manage to neuter a wave of Equihash ASICs, this will not be the end of the discussion. Inevitably, some new ASIC will arise, and we may have to go through this process again. I\u2019m all for a Sisyphean effort now and again, but perhaps there is a better solution\u2014one that subverts the entire \u201cASIC resistance\u201d debate? Eventually we will need one, because I\u2019m not sure how sustainable the whack-a-mole strategy will be for the community. There has to be a better way\u2014and I think it starts with reframing the discussion away from \u201cASIC resistance\u201d and towards the perceived goals of ASIC resistance\u2014decentralization, less concentrated proof of work, and accessibility to the network. But in the meantime, we must act, and act we shall. Update: the original version of this post implied that the Foundation was taking a definitive stance for ASIC resistance. It has been edited for clarity to indicate that the Foundation is putting resources into researching ASIC resistance: advantages, disadvantages and potential implementation paths. Early feedback on this post encouraged us to submit this update, but for all edits, you can see full commit history on on GitHub and track changes made here. For people in the community who do not like this approach, or demand simpler and swifter action: we know you don\u2019t want to accept \u2018It\u2019s complicated and we\u2019re still figuring it out.\u2019 But, it\u2019s complicated and we\u2019re still figuring it out. We\u2019ll work together to learn as fast as we can and take the appropriate action as fast as we can. Thank you for your support. Thanks to Andrew Miller, Sonya Mann, and the #the-zcash-foundation community channel for their input and feedback.","time":1525806950,"title":"The Zcash Foundation's Statement on ASIC Resistance","type":"story","url":"https:\/\/z.cash.foundation\/blog\/statement-on-asics\/","label":7,"label_name":"random"},{"by":"jjuhl","descendants":0,"id":17023862,"kids":"None","score":2,"text":"<!--\n\te9 = new Object();\n    e9.size = \"728x90\";\n\/\/-->\n Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 10,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter or contacted via MichaelLarabel.com. The mission at Phoronix since 2004 has centered around enriching the Linux hardware experience. In addition to supporting our site through advertisements, you can help by subscribing to Phoronix Premium. You can also use our NewEgg.com shopping links when making online purchases or contribute to Phoronix through a PayPal tip.  Legal Disclaimer, Privacy Policy | Contact Copyright \u00a9 2004 - 2018 by Phoronix Media. All trademarks used are properties of their respective owners. All rights reserved.","time":1525806932,"title":"Qt 5.11 Release Candidate Arrives, Final Release May Come Early","type":"story","url":"https:\/\/www.phoronix.com\/scan.php?page=news_item&px=Qt-5.11-Release-Candidate","label":7,"label_name":"random"},{"by":"mauliknshah","descendants":12,"id":17023837,"kids":"[17024157, 17024151, 17024206]","score":38,"text":"\n\n\n\n            var postLoadFunctions = {};\n            var foresee_enabled = 1\n            var dynamic_yield_enabled = 1\n                    \n\n\n\n\tCNBC_Comscore = 'Technology';\n\n\n\n\t\tvar mpscall = {\n\t\t\t\t\t\t'node_brand' : '2'  ,  \n\n\t\t\t\t\t'site' : 'cnbc.com-relaunch'  ,  \n\n\t\t\t\t\t'content_id' : '105193332'  ,  \n\n\t\t\t\t\t'path' : '\/id\/105193332'  ,  \n\n\t\t\t\t\t'is_content' : '1'  ,  \n\n\t\t\t\t\t'is_sponsored' : '0'  ,  \n\n\t\t\t\t\t'adunits' : 'Top Banner|Badge A|Badge B|Badge C|Badge D|Flex Ad First|Box Ad 1|Non Iframe Custom|Inline Custom|Movable Box Ad|Responsive Rectangle'  ,  \n\n\t\t\t\t\t'keywords' : '~'  ,  \n\n\t\t\t\t\t'cat' : 'Technology|Enterprise'  ,  \n\n\t\t\t\t\t'cag[configuration_franchise]' : 'Enterprise'  ,  \n\n\t\t\t\t\t'cag[attribution_author]' : 'Jordan Novet'  ,  \n\n\t\t\t\t\t'cag[related_primary]' : 'Microsoft Corp|Amazon.com Inc|NVIDIA Corp|Alphabet Class A|Technology|US: News'  ,  \n\n\t\t\t\t\t'cag[attribution_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[type_franchise]' : 'Enterprise|US: News|Technology'  ,  \n\n\t\t\t\t\t'cag[type_creator]' : 'Jordan Novet'  ,  \n\n\t\t\t\t\t'cag[type_company]' : 'Microsoft Corp|Amazon.com Inc|NVIDIA Corp|Alphabet Class A'  ,  \n\n\t\t\t\t\t'cag[type_tag]' : 'Technology'  ,  \n\n\t\t\t\t\t'cag[type_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[brand]' : 'none'  ,  \n\n\t\t\t\t\t'cag[template]' : 'story_simple'  ,  \n\n\t\t\t\t\t'cag[device]' : 'web'  ,  \n\n\t\t\t\t\t'hline' : 'Enterprise'  ,  \n\n\t\t\t\t\t'type' : 'cnbcnewsstory'  ,  \n\n\t\t\t\t\t'template' : 'story_simple'  ,  \n\n\t\t\t\t\t'title' : 'Google launches the third version of its A.I. chips, an alternative to Nvidia&amp;#039;s'  ,  \n\n\t\t\t\t\t'pubdate' : '1525802569'  ,  \n\n\t\t\t\t\t'stitle' : 'Google TPU Novet 180508 SF'  ,  \n\n\t\t\t\t\t'byline' : 'Jordan Novet'  ,  \n\n\t\t\t\t\t'subtype' : 'section'  ,  \n\n\t\t\t\t\t'id' : '105193332'  ,  \n\n\t\t\t\t\t'nid' : '105193332'  \n\n\t\t\n\t\t}, mpsopts = {\n\t\t  \n\t\t\"host\" : 'mps.cnbc.com',\n\t\t\"updatecorrelator\" : true\n\n\t\t};\n\n\t\tvar mps = mps || {};\n\t\tmps._ext = mps._ext || {};\n\t\tmps._adsheld = [];\n\t\tmps._queue = mps._queue || {};\n\t\tmps._queue.mpsloaded = mps._queue.mpsloaded || [];\n\t\tmps._queue.mpsinit = mps._queue.mpsinit || [];\n\t\tmps._queue.gptloaded = mps._queue.gptloaded || [];\n\t\tmps._queue.adload = mps._queue.adload || [];\n\t\tmps._queue.adclone = mps._queue.adclone || [];\n\t\tmps._queue.adview = mps._queue.adview || [];\n\t\tmps._queue.refreshads = mps._queue.refreshads || [];\n\t\tmps.__timer = Date.now ? Date.now() : (function() { return +new Date })();\n\t\tmps.__intcode = \"v2\";\n\t\tif (typeof mps.getAd != \"function\") mps.getAd = function(adunit) {\n\t\t    if (typeof adunit != \"string\") return false;\n\t\t    var slotid = \"mps-getad-\" + adunit.replace(\/\\W\/g, \"\");\n\t\t    if (!mps._ext || !mps._ext.loaded) {\n\t\t        mps._queue.gptloaded.push(function() {\n\t\t            typeof mps._gptfirst == \"function\" && mps._gptfirst(adunit, slotid);\n\t\t            mps.insertAd(\"#\" + slotid, adunit)\n\t\t        });\n\t\t        mps._adsheld.push(adunit)\n\t\t    }\n\t\t    return '<div id=\"' + slotid + '\" class=\"mps-wrapper\" data-mps-fill-slot=\"' + adunit + '\"><\/div>'\n\t\t};\n\t\t(function() {\n\t\t    head = document.head || document.getElementsByTagName(\"head\")[0], mpsload = document.createElement(\"script\");\n\t\t    mpsload.src = \"\/\/\" + mpsopts.host + \"\/fetch\/ext\/load-\" + mpscall.site + \".js?nowrite=2\";\n\t\t    mpsload.id = \"mps-load\";\n\t\t    head.insertBefore(mpsload, head.firstChild)\n\t\t})();\n\n\t\n\n\n\n\n  \n\n\n\nwindow.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var o=e[n]={exports:{}};t[n][0].call(o.exports,function(e){var o=t[n][1][e];return r(o||e)},o,o.exports)}return e[n].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<n.length;o++)r(n[o]);return r}({1:[function(t,e,n){function r(t){try{c.console&&console.log(t)}catch(e){}}var o,i=t(\"ee\"),a=t(19),c={};try{o=localStorage.getItem(\"__nr_flags\").split(\",\"),console&&\"function\"==typeof console.log&&(c.console=!0,o.indexOf(\"dev\")!==-1&&(c.dev=!0),o.indexOf(\"nr_dev\")!==-1&&(c.nrDev=!0))}catch(s){}c.nrDev&&i.on(\"internal-error\",function(t){r(t.stack)}),c.dev&&i.on(\"fn-err\",function(t,e,n){r(n.stack)}),c.dev&&(r(\"NR AGENT IN DEVELOPMENT MODE\"),r(\"flags: \"+a(c,function(t,e){return t}).join(\", \")))},{}],2:[function(t,e,n){function r(t,e,n,r,o){try{d?d-=1:i(\"err\",[o||new UncaughtException(t,e,n)])}catch(c){try{i(\"ierr\",[c,s.now(),!0])}catch(u){}}return\"function\"==typeof f&&f.apply(this,a(arguments))}function UncaughtException(t,e,n){this.message=t||\"Uncaught error with no additional information\",this.sourceURL=e,this.line=n}function o(t){i(\"err\",[t,s.now()])}var i=t(\"handle\"),a=t(20),c=t(\"ee\"),s=t(\"loader\"),f=window.onerror,u=!1,d=0;s.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(p){\"stack\"in p&&(t(12),t(11),\"addEventListener\"in window&&t(6),s.xhrWrappable&&t(13),u=!0)}c.on(\"fn-start\",function(t,e,n){u&&(d+=1)}),c.on(\"fn-err\",function(t,e,n){u&&(this.thrown=!0,o(n))}),c.on(\"fn-end\",function(){u&&!this.thrown&&d>0&&(d-=1)}),c.on(\"internal-error\",function(t){i(\"ierr\",[t,s.now(),!0])})},{}],3:[function(t,e,n){t(\"loader\").features.ins=!0},{}],4:[function(t,e,n){function r(){C++,M=y.hash,this[u]=b.now()}function o(){C--,y.hash!==M&&i(0,!0);var t=b.now();this[l]=~~this[l]+t-this[u],this[d]=t}function i(t,e){E.emit(\"newURL\",[\"\"+y,e])}function a(t,e){t.on(e,function(){this[e]=b.now()})}var c=\"-start\",s=\"-end\",f=\"-body\",u=\"fn\"+c,d=\"fn\"+s,p=\"cb\"+c,h=\"cb\"+s,l=\"jsTime\",m=\"fetch\",v=\"addEventListener\",w=window,y=w.location,b=t(\"loader\");if(w[v]&&b.xhrWrappable){var g=t(9),x=t(10),E=t(8),O=t(6),R=t(12),P=t(7),T=t(13),S=t(\"ee\"),N=S.get(\"tracer\");t(14),b.features.spa=!0;var M,j=w[v],C=0;S.on(u,r),S.on(p,r),S.on(d,o),S.on(h,o),S.buffer([u,d,\"xhr-done\",\"xhr-resolved\"]),O.buffer([u]),R.buffer([\"setTimeout\"+s,\"clearTimeout\"+c,u]),T.buffer([u,\"new-xhr\",\"send-xhr\"+c]),P.buffer([m+c,m+\"-done\",m+f+c,m+f+s]),E.buffer([\"newURL\"]),g.buffer([u]),x.buffer([\"propagate\",p,h,\"executor-err\",\"resolve\"+c]),N.buffer([u,\"no-\"+u]),a(T,\"send-xhr\"+c),a(S,\"xhr-resolved\"),a(S,\"xhr-done\"),a(P,m+c),a(P,m+\"-done\"),E.on(\"pushState-end\",i),E.on(\"replaceState-end\",i),j(\"hashchange\",i,!0),j(\"load\",i,!0),j(\"popstate\",function(){i(0,C>1)},!0)}},{}],5:[function(t,e,n){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t(\"ee\"),i=t(\"handle\"),a=t(12),c=t(11),s=\"learResourceTimings\",f=\"addEventListener\",u=\"resourcetimingbufferfull\",d=\"bstResource\",p=\"resource\",h=\"-start\",l=\"-end\",m=\"fn\"+h,v=\"fn\"+l,w=\"bstTimer\",y=\"pushState\",b=t(\"loader\");b.features.stn=!0,t(8);var g=NREUM.o.EV;o.on(m,function(t,e){var n=t[0];n instanceof g&&(this.bstStart=b.now())}),o.on(v,function(t,e){var n=t[0];n instanceof g&&i(\"bst\",[n,e,this.bstStart,b.now()])}),a.on(m,function(t,e,n){this.bstStart=b.now(),this.bstType=n}),a.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),this.bstType])}),c.on(m,function(){this.bstStart=b.now()}),c.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),\"requestAnimationFrame\"])}),o.on(y+h,function(t){this.time=b.now(),this.startPath=location.pathname+location.hash}),o.on(y+l,function(t){i(\"bstHist\",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance[\"c\"+s]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"c\"+s]()},!1):window.performance[f](\"webkit\"+u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"webkitC\"+s]()},!1)),document[f](\"scroll\",r,{passive:!0}),document[f](\"keypress\",r,!1),document[f](\"click\",r,!1)}},{}],6:[function(t,e,n){function r(t){for(var e=t;e&&!e.hasOwnProperty(u);)e=Object.getPrototypeOf(e);e&&o(e)}function o(t){c.inPlace(t,[u,d],\"-\",i)}function i(t,e){return t[1]}var a=t(\"ee\").get(\"events\"),c=t(22)(a,!0),s=t(\"gos\"),f=XMLHttpRequest,u=\"addEventListener\",d=\"removeEventListener\";e.exports=a,\"getPrototypeOf\"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+\"-start\",function(t,e){var n=t[1],r=s(n,\"nr@wrapped\",function(){function t(){if(\"function\"==typeof n.handleEvent)return n.handleEvent.apply(n,arguments)}var e={object:t,\"function\":n}[typeof n];return e?c(e,\"fn-\",null,e.name||\"anonymous\"):n});this.wrapped=t[1]=r}),a.on(d+\"-start\",function(t){t[1]=this.wrapped||t[1]})},{}],7:[function(t,e,n){function r(t,e,n){var r=t[e];\"function\"==typeof r&&(t[e]=function(){var t=r.apply(this,arguments);return o.emit(n+\"start\",arguments,t),t.then(function(e){return o.emit(n+\"end\",[null,e],t),e},function(e){throw o.emit(n+\"end\",[e],t),e})})}var o=t(\"ee\").get(\"fetch\"),i=t(19);e.exports=o;var a=window,c=\"fetch-\",s=c+\"body-\",f=[\"arrayBuffer\",\"blob\",\"json\",\"text\",\"formData\"],u=a.Request,d=a.Response,p=a.fetch,h=\"prototype\";u&&d&&p&&(i(f,function(t,e){r(u[h],e,s),r(d[h],e,s)}),r(a,\"fetch\",c),o.on(c+\"end\",function(t,e){var n=this;e?e.clone().arrayBuffer().then(function(t){n.rxSize=t.byteLength,o.emit(c+\"done\",[null,e],n)}):o.emit(c+\"done\",[t],n)}))},{}],8:[function(t,e,n){var r=t(\"ee\").get(\"history\"),o=t(22)(r);e.exports=r,o.inPlace(window.history,[\"pushState\",\"replaceState\"],\"-\")},{}],9:[function(t,e,n){var r=t(\"ee\").get(\"mutation\"),o=t(22)(r),i=NREUM.o.MO;e.exports=r,i&&(window.MutationObserver=function(t){return this instanceof i?new i(o(t,\"fn-\")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype)},{}],10:[function(t,e,n){function r(t){var e=a.context(),n=c(t,\"executor-\",e),r=new f(n);return a.context(r).getCtx=function(){return e},a.emit(\"new-promise\",[r,e],e),r}function o(t,e){return e}var i=t(22),a=t(\"ee\").get(\"promise\"),c=i(a),s=t(19),f=NREUM.o.PR;e.exports=a,f&&(window.Promise=r,[\"all\",\"race\"].forEach(function(t){var e=f[t];f[t]=function(n){function r(t){return function(){a.emit(\"propagate\",[null,!o],i),o=o||!t}}var o=!1;s(n,function(e,n){Promise.resolve(n).then(r(\"all\"===t),r(!1))});var i=e.apply(f,arguments),c=f.resolve(i);return c}}),[\"resolve\",\"reject\"].forEach(function(t){var e=f[t];f[t]=function(t){var n=e.apply(f,arguments);return t!==n&&a.emit(\"propagate\",[t,!0],n),n}}),f.prototype[\"catch\"]=function(t){return this.then(null,t)},f.prototype=Object.create(f.prototype,{constructor:{value:r}}),s(Object.getOwnPropertyNames(f),function(t,e){try{r[e]=f[e]}catch(n){}}),a.on(\"executor-start\",function(t){t[0]=c(t[0],\"resolve-\",this),t[1]=c(t[1],\"resolve-\",this)}),a.on(\"executor-err\",function(t,e,n){t[1](n)}),c.inPlace(f.prototype,[\"then\"],\"then-\",o),a.on(\"then-start\",function(t,e){this.promise=e,t[0]=c(t[0],\"cb-\",this),t[1]=c(t[1],\"cb-\",this)}),a.on(\"then-end\",function(t,e,n){this.nextPromise=n;var r=this.promise;a.emit(\"propagate\",[r,!0],n)}),a.on(\"cb-end\",function(t,e,n){a.emit(\"propagate\",[n,!0],this.nextPromise)}),a.on(\"propagate\",function(t,e,n){this.getCtx&&!e||(this.getCtx=function(){if(t instanceof Promise)var e=a.context(t);return e&&e.getCtx?e.getCtx():this})}),r.toString=function(){return\"\"+f})},{}],11:[function(t,e,n){var r=t(\"ee\").get(\"raf\"),o=t(22)(r),i=\"equestAnimationFrame\";e.exports=r,o.inPlace(window,[\"r\"+i,\"mozR\"+i,\"webkitR\"+i,\"msR\"+i],\"raf-\"),r.on(\"raf-start\",function(t){t[0]=o(t[0],\"fn-\")})},{}],12:[function(t,e,n){function r(t,e,n){t[0]=a(t[0],\"fn-\",null,n)}function o(t,e,n){this.method=n,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],\"fn-\",this,n)}var i=t(\"ee\").get(\"timer\"),a=t(22)(i),c=\"setTimeout\",s=\"setInterval\",f=\"clearTimeout\",u=\"-start\",d=\"-\";e.exports=i,a.inPlace(window,[c,\"setImmediate\"],c+d),a.inPlace(window,[s],s+d),a.inPlace(window,[f,\"clearImmediate\"],f+d),i.on(s+u,r),i.on(c+u,o)},{}],13:[function(t,e,n){function r(t,e){d.inPlace(e,[\"onreadystatechange\"],\"fn-\",c)}function o(){var t=this,e=u.context(t);t.readyState>3&&!e.resolved&&(e.resolved=!0,u.emit(\"xhr-resolved\",[],t)),d.inPlace(t,y,\"fn-\",c)}function i(t){b.push(t),l&&(x?x.then(a):v?v(a):(E=-E,O.data=E))}function a(){for(var t=0;t<b.length;t++)r([],b[t]);b.length&&(b=[])}function c(t,e){return e}function s(t,e){for(var n in t)e[n]=t[n];return e}t(6);var f=t(\"ee\"),u=f.get(\"xhr\"),d=t(22)(u),p=NREUM.o,h=p.XHR,l=p.MO,m=p.PR,v=p.SI,w=\"readystatechange\",y=[\"onload\",\"onerror\",\"onabort\",\"onloadstart\",\"onloadend\",\"onprogress\",\"ontimeout\"],b=[];e.exports=u;var g=window.XMLHttpRequest=function(t){var e=new h(t);try{u.emit(\"new-xhr\",[e],e),e.addEventListener(w,o,!1)}catch(n){try{u.emit(\"internal-error\",[n])}catch(r){}}return e};if(s(h,g),g.prototype=h.prototype,d.inPlace(g.prototype,[\"open\",\"send\"],\"-xhr-\",c),u.on(\"send-xhr-start\",function(t,e){r(t,e),i(e)}),u.on(\"open-xhr-start\",r),l){var x=m&&m.resolve();if(!v&&!m){var E=1,O=document.createTextNode(E);new l(a).observe(O,{characterData:!0})}}else f.on(\"fn-end\",function(t){t[0]&&t[0].type===w||a()})},{}],14:[function(t,e,n){function r(t){var e=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<d;r++)t.removeEventListener(u[r],this.listener,!1);if(!e.aborted){if(n.duration=a.now()-this.startTime,4===t.readyState){e.status=t.status;var i=o(t,this.lastSize);if(i&&(n.rxSize=i),this.sameOrigin){var s=t.getResponseHeader(\"X-NewRelic-App-Data\");s&&(e.cat=s.split(\", \").pop())}}else e.status=0;n.cbTime=this.cbTime,f.emit(\"xhr-done\",[t],t),c(\"xhr\",[e,n,this.startTime])}}}function o(t,e){var n=t.responseType;if(\"json\"===n&&null!==e)return e;var r=\"arraybuffer\"===n||\"blob\"===n||\"json\"===n?t.response:t.responseText;return l(r)}function i(t,e){var n=s(e),r=t.params;r.host=n.hostname+\":\"+n.port,r.pathname=n.pathname,t.sameOrigin=n.sameOrigin}var a=t(\"loader\");if(a.xhrWrappable){var c=t(\"handle\"),s=t(15),f=t(\"ee\"),u=[\"load\",\"error\",\"abort\",\"timeout\"],d=u.length,p=t(\"id\"),h=t(18),l=t(17),m=window.XMLHttpRequest;a.features.xhr=!0,t(13),f.on(\"new-xhr\",function(t){var e=this;e.totalCbs=0,e.called=0,e.cbTime=0,e.end=r,e.ended=!1,e.xhrGuids={},e.lastSize=null,h&&(h>34||h<10)||window.opera||t.addEventListener(\"progress\",function(t){e.lastSize=t.loaded},!1)}),f.on(\"open-xhr-start\",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),f.on(\"open-xhr-end\",function(t,e){\"loader_config\"in NREUM&&\"xpid\"in NREUM.loader_config&&this.sameOrigin&&e.setRequestHeader(\"X-NewRelic-ID\",NREUM.loader_config.xpid)}),f.on(\"send-xhr-start\",function(t,e){var n=this.metrics,r=t[0],o=this;if(n&&r){var i=l(r);i&&(n.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{\"abort\"===t.type&&(o.params.aborted=!0),(\"load\"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||\"function\"!=typeof e.onload))&&o.end(e)}catch(n){try{f.emit(\"internal-error\",[n])}catch(r){}}};for(var c=0;c<d;c++)e.addEventListener(u[c],this.listener,!1)}),f.on(\"xhr-cb-time\",function(t,e,n){this.cbTime+=t,e?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&\"function\"==typeof n.onload||this.end(n)}),f.on(\"xhr-load-added\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&!this.xhrGuids[n]&&(this.xhrGuids[n]=!0,this.totalCbs+=1)}),f.on(\"xhr-load-removed\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&this.xhrGuids[n]&&(delete this.xhrGuids[n],this.totalCbs-=1)}),f.on(\"addEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-added\",[t[1],t[2]],e)}),f.on(\"removeEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-removed\",[t[1],t[2]],e)}),f.on(\"fn-start\",function(t,e,n){e instanceof m&&(\"onload\"===n&&(this.onload=!0),(\"load\"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),f.on(\"fn-end\",function(t,e){this.xhrCbStart&&f.emit(\"xhr-cb-time\",[a.now()-this.xhrCbStart,this.onload,e],e)})}},{}],15:[function(t,e,n){e.exports=function(t){var e=document.createElement(\"a\"),n=window.location,r={};e.href=t,r.port=e.port;var o=e.href.split(\":\/\/\");!r.port&&o[1]&&(r.port=o[1].split(\"\/\")[0].split(\"@\").pop().split(\":\")[1]),r.port&&\"0\"!==r.port||(r.port=\"https\"===o[0]?\"443\":\"80\"),r.hostname=e.hostname||n.hostname,r.pathname=e.pathname,r.protocol=o[0],\"\/\"!==r.pathname.charAt(0)&&(r.pathname=\"\/\"+r.pathname);var i=!e.protocol||\":\"===e.protocol||e.protocol===n.protocol,a=e.hostname===document.domain&&e.port===n.port;return r.sameOrigin=i&&(!e.hostname||a),r}},{}],16:[function(t,e,n){function r(){}function o(t,e,n){return function(){return i(t,[f.now()].concat(c(arguments)),e?null:this,n),e?void 0:this}}var i=t(\"handle\"),a=t(19),c=t(20),s=t(\"ee\").get(\"tracer\"),f=t(\"loader\"),u=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=u);var d=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],p=\"api-\",h=p+\"ixn-\";a(d,function(t,e){u[e]=o(p+e,!0,\"api\")}),u.addPageAction=o(p+\"addPageAction\",!0),u.setCurrentRouteName=o(p+\"routeName\",!0),e.exports=newrelic,u.interaction=function(){return(new r).get()};var l=r.prototype={createTracer:function(t,e){var n={},r=this,o=\"function\"==typeof e;return i(h+\"tracer\",[f.now(),t,n],r),function(){if(s.emit((o?\"\":\"no-\")+\"fn-start\",[f.now(),r,o],n),o)try{return e.apply(this,arguments)}finally{s.emit(\"fn-end\",[f.now()],n)}}}};a(\"setName,setAttribute,save,ignore,onEnd,getContext,end,get\".split(\",\"),function(t,e){l[e]=o(h+e)}),newrelic.noticeError=function(t){\"string\"==typeof t&&(t=new Error(t)),i(\"err\",[t,f.now()])}},{}],17:[function(t,e,n){e.exports=function(t){if(\"string\"==typeof t&&t.length)return t.length;if(\"object\"==typeof t){if(\"undefined\"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if(\"undefined\"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!(\"undefined\"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(e){return}}}},{}],18:[function(t,e,n){var r=0,o=navigator.userAgent.match(\/Firefox[\/s](d+.d+)\/);o&&(r=+o[1]),e.exports=r},{}],19:[function(t,e,n){function r(t,e){var n=[],r=\"\",i=0;for(r in t)o.call(t,r)&&(n[i]=e(r,t[r]),i+=1);return n}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],20:[function(t,e,n){function r(t,e,n){e||(e=0),\"undefined\"==typeof n&&(n=t?t.length:0);for(var r=-1,o=n-e||0,i=Array(o<0?0:o);++r<o;)i[r]=t[e+r];return i}e.exports=r},{}],21:[function(t,e,n){e.exports={exists:\"undefined\"!=typeof window.performance&&window.performance.timing&&\"undefined\"!=typeof window.performance.timing.navigationStart}},{}],22:[function(t,e,n){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t(\"ee\"),i=t(20),a=\"nr@original\",c=Object.prototype.hasOwnProperty,s=!1;e.exports=function(t,e){function n(t,e,n,o){function nrWrapper(){var r,a,c,s;try{a=this,r=i(arguments),c=\"function\"==typeof n?n(r,a):n||{}}catch(f){p([f,\"\",[r,a,o],c])}u(e+\"start\",[r,a,o],c);try{return s=t.apply(a,r)}catch(d){throw u(e+\"err\",[r,a,d],c),d}finally{u(e+\"end\",[r,a,s],c)}}return r(t)?t:(e||(e=\"\"),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,e,o,i){o||(o=\"\");var a,c,s,f=\"-\"===o.charAt(0);for(s=0;s<e.length;s++)c=e[s],a=t[c],r(a)||(t[c]=n(a,f?c+o:o,i,c))}function u(n,r,o){if(!s||e){var i=s;s=!0;try{t.emit(n,r,o,e)}catch(a){p([a,n,r,o])}s=i}}function d(t,e){if(Object.defineProperty&&Object.keys)try{var n=Object.keys(t);return n.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(r){p([r])}for(var o in t)c.call(t,o)&&(e[o]=t[o]);return e}function p(e){try{t.emit(\"internal-error\",e)}catch(n){}}return t||(t=o),n.inPlace=f,n.flag=a,n}},{}],ee:[function(t,e,n){function r(){}function o(t){function e(t){return t&&t instanceof r?t:t?s(t,c,i):i()}function n(n,r,o,i){if(!p.aborted||i){t&&t(n,r,o);for(var a=e(o),c=l(n),s=c.length,f=0;f<s;f++)c[f].apply(a,r);var d=u[y[n]];return d&&d.push([b,n,r,a]),a}}function h(t,e){w[t]=l(t).concat(e)}function l(t){return w[t]||[]}function m(t){return d[t]=d[t]||o(n)}function v(t,e){f(t,function(t,n){e=e||\"feature\",y[n]=e,e in u||(u[e]=[])})}var w={},y={},b={on:h,emit:n,get:m,listeners:l,context:e,buffer:v,abort:a,aborted:!1};return b}function i(){return new r}function a(){(u.api||u.feature)&&(p.aborted=!0,u=p.backlog={})}var c=\"nr@context\",s=t(\"gos\"),f=t(19),u={},d={},p=e.exports=o();p.backlog=u},{}],gos:[function(t,e,n){function r(t,e,n){if(o.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[e]=r,r}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){o.buffer([t],r),o.emit(t,e,n)}var o=t(\"ee\").get(\"handle\");e.exports=r,r.ee=o},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||\"object\"!==e&&\"function\"!==e?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i=\"nr@id\",a=t(\"gos\");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!x++){var t=g.info=NREUM.info,e=p.getElementsByTagName(\"script\")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return u.abort();f(y,function(e,n){t[e]||(t[e]=n)}),s(\"mark\",[\"onload\",a()+g.offset],null,\"api\");var n=p.createElement(\"script\");n.src=\"https:\/\/\"+t.agent,e.parentNode.insertBefore(n,e)}}function o(){\"complete\"===p.readyState&&i()}function i(){s(\"mark\",[\"domContent\",a()+g.offset],null,\"api\")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(c=Math.max((new Date).getTime(),c))-g.offset}var c=(new Date).getTime(),s=t(\"handle\"),f=t(19),u=t(\"ee\"),d=window,p=d.document,h=\"addEventListener\",l=\"attachEvent\",m=d.XMLHttpRequest,v=m&&m.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:m,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var w=\"\"+location,y={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",agent:\"js-agent.newrelic.com\/nr-spa-1039.min.js\"},b=m&&v&&v[h]&&!\/CriOS\/.test(navigator.userAgent),g=e.exports={offset:c,now:a,origin:w,features:{},xhrWrappable:b};t(16),p[h]?(p[h](\"DOMContentLoaded\",i,!1),d[h](\"load\",r,!1)):(p[l](\"onreadystatechange\",o),d[l](\"onload\",r)),s(\"mark\",[\"firstbyte\",c],null,\"api\");var x=0,E=t(21)},{}]},{},[\"loader\",2,14,5,3,4]);\n;NREUM.info={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",licenseKey:\"356631dc7f\",applicationID:\"22258555\",sa:1}\n\n\nvar admantx_url =\ndocument.addEventListener(\"DOMContentLoaded\", function(event) {\n  mps._queue.mpsloaded.push(function(){\n    mps._log('**** LOADED: cnbc-cms-header-insert');\n    if (window.mps) {\n        if (window.CNBC_Premium && CNBC_Premium.isPremium && document.cookie.indexOf('ispro=true') == -1 && (mps.pagevars.type!=\"franchise\")) {\n          mps.nlformtypes = mps.nlformtypes || [];\n          mps.nlformtypes.push('paywall');\n        }\n\n        \/\/<!-- Omniture s_code path -->\n        mps.scodePath=\"\/\/fm.cnbc.com\/applications\/cnbc.com\/staticcontent\/scripts\/omniture\/s_code.js?v=1.6.4.1\";\n        \/\/<!-- end: Omniture s_code path -->\n\n        \/\/<!-- Google PII Fix BEGIN -->\n        mps._queue.mpsinit.push(function() {\n          (function(){\n            mps._urlContainsEmail = function() {\n              var _qs = window.location.href;\n              if (!_qs) {\n                return false;\n              }\n              var _regex = \/([^=&\/<>()[].,;:s@\"]+(.[^=&\/<>()[].,;:s@\"]+)*)@(([[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}])|(([a-zA-Z-0-9]+.)+[a-zA-Z]{2,}))\/;\n              return _regex.test(_qs);\n            };\n            if (mps._urlContainsEmail()) {\n              mps._debug('[MPS]: email address detected in url, bypass gpt.');\n              if (mps.response && mps.response.dart && typeof(mps.response.dart.adunits) === 'object') {\n                if (typeof(window._mpspixZ) != 'string') {\n                  window._mpspixZ = (function(a){var b=\"abcdefghiklmnopqrstuvwxyz\".split(\"\");a||(a=Math.floor(Math.random()*b.length));for(var c=\"\",d=0;d<a;d++)c+=b[Math.floor(Math.random()*b.length)];return c})(12)\n                }\n                for (var i in mps.response.dart.adunits) {\n                  var pixelurl = ((document.location.protocol === 'https') ? 'https' : 'http') + ':\/\/pix.nbcuni.com\/a-pii.gif?X=piiblock&S=' + mps.pagevars.instance + '&P=' + mps.pagevars.mpsid + '&A=' + i + '&U=' + encodeURIComponent(window.location.href) + '&_=' + window._mpspixZ;\n                  mps.response.dart.adunits[i].data = '<img id=\"div-gpt-x-0\" class=\"mps-slot\" data-mps-slot=\"x\" data-mps-loadset=\"0\" style=\"width:0;height:0;margin:0;padding:0;border:0;display:none;\" src=\"' + pixelurl + '\"\/>';\n                }\n              }\n              mps.cloneAd = function() { return false; }\n              return true;\n            } else {\n              return false;\n            }\n          })();\n        });\n        \/\/<!-- Google PII Fix END -->\n   }\n });\n});\n\n\n\n    var setAdblockerCookie = function(adblocker) {\n        var d = new Date();\n        d.setTime(d.getTime() + 60 * 60 * 24 * 30 * 1000);\n        document.cookie = \"__adblocker=\" + (adblocker ? \"true\" : \"false\") + \"; expires=\" + d.toUTCString() + \"; path=\/\";\n    }\n    var script = document.createElement(\"script\");\n    script.setAttribute(\"async\", true);\n    script.setAttribute(\"src\", \"\/\/www.npttech.com\/advertising.js\");\n    script.setAttribute(\"onerror\", \"setAdblockerCookie(true);\");\n    script.setAttribute(\"onload\", \"setAdblockerCookie(false);\");\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n\n\n\n\n    var admantx_url = ('https:'==document.location.protocol?'https:':'http:')+'\/\/usasync01.admantx.com\/admantx\/service?request=' + escape('{\"decorator\":\"template.nbc_template\",\"key\":\"62263fff3cc1d07f85c7f8261a0c8f7dc096b35f59c82a713f20a9db8d562ff2\",\"method\":\"descriptor\",\"filter\":\"default\",\"mode\":\"async\",\"type\":\"URL\",\"body\":\"' + escape(document.location.href) + '\"}');\n\n    function admantx_callback(data) {\n        if (top.__nbcudigitaladops_inject && top.__nbcudigitaladops_inject.dtprm) {\n            if (data && data.admants) {\n                if (data.status == \"OK\") {\n                    for (var x = 0; x < data.admants.length; x++) {\n                        var kv = 'adg=' + data.admants[x] + ';';\n                        top.__nbcudigitaladops_inject.dtprm(kv)\n                    }\n                } else {\n                    top.__nbcudigitaladops_inject.dtprm(\"asg=othererror;\")\n                }\n            }\n        }\n    }\n    (function() {\n        var sct = document.createElement('script');\n        sct.src = admantx_url;\n        sct.type = 'text\/javascript';\n        sct.async = 'true';\n        var domel = document.getElementsByTagName('script')[0];\n        domel.parentNode.insertBefore(sct, domel);\n    })();\n\n\n\n\n  if(typeof window.MediaSource !== 'function') {\n    if(typeof document.getElementsByTagName('meta')['tp:PreferredRuntimes'] === 'object') {\n        document.getElementsByTagName('meta')['tp:PreferredRuntimes'].setAttribute(\"content\", \"flash,html5\");\n    }\n}\n\n\n\n\n\n\n Google on Tuesday announced that it has developed a third generation of its special chips for artificial intelligence. The new Tensor Processing Units (TPUs) will help Google improve applications that use artificial intelligence to do things like recognize words people are saying in audio recordings, spot objects in photos and videos, and pick up underlying emotions in written text. As such, the chips represent an alternative to Nvidia's graphics processing units. Also, if the new version is anything like its predecessor, it will also become accessible to third-party developers through Google's public cloud service, which could help Google compete with Amazon and Microsoft.  Earlier this week Microsoft announced the early availability of special chips in its Azure cloud. Pichai boasted about the vast computing power that's possible when people use large fleets of these third-generation TPUs.  \"Each of these pods is now eight times more powerful than last year's version -- well over 100 petaflops,\" he said. For context, a box containing 16 of Nvidia's latest GPUs offers two petaflops of computing power. The chips are liquid-cooled -- a feature that's sometimes used for high-performance computing chips or some performance-oriented chips in people's PCs.  Last year's version is already showing good results. Test results posted in recent months suggest that the second-generation TPUs could deliver better performance than existing options with GPUs in certain scenarios, although the TPUs do have certain limitations, like lacking support for the Facebook-backed PyTorch AI software framework. The PyTorch open-source community has been working to change that. Google first announced the TPU initiative in 2016.  Playing Share this video... Watch Next...","time":1525806784,"title":"Google launches the third version of its A.I. chips, an alternative to Nvidia's","type":"story","url":"https:\/\/www.cnbc.com\/2018\/05\/08\/google-launches-tpu-3-point-0-third-version-of-ai-chips.html","label":0,"label_name":"biz-news"},{"by":"bdibs","descendants":0,"id":17023833,"kids":"None","score":1,"text":"  Analytics with insight. by \nBrandon\n \u00b7\nPublished May 5, 2018\n\u00b7 Updated May 5, 2018\n Photo by Igor Ovsyannykov A\/B testing is an effective way to test different variations of a web page or application screen. Your users are split between a control (what you\u2019re testing against) and the variation(s). Data driven decisions from the results of A\/B testings can improve your conversion rate, sign up rate, and lifetime value of your users, improving your bottom line. Without constant experimentation, you\u2019re possibly missing out on revenue or other conversions that add up quickly. There\u2019s almost limitless possibilities for what you\u2019re able to test. Some examples include: There are a few general guidelines you should follow when performing any A\/B tests: \u00a0 \n\n\n Tags: a\/b testinganalyticsexperimentroitesting \nMay 5, 2018\n \u00a0by \nBrandon\n \u00b7 Published May 5, 2018\n \nMay 5, 2018\n \u00a0by \nBrandon\n \u00b7 Published May 5, 2018\n \nMay 5, 2018\n \u00a0by \nBrandon\n \u00b7 Published May 5, 2018\n\u00b7 Last modified May 6, 2018  Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n Follow: Business \/ Experimentation An Introduction to A\/B Testing 5 May, 2018 Business \/ Experimentation 5 Testing Mistakes That You Should Avoid at All Costs 5 May, 2018 Analytics The Importance of Analytics 5 May, 2018 Analytics Analytics For Ecommerce Websites 5 May, 2018 Business Why You Should Focus on Customer Retention 6 May, 2018 Emberbase \u00a9 2018. All Rights Reserved.","time":1525806770,"title":"An Introduction to A\/B Testing","type":"story","url":"https:\/\/blog.emberbase.com\/2018\/05\/05\/introduction-ab-testing\/","label":1,"label_name":"business"},{"by":"nikse","descendants":0,"id":17023823,"kids":"None","score":2,"text":"Taking teenage rebellion to a new level, Oklahoma high school senior George Wang stunned teachers and fellow students when in April he co-authored a chemistry paper up-ending a long-held belief about carbon, a fundamental element of life. In his Journal of Molecular Modeling paper, Wang and his two mentors showed that carbon, long thought to be only capable of forming four bonds, can actually form seven. In publishing this unprecedented work, Wang joins a community of theoretical chemists who explain to Inverse that the discovery is far more than a mere scientific achievement. Among the scientists who reviewed Wang\u2019s work before it was published in the journal was Joel Liebman, Ph.D., a professor of chemistry at the University of Maryland, Baltimore County (UMBC). \u201cI recall getting it, finding myself liking it,\u201d he tells Inverse, \u201cbut never dreaming it was a high school student, never asking it, never challenging it.\u201d The names and affiliations on the paper, which also included Wang\u2019s high school chemistry professor at the Oklahoma School of Science and Mathematics, Dr. A.K. Fazlur Rahman, and University of Oklahoma chemist Bin Wang, Ph.D., weren\u2019t familiar to Liebman, who only found out that George was in high school when Inverse wrote to him in late April. \u201cTo find out a high school student was involved, I think the most remarkable thing about it is the degree of encouragement that the student got,\u201d says Liebman.  \u201cI am very encouraged by the chutzpah, the gutsiness of it, to simply ask the question and follow up on it,\u201d Liebman says. \u201cIt\u2019s a wonderful, wonderful thing.\u201d Article continues below Take any introductory chemistry course and you\u2019re guaranteed to learn that carbon, by nature of the number of electrons circling its nucleus, is capable of forming four bonds with other elements, forming basic organic chemicals like carbon dioxide, ethanol, and methane. Under very rare circumstances, however, an unusual form of carbon called a \u201chypercarbon\u201d can occur, and this can, theoretically, form more than four bonds. In the 1950s, scientists showed it could sometimes form five bonds, and in 2016, a German team showed in a celebrated paper that it could form six. Wang\u2019s work, showing that seven bonds are theoretically possible, wowed even Moritz Malischewski, Ph.D., of the Free University of Berlin, who co-authored the 2016 paper. \u201cEven for a chemist that has prepared a molecule containing hexacoordinated carbon the possible existence of a heptacoordinated carbon atom is fascinating,\u201d he tells Inverse in an email. \u201cIt is truly exceptional that a high school student is reading about very recent chemistry research and wants to get involved and perform research on his own.\u201d Wang did the work on his own after being challenged by Rahman, an organic chemist by training, during a high school chemistry lesson. \u201cI asked the students, is it possible that it can make more than six?\u201d Rahman, who presented Malischewski\u2019s paper as a springboard, told Inverse in a previous interview. Taking up the challenge, Wang fiddled around with the VASP atom modeling program, which he\u2019d learned to use with the help of a mentor and online manuals, and presented his calculations to Rahman. With the help of Bin Wang, a physical chemist, the team got the paper published in a peer-reviewed international journal. George, who was a junior at the time he made the calculations, got top billing on the paper. What made his work all the more remarkable was the fact that it came out of Oklahoma, a state with a notoriously abysmal education system that only recently ended a massive teacher\u2019s strike, meant to protest the lack of funding and resources available to teachers. \u201cWe have congressmen, senators, who don\u2019t believe in science,\u201d Rahman told Inverse, explaining that Wang\u2019s example underscored the need for better support for teachers, who students in turn rely on for intellectual support. Liebman, a chemist whose career stretches back to the 1970s, emphatically supports the need for better mentorship in the sciences. \u201cGood mentoring is rather rare,\u201d he says, applauding Rahman\u2019s role in Wang\u2019s work. \u201cTo have someone who is not threatened but is rather encouraging \u2014 that is rather rare.\u201d Noting that the field of chemistry is traditionally \u201cinternational and intercultural,\u201d he is encouraged that Wang and Rahman, by building on the work of the German team and George Andrew Olah, Ph.D., the Nobel Prize-winning chemist that pioneered hypercarbon chemistry, are continuing that legacy. Arguably, the most important effect of Rahman\u2019s mentorship is not that Wang found the answer to the daring question posed in his high school class but that Wang was encouraged to pursue research that is by nature rebellious.  Theoretical chemistry \u2014 and the discipline of science as a whole \u2014 is built on the exploration of the unknown and the unimaginable. Perhaps Wang\u2019s willingness to do so can be chalked up to his youth; maybe it\u2019s just that he isn\u2019t as well acquainted with the \u201crules\u201d of chemistry just yet. Whatever the case, his story illustrates why creativity is so highly valued in the field. \u201cIt is important that theoretical chemists propose interesting but hard-to-reach molecules and maybe, one day, they will be synthesized in the lab,\u201d says Malischewski. \u201cI know too many people, when confronted with the question that was asked \u2014 is it possible to have seven bonds \u2014 would have answered, \u2018Of course not!\u2019\u201d says Liebman. \u201cThis boy said, \u2018What\u2019s the big deal?\u2019\u201d Love space? Listen to the latest episode of our new podcast: I Need My Space","time":1525806715,"title":"High Schooler's Unprecedented Carbon Discovery Has Scientists Wowed","type":"story","url":"https:\/\/www.inverse.com\/article\/44600-oklahoma-student-carbon-7-bonds-george-wang","label":7,"label_name":"random"},{"by":"walterbell","descendants":0,"id":17023820,"kids":"None","score":1,"text":"\nBy Nick Barrett \r\n\tThere was a time not so long ago when behavioural scientists believed they'd found a sweet spot between hands-off Reaganomics and the paternalistic nanny state. \r\n\tGovernments wanted citizens to behave in ways that benefited them, but didn't want to be seen to be telling them what to do or how to live. The answer was to be found in 'choice architecture', a term popularised by Richard Thaler and Cass Sunstein in their 2008 book 'Nudge'. \r\n\tSince then, this theory has come to impact on millions of our day-to-day decisions, from whether we buy fruit or veg to how we urinate. It is always operating under-the-radar, affecting you on a subconscious level. But it is not always so generous. The row over Theresa May's 'hostile environment' shows what happens when Nudge theory goes dark. \r\n\tIt's hard to argue against the many benefits of choice architecture. People are more likely to agree to become organ donors, for instance, when they are asked to opt-out as opposed to opting-in. This way the NHS can get its hands on more organs without that state forcibly harvesting your liver. Similarly, people are more likely to leave money to charity after they die if they are presented with the option to do so while registering their will. \r\n\tIt's easy to see why these ideas would be attractive to policymakers who could suddenly encourage more people to live healthy lives, save for a pension and pay their taxes earlier without the use of coercion. The individual was free. And as long as the government's preferred outcome was presented as the path of least resistance, most of us would be inclined to do the right thing. People would make better choices for themselves and governments would become more efficient. \r\n\tThese ideas have been seized upon by politicians around the world, on both the left and right. In 2009, Cass Sunstein was hired by the Obama White House. A year later David Cameron set up a 'nudge unit' in Downing Street. Over the last eight years, almost every interaction between the state and the individual has been amended with apparent (and some not so apparent) elements of choice architecture, including applications that do not necessarily have the individual's best interests at heart. \r\n\tIn 2010 the Conservatives came into power with two decade-defining targets. One was to eliminate the deficit by 2015 by dramatically reducing government spending. The other was to reduce immigration to the tens of thousands. The government then employed behavioural science to these ends, tasked with nudging the public away from interacting with the welfare state and creating a sprawling bureaucracy to deter legal and illegal migrants from living an ordinary life in Britain. \r\n\tAcross the country, many can only see a GP by waiting outside a surgery in the wind and the rain so that they can ask for an appointment at the reception desk at 8am. This is no accident. A recent survey revealed that one in five people have given up trying seeing a doctor more than once in the last year because they had trouble getting through to their surgery. \r\n\tThe aim is to ensure that nobody goes to see a GP unless they feel like they absolutely have to. Most people are aware that the government uses choice architecture to make the service that we are entitled to feel as if they were a desperate last resort. \r\n\tIn the world of online user experience (UX), web-designs that deliberately make your experience harder are known as \"dark patterns\", a term coined by the user research specialist Harry Brignull. You might encounter dark patterns when airline websites force you to opt out of buying travel insurance or when newsletters bury a tiny 'unsubscribe' button deep within their small print. Amazon, which patented the one-click purchase back in 1999, requires would-be escapees to click on a minimum of eight unmarked links at the bottom of the page before they can delete their Amazon account. \r\n\tJust as a website can use a big yellow button to make buying a book or signing up to a newsletter inviting, governments can use nudge theory to make saving money for your pension easy and user-friendly. But it can also establish its own dark patterns too and the biggest government dark pattern of all is the hostile environment policy established in 2012 to encourage migrants to leave the country. \r\n\tThe policy meant that without the right paperwork, people were deprived of health services, employment rights and access to housing and effectively excluded from mainstream society. They were not barred. The circumstances were simply created to nudge them into leaving the country. \r\n\tFor six years the hostile environment persecuted the least visible among us. It was only when its effects on the Windrush generation were revealed that the policy\u2019s inherent prejudice became clear to all. What could once be seen as firm but fair suddenly looked cruel and unusual. These measures might have been defensible if the legal migration process hadn\u2019t been turned into a painfully punitive process for anybody arriving from outside of the EU. \r\n\tThese bureaucratic dark patterns allow the government to celebrate immigration in public while simultaneously strangling it with red tape behind the closed doors of the Home Office. The government can do anything and everything to drive down the number of migrants while simultaneously providing Michael Gove with enough plausible deniability to tell the Today Programme that Britain is \"the most immigration-friendly country in the EU\" and walk away unchallenged. After all, migrants are welcome here - but terms and conditions apply. \r\n\tNudge theory was initially conceived and applied as an exciting idea with noble purposes but, as Harry Brignull has remarked, \"pretty much any usability principle there is you can just invert and use for an evil purpose\". The government can't dramatically refine access to the welfare state or round up immigrants in the street, but it can use choice architecture to discourage people from accessing public services or living in a country they are legally entitled to be in. And best of all: They don't ever have to admit what they are doing. \nNicholas Barrett is a journalist based in London. You can follow him on Twitter here. \nThe opinions in politics.co.uk's Comment and Analysis section are those of the author and are no reflection of the views of the website or its owners. Advertise your job vacancies here \n\r\n                \u00a9 2004-2018\r\n            ","time":1525806706,"title":"Hostile environment: The dark side of nudge theory","type":"story","url":"http:\/\/www.politics.co.uk\/comment-analysis\/2018\/05\/01\/hostile-environment-the-dark-side-of-nudge-theory","label":7,"label_name":"random"},{"by":"jonbaer","descendants":22,"id":17023805,"kids":"[17024695, 17024084, 17024182, 17024064, 17024272]","score":60,"text":"Front page layout Site theme Sign up or login to join the discussions! \nSam Machkovech\n    -  May 8, 2018 6:04 pm UTC\n Tuesday's\u00a0I\/O keynote\u00a0included a segment on Google Assistant with a slew of newly announced features, but none was as startling as its rollout of Google Duplex: a voice-powered service that pretends to be human and calls businesses on your behalf. Google CEO Sundar Pichai played back two phone conversations that he alleged were 100-percent legitimate, in which Google's AI-driven voice service called real-world businesses and scheduled appointments based on a user's data. In both cases, voices that sounded decidedly more human and realistic than the default female Google Assistant voice used seemingly natural speech patterns. Phrases like \"um\" and a decidedly West Coast question-like tilt could be heard as Google Duplex confirmed both a salon appointment and a dinner reservation. (The two calls were completed with different voices: one male, one female.) \"We are still developing this technology,\" Pichai told the I\/O crowd, and\u00a0he admitted many calls in Duplex's testing phase \"didn't quite go as expected.\" (Pichai did not play any sample audio of these failed tests.) Pichai says the service will also automatically call businesses during holidays and special events to discover whether shops will be opened or closed and update Google's listings accordingly. That part of Duplex's service will launch \"as an experiment in the coming weeks.\" The fuller Duplex service did not receive as formal a launch window. [Update, 3:05 p.m.: After I\/O's conclusion,\u00a0a lengthy breakdown of Duplex\u00a0went live at the Google Blog. The post includes more sample Duplex calls, including a Google attempt to confirm hours of operation, as previously described in Pichai's speech, along with apparent successes in responding to interruptions. One call's audio, at the bottom of the blog post, includes more awkward and sometimes even rude-sounding speech and tone from Duplex's robotic speakers than in other examples, but the text doesn't acknowledge this disconnect.] Another Google Assistant option, dubbed\u00a0\"Pretty Please,\" will debut \"later this year\" as an option that can be toggled on a per-device basis. It will, quite simply, require that users say \"please\" when issuing a voice command before Google Assistant will respond to it. The I\/O presentation included a brief video of families using home Google Assistant devices and laughing while checking each other's politeness. This goes one further than Amazon's Echo line of devices, which will roll out a \"politeness affirmation\" option tomorrow, May 9, dubbed the \"magic word\" feature. Amazon's version will simply offer affirmation when a user adds \"please\" to a search query. Additionally, Google Assistant will soon receive a \"continued conversation\" option that users can elect to toggle. This will allow Google Assistant to continue listening to users after a question has been answered for the sake of follow-up queries. The idea being, people with many repeat queries will be able to ask multiple questions in a row, often sandwiched together. Google's Scott Huffman explained that, in many cases, \"and\" qualifiers can be inserted into questions to ask two things at once, though not always separated as two clean, separate phrases. The continued conversation system will take these into account, along with brief conversational phrases between questions. And more voices are coming to Google Assistant, with six new American-accented voices coming \"as of today.\" Pichai introduced the new voices, who all greeted the crowd (though the final voice robotically pronounced Pichai's name). He emphasized that real-voice recordings were used and processed by Google's \"Wavenet\" system, which he also showed being used by singer John Legend\u2014whose own voice will be used in a Google Assistant feature \"later this year.\" You must login or create an account to comment. Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox.","time":1525806621,"title":"Google Duplex will call salons, restaurants, and pretend to be human for you","type":"story","url":"https:\/\/arstechnica.com\/gadgets\/2018\/05\/google-duplex-will-call-salons-restaurants-and-pretend-to-be-human-for-you\/","label":9,"label_name":"tech"},{"by":"Tomte","descendants":0,"id":17023798,"kids":"None","score":1,"text":"\nChromostereopsis\nAdvancing color and receding color For many people, the effect is larger when they see the figures with a\ndistance of more than 1 meter.  \"The end of the earth\" For more than half people, the red ring appears to be in front of the blue\n\"earth\". In addition, they appear to move. Copyright Akitaoka Kitaoka 2004 (December 29)  \"Blinds\" The image appears to be blinds, in which yellow appears in front while\nblue appears behind. Copyright Akitaoka Kitaoka 2004 (October 22)  \"Vertebral column\" The array of gold rectangles appears to form a column. I am not sure that\nthis effect is really chromostereopsis. In addition, there is lightness\nillusion in the uniform blue background, which is grating induction.1) Copyright A.Kitaoka 2004 (July 17) 1) McCourt, M. E. (1982) A spatial frequency dependent grating-induction\neffect. Vision Research, 22, 119-134.  \"Kamaboko\"* *kind of food For many people, the red is located on the top of the illusory bulges. Copyright A.Kitaoka 2003   \"A black heart\" For more than half people, the black heart appears to be behind the brown\nsurround. In addition, the heart appears to move a little. Copyright Akitaoka Kitaoka 2005 (March 2) cf.\nFaubert, J. (1995) Colour induced steropsis in images with achromatic information\nand only one other colour. Vision Research, 35, 3161-3167.\nFaubert, J. (1994) Seeing depth in colour: More than just what meets the\neyes. Vision Research, 34, 1165-1186.  \"Floating heart\"* *The \"Trick Eyes\" version For more than half observers, the black heart appears to float in front\nof the surround. However, some observers see the reversal. In this version,\nthe heart appears to move frequently. Copyright A.Kitaoka 2002\n(c)Akiyoshi Kitaoka \"Trick eyes\" Tokyo: KANZEN 2002  *I found a similar image in a previous paper, in which the inset was of\nsquare shape, the stimuli were random dots, and the background is green\nand the surround is red. It seems that this paper is always cited in the\nstudy of chromostereopsis.\nFaubert, J. (1994) Seeing depth in colour: more than just what meets the \neyes. Vision Research, 34, 1165-1186   \"Chochin\"* *a kind of light For many people, the red is located on the top of the illusory bulges. Copyright A.Kitaoka 2003   \"Floating heart\" For more\n\nthan half observers, the red heart appears to float in front of\n\nthe blue surround (upper panel) while the blue heart appears to\n\nsink behind the red surround (lower panel). However, some\n\nobservers see the reversal. This demonstration depends on\n\nchromostereopsis. In addition, the hearts sometimes appear to\n\nmove. Copyright A.Kitaoka 2002   \"Flattening\n\nheart\" The\n\nchromostereopsis reduces or disappears when the background is\n\nwhite. Those who see the red heart in front of the blue surround\n\nwhen the background is black (\"Floating heart\"), tend\n\nto see the red BEHIND the blue when the background is white.  Copyright A.Kitaoka 2002 Back","time":1525806561,"title":"Chromostereopsis","type":"story","url":"http:\/\/www.ritsumei.ac.jp\/~akitaoka\/scolor-e.html","label":7,"label_name":"random"},{"by":"Tomte","descendants":0,"id":17023797,"kids":"None","score":1,"text":"\n    RAUC is a lightweight update client that runs on your Embedded Linux device and\n    reliably controls the procedure of updating your device with a new firmware\n    revision. RAUC is also the tool on your host system that lets you create, inspect\n    and modify update artifacts for your device.\n     \n\n\u00a0Download\n\n\u00a0Visit on GitHub\n RAUC uses X.509 cryptography to sign your update bundles RAUC uses full image updates of redundant slots RAUC requires no fixed device \/ partition setup but instead\n      can be adapted to your needs \n      The best way to start using RAUC is reading our Documentation.\n       \n      Note that designing a robust redundancy and update mechanism\n      for your device requires a lot of design considerations and\n      additional configuration that cannot be fully covered by a\n      generic update tool.\n       You may always integrate RAUC manually (read the Integration\n      chapter for this). But, for those who like a quick start, here are the 3\n      shortcuts that will make it easy to use RAUC on your next\n      device.\n       Use the meta-rauc\n            layer to integrate RAUC into your Yocto BSP Full RAUC support in PTXdist was added in 2017.04.0 RAUC support in Buildroot was added with 2017.08 release Post your question to the mailing list, use\n      the GitHub Issue Tracker\n Join the discussions on the mailing list, or feel\n      free to send GitHub Pull Requests\n","time":1525806550,"title":"RAUC: Safe and Secure Updating","type":"story","url":"https:\/\/www.rauc.io\/","label":3,"label_name":"dev"},{"by":"SwellJoe","descendants":0,"id":17023789,"kids":"None","score":2,"text":"By Zoya Teirstein When Peter Coughlin was in his sophomore year at James Madison University in northern Virginia, he was besieged by a strange and unsettling illness. At random times throughout the night, Coughlin would wake up with hives, full body chills, and raging fevers. These episodes always ended up with him in the bathroom, throwing up until his stomach was empty. After keeping a food journal for nearly a year, Coughlin realized his symptoms occurred after eating meat, primarily pork. \u201cI essentially spent a week proving my point,\u201d Coughlin says. \u201cI\u2019d eat a bunch of red meat, and go through a series of pretty severe reactions.\u201d When he finally went to the hospital in 2016, the doctor tested him for all the usual allergies and was flummoxed by the lack of results. She gave him a strong antihistamine and an EpiPen and sent him home. Frustrated, Coughlin started researching. He found similarities between his symptoms and documented cases of something called alpha-gal allergy. A major study on the allergic reaction had been done right across the Blue Ridge Mountains at the University of Virginia. Suddenly, his hiking trips in that very mountain range came into focus: \u201cI kept pulling ticks off of me,\u201d he says. According to the research, those little brown bugs, marked by a telltale white spot, were to blame for his meat allergy. Coughlin was bit by lone star ticks. Alpha-gal isn\u2019t your typical hayfever-like allergy. It\u2019s a severe, delayed-reaction immune response, which means it hits hours after someone who suffers from the allergy eats meat. People with alpha-gal describe their episodes as terrifying experiences that can land you in the emergency room and change the way you live your life. \u201cI was disheartened,\u201d Coughlin says. \u201cI\u2019m a big eater.\u201d Even a decade ago, only small populations of lone star ticks were found in the northeastern U.S. As climate change shifts temperatures and humidity levels across the country, many types of ticks, which thrive in warm, humid weather, are able to expand their ranges. The EPA even uses Lyme disease, which is transmitted by blacklegged ticks, as an indicator to track where the country is warming. The spread of lone stars has been linked to climate change, and now, the ticks have made it all the way up through Maine, imparting severe red meat allergies on unsuspecting carnivores\u200a\u2014\u200aand offering a window into our changing world and its effect on human health. As lone stars expand into new communities this summer, the ticks are poised to catch people off guard. And just like Coughlin, these little fellows are big eaters. As you read this, millions of tiny, black-and-brown-legged creatures are beginning to reawaken after laying dormant underneath layers of last year\u2019s leaf cover. Ticks are only second to mosquitoes as vectors for human disease. This week, the Centers for Disease Control and Prevention published a report showing illnesses from ticks, fleas, and mosquitoes are on the rise. Disease cases in the U.S. more than tripled between 2004 and 2016, and the report found that we\u2019re ill-equipped to tackle the growing problem. Large swaths of the eastern U.S. are already dealing with an epidemic of Lyme disease, an illness that can rob you of your short-term memory, your motor functions, and, very rarely, even your life. And every so often, it seems, the ticks that rouse themselves from the leaf litter are armed with unexpected and mysterious pathogens, like the resurfaced Powassan virus or Pacific Coast tick fever. The CDC report says seven new tick-borne infections have been recorded since 2004. The organization hasn\u2019t recognized alpha-gal allergies yet. \u201cIt\u2019s scary,\u201d says Graham Hickling, the director of the University of Tennessee\u2019s Center for Wildlife Health. \u201cPretty much every year, we\u2019re finding something new.\u201d A combination of factors has allowed lone stars to conquer territories far outside their known range. Climate change is among them. It\u2019s likely affecting the viability rates for the thousands of eggs that a single lone star can lay at a time. \u201cWhen we start getting these warm seasons, high rainfall kind of years, that probably means that those 2,000 baby ticks do a lot better,\u201d Hickling says. That\u2019s not the only way climate change is aiding survival rates. Many ticks go dormant during the winter, when consecutive below-freezing days and nights turn them into sesame-sized popsicles. But as warming keeps taking days out of the region\u2019s cold season, ticks are able to stay active for longer. Hickling says a benign climate is helpful for ticks and what they carry: \u201cThere are more opportunities for those viruses to start infecting us.\u201d Holly Gaff, a tick-borne disease expert at Old Dominion University in Norfolk, Virginia, also points to one of the tick\u2019s favorite hosts, the white-tailed deer. Deer can travel several miles in the days or even weeks it takes for lone stars to feed on them, eventually dropping the ticks a long way from where they first picked them up. Reforestation efforts in the eastern U.S. that began in the 20th century, coupled with a slump in hunting, have led to an explosion in white-tail deer populations. The growth of suburbs means there are plenty of people pressed up against these wooded areas. Gaff calls this combination of factors\u200a\u2014\u200ahigher deer populations, people living next to fragmented forests, a friendlier climate\u200a\u2014\u200athe \u201cperfect storm\u201d for lone star\u2013tick proliferation. \u201cWhen you have nature in balance you get some ticks, but not like this,\u201d Gaff says. Already, at least 600 known cases of alpha-gal have occured north of the Mason-Dixon line, according to University of North Carolina allergist Scott Commins, one of the researchers who discovered the connection between ticks and alpha-gal. But that\u2019s probably only a fraction of the incidences. It\u2019s a difficult pathology to diagnose, and doctors aren\u2019t required to report alpha-gal to the CDC. Compared to blacklegged ticks, lone stars are much more aggressive. Blacklegged ticks behave in relatively predictable ways\u200a\u2014\u200athey hang out in leafy undergrowth, arms and legs outstretched in case a hapless animal or human passes by. According to Ellen Stromdahl, a researcher with the United States Army Public Health Center, blackleggeds are relatively small and weak. Lone stars, on the other hand, hunt in packs and travel at surprising speeds, emerging from the leaf litter like a swarm of thirsty, galloping lentils. \u201cIf you sit in the middle of the woods breathing out CO2, you\u2019ll get a fan club of lone stars pretty quickly,\u201d Hickling says. On top of lone stars\u2019 rapacious mentality, Old Dominion\u2019s Gaff says that after conducting a series of experiments, the bugs \u201cseem to be invincible.\u201d She\u2019s tried freezing them\u200a\u2014\u200abut they came crawling out of the freezer after seven days on ice. Next, she tried drowning them, figuring that sea-level rise on Virginia\u2019s coast could end up doing humanity a favor by drowning out tick populations. Her team submerged lone stars in salt, fresh, and brackish water. Every single tick lasted for at least 30 days in each condition\u200a\u2014\u200athe last lone star died after 74 days. It only takes one bite from a lone star tick for an unsuspecting victim to develop a meat allergy that can last months, years, or even an entire lifetime. Here\u2019s how scientists think it goes down: Alpha-gal is a sugar molecule found in nearly all mammals, except humans and a few other primates. A lone star carrying alpha-gal (or an alpha-gal-like substance) bites a person and spreads it to their blood through the tick\u2019s saliva. Then, the molecule essentially rewires the body\u2019s immune system, prompting it to produce an overload of alpha-gal antibodies. When that person goes in for a cheeseburger, their body has a life-threatening reaction to the sugar in the meat. As recently as a few years ago, the link between lone stars and this allergic reaction was controversial. In 2011, a team of University of Virginia allergists presented its hypothesis in front a group of tick experts. The scientists\u2019 reaction was dismissive. \u201cWe thought, \u2018These guys are full of stuffing,\u2019\u201d Gaff recalls. That team was led by Thomas Platts-Mills, who initially made the connection between lone stars and alpha-gal. Platts-Mills applied insights from his study of patients who were taking the cancer drug cetuximab. Some patients were allergic to the drug, which contains alpha-gal. The team looked into what could be causing the reaction and discovered the link between lone stars and alpha-gal antibodies. As more people started turning up in emergency rooms with sudden and inexplicable reactions to meat, other researchers began coming around to the idea that a sesame-sized insect could, in fact, instill a lifelong aversion to red meat in full-sized human beings. Platts-Mills is now working on mapping cases of the allergy. One such case took place in Lake of the Ozarks, Missouri. John Beckett, a self-professed meat lover, was besieged by a pack of lone stars when he was cleaning out underbrush from an old car lot in 2014. Two weeks later, he was chowing down on hamburgers with some friends at a dock party on the lake when he started breaking out in hives. Over time, Beckett figured out that he felt sick every time he ate red meat. The hives weren\u2019t enough to make him stop, though. It wasn\u2019t until he wound up in the emergency room\u200a\u2014\u200aafter eating a cowboy rib eye from one of his favorite restaurants\u200a\u2014\u200athat he decided to kick red meat out of his diet once and for all. \u201cThe hives had closed my airways,\u201d Beckett says. \u201cI thought I was going to die that night.\u201d When he finally went to an allergist and got his blood tested, his doctor told him the levels of alpha-gal antibodies in Beckett\u2019s system were the highest he had ever seen. \u201cYou gave me bragging rights,\u201d Beckett remembers his doctor telling him. That was four years ago. Beckett stopped eating meat, and the amount of alpha-gal antibodies in his blood declined only slightly. Mark Vandewalker, an allergist who\u2019s been treating patients in Missouri since 1990, has noticed an uptick in patients exhibiting anaphylaxis, or a systemic allergic reaction, to meat. He sees patients come in with hives, swelling, itching, and, occasionally, some respiratory difficulties. \u201cInitially, I didn\u2019t even believe that the condition was real,\u201d Vandewalker says. \u201cBut now, having seen so many cases of my own, I think that it\u2019s impossible to deny that this is a very unusual, but a very real, form of food-induced anaphylaxis.\u201d The vast majority of food-related anaphylaxis occurs within minutes after eating, but alpha-gal is one of the rare allergies that doesn\u2019t work that way. \u201cWhat\u2019s odd is that it\u2019s happening in the middle of the night,\u201d Vandewalker explains. \u201cThese episodes have occurred three, four, five, even up to eight hours after eating.\u201d That makes it harder to diagnose, which is why patients with alpha-gal are often sent home from medical facilities without answers. On a trip to visit his family in Leesburg, Virginia, last year, Peter Coughlin was bitten by a blacklegged tick. He contracted Lyme disease, which required him to go on a regimen of antibiotics. A few months later, he reunited with a bunch of his high school friends, and the group decided to go out to eat. It had been two years since his alpha-gal symptoms began popping up, and Coughlin explains he was ready to jeopardize his health in the name of grilled steak. \u201cI just said, \u2018Fuck it,\u2019\u201d he recalls. \u201cI filled my pocket with Benadryl and went to Korean barbecue.\u201d This time Coughlin didn\u2019t have an allergic reaction. The Benadryl he had brought stayed in his pocket. According to Vandewalker, the Missouri physician, alpha-gal can eventually retreat to the point where eating red meat again is possible. Doctors and researchers don\u2019t know, however, how long the antibodies will linger patient to patient\u200a\u2014\u200aremember, John Beckett\u2019s levels were still high four years after he was bit\u200a\u2014\u200aand they don\u2019t know how to counteract it besides telling patients to lay off the red meat. Though alpha-gal remains somewhat mysterious, there is some good news about the ticks that carry it. While in some areas up to 50 percent of blacklegged ticks carry some kind of infectious disease\u200a\u2014\u200aLyme, Babesia, Anaplasmosis\u200a\u2014\u200athe rate of transmissible illnesses found in lone stars (like Rocky Mountain Fever) is much lower, around 10 to 20 percent. What\u2019s more, a recent study published by the Army Public Health Center indicates that lone stars can\u2019t carry Lyme disease at all. Stromdahl, the Army entomologist, surveyed 54 studies from 35 different research groups involving 52,000 ticks and found that a chemical in lone star saliva kills Borrelia\u200a\u2014\u200athe bacteria that causes Lyme. \u201cYou never want to say never with ticks or insects and what they can carry,\u201d she says. \u201cBut we presented a lot of evidence that they don\u2019t.\u201d But the reality is that we\u2019re living in a warming world, and one of the consequences of that is a tick expansion. And while a group of scientists is working on a vaccine for alpha-gal, others are devising ways to attack the issue at its root\u200a\u2014\u200aby eliminating the ticks from highly populated areas. Gaff at Old Dominion conducts studies using a robot called a tickbot, which moves around dragging a rag soaked with permethrin (a common treatment for lice that also kills ticks). The bot, which has a little piece of dry ice embedded in its center, breathes out CO2 and attracts ticks to the toxic rag. Richard Ostfeld, a disease ecologist at the Cary Institute of Ecosystem Studies in Upstate, New York, is conducting tick experiments on entire neighborhoods, which he calls \u201ctick towns.\u201d Twenty-four communities volunteered for the experiment, and some are outfitted with a naturally occurring fungus that sucks the life force out of ticks. Others have little contraptions called \u201cbait boxes\u201d that dab rodents with a small dose of Frontline, the flea and tick medicine for cats and dogs. According to Ostfeld, these preventive measures are \u201cprobably our best hope at clobbering ticks.\u201d Tickbots and tick towns aren\u2019t much comfort to people already living with Lyme or alpha-gal, but they\u2019re our best shot at keeping people who are still unaffected safe. For the alpha-gal allergic among us, the spread of lone stars means the end of traditions that once seemed reassuringly permanent\u200a\u2014\u200alike eating hamburgers at a dock party on the Lake of the Ozarks. Those get-togethers aren\u2019t what they once were for John Beckett. But he\u2019s playing the long game. \u201cI\u2019m trying my best not to get bitten a second time,\u201d he says, adding he reckons his blood levels will have evened out in a few decades. \u201cBy the time I\u2019m 80 I might be able to eat meat again.\u201d By clapping more or less, you can signal to us which stories really stand out. A nonprofit news org for people who want a planet that doesn\u2019t burn and a future that doesn\u2019t suck.","time":1525806518,"title":"Lone Star Ticks Are a Carnivore\u2019s Nightmare and They\u2019re Just Waking Up","type":"story","url":"https:\/\/medium.com\/@grist\/lone-star-ticks-are-a-carnivores-nightmare-and-they-re-just-waking-up-3f90e57548ad","label":7,"label_name":"random"},{"by":"dean","descendants":0,"id":17023783,"kids":"None","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000","time":1525806476,"title":"The Gambler Who Cracked the Horse-Racing Code","type":"story","url":"https:\/\/www.bloomberg.com\/businessweek","label":0,"label_name":"biz-news"},{"by":"ajb413","descendants":0,"id":17023764,"kids":"None","score":1,"text":"Flexible and feature-rich realtime chat APIs for web, IOS, Android and connected devices.  Securely monitor, control, provision and stream data between internet-connected devices. From geotracking, to alerts, to  up-to-the-millisecond updates, keep everyone, and everything realtime. Open and extensible chat SDK for accelerating chat development. A global network infrastructure powering API for realtime updates and IoT.  Learn how to use ChatEngine, Pub\/Sub, Presence, Access Manager, Storage & Playback, and Push Notifications with your clients. Develop and deploy. Tutorials, code walkthroughs, and PubNub news on the PubNub Blog. IoT security has been a predominant topic of discussion, both for IoT businesses and their end users, for the last decade. Several widely publicized incidents demonstrated how easy it was for unsecured IoT devices to be manipulated for malicious intent. These incidents brought gravitas to the conversation, highlighting the need for a better standard of security in IoT. One of the most concerning breaches was the Dyn distributed denial of service (DDoS) attack in the fall of 2016, which targeted a security vulnerability in a bunch of baby monitor cameras. Since these devices had the ability to be seen consumer to consumer (C2C) at the command and control server level (a somewhat PubNub like service that was not secure), those pathways were left open. When the attack hit, this vulnerability was leveraged in order to take over every single one of these devices to form a botnet. The botnet then proceeded to start spamming a bunch of UDP datagrams over Port 53. This flooded Dyn\u2019s DNS network, taking them down completely and preventing users from accessing sites like Netflix, Twitter, Amazon, and thousands of other major websites. What happened was, when Dyn\u2019s servers were flooded by the attack, they were shut down. As a result, users were no longer able to access the IP address of websites for apps and other things like that, things that would allow you to visit Twitter.com or order a taxi. Chaos ensued, which was surely the intent. That one incident completely destroyed Dyn. It tanked their reputation and that of all the businesses connected to it. All due to insecure IoT device connectivity. As one attack was mitigated, another began. Further compounding the situation was that the DNS protocol made it difficult to distinguish legitimate traffic from malicious traffic. The first attack prompted a flood of legitimate \u201cretries\u201d as servers attempted to refresh their caches. This amplified the traffic volume exponentially, preventing Dyn from accurately identifying the endpoints. In the end, it was concluded that there were an estimated 100K endpoints and that the attack came from Mirai botnets. Now, Dyn has rebranded as Oracle, but not without significant damage, both financial and otherwise. On the plus side, this incident started a significant discussion and much innovation around the security of IoT and how this could be approached on a more standardized basis. One of the most concerning discoveries was that the IoT devices used in the attack were targeted because they still had their default (factory) passwords stored. This allowed the attackers an easy in as they only had to include those sign-in credentials in the code. Once the source code had been identified, it was ascertained that it contained default credentials for more than 60 different IoT devices. The takeaways were many-layered but largely positive. At least, for everybody except Dyn. (As an analogy, consider that the rearview mirror was invented after the first Indianapolis 500 race. Think about it). First, companies learned that having a secondary DNS provider is a good idea in case of any future attacks. Second, one could assume that it would underscore the importance of changing passwords upon powering up a new IoT device, but that is something that is entirely in the hands of the end user\u2026 so, largely, uncontrollable. The potential for DDoS attacks that are as devastating as the Dyn incident still remains, but at least with regard to the threats we do know about, we can prepare. As our dependence on IoT grows\u2014in business and manufacturing in addition to our personal lives\u2014there is still great potential for harm to be done. A more secure internet, stronger security and stability in IoT device development, and ongoing vigilance are indicated. Even before the disastrous Dyn breach, PubNub has always had a primary focus on security. Their network is secure, tunneled through transport layer security (TLS), and encrypted. The TLS establishes a secure platform that offers complete privacy and data integrity between two communicating devices or applications. TLS is the most common internet security protocol in use today. It is leveraged by browsers or any application that needs to transfer information safely and securely, like instant messaging, chat, VoIP, and VPN connections. The way PubNub makes this technology available to their customers is through a software developer kit (SDK), which is activated on the device when the app is initialized. In fact, this security feature has been standardized across all PubNub based applications from the very start and not simply as a response to growing security concerns. Though admittedly, they do not do a lot of marketing in the IoT sphere, the solution was ideal and timely, prompting many IoT innovators to choose PubNub as their platform of choice. Security is our number one concern. Following the Dyn security event, they had some noteworthy visitors: We\u2019ve had some interesting communication from the United States government, prompting them to visit us onsite to talk about this issue specifically. They realize that a lot of IoT vendors are using unsecured networks and they wanted to make sure that we were aware of the landscape. They gave us a whole bunch of tips and tricks, which, as it turns out, we were already using. After the D Day of the Dyn DNS takedown, we started seeing a ton of customers coming to us specifically for our security. From a developer standpoint, the big benefit of using PubNub is that by adopting their data streaming network, all the security essentials they need are available right out of the box. PubNub is compliant with a range of security legislation that includes HIPAA, the GDPR, and the EU-US Privacy Shield. It also supports geographical limitations as to where messages are stored, due in large part to the availability of their data centers all over the world. Since all connections are outbound from the client, there are no open inbound ports required. Regional attacks are avoided using a system of intelligent data center routing protocols. PubNub uses point-to-point TLS network encryption (encrypts messages in transit) and AES encryption, a protocol that is used by the United States government to encrypt sensitive messages and classified information. Leveraging these and various other security features, PubNub gives their clients the ability to separate multiple channels, giving them access to any device, user, channel or key on the network. If any malicious activity is detected, the endpoint can be isolated and shut down immediately without any disruption to the rest of the network. Combined, these protocols are a powerhouse of next generation security. And though they are certainly not specific to IoT, they are making the technology better, more reliable, and more viable as a life enhancing product, which is exactly what it\u2019s meant to be. Competition in this space continues to mount, but insofar as PubNub is a pioneer that has always demonstrated their commitment to a long range vision, it\u2019s difficult to imagine a new player who could approach the same problems with as much passion and creativity. It would seem that we have imagined every possibility long before it was a sparkle in anybody else\u2019s eye. Not content to rest on their past accomplishments, PubNub continues to innovate new solutions and applications around the things they do best, but security is always at the forefront of every new initiative. If the name of the game is \u201clet\u2019s make this as easy as possible and deliver top quality and security from end-to-end\u201d, then they, and by proxy, their clientele, are already well ahead of the curve. From the end user, and for all the developers in between, PubNub is providing working solutions that make our lives better. Whether you realize it or not, they have already made a difference in how we see the world, the internet, and all the ways in which we connect to it. By strengthening confidence in the security of our connected devices, it will encourage companies to innovate, bring new products to market, and create a spectacular future. One that puts the \u201cfun\u201d back in function, while bringing our wildest imaginings into sharp focus, proving decisively that, with the right supports, anything is possible.   Categories: Insights  The PubNub Data Stream Network powers thousands of apps, streaming 1.9 Trillion messages to over 330 million devices a month.  We have a Beta version of our new Debug Console available to try, would you like to check it out?","time":1525806349,"title":"Security in the Internet of Things: What It Is and Why It Matters","type":"story","url":"https:\/\/www.pubnub.com\/blog\/internet-of-things-security\/","label":3,"label_name":"dev"},{"by":"wglb","descendants":0,"id":17023761,"kids":"None","score":1,"text":"This article contains guidance for developers to assist with identifying and mitigating speculative execution side channel hardware vulnerabilities in C++ software. These vulnerabilities can disclose sensitive information across trust boundaries and can affect software that runs on processors that support speculative, out-of-order execution of instructions. This class of vulnerabilities was first described in January, 2018 and additional background and guidance can be found in Microsoft's security advisory. The guidance provided by this article is related to the class of vulnerabilities represented by CVE-2017-5753, also known as Spectre variant 1. This hardware vulnerability class is related to side channels that can arise due to speculative execution that occurs as a result of a conditional branch misprediction. The Visual C++ compiler in Visual Studio 2017 (starting with version 15.5.5) includes support for the \/Qspectre switch provides a compile-time mitigation for a limited set of potentially vulnerable coding patterns related to CVE-2017-5753. The documentation for the \/Qspectre flag provides more information on its effects and usage.  An accessible introduction to speculative execution side channel vulnerabilities can be found in the presentation titled The Case of Spectre and Meltdown by one of the research teams that discovered these issues. Modern CPUs provide higher degrees of performance by making use of speculative and out-of-order execution of instructions. For example, this is often accomplished by predicting the target of branches (conditional and indrect) which enables the CPU to begin speculatively executing instructions at the predicted branch target, thus avoiding a stall until the actual branch target is resolved. In the event that the CPU later discovers that a misprediction occurred, all of the machine state that was computed speculatively is discarded. This ensures that there are no architecturally visible effects of the mispredicted speculation. While speculative execution does not affect the architecturaly visible state, it can leave residual traces in non-architectural state, such as the various caches that are used by the CPU. It is these residual traces of speculative execution that can give rise to side channel vulnerabilities. To better understand this, consider the following code fragment which provides an example of CVE-2017-5753 (Bounds Check Bypass): In this example, ReadByte is supplied a buffer, a buffer size, and an index into that buffer. The index parameter, as specified by untrusted_index, is supplied by a less privileged context, such as a non-administrative process. If untrusted_index is less than buffer_size, then the character at that index is read from buffer and used to index into a shared region of memory referred to by shared_buffer. From an architectural perspective, this code sequence is perfectly safe as it is guaranteed that untrusted_index will always be less than buffer_size. However, in the presence of speculative execution, it is possible that the CPU will mispredict the conditional branch and execute the body of the if statement even when untrusted_index is greater than or equal to buffer_size. As a consequence of this, the CPU may speculatively read a byte from beyond the bounds of buffer (which could be a secret) and could then use that byte value to compute the address of a subsequent load through shared_buffer.  While the CPU will eventually detect this misprediction, residual side effects may be left in the CPU cache that reveal information about the byte value that was read out of bounds from buffer. These side effects can be detected by a less privileged context running on the system by probing how quickly each cache line in shared_buffer is accessed. The steps that can be taken to accomplish this are: Invoke ReadByte multiple times with untrusted_index being less than buffer_size. The attacking context can cause the victim context to invoke ReadByte (e.g. via RPC) such that the branch predictor is trained to be not-taken as untrusted_index is less than buffer_size. Flush all cache lines in shared_buffer. The attacking context must flush all of the cache lines in the shared region of memory referred to by shared_buffer. Since the memory region is shared, this is straightforward and can be accomplished using intrinsics such as _mm_clflush. Invoke ReadByte with untrusted_index being greater than buffer_size. The attacking context causes the victim context to invoke ReadByte such that it incorrectly predicts that the branch will not be taken. This causes the processor to speculatively execute the body of the if block with untrusted_index being greater than buffer_size, thus leading to an out-of-bounds read of buffer. Consequently, shared_buffer is indexed using a potentially secret value that was read out-of-bounds, thus causing the respective cache line to be loaded by the CPU. Read each cache line in shared_buffer to see which is accessed most quickly. The attacking context can read each cache line in shared_buffer and detect the cache line that loads significantly faster than the others. This is the cache line that is likely to have been brought in by step 3. Since there is a 1:1 relationship between byte value and cache line in this example, this allows the attacker to infer the actual value of the byte that was read out-of-bounds. The above steps provide an example of using a technique known as FLUSH+RELOAD in conjunction with exploiting an instance of CVE-2017-5753. Developing secure software using a process like the Security Development Lifecycle (SDL) typically requires developers to identify the trust boundaries that exist in their application. A trust boundary exists in places where an application may interact with data provided by a less-trusted context, such as another process on the system or a non-administrative user mode process in the case of a kernel-mode device driver. The new class of vulnerabilities involving speculative execution side channels is relevant to many of the trust boundaries in existing software security models that isolate code and data on a device. The following table provides a summary of the software security models where developers may need to be concerned about these vulnerabilities occurring: Applications that have attack surface exposed to any of the above trust boundaries should review code on the attack surface to identify and mitigate possible instances of speculative execution side channel vulnerabilities. It should be noted that trust boundaries exposed to remote attack surfaces, such as remote network protocols, have not been demonstrated to be at risk to speculative execution side channel vulnerabilities. Speculative execution side channel vulnerabilities can arise as a consequence of multiple coding patterns. This section describes potentially vulnerable coding patterns and provides examples for each, but it should be recognized that variations on these themes may exist. As such, developers are advised to take these patterns as examples and not as an exhaustive list of all potentially vulnerable coding patterns. In general, speculative execution side channels related to conditional branch misprediction can arise when a conditional expression operates on data that can be controlled or influenced by a less-trusted context. For example, this can include conditional expressions used in if, for, while, switch, or ternary statements. For each of these statements, the compiler may generate a conditional branch that the CPU may then predict the branch target for at runtime. For each example, a comment with the phrase \"SPECULATION BARRIER\" is inserted where a developer could introduce a barrier as a mitigation. This is discussed in more detail in the section on mitigations. This category of coding patterns involves a conditional branch misprediction that leads to a speculative out-of-bounds memory access. This coding pattern is the originally described vulnerable coding pattern for CVE-2017-5753 (Bounds Check Bypass). The background section of this article explains this pattern in detail. Similarly, an array out-of-bounds load may occur in conjunction with a loop that exceeds its terminating condition due to a misprediction. In this example, the conditional branch associated with the x < buffer_size expression may mispredict and speculatively execute the body of the for loop when x is greater than or equal to buffer_size, thus resulting in a speculative out-of-bounds load. This coding pattern involves the case where a conditional branch misprediction can lead to an out-of-bounds access to an array of function pointers which then leads to an indirect branch to the target address that was read out-of-bounds. The following snippet provides an example that demonstrates this.  In this example, an untrusted message identifier is provided to DispatchMessage through the untrusted_message_id parameter. If untrusted_message_id is less than MAX_MESSAGE_ID, then it is used to index into an array of function pointers and branch to the corresponding branch target. This code is safe architecturally, but if the CPU mispredicts the conditional branch, it could result in DispatchTable being indexed by untrusted_message_id when its value is greater than or equal to MAX_MESSAGE_ID, thus leading to an out-of-bounds access. This could result in speculative execution from a branch target address that is derived beyond the bounds of the array which could lead to information disclosure depending on the code that is executed speculatively. As with the case of an array out-of-bounds load feeding another load, this condition may also arise in conjunction with a loop that exceeds its terminating condition due to a misprediction. This category of coding patterns involves a conditional branch misprediction that leads to a speculative type confusion. The coding patterns in this section will refer to the example code below. This coding pattern involves the case where a speculative type confusion can result in an out-of-bounds or type-confused field access where the loaded value feeds a subsequent load address. This is similar to the array out-of-bounds coding pattern but it is manifested through an alternative coding sequence as shown above. In this example, an attacking context could cause the victim context to execute ProcessType multiple times with an object of type CType1 (type field is equal to Type1). This will have the effect of training the conditional branch for the first if statement to predict not taken. The attacking context can then cause the victim context to execute ProcessType with an object of type CType2. This can result in a speculative type confusion if the conditional branch for the first if statement mispredicts and executes the body of the if statement, thus casting an object of type CType2 to CType1. Since CType2 is smaller than CType1, the memory access to CType1::field2 will result in a speculative out-of-bounds load of data that may be secret. This value is then used in a load from shard_buffer which can create observable side effects, as with the array out-of-bounds example described previously. This coding patterns involves the case where a speculative type confusion can result in an unsafe indirect branch during speculative execution. In this example, an attacking context could cause the victim context to execute ProcessType multiple times with an object of type CType2 (type field is equal to Type2). This will have the effect of training the conditional branch for the first if statement to be taken and the else if statement to be not taken. The attacking context can then cause the victim context to execute ProcessType with an object of type CType1. This can result in a speculative type confusion if the conditional branch for the first if statement predicts taken and the else if statement predicts not taken, thus executing the body of the else if and casting an object of type CType1 to CType2. Since the CType2::dispatch_routine field overlaps with the char array CType1::field1, this could result in a speculative indirect branch to an unintended branch target. If the attacking context can control the byte values in the CType1::field1 array, they may be able to control the branch target address. Speculative execution side channel vulnerabilities can be mitigated by making changes to source code. These changes can involve mitigating specific instances of a vulnerability, such as by adding a speculation barrier, or by making changes to the design of an application to make sensitive information inaccessible to speculative execution. A speculation barrier can be manually inserted by a developer to prevent speculative execution from proceeding along a non-architectural path. For example, a developer can insert a speculation barrier before a dangerous coding pattern in the body of a conditional block, either at the beginning of the block (after the conditional branch) or before the first load that is of concern. This will prevent a conditional branch misprediction from executing the dangerous code on a non-architectural path by serializing execution. The speculation barrier sequence differs by hardware architecture as described by the following table: For example, the following code pattern can be mitigated by using the _mm_lfence intrinsic as shown below. The Visual C++ compiler in Visual Studio 2017 (starting with version 15.5.5) includes support for the \/Qspectre switch which automatically inserts a speculation barrier for a limited set of potentially vulnerable coding patterns related to CVE-2017-5753. The documentation for the \/Qspectre flag provides more information on its effects and usage. It is important to note that this flag does not cover all of the potentially vulnerable coding patterns and as such developers should not rely on it as a comprehensive mitigation for this class of vulnerabilities. Another technique that can be used to mitigate speculative execution side channel vulnerabilities is to remove sensitive information from memory. Software developers can look for opportunities to refactor their application such that sensitive information is not accessible during speculative execution. This can be accomplished by refactoring the design of an application to isolate sensitive information into separate processes. For example, a web browser application can attempt to isolate the data associated with each web origin into separate processes, thus preventing one process from being able to access cross-origin data through speculative execution. Guidance to mitigate speculative execution side-channel vulnerabilities Mitigating speculative execution side channel hardware vulnerabilities Note The feedback system for this content will be changing soon. Old comments will not be carried over. If content within a comment thread is important to you, please save a copy. For more information on the upcoming change, we invite you to read our blog post.","time":1525806322,"title":"C++ Developer Guidance for Speculative Execution Side Channels","type":"story","url":"https:\/\/docs.microsoft.com\/en-us\/cpp\/security\/developer-guidance-speculative-execution","label":3,"label_name":"dev"},{"by":"huphtur","descendants":0,"id":17023758,"kids":"None","score":2,"text":"The apps, books, movies, music, TV shows, and art are inspiring our some of the most creative people in business this month The struggles and triumphs of prominent women in leadership positions The major tech ecosystems that battle for our attention and dollars What\u2019s next for hardware, software, and services Our annual guide to the businesses that matter the most Leaders who are shaping the future of business in creative ways New workplaces, new food sources, new medicine--even an entirely new economic system Celebrating the best ideas in business An award-winning team of journalists, designers, and videographers who tell brand stories through Fast Company's distinctive lens Amazon\u2019s unstoppable growth has set the entire world of retail back on its heels, but at least one company has the footprint to stand in its way. With $500 billion in annual sales and 11,700 locations across the world, Walmart is still more than three times the size of Amazon in yearly revenue\u2013and it\u2019s not about to be left behind. So over the past two years, Walmart has invested heavily in e-commerce. It snatched up Jet.com, Bonobos, and Modcloth to expand its online footprint into hip millennial territory, along with the delivery service Parcel to expedite its shipping. More recently, it started offering Amazon-style conveniences like free two-day shipping with no membership fees, and one-button reorder options. Such updates have enabled Walmart\u2019s online sales to grow 50% between 2016 and 2017, but that growth is already slowing. Jet.com faltered as Walmart\u00a0struggled to woo the urban market. Walmart may be the bigger retailer, but Amazon\u2019s online revenue in 2017 was almost 10 times\u00a0that of Walmart\u2019s ($118.57 billion vs. $11.5 billion, respectively)\u2013and Amazon is still growing at a faster rate despite this difference in scale. This week, Walmart is debuting a totally redesigned website\u2013and it\u2019s anything but an Amazon clone. Whereas Amazon feels like a digital warehouse stacked to the brim with seemingly random recommendations on its home page and an endless, searchable list of products, the new Walmart.com aims to be warm and approachable, with imagery that evokes a lifestyle brand rather than a place to just get good deals. Walmart plans to balance algorithmic recommendations with human curation. And most of all, it wants to leverage its greatest asset\u2013those 4,700 physical stores\u2013to take on the competition. \u201cWalmart.com is not just a national e-commerce retailer, but actually an extension of your local Walmart,\u201d says Jordan Sweetnam, SVP, customer experience & product at Walmart U.S. eCommerce. Take a look at the new Walmart home page, and three things stick out. First, it\u2019s topped with a wide photo of people in their homes. Second, it recommends you order groceries, probably to be picked up at a local store. Third, the ratio of white space to product photos is far higher\u2013the company has pared down the sheer number of offerings. \u201cOne school of thought is that more is better. You give a shortcut to anything you need,\u201d says \u00a0Sweetnam. \u201cBut it can be overwhelming for new customers.\u201d Instead of a product dump, Walmart.com is \u201clocal and personal.\u201d The personal comes largely from the option to \u201cEasy Reorder\u201d recent products, and continues as you scroll into category shopping down the page. The local comes from a new trending module, which shows three products that are selling fast in physical Walmart stores near you. \u201cYou go into Walmarts across the country, and while 80% of the store is the same, there\u2019s different inventory highlighted regionally. It\u2019s not just a selling tactic. People are excited when they walk into their local Walmart and see [local sports items] stacked,\u201d says Sweetnam. \u201cWhen we started prototyping [the trending module], we got a really similar reaction.\u201d For Sweetnam, who lives in San Jose, CA, he\u2019s seen Sharks gear as the hockey team made the playoffs and box fans as the town experiences a heat wave. Likewise, in my home town of Chicago, our first real days of spring have induced coolers, barbeques, and pink children\u2019s bicycles to rise to the top. Some of these suggestions can actually get hyperlocal, depending on just how many Walmarts are in a town\u2013like we see in the city of Houston, which has 33 Walmarts dotting the city\u00a0rather than just a few. \u201cIt creates this connection,\u201d says Sweetnam. \u201cThis isn\u2019t a generic website, this is actually my Walmart.\u201d The redesigned site features photography heavily\u2013a pointed choice that tips the scale against the kind of information density you\u2019ll find on Amazon or other e-commerce sites. The images feel like that of a \u201clifestyle\u201d brand, though the company disagrees with me on that word, insisting they\u2019re still focused on \u201ceveryday low prices.\u201d But on the new site, even Walmart mainstays like Tide and Swiffer Wet Jets are photographed in situ, as if they\u2019re the stars of a perfectly orderly home. Contrast this to Amazon, which lists products shot (or Photoshopped) in front of soulless white backdrops. Walmart.com may be going for deal shoppers, but its vibe is borderline aspirational. \u201cThe imagery is very different. It\u2019s much more focused on the human elements . . . showing real-life products in use and in\u00a0context,\u201d confirms Sweetnam. \u201cIn our customer testing, it draws people in more. It\u2019s less like you\u2019re being sold a product, and more like, \u2018I also have a kid in a highchair and see cereal thrown on the ground!'\u201d As Sweetnam puts it, this photography is meant to transform the quick chore of ordering more Cheerios or paper towels into something more akin to a \u201clife journey.\u201d This life journey continues into new verticals, in which Walmart offers a more tailored window shopping experience for topics like furniture and, soon, fashion. Before, a search for furniture would bring you to one of those massive results pages like you get on other e-commerce sites, with a sidebar full of toggles to tailor your search. Walmart.com is meant to feel like a specialty store, with a photo-heavy mix of algorithmic recommendation and merchandiser-curated selections that mine your shopping history. \u201cWe spent a lot of time building up the personas, [like] \u2018I have a family with two kids. I have a dog,\u201d says Sweetnam. These personas were constructed by Walmart based upon your shopping history, but they weren\u2019t all that deeply utilized by Walmart.com, which only used them to funnel you into generic lists like \u201cback to school\u201d or whatever else might fit that generalized portrait of you the best. Now, Walmart is using your persona to create curated product pages just for you, by allowing its own merchandisers to make the best recommendations for your persona. \u201cWhat we can get to now is, looking at differences [in shoppers], the merchants can target specific products,\u201d says Sweetnam. \u201cIf it knows I have two kids, 8-10, and it knows it\u2019s raining, it\u2019s not just sharing generic rain gear. We can actually highlight rain gear for boys and girls, and a merchants can say, \u2018for boys, for girls, these are the best products. The best value, the best brand, the best sellers.\u2019 \u201cAggregating it up a level, if you are now looking for that midcentury modern coffee or dining table\u2013it might say, \u2018Mark, you\u2019re looking for a dining room!\u2019 Most people aren\u2019t just looking for a dining table. They need chairs, lights for the kitchen, and we can tailor merchandising that speaks to that dining mission, in the style, midcentury modern.\u201d Walmart is rolling out its redesign this week, and it will inevitably begin collecting lots of real-world data on the updates. No doubt, things will change within the year as the design is optimized. Could Walmart, against all odds, solve some of Amazon\u2019s worst problems, and translate the best of its brick and mortar\u2013like the friendliness of its greeters, and local preferences of its shoppers\u2013into an online experience? Time will tell. \u201cWhat we heard from customer feedback is, \u2018it\u2019s dangerously enticing.\u2019 It\u2019s kind of sucking people into these journeys of discovery [for products] they just weren\u2019t aware we had before,\u201d says Sweetnam. \u201cIn the stores, it\u2019s easy. You can\u2019t miss the home aisle in a physical store. On a website, it\u2019s not as intuitive.\u201d Mark Wilson is a senior writer at Fast Company. He started Philanthroper.com, a simple way to give back every day.  More Co.Design Daily Newsletter","time":1525806290,"title":"Walmart.com Redesigns as the Anti-Amazon","type":"story","url":"https:\/\/www.fastcodesign.com\/90170513\/exclusive-walmart-com-redesigns-as-the-anti-amazon","label":7,"label_name":"random"},{"by":"jell","descendants":0,"id":17023737,"kids":"None","score":1,"text":"","time":1525806171,"title":"Apple BACKS DOWN on anti screen repair firmware","type":"story","url":"https:\/\/www.youtube.com\/watch?v=_yIl-OTDRXo","label":7,"label_name":"random"},{"by":"elsewhen","descendants":0,"id":17023723,"kids":"None","score":2,"text":"When Google first announced Google Lens last year, it was described as a kind of search in reverse. Rather than type a text query to find image results, you could point your phone\u2019s camera at an object, like a dog or a plant, to find text-based information. Lens was not only a statement about your camera as an input device but also a most Google-y expression of technology: It combined search, computer vision, AI, and AR, and put it all in apps that weren\u2019t limited to one ecosystem. At this year\u2019s developers conference, Google announced the most significant update yet to Google Lens\u2014one that emphasizes shopping, text-reading, and additional language support. And to make Lens more convenient for people to use, Google has convinced a bunch of handset partners to offer Lens as an option right in the native camera app. The new features, which roll out at the end of May, represent Google\u2019s next steps to make your smartphone camera \"like a visual browser for the world around you,\" says Aparna Chennapragada, vice president of product for AR, VR, and vision-based products at Google. \"By now people have the muscle memory for taking pictures of all sorts of things\u2014not just sunsets and selfies but the parking lot where you parked, business cards, books to read,\" Chennapragada says. \"That\u2019s a massive behavior shift.\" In other words, Google\u2019s vision of the future still involves searching for things. Now it's just by whipping out your phone and pointing the camera at something, a behavior that\u2019s become second nature to smartphone users. But Google knows it\u2019s not the only tech company working on visual search, so it\u2019s trying to wedge Lens into places you\u2019re already active on your phone. Earlier versions of Lens could be accessed through Google Assistant and Google Photos; the new version will be built directly into the camera on more than ten different Android phones. This includes Google\u2019s Pixel phones; handsets from Asus, Motorola, Xiaomi, and OnePlus; the new LG G7 ThinQ; and more. On the G7 ThinQ, Lens will also have a physical button\u2014press it twice and the Lens camera automatically opens\u2014the same way that Bixby has a dedicated button on Samsung flagship phones. In a demo of the new features, launching Lens with a physical button worked like it was supposed to on the LG G7 ThingQ. On phones without a dedicated Lens button, Lens appears as one of the main options in the camera app, the same way that video recording does. Another thing that\u2019s new about Lens: The camera app starts scanning the space around you as soon as you open it. \"We realized that you don\u2019t always know exactly the thing you want to get the answer on,\" says Clay Bavor, Google\u2019s vice president of virtual and augmented reality. \"So instead of having Lens work where you have to take a photo to get an answer, we\u2019re using Lens Real-Time, where you hold up your phone and Lens starts looking at the scene [then].\" This scanning function appears as a series of AR dots, mapping the world around you, before a virtual button appears as a ready-to-go signal. Both native camera access and the Lens Real-Time feature contribute to a faster visual search experience, but the latter also mean Lens grabs information you may not need it to. In one instance, I pointed the new Lens at a pair of shoes only to get search results for the restaurant Nopalito, because the restaurant\u2019s menu was sitting on a shelf below the shoes and Lens had picked up on it as I raised the camera. It also wasn\u2019t 100 percent accurate when it came to shopping, one of the other key features of the new Lens. At one point, Lens identified a large gray sweater as an elephant. But the version of the app I saw was still in beta, and Google says the misidentification will be fixed by the time it rolls out at the end of the month. And in general, the shopping results were impressive. An earlier version of Lens might simply identify the object as a sweater, or a pillow, or a pair of shoes. The new Lens has something Google calls \"Style Match\": It found a match for all three items, showed options for where to buy them, and recommended similar items. It even knew the pillow I brought with me for the demo was from Etsy.com. If the first version of Lens was about pets and plants, this version might be defined by clothes and home decor. The new Google Lens will also support Spanish, Portuguese, French, German, and Italian\u2014which, it\u2019s worth noting, is different from translation. Lens has always been able to translate languages supported by Google Translate. This update just means if you\u2019re a native speaker in one of those new languages, you can run a version of Lens that\u2019s specific to that language. Of course, Google already has all of that information indexed, whether it\u2019s puppy breeds, restaurant menus, clothing inventory, or foreign languages. So why is it so hard to bring it all to Lens search? Chennapragada insists that it\u2019s quite difficult to provide on-the-fly context for visual objects in what she calls a \"very unstructured, noisy situation.\" \"We\u2019ve always used vision technology in our image recognition algorithms, but in a very measured way,\" she says. Bavor says it\u2019s also the sheer number of objects that exist in the world that makes visual search a unique challenge. \"In the English language there\u2019s something like 180,000 words, and we only use 3,000 to 5,000 of them. If you\u2019re trying to do voice recognition, there\u2019s a really small set of things you actually need to be able to recognize. Think about how many objects there are in the world, distinct objects, billions, and they all come in different shapes and sizes,\" Bavor says. \"So the problem of search in vision is just vastly larger than what we've seen with text or even with voice.\" It\u2019s a problem that many others are trying to tackle as well. Facebook, Amazon, and Apple have begun been building their own visual search platforms or acquiring technology companies that analyze photo content. Last February, Pinterest launched its own Lens tool, which lets users search the site using the Pinterest camera. Pinterest Lens also happens to power Samsung\u2019s Bixby Vision. There are smaller competitors too: The AR app platform Blippar can recognize flowers, public faces, and famous landmarks through a smartphone\u2019s camera. Even high schoolers are building \"smart lens\" apps. For Google, though, expectations might be higher, given that the company defined online search as the world now knows it. Can it do the same for visual search? More importantly, can it do it without creating visual algorithms that are biased or even downright offensive? The sweater I saw misidentified as an elephant was a benign example, but shows how a seemingly simple object could be mistaken for something else. One advantage text-based queries have is that they tend to be explicit, whereas object or person recognition is still open to a lot of algorithmic misinterpretation. \"A key approach we\u2019ve taken in building Lens is to make the system identify why errors happen, and build improvements that help mitigate those errors,\" Chennapragada wrote in an email when I asked what Google was doing to ensure accuracy in visual search. \"This is a similar philosophy to what we have done with Search and Autocomplete.\" She went on to write that Lens is solving a complex problem as part of a \"multi-year journey,\" and that it\u2019s hard to recognize and understand the billions of objects in the world. Still, it\u2019s clear that Google executives are excited about visual search\u2014and not just its potential, but what it can do right now. At Google\u2019s offices the day of the Lens demo, Bavor pulled out his smartphone and showed me a photo he\u2019d taken of Datsun 1500 Roadster that he spotted from the back of his Lyft ride. \u201cYou think about how you\u2019d formulate that query. \u2018Old car with round headlights and a big grille and a curvy line on the side and it\u2019s a convertible and it has silver pointy\u2019\u2026 What is the query you would even write? And I Lensed it, and oh, it\u2019s the Datsun 1500,\" says Bavor. \"There\u2019s literally no query I could have written to figure that out.\" Virtual assistants like Google Home and Amazon Alexa can be amazing but what are they doing with all of your questions? Here's how to control all of that data. CNMN Collection Use of this site constitutes acceptance of our user agreement (effective 3\/21\/12) and privacy policy (effective 3\/21\/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast.","time":1525806102,"title":"Lens, Google's Visual Search Tool, Gets Its Most Significant Update Yet","type":"story","url":"https:\/\/www.wired.com\/story\/lens-google-visual-search-tool-update\/","label":7,"label_name":"random"},{"by":"thatssosid","descendants":0,"id":17023690,"kids":"None","score":1,"text":"Example - The Shawshank Redemption \ud83c\udfa5\nIMDb Link - https:\/\/www.imdb.com\/title\/tt0111161\/ \ud83d\udc48\nIMDb ID   - tt0111161 \u2705\n","time":1525805905,"title":"Show HN: IMDb Movie Streamer","type":"story","url":"https:\/\/paperdownloader.cf\/movie\/","label":7,"label_name":"random"},{"by":"johnshades","descendants":0,"id":17023685,"kids":"None","score":1,"text":"The apps, books, movies, music, TV shows, and art are inspiring our some of the most creative people in business this month The struggles and triumphs of prominent women in leadership positions The major tech ecosystems that battle for our attention and dollars What\u2019s next for hardware, software, and services Our annual guide to the businesses that matter the most Leaders who are shaping the future of business in creative ways New workplaces, new food sources, new medicine--even an entirely new economic system Celebrating the best ideas in business An award-winning team of journalists, designers, and videographers who tell brand stories through Fast Company's distinctive lens Modern AI systems can recognize faces, drive cars and understand human speech, but experts often say it can sometimes be difficult to understand how they\u2019re actually making decisions. And if their virtual thought processes have hidden logic errors or blind spots, that can lead to serious safety or other concerns, even if their decisions seem to mostly come out right. For instance, recent research from Stanford and MIT found that mainstream face recognition and analysis algorithms often performed worse on pictures of people who aren\u2019t white and male, likely because they were trained on skewed datasets. And researcher Janelle Shane, who runs the website AIWeirdness, recently pointed out the propensity that\u00a0Microsoft Azure\u2019s computer vision API has for spotting sheep in photographs of grassy fields, even where there are no animals at all, presumably because so many of the images of fields it has seen before were dotted with sheep. Ultimately, AI systems are only useful and safe as long as the goals they\u2019ve learned actually mesh with what humans want them to do, and it can often be hard to know if they\u2019ve subtly learned to solve the wrong problems or make bad decisions in certain conditions. To make AI easier for humans to understand and trust, researchers at the nonprofit research organization OpenAI have proposed training algorithms to not only classify data or make decisions, but to justify their decisions in debates with other AI programs in front of a human or AI judge. \u201cGiven a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information,\u201d write OpenAI researchers Geoffrey Irving, Paul Christiano and Dario Amodei in a new research paper. The San Francisco-based AI lab is funded by Silicon Valley luminaries including Y Combinator President Sam Altman and Tesla CEO Elon Musk, with a goal of building safe, useful AI to benefit humanity. Since human time is valuable and usually limited, the researchers say the AI systems can effectively train themselves in part by debating in front of an AI judge designed to mimic human decision making, similar to how software that plays games like Go or chess often trains in part by playing against itself. In an experiment described in their paper, the researchers set up a debate where two software agents work with a standard set of handwritten numerals, attempting to convince an automated judge that a particular image is one digit rather than another digit, by taking turns revealing one pixel of the digit at a time. One bot is programmed to tell the truth, while another is programmed to lie about what number is in the image, and they reveal pixels to support their contentions that the digit is, say, a five rather than a six. Microsoft\u2019s computer vision API incorrectly determined this image contains sheep [Image: courtesy Janelle Shane \/ aiweirdness.com]The truth-telling bots tend to reveal pixels from distinctive parts of the digit, like the horizontal line at the top of the numeral \u201c5,\u201d while the lying bots, in an attempt to deceive the judge, point out what amount to the most ambiguous areas, like the curve at the bottom of both a \u201c5\u201d and a \u201c6.\u201d The judge ultimately \u201cguesses\u201d which bot is telling the truth based on the pixels that have been revealed. The image classification task, where most of the image is invisible to the judge, is a sort of stand-in for complex problems where it wouldn\u2019t be possible for a human judge to analyze the entire dataset to judge bot performance. The judge would have to rely on the facets of the data highlighted by debating robots, the researchers say. \u201cThe goal here is to model situations where we have something that\u2019s beyond human scale,\u201d says Irving, a member of the AI safety team at OpenAI. \u201cThe best we can do there is replace something a human couldn\u2019t possibly do with something a human can\u2019t do because they\u2019re not seeing an image.\u201d The researchers use image sampling, rather than natural language debates, in their early experiments because the ability of AI systems to carry on a spirited, coherent verbal argument is still a challenge. \u201cText-based debates would be possible today,\u201d says Amodei. \u201cBut they won\u2019t be very good.\u201d In the image test, a judge trained to identify numbers after seeing arbitrary sets of six pixels can figure out which bot is telling the truth 88.9% of the time after each bot has the chance to reveal three pixels of its choice. By comparison, it can guess accurately 59.4% of the time after seeing six random pixels from the images. That is, even with the debaters able to see the whole image while the judge can only see disjointed fractions of it, the debate still results in correct answers, since it\u2019s easier for the truth-telling bot to find evidence to support its position. To test their hypothesis\u2014that two debaters can lead to honest behavior even if the debaters know much more than the judge\u2014the researchers have also devised an interactive demonstration of their approach, played entirely by humans and now available online. In the game, two human players are shown an image of either a dog or a cat and argue before a judge as to which species is represented. The contestants are allowed to highlight rectangular sections of the image to make their arguments\u2014pointing out, for instance, a dog\u2019s ears or cat\u2019s paws\u2014but the judge can \u201csee\u201d only the shapes and positions of the rectangles, not the actual image. While the honest player is required to tell the truth about what animal is shown, he or she is allowed to tell other lies in the course of the debate. \u201cIt is an interesting question whether lies by the honest player are useful,\u201d the researchers write. \n\n\nOpenAI found that an adversarial game between two human debaters can produce honest behavior, even if they know much more than the judge [Image: OpenAI]The judge is also able to pose questions, and each player is also allowed to reveal one pixel of the image over the course of the entire debate. \u201cWe\u2019ve played this game informally at OpenAI, and the honest agent indeed tends to win, though to make it fair to the liar we usually limit the rate at which the judge can solicit information (it\u2019s cognitively difficult to construct a detailed lie),\u201d the researchers write. One open question is whether training and deploying AI systems that can formulate debates as well as make decisions will take significantly more computing power than ones that just take action, something that will have to be determined through future experiments, they say. \u201cWe hope the performance penalty is small,\u201d says Amodei. The researchers emphasize that it\u2019s still early days, and the debate-based method still requires plenty of testing before AI developers will know exactly when it\u2019s an effective strategy or how best to implement it. For instance, they may find that it may be better to use single judges or a panel of voting judges, or that some people are better equipped to judge certain debates. It also remains to be seen whether humans will be accurate judges of sophisticated robots working on more sophisticated problems. People might be biased to rule in a certain way based on their own beliefs, and there could be problems that are hard to reduce enough to have a simple debate about, like the soundness of a mathematical proof, the researchers write. Other less subtle errors may be easier to spot, like the sheep that Shane noticed had been erroneously labeled by Microsoft\u2019s algorithms. \u201cThe agent would claim there\u2019s sheep and point to the nonexistent sheep, and the human would say no,\u201d Irving writes in an email to Fast Company. But deceitful bots might also learn to appeal to human judges in sophisticated ways that don\u2019t involve offering rigorous arguments, Shane suggested. \u201cI wonder if we\u2019d get kind of demagogue algorithms that would learn to exploit human emotions to argue their point,\u201d she says. Even before AI systems are advanced enough to hold sophisticated verbal beliefs, some of those questions can still be tested by having humans argue before a less knowledgeable judge. \u201cWe are optimistic that we can learn a great deal about these issues by conducting debates between humans, in domains where experts have much more time than the judge, have access to a large amount of external information, or have expertise that the judge lacks,\u201d they write. \u201cI think it\u2019s going to be a really tough thing to do,\u201d Shane said about OpenAI\u2019s debate method, \u201cbut I want to see them try it\u00a0because I think it would be really interesting and probably really entertaining.\u201d Steven Melendez is an independent journalist living in New Orleans.  More Technology Newsletter","time":1525805865,"title":"Why Scientists Think AI Systems Should Debate Each Other","type":"story","url":"https:\/\/www.fastcompany.com\/40569116\/why-scientists-think-ai-systems-should-debate-each-other","label":5,"label_name":"ml"},{"by":"cirrus-clouds","descendants":0,"id":17023684,"kids":"None","score":2,"text":" Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp LinkedIn Copy this link These are external links and will open in a new window The Home Office has denied taking \"arbitrary\" decisions on asylum cases in order to meet deportation targets, but an asylum caseworker says staff have to work so fast that the results are a \"lottery\" - one that could result in people being sent home to their deaths. He contacted the BBC because he wants the public to know how the system operates. As he would lose his job if identified, we have called him \"Alex\". Every day Alex reads the case files of people who have fled armed conflict. People who have been persecuted because of their politics, race, religion or sexuality. People who have experienced torture and sexual violence. It's his job to decide whether these people, all asylum seekers, should be allowed to stay in the UK or be deported. And yet, when he walks into work, he is greeted by a scene that wouldn't look out of place at a call centre selling double glazing. A leader board hangs on the wall displaying who is hitting their targets and who isn't, and performance managers pace the floor asking for updates on progress as often as once an hour. Staff who don't meet their targets risk losing their jobs. \"There is an obsession among management with unachievable 'stats' - human beings with complex lives are reduced just to numbers,\" says Alex who has been a decision-maker for the Home Office for almost a year. \"These are people waiting for a decision to be made on their lives - it is probably one of the biggest things they will ever have to go through. \"Given what we are dealing with, this is not the environment for pushy managers who try to drive results through fear and intimidation.\" Alex is one of 140 decision-makers based in an office in Bootle, just outside Liverpool. Most were recruited last year to clear a backlog of 10,000 of asylum cases within 12 months - a project known as Next Generation Casework. Source: Asylum Aid The focus is on cases classified by the Home Office as \"non-straightforward\", including pregnant women, people who claim to have been tortured and those with mental health conditions. But no matter how complex the case, Alex is expected to make five decisions to grant or refuse asylum seekers a week, justified by a letter that can be anything between 5,000 and 17,000 words long (that is, between two or seven times the length of this article). Anyone consistently hitting three or less is put on an \"improvement plan\" - and will be sacked if they don't improve in four weeks, Alex says. \"People will often take decisions based on what the easiest result will be to get through the decision as quickly as possible,\" says Alex. Sometimes the easiest decision will be to grant asylum, sometimes it will be to refuse it. \"In that sense, asylum seekers face a lottery,\" he says. The Home Office told the BBC it didn't recognise the picture painted by Alex and insisted that staff had an \"appropriate\" workload. I went to Liverpool to meet Alex and to see his Home Office ID. While there, I also spoke to officials from the Public and Commercial Services Union, who confirmed several aspects of his story. Under the 1951 Geneva Convention, asylum seekers must show that: Asked what he means by saying that decision-makers sometimes take the \"easiest\" route to a decision, rather than the fairest, Alex asks me to imagine that an applicant has given several reasons why he or she needs asylum. In this case, a decision-maker may home in on just one of the reasons, Alex says, rather than considering whether the whole story adds up. In this case the application is likely to be approved, when perhaps it shouldn't be. But equally, if someone's application contains inconsistencies - regarding dates for example - this can be used as an easy way to refuse an application.  \"In reality, some inconsistencies might be down to the person having a mental health problem, or just simply that it has been such a long time between making the claim and having an interview that they've forgotten precise dates of things,\" says Alex. There is nothing stopping decision-makers from doing their own research - for example, putting in a call to a UK church where someone claiming asylum on grounds of religious persecution claims to have been worshipping. \"But there are no extra points for going the extra mile - in fact, it only hurts your targets because it takes up time. So people normally just go on the information they've been given,\" says Alex. Some of this information comes from two interviews - an initial interview when the asylum seeker first arrives in the country, and a second in-depth interview, conducted by decision-makers like Alex. These interviews are supposed to last two-and-a-half hours and staff are criticised if they take any longer, says Alex. \"That target is in people's minds constantly and it's wrong, because how do you fit into two-and-a-half hours someone's story of how they've upped sticks and left the place they were born, the place their family is?\" The pressure to get things done quickly means interviews may be rushed, especially if a decision-maker has two to do on the same day. \"We are reluctant to offer breaks, we might be abrupt with asylum seekers, rather than empathetic because we simply need to power through the interview as quickly as possible,\" Alex says. Until the beginning of this year, Bootle staff would interview asylum seekers face-to-face at the Capital building in central Liverpool. But now they increasingly do the interviews over Skype.  The asylum seeker will beam in from one location, the interpreter, if needed, from another - and Alex from a small booth in Bootle. It means they've been able to interview asylum seekers living in Leicester, Sheffield, London and Glasgow. But the video link often glitches and cuts out throughout interviews. The charity Asylum Aid, which gives legal support to asylum seekers, says it has heard of connections being so bad that it's difficult to make out what is being said. \"In a matter of life and death, which is what an asylum interview is, that is unacceptable,\" says spokesman Ciaran Price. \"Anyone who has ever done a video conference knows it is not as easy to put a point across. The Home Office regularly take into account body language, it will be very difficult to make a judgement about how traumatised someone is when you're relying on a grainy video that keeps freezing.\" Alex says it isn't uncommon for people to break down into tears and in that situation, it is good to be in the same room. \"I can be sympathetic and encourage them to have a break. I can get them some water and sit quietly with them while they recompose themselves,\" he says.  Some days it feels too cruel to do otherwise, even if it means forfeiting a target. Alex will often go home after a tough day and break down into tears himself.  Sometimes it's not possible for one decision-maker to follow a case all the way through, and in such cases Alex has to rely on notes taken by another interviewer. Reading the case files it becomes clear when the interview has been rushed, as key details will be missing. For example, it's possible to check whether applicants come from the country they claim to come from by asking the right questions - questions about key landmarks in their town, perhaps, the name of the local public transport network or the country's last-but-one leader. But sometimes interviewers have failed to do this. \"If someone is undocumented, how can you assume their nationality without asking questions?\" asks Alex.  \"The files are often missing key details and they've forgotten to ask key questions, which makes it very difficult for me to a make a decision.\"  Again, this can be because the interviewer is rushing. It's rare to have time to read through the applicant's file before going into the interview, Alex says, or to carry out research into the applicant's home country.  When Asylum Aid represented a gay client from Vietnam recently, the Home Office caseworker referred to a Lonely Planet guide to establish whether or not it would be safe for him to return home. Based on the guide's description of Ho Chi Minh city, the caseworker suggested it would be safe for him to go back. \"The target audience for Lonely Planet isn't a Home Office decision-maker. It's a holiday-maker, probably Western, with cash to spend. Unsurprisingly, it doesn't offer holiday-makers the level of detail about the human rights situation that is needed in deciding a person's fate,\" says spokesman Ciaran Price. \"This is a ridiculous source of objective evidence to use in a decision letter, and is a strong example of Home Office staff relying on information that's quickly available and easy to find - not what is suitable in an individual's case.\" Many of the decision-makers in the Bootle centre are young graduates, with no previous experience of this kind of work and only two weeks of training before they start doing interviews, Alex says.  Everyone else in the office is a temporary worker, employed via a High Street recruitment agency. This includes the performance managers driving the decision-makers to work faster. \"They typically come from sales backgrounds and have never done any work involving asylum seekers or immigration themselves. They have no understanding of the process or how important it is to do things sensitively and properly,\" Alex says.  He says there are no quotas for the number of applications that must be rejected, the only target is speed - everyone is made acutely aware that the national backlog of cases in progress is in the tens of thousands and that the Home Office is under fire for long delays. But speed affects quality, he says, and the decisions are sometimes overturned on appeal. According to the Law Society, almost 50% of UK immigration and asylum appeals are upheld - evidence of \"serious flaws in the way visa and asylum applications are being dealt with\". Asked to comment on Alex's allegations a Home Office spokeswoman said: \"We do not recognise these claims made by an anonymous source.  We have a dedicated and hardworking team who are committed to providing a high level of service with often complex asylum claims. Their individual workload is appropriate and dependent on their level of experience and seniority.\" She added that caseworkers received a proper level of training, and further mentoring if they struggled \"to progress cases in line with expected standards\". There were also internal audit procedures, she said, to ensure that decision-makers do not simply make what they deem to be the quickest decision. Across the UK, she said, most interviews with asylum applicants took place face-to-face, though video-interviewing trials would continue. The spokeswoman said that appeals could be upheld for a number of reasons, including the presentation of new material not available at the time of the initial decision. Despite the Bootle centre's emphasis on speed, it has failed to clear the backlog as fast as had been hoped.  There used to be a big poster on the wall of a winding road with a plastic toy car attached, which was moved to indicate progress towards the 10,000 target. It was taken down some months ago, when it became clear that this would be impossible. Towards the end of March, coming up to the centre's one-year anniversary, it was announced that 5,000 cases had been completed. (The person who made the 5,000th decision was rewarded with vouchers and some chocolate.) Problems with staff retention were one factor that prevented the car moving faster. More than a quarter of Home Office staff who take decisions on asylum cases quit over a six-month period, according to a report by David Bolt, the chief inspector of borders and immigration.  Alex is looking for another job, and so are lots of his colleagues. \"I struggle with my job from a moral perspective,\" he says. \"The thing that gets me the most is, if someone is telling the truth but I make the wrong decision and send them back, I'm signing their death warrant.\" Illustrations by Tom Humberstone Follow Kirstie Brewer on Twitter @kirstiejbrewer They often fled their homelands to escape sexual abuse - but for many asylum seekers, it continues in the UK. Fear of deportation typically means they don't tell police, but one effect of the Harvey Weinstein revelations is that they have now begun to talk about their experiences among themselves. Secret world: The women who cannot report sexual abuse Join the conversation - find us on Facebook, Instagram, YouTube and Twitter.","time":1525805856,"title":"UK asylum decision-maker: 'It's a lottery'","type":"story","url":"http:\/\/www.bbc.co.uk\/news\/stories-43555766","label":7,"label_name":"random"},{"by":"baddash","descendants":0,"id":17023661,"kids":"None","score":2,"text":"Onstage at I\/O 2018, Google showed off a stunning, in-the-works capability of Google Assistant: in the not too distant future, it\u2019s going to make phone calls on your behalf. CEO Sundar Pichai played back a phone call recording that he said was placed by the Assistant to a hair salon. The voice sounded incredibly natural; the person on the other end had no idea they were talking to a digital AI helper. Google Assistant even dropped in a super casual \u201cmmhmmm\u201d early in the conversation. Pichai reiterated that it was a real call using Assistant and not some staged demo. This is next-level AI stuff, but Google\u2019s chief executive said it\u2019s still very much under development. Google plans to conduct early testing of Duplex inside Assistant this summer \u201cto help users make restaurant reservations, schedule hair salon appointments, and get holiday hours over the phone.\u201d Pichai says the Assistant can \u201chandle the interaction gracefully\u201d and react intelligently even when a conversation isn't totally straightforward and veers a bit off course. \u201cWe\u2019ve been working on this technology for many years,\u201d he said. It\u2019s called Google Duplex. \u201cWe\u2019re still developing this technology, and we want to work hard to get this technology and the expectations right.\u201d Google has published a blog post with more details and soundbites of Duplex in action. \u201cThe technology is directed towards completing specific tasks, such as scheduling certain types of appointments. For such tasks, the system makes the conversational experience as natural as possible, allowing people to speak normally, like they would to another person, without having to adapt to a machine.\u201d Google envisions other use cases like having Assistant call businesses and inquire about their hours to help keep Maps listings up to date.  The company says it wants to be transparent about Duplex, as a voice that sounds this realistic is certain to raise some questions.  In current testing, Google notes that Duplex successfully completes most conversations and tasks on its own without any intervention from a person on Google\u2019s end. But there are cases where it gets overwhelmed and hands off to a human operator. This section on the ins and outs of Duplex is very interesting:   The Google Duplex system is capable of carrying out sophisticated conversations and it completes the majority of its tasks fully autonomously, without human involvement. The system has a self-monitoring capability, which allows it to recognize the tasks it cannot complete autonomously (e.g., scheduling an unusually complex appointment). In these cases, it signals to a human operator, who can complete the task. To train the system in a new domain, we use real-time supervised training. This is comparable to the training practices of many disciplines, where an instructor supervises a student as they are doing their job, providing guidance as needed, and making sure that the task is performed at the instructor\u2019s level of quality. In the Duplex system, experienced operators act as the instructors. By monitoring the system as it makes phone calls in a new domain, they can affect the behavior of the system in real time as needed. This continues until the system performs at the desired quality level, at which point the supervision stops and the system can make calls autonomously. Command Line delivers daily updates from the near-future.","time":1525805727,"title":"Google Assistant's Real World Demo \u2013 Booking a Haircut on the Phone","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17332070\/google-assistant-makes-phone-call-demo-duplex-io-2018","label":5,"label_name":"ml"},{"by":"ohjeez","descendants":0,"id":17023648,"kids":"None","score":1,"text":"Given how much tech and privacy policy have been appearing in the headlines recently,\nI was curious to know more about the groups that actually guide these technologies\nthat impact millions of peoples\u2019 lives.  The World Wide Web Consortium \u2014 W3C for short \u2014 is one of these groups, responsible for codifying a set of standards for the web.\nAlong with ECMA International, a standards body to created to oversee the development of JavaScript (ECMAScript is the ES in ES6), the W3C creates proposals that are\nstandardized and adopted by browsers. The W3C is split up into working groups, which consist of invited experts and representatives from member organizations. I collected data on 309 organizations across 41 working groups. Most of the organizations are private companies, but there are some universities and non-profits as well. Not surprisingly, the most well-represented organizations are browser manufacturers like Google, Microsoft, Apple, Mozilla, Baidu, and Alibaba. Other tech giants like Adobe, IBM, and Intel are represented too. To help better understand what these companies are most interested in, I built a small interactive tool to browse organizations and see what groups their representatives are working with. Click on the name of an organization to see associated working groups, and use the control panel to refine your search and see more or fewer organizations. Scroll down for methodology.\n \n    Showing companies holding more than 50.00 seats.\n   \n    Search  \n   Normalize by group size?  I\u2019m not affiliated with the W3C in any way, this post was simply made using publicly accessible data on their website. This data was obtained from the W3C website in April 2018. Some organizations were listed as being members of a group but having no current representatives, in this case they were given a value of 0.5 representatives. The group size totals reflect the total number of organizational members, not including invited experts, whose participation is not affiliated with a specific organization. The member groups are sized according to the selected organization\u2019s level of participation in each group. The layout is calculated by react-masonry-component, which although\nimperfect, provides a good at-a-glance overview of each organization\u2019s major interests. If you\u2019d like to visualize this data in a different way, feel free to take it from the github repo linked below, in the data folder.","time":1525805664,"title":"Who Shapes the Open Web? Interactive Tool Explores W3C Working Groups","type":"story","url":"https:\/\/mathisonian.github.io\/who-shapes-the-open-web\/","label":7,"label_name":"random"},{"by":"monsieurpng","descendants":0,"id":17023642,"kids":"None","score":4,"text":"Google  unveiled some of the new features in the next version of Android at its developer conference. One feature looked particularly familiar. Android P will get new navigation gestures to switch between apps. And it works just like the iPhone X. \u201cAs part of Android  P, we\u2019re introducing a new system navigation that we\u2019ve been working on for more than a year now,\u201d VP of Android Engineering Dave Burke said. \u201cAnd the new design makes Android multitasking more approachable and easier to understand.\u201d While Google has probably been working on a new multitasking screen for a year, it\u2019s hard to believe that the company didn\u2019t copy Apple. The iPhone X was unveiled in September 2017. On Android P, the traditional home, back and multitasking buttons are gone. There\u2019s a single pill-shaped button at the center of the screen. If you swipe up from this button, you get a new multitasking view with your most recent apps. You can swipe left and right and select the app you\u2019re looking for. If you swipe up one more time, you get the app drawer with suggested apps at the very top. At any time, you can tap on the button to go back to the home screen. These gestures also work when you\u2019re using an app. Android P adds a back button in the bottom left corner if you\u2019re in an app. But the most shameless inspiration is the left and right gestures. If you swipe left and right on the pill-shaped button, you can switch to the next app, exactly like on the iPhone X. You can scrub through multiple apps. As soon as you release your finger, you\u2019ll jump to the selected app. You can get Android P beta for a handful of devices starting today. End users will get the new version in the coming months. It\u2019s hard to blame Google with this one as the iPhone X gestures are incredibly elegant and efficient \u2014 and yes, it looks a lot like the Palm Pre. Using a phone that runs the current version of Android after using the iPhone X is much slower as it requires multiple taps to switch to the most recent app. Apple moved the needle and it\u2019s clear that all smartphones should work like the iPhone X. But Google still deserves to be called out. ","time":1525805620,"title":"Android blatantly copies the iPhone X navigation gestures","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/android-blatantly-copies-the-iphone-x-navigation-gestures\/","label":9,"label_name":"tech"},{"by":"meredithah","descendants":1,"id":17023632,"kids":"[17024607]","score":13,"text":"By Justin KanMay 1, 2018 The key to finding and meeting VCs isn\u2019t cold emails or playing a high-volume game \u2014 it\u2019s all in the introductions. And anyone can get them. I\u2019ve been fortunate enough to have success in building technology businesses and have established great connections. This made our Series A process for Atrium much more straightforward. To be clear, establishing yourself in Silicon Valley makes it much easier to meet investors. But it\u2019s also beside the point. All too many founders say \u2018I\u2019m not connected in Silicon Valley, I\u2019m not successful, so I\u2019ll never be able to raise money for my business\u2019. Of all the common questions I get, \u201cHow do I meet investors?\u201d ranks at or near the top. As a founder who went through an exit, and later invested in companies myself, I have experience from both sides of the table. There are two critical factors to consider when you want to fundraise for your startup: It\u2019s not who you know: it\u2019s who your network knows. Building and leveraging your network to get intros is the tried and true way to meet investors and get funding. I\u2019m going to show you the process and explain how I was able to raise a Series A with 0 investors in my contact list. But first . . . It\u2019s important to precede a list of suggestions with a stern warning of how you can shoot yourself in the foot before you begin. The amount of bad advice out there about meeting investors is appalling and detrimental to the startup community. It\u2019s hard to know where to start, but here is a brief list of what not to do. When I was a YC partner, I would get emails every day from random people saying \u201cInvest in my startup.\u201d No intro. Not even a good explanation of what they do or what\u2019s in it for me. Nothing. Have empathy and think about your audience (this applies to all aspects of business communication, not exclusively to investors). For 99.99% of those inquiries, I just didn\u2019t have time to actually respond to them. Otherwise, I would spend my entire day talking to random people. Cold email is almost never going to work and is a subpar use of your time \u2014 which is the single-most valuable asset for a startup founder. We\u2019ll get more into the opposite of this in the how-to section. I wish this went without saying, but unfortunately it doesn\u2019t. One of my partners at YC had a founder show up outside his house. Not cool. Don\u2019t track someone through social media, or show up at an investor\u2019s house and expect that to not creep them out (yes, this has happened to me before). Even if you identified the right investor, this is simply the wrong way to go about it. Investors don\u2019t respond to this \u2014 it\u2019s a perverse incentive. Rewarding this type of behavior will lead to its proliferation, even if you found a great fit. Don\u2019t waste your time on investors who don\u2019t look at your space, or haven\u2019t shown a propensity for investing at your current stage. What\u2019s the point? Investors typically have a focus for both:\na) Industry (e.g. biotech, health, enterprise SaaS)\nb) Funding round (seed, growth, Series A) When you contact an investor who doesn\u2019t meet both of your criteria, you may be ruining an opportunity for future consideration. The importance of networks means your reputation is of utmost importance. If word gets around that you are aimlessly sending cold emails, you might miss out on a future opportunity. We\u2019ll go over how to find and target the right investors in the next section. There\u2019s a delicate balance between highly personalized\/targeted outreach, and acknowledging that it\u2019s ultimately a numbers game. The funnel metaphor couldn\u2019t be more apt. You\u2019re only going to convert so many, so you need to get enough in the top (the \u201cvolume\u201d). But identifying too many prospects can \u2014 intentionally or not \u2014 detract from the quality of each individual relationship, thereby limiting your chances of closing a deal. Find the sweet spot of just enough, but not too many. A lot of very smart people I really respect say \u201cdon\u2019t talk to associates.\u201d I actually disagree with this. Per the point above about cold email, connections are king \u2014 and better than not getting in touch at all. Obviously talking directly to a partner is preferable, but it\u2019s realistically not always an option. Take what you can get. If you have an \u201cin\u201d with an associate, you\u2019re only one level removed from the investor. Like all aspects of startups and business, there is no universal hack that magically works. Sure, some people have used counterintuitive and ingenious methods to get investor attention. But this is not a hack; rather, it\u2019s a different approach to the same principles (which I explain below). And one last point: you should know whether you\u2019re even ready to talk to investors in the first place. More on that here \u2013 Guide to Raising a Series A. The following 3 steps will be broken down and provide an actionable way for you to start meeting the right investors \u2014 aka those who will be interested in your company. You don\u2019t want any investors; you want people who have the ideal resume, background, and demonstrated interests in funding your startup. The answers to two basic questions will guide the filtration process: If you\u2019re a biotech company, don\u2019t target investors who haven\u2019t been involved in biotech. If you\u2019re raising a seed round, don\u2019t target investors who exclusively invest at later stages. You want to respect everyone\u2019s time and carefully curate your list. This simple rule alone will narrow down your scoping process. Don\u2019t just stop at the firm. Target specific partners, angel investors, etc. You want the people with the relevant background. At most firms there are different folks who specialize in different industries. If you\u2019re a biotech company, target the biotech partner. Choose the person who\u2019s most relevant to you. Knowing that a certain firm has invested in your space is only the starting point. You want the person with influence who can make a decision about your company. I usually track my curated list of investors in a Google Sheet with all relevant details. I don\u2019t want to be too prescriptive about the length of the list, as there\u2019s no magic number. However, I generally find that a minimum of 10, and ideally in the 15-20 range is good. Too many more and it\u2019s hard to do it well and target effectively; much less and you\u2019re not feeding the funnel. Use your network\nAnother source of investor leads \u2014 and your best bet at actually reaching them \u2014 lies within your network. This requires a distinction. The next point is about using your network to get connected to the investors you\u2019ve already identified. Here, I mean that you actually use that network to identify the investors in the first place (at which point the connection is predetermined). Talk to your friends and network about your company and lay out your investor criteria (industry + investment stage) to see who someone knows. Odds are, you\u2019ll get multiple leads this way. People are always much more receptive to introductions from people that they know and trust. The \u201cwarmer\u201d the person that\u2019s giving you the intro, the better. Your primary objective with this process is to find the best person to give you an intro; someone who is seen as credible to the investor.\nExisting investors\nIf you\u2019ve raised a seed and are targeting a Series A, you have the best type of referral: an investor in your company. Referrals from investors are the strongest because they\u2019re putting their money where their mouth is. It\u2019s a good signal if I recommend a startup that I\u2019ve recently become acquainted with to an investor friend; it\u2019s an extremely strong signal if I\u2019ve already put my own cash on the line. Find out how your current investors can connect you with someone on your list, and use them as a method for building the list in the first place. When you are asking your investors who they can intro you to, feel free to share the Google sheet you are using to track with them. This is much better than just asking \u2018What investors do you know that would be interested in my company?\u2019 Odd\u2019s are that that investor doesn\u2019t keep a list of other investors and their interests in their head \u2014 it\u2019s much easier for them to look at a list and see who they know. There\u2019s an awkward stage between a cold email and a fully-vetted introduction: a \u201csingle opt-in\u201d email. That\u2019s when someone I know makes an unsolicited intro, without first running it by me. There\u2019s a lot of companies that I\u2019m not interested in meeting with. Maybe I\u2019m not interested in the space, have a conflicting investment, or have soured on the space. That\u2019s why double opt-in has become a standard best practice for introductions. Both parties get briefed on the other so that when the introduction happens, you can move forward productively. You don\u2019t waste anyone\u2019s time. The alternative \u2014 single opt-in, or none at all \u2014 is a waste of everyone\u2019s time. It also puts the recipient in an awkward position. I\u2019ve had to tell a friend or acquaintance \u201cno thanks,\u201d putting everyone in an uncomfortable spot. Having a person who can act as a connection to an investor isn\u2019t enough. To get an investor\u2019s attention and not be a hindrance, you need the double-opt-in. Here\u2019s how double-opt-in works: It\u2019s that simple. The magic is in the email, the strength of the connection, and the ultra-relevance of the investor (as established in step 1). This is why I say there are no excuses if you really want to meet investors. In fact, I put together an email copy and detailed video explanation to help you do the same. Get Email Copy I had 0 startup connections when I first started fundraising, resulting in my firm belief that anyone can do it. Having connections isn\u2019t the requirement; it\u2019s access to connections that matters, and this is something entirely within your control. The first thing to do is go lean on your friends who are entrepreneurs. Every entrepreneur has friends who are entrepreneurs. When I first showed up in Silicon Valley, I leaned on Adam Smith and Matt Brezina, the founders of Xobni, a YC-backed company. They were friends of mine I\u2019d met in Boston, providing my access to the people I needed to meet. They had raised a seed round from a lot of great angel investors, including Paul Buchheit and others. We asked them for some intros to their investors and extended network. This got us in touch with Aydin Senkut (Felicis Ventures), and eventually Paul Buchheit (the creator of Gmail). Those intros led to the initial funding for Justin TV, which eventually morphed into Twitch. We didn\u2019t have any investor connections, but our friends did \u2014 and that was how I got my start in the Valley. Let\u2019s say you just got here and don\u2019t know anyone. No friends, no network, nada. It\u2019s still doable. People are intimidated by the concept of \u201cnetworking,\u201d but meeting people is surprisingly simple. You can come to the Bay Area and meet entrepreneurs very easily. Sure, you might not be able to meet Steve Huffman from Reddit or Patrick Collison from Stripe on day one. But, you can meet somebody who\u2019s raised seed money, Series A, maybe even more \u2014 in a relatively short period of time. Meet people at networking dinners and conferences. If it isn\u2019t these folks, then I guarantee they know other people who are founders or angel investors. You can meet founders\/angels and their extended network. Founders and investors have a diversity of friends and partners \u2014 including people who personally aren\u2019t founders or investors. You can often get an intro to a founder and investor through one of the senior employees at their companies. I\u2019ve talked to friends of my employees about startup investment opportunities. Another idea: lean on people doing an accelerator like Y Combinator or 500 Startups. You either know someone going through the program, or can meet someone who knows someone at any event I described above. You can talk to friends at Y Combinator, 500 Startups, or another accelerator . . . or you can try and get your startup in. This is a phenomenal way to get access to networks. It\u2019s like seed fundraising on easy mode. Based on the strong network you inherit, you have access to practically anyone in Silicon Valley (using the framework from the previous section). Remember that meeting investors comes down to warm intros from people who the investor trusts. YC provides everything you could want in that respect. It\u2019s also great credentialing and a brand stamp on you as well. This type of endorsement almost certainly leads to a valuation bump compared to the Silicon Valley average. Or you can use a fundraising bootcamp. There are many of these \u2014 here at Atrium, we have Atrium Scale. This provides startups with the credible intro you need to talk to investors, but also helps you maximize your opportunity with that intro. You learn how to: Then before and after the bootcamp we help secure intros (the \u201cwarm lead\u201d you need). There are plenty of well-connected people who do bootcamps, and this is an excellent complement (or even standalone method) for meeting investors and getting intros. Meeting investors seems daunting but is like anything in the startup world: apply first principles thinking and boil it down to the core idea (getting a warm introduction). There are shortcuts you can take to getting introductions, such as bootcamps like Atrium Scale, but if you put in the work and follow this process you will give your startup a chance. Justin Kan is an internet entrepreneur and investor known for founding various companies, including Twitch--a video game streaming platform (acquired by Amazon for $970mm). He served as Partner at Y Combinator, where he impacted over 900 companies and funded more than 130. Currently, Justin serves as CEO of Atrium, where he's building technology to revolutionize the $450bn legal industry. Everything you need to know about early stage fundraising from our unique perspective helping our clients close deals everyday. By Justin KanMay 1, 2018 A few years ago I wrote The Founder\u2019s Guide To Selling Your Company that was pretty well received by the\u2026 Read On Startup advice, war stories, and best practices.\u00a0 Found exclusively in the Atrium newsletter. First Name\nLast Name\nEmail\n \u00a9Atrium\nAtrium LLP is a registered limited liability partnership under the laws of the State of California.\nAtrium LTS is a [non-law] corporation incorporated under the laws of the State of Delaware.","time":1525805566,"title":"How to Find Investors and Get Email Intros","type":"story","url":"https:\/\/www.atrium.co\/blog\/how-to-find-meet-investors\/","label":7,"label_name":"random"},{"by":"hapnin","descendants":0,"id":17023596,"kids":"None","score":3,"text":"A minimalistic commandline tool to manage encrypted volumes aka The Crypto Undertaker  More information and updates on website: https:\/\/www.dyne.org\/software\/tomb Get the stable .tar.gz signed release for production use! Download it from https:\/\/files.dyne.org\/tomb For the instructions on how to get started using Tomb, see INSTALL.   Tomb aims to be a free and open source system for easy encryption and\nbackup of personal files, written in code that is easy to review and\nlinks well reliable GNU\/Linux components. Tomb's ambition is to provide military-grade security by way of: At present, Tomb consists of a simple shell script (Zsh) using\nstandard filesystem tools (GNU) and the cryptographic API of the Linux\nkernel (cryptsetup and LUKS). Tomb can also produce machine parsable\noutput to facilitate its use inside graphical applications. To create a Tomb, do: To open it, do and after you are done or if you are in a hurry This tool can be used to dig .tomb files (LUKS volumes), forge keys\nprotected by a password (GnuPG encryption) and use the keys to lock\nthe tombs. Tombs are like single files whose contents are inaccessible\nin the absence of the key they were locked with and its password. Once open, the tombs are just like normal folders and can contain\ndifferent files, plus they offer advanced functionalities like bind\nand execution hooks and fast search, or they can be slammed close even\nif busy. Keys can be stored on separate media like USB sticks, NFC,\non-line SSH servers or bluetooth devices to make the transport of data\nsafer: one always needs both the tomb and the key, plus its password,\nto access it. The tomb script takes care of several details to improve user's\nbehaviour and the security of tombs in everyday usage: protects the\ntyping of passwords from keyloggers, facilitates hiding keys inside\nimages, indexes and search a tomb's contents, mounts directories in\nplace, lists open tombs and selectively closes them, warns the user\nabout free space and last time usage, etc. Death is the only sure thing in life. That said, Tomb is a pretty\nsecure tool especially because it is kept minimal, its source is\nalways open to review (even when installed) and its code is easy to\nread with a bit of shell script knowledge. All encryption tools being used in Tomb are included as default in\nmany GNU\/Linux operating systems and therefore are regularly peer\nreviewed: we don't add anything else to them really, just a layer of\nusability. The file KNOWN_BUGS.md contains some notes on known\nvulnerabilities and threat model analysis. In absence or malfunction of the Tomb script it is always possible to\naccess the contents of a Tomb only using a dm-crypt enabled Linux\nkernel, cryptsetup, GnuPG and any shell interpreter issuing the\nfollowing commands as root: One can change the last argument \/mnt to where the Tomb has to be\nmounted and made accessible. To close the tomb then use: Tomb is an evolution of the 'mknest' tool developed for the\ndyne:bolic 100% Free GNU\/Linux\ndistribution in 2001: its 'nesting' mechanism allowed the liveCD users\nto encrypt and make persistent home directories. Since then the same\nshell routines kept being maintained and used for dyne:bolic until\n2007, when they were ported to work on more GNU\/Linux distributions. As of today, Tomb is a very stable tool also used in mission critical\nsituations by a number of activists in dangerous zones. It has been\nreviewed by forensics analysts and it can be considered to be safe for\nmilitary grade use where the integrity of information stored depends\non the user's behaviour and the strength of a standard AES-256 (XTS\nplain) encryption algorithm. Tomb can be used in conjunction with some other software applications,\nsome are developed by Dyne.org, but some also by third parties.  Secrets is a software that can be operated on-line and on-site to split a Tomb key in shares to be distributed to peers: some of them have to agree to combine back the shares in order to retrieve the key. zuluCrypt is a graphical application to manage various types of encrypted volumes on GNU\/Linux, among them also Tombs, written in C++. Mausoleum is a graphical interface to facilitate the creation and management of tombs, written in Python. pass-tomb is a console based wrapper of the excellent password keeping program pass that helps to keep the whole tree of password encrypted inside a tomb. It is written in Bash. If you are writing a project supporting tomb volumes or wrapping tomb, let us know! Tomb qualifies as sound for use on information rated as \"top secret\"\nwhen used on an underlying stack of carefully reviewed hardware\n(random number generator and other components) and software (Linux\nkernel build, crypto modules, device manager, compiler used to built,\nshell interpreter and packaged dependencies). Tomb volumes are fully compliant with the FIPS 197 advanced encryption\nstandard published by NIST and with the following industry standards: Tomb implementation is known to address at least partially issues raised in: Any help on further verification of compliancy is very welcome, as the\naccess to ISO\/IEC document is limited due to its expensive nature. Anyone planning to use Tomb to store and access secrets should not use\nthe latest development version in Git, but use instead the .tar.gz\nrelease on https:\/\/files.dyne.org\/tomb . The stable version will\nalways ensure backward compatibility with older tombs: we make sure it\ncreates sane tombs and keys by running various tests before releasing\nit. The development version in Git might introduce sudden bugs and is\nnot guaranteed to produce backward- or forward-compatible tombs and keys.\nThe development version in Git should be used to report bugs, test new\nfeatures and develop patches. So be warned: do not use the latest Git version in production\nenvironments, but use a stable release versioned and packed as\ntarball on https:\/\/files.dyne.org\/tomb  Donations are very welcome, please go to https:\/\/www.dyne.org\/donate Translations are also welcome: they can be contributed editing sending\nthe .po files in extras\/translations. The code is pretty short and readable. There is also a collection of\nspecifications and design materials in the doc directory. To contribute code and reviews visit https:\/\/github.com\/dyne\/Tomb If you plan to commit code into Tomb, please keep in mind this is a\nminimalist tool and its code should be readable. Guidelines on the\ncoding style are illustrated in doc\/HACKING.txt. Tomb's developers can be contacted using the issues on GitHub or over\nIRC on https:\/\/irc.dyne.org channel #dyne (or direct port 9999 SSL) Tomb is Copyright (C) 2007-2018 by the Dyne.org Foundation and\nmaintained by Denis Roio jaromil@dyne.org. More information on all\nthe developers involved is found in the AUTHORS file. This source code is free software; you can redistribute it and\/or\nmodify it under the terms of the GNU Public License as published by\nthe Free Software Foundation; either version 3 of the License, or\n(at your option) any later version. This source code is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  Please refer\nto the GNU Public License for more details. You should have received a copy of the GNU Public License along with\nthis source code; if not, write to: Free Software Foundation, Inc.,\n675 Mass Ave, Cambridge, MA 02139, USA.","time":1525805276,"title":"Tomb: A minimalistic commandline tool to manage encrypted volumes","type":"story","url":"https:\/\/github.com\/dyne\/Tomb\/blob\/master\/README.md","label":3,"label_name":"dev"},{"by":"heitortsergent","descendants":0,"id":17023590,"kids":"None","score":6,"text":"Guest post by Ian Dugas, Sales Manager at IOpipe. I\u2019ve worked for companies ranging from startups to one traded on the public market. In every one of those, I was largely\u2014if not entirely\u2014abstracted away from the product development and engineering teams. Now that I am at seed stage startup called\u00a0IOpipe, where I was the first sales hire, I engage directly with the product team. That\u2019s right, I\u2019m a salesperson. So why am I, a salesperson, writing about a developer tool like LaunchDarkly? Well like most people (hopefully) who work in technology, I\u2019m a fan of discovering efficiencies and applying them to processes and workflows. LaunchDarkly has uncovered new efficiencies for IOpipe and how we engage with our customers. The value here transcends product development. IOpipe is obsessively focused on iterating the product at breakneck speeds. In other words, being efficient\u2014as a company\u2014is mission-critical for our success. It takes extreme focus for a small team to iterate as quickly as we\u2019ve been able to and continue to. I\u2019ve already learned many things working in this paradigm. Chief among them are two closely related lessons: IOpipe provides powerful debugging, monitoring, and observability tools for enterprises. \u201cEnterprises\u201d is the operative word in the preceding sentence as our touches with prospects and customers are always deliberate and often strategic. For that reason, the release of beta and test features are not done en masse, and are instead done in a controlled and manual fashion. The problem, however, was that we had to find a way to open test features for specific users without Sales disrupting product sprints. That\u2019s when my developer colleagues introduced me to LaunchDarkly and feature flags\u2014something we\u2019ve been using on the Product side of the business for quite a while. With LaunchDarkly, once our company determines a feature or capability is ready for user testing, Sales can go in and directly unlock for a specific user. So, with LaunchDarkly, our process went from: 1)\u00a0Sales identifies test user\u00a0 \u2192\u00a0\u00a02) Sales requests feature from Dev\u00a0\u00a0\u2192\u00a0\u00a03) Dev unlocks feature\u00a0\u00a0\u2192\u00a0\u00a04) Dev notifies Sales To: 1)\u00a0Sales identifies test user\u00a0\u00a0\u2192\u00a0\u00a02) Sales unlocks feature for test user LaunchDarkly essentially removes two steps from our process, and completely eliminates the need for developers to break flow in order to perform a simple but very important task. i.e.\u00a0Exactly what we were looking for.\u00a0It also took the time to implement a feature for a customer from 45 minutes plus a developer interruption, to ~1 minute. Let me give you a real example\u2026 IOpipe recently GA\u2019d support for Java. Prior to releasing this capability, we performed two rounds of tests (alpha, beta) with actual enterprise users. At the time, only a subset of our customers and prospects had requested Java support. They were of the larger variety. Within that subset, we could only have a few strategic users test for testing to be effective. I was responsible for engaging the users and qualifying strategic prospects and customers willing to partner with us for Java testing. With LaunchDarkly I was able to unlock the feature as I spoke with the target users. Without LaunchDarkly, unlocking Java would\u2019ve become a \u201clet me get back to you\u201d type of exercise, which no one enjoys, and is also\u2026well, inefficient. To recap, since my sales team has begun using LaunchDarkly, we\u2019ve been able to minimize developer distractions, so they can preserve focus, and we\u2019ve been able to independently roll out beta features. This has allowed us to quickly deliver functionality without being a burden to engineering, and participate more actively in the feedback process with developers and product managers. *\u00a0 *\u00a0 * Ian Dugas\u00a0has been in tech for the last 10 years in roles spanning Sales, Professional Services, Customer Success, Business Development, and Operations. He\u2019s sold, and delivered, cloud and on-premise solutions to companies and government entities big and small. He has an affinity for bulldogs, and can be found on Twitter\u00a0here.  Copyright \u00a9 2018 Catamorphic Co. ","time":1525805232,"title":"Giving Sales Control of Feature Flags","type":"story","url":"https:\/\/blog.launchdarkly.com\/giving-sales-control-of-feature-flags\/","label":1,"label_name":"business"},{"by":"LVB","descendants":0,"id":17023587,"kids":"None","score":2,"text":"New in OS X: Get MacRumors Push Notifications on your Mac Subscribe to the MacRumors Newsletter \r\n  Nice! Now would be even nicer if it was shorter then 7 days. 1 day is fine with me. A couple of hours would be fine with me too. Yeah, there are never any reasons for the government to need access. Certainly not thousands of children molested and their exploited photos traded around with pedophiles. There's no white collar crime or terrorism in the world. Yup, you've got it, there's never a good reason that they may need to access a device. Never. An unintended downside: imagine you have a close friend or family member pass away. You\/their family want to access their devices afterwards for photos, remembrance, information about their final days, etc. Absent the biometric and passcode (which I am assuming you don't have) you will need the cord access to get in. With a billion devices out there, I am thinking this will happen more frequently than the law enforcement access it is intended to prevent.\n\nAnd to anyone who wonders why this would even be necessary, I submit you have not suddenly lost a loved one. Not uncommon to go looking for answers or solace in their devices, notify friends of the passing, etc. Asking people to try to get in there within 7 days isn't always realistic or feasible. After reading all the comments... WHAT SORT OF STUFF ARE YOU ALL HIDING IN YOUR PHONES?\n\nWhat are you so scared of? Preview at WWDC likely in June, followed by September launch. WWDC 2018 takes place June 4 to June 8 in San Jose, California.  Three new iPhones expected: 5.8\" and 6.5\" OLED plus a 6.1\" LCD. \u00a0 MacRumors attracts a broad audience\n        of both consumers and professionals interested in\n        the latest technologies and products. We also boast an active community focused on\npurchasing decisions and technical aspects of the iPhone, iPod, iPad, and Mac platforms.","time":1525805210,"title":"iOS 11.4 Disables Lightning Connector After 7 Days","type":"story","url":"https:\/\/www.macrumors.com\/2018\/05\/08\/ios-11-4-usb-restricted-mode\/","label":7,"label_name":"random"},{"by":"PaulVanOijen","descendants":0,"id":17023563,"kids":"None","score":1,"text":"Motion is an inescapable element of design these days. The days that motion design sat squarely in the domain of animators and VFX specialists are long gone. Major companies such as Google and IBM treat motion as a key part of their design guidelines. Designers around the globe have come to the realization that motion can be much more than a fanciful supplement to a user interface. It can be an incredibly powerful tool used to guide the user to where we want or need them to go. As a result of motion design\u2019s recognition as powerful tool to enhance the user experience, the frontpages of design-focused websites are at any one time filled with a dozen or so animated GIFs, featuring elements that slide, jump or scale in and out the screen. You might take a look at these and find some inspiration and consequently set off to create a similar animation for a project of your own. You toil for a few hours (if not days) in your prototyping tool of choice, be it Principle, Flinto, After Effects or whatnot. And in the end, you have this snazzy animation that you think will be perfect for your project. So you render the video of the animation, double check it to make sure everything is moving at the right pace and time, and store it somewhere safe. There you go. All done. Now you just need to send it over to your engineering team and it'll be implemented in no time. Great! Cue the engineer. They get handed a video that's barely a couple of seconds long, maybe a vague description of the animation, but that's probably all. Their instructions? That's it. That's all they get. A video or GIF and the instructions to replicate the animation. Unsurprisingly, even the most experienced of engineers are often hesitant to work on any form of motion design. It takes too much time, isn't worth the effort, they complain. And rightfully so. Imagine being handed a brush, some paint, a blank canvas and a grainy printout of a Picasso. Your instructions? Make it look like this. You'd be pissed off too, wouldn't you? Any designer (or engineer) who's worked on motion in a team has probably come across the scenario outline above. You might've wondered if there was a better solution for handing over motion designs. A better solution to collaboratively work on building and shipping your animation. Enter the animation graph. An animation graph (or motion graph, if you will) is a visual representation of the timeline of your animation. It is to your animation what a legend is to a map. An insightful companion that will make it easier to grasp the specifics. The graph allows you to quickly and easily answer a lot of questions concerning the animation, such as: At a quick glance, you can gather a lot of information from the graph, without having to dig through the often convoluted timeline of whatever prototyping or animation tool is being used. At the core of the motion design handover sits the video. Or GIF, in this case. It's a quick and easy demonstration of what you're trying to achieve and can be relied upon as a reference while you're building or reviewing the animation. The graph essentially serves as a companion to the animation. It's the nitty- gritty of your motion design. Using this, engineers can nail the specifics of the animation without having to even think about things like prototyping tools or After Effects timelines. There's no need to hover over their shoulder to make sure everything's pixel-perfect. The graph defines which elements are being changed, so that no parts of the animation are left out. Using the graph, a couple of things can be established: Assuming the curves are set up in advance, engineers need only set the transition start and duration, apply the pre-defined curve and the affected property. The graph doesn't specify the exact changes though, e.g. change opacity from 0% to 100% or Y-position from 450 to 550. These still need to be either documented specifically by the designer or eyeballed by the engineer. But it's a hell of a lot better than \"Make it look like this\". Thank you for reading! Continue reading about Motion Design: By clapping more or less, you can signal to us which stories really stand out. Dutchman lost in the Nordics. Product Designer @smartlyio \/\/ @paulvanoijen","time":1525805098,"title":"Motion Design handovers made easy","type":"story","url":"https:\/\/medium.com\/@paulvano\/motion-design-handovers-made-easy-83cad5f2e12a","label":7,"label_name":"random"},{"by":"anarbadalov","descendants":0,"id":17023554,"kids":"None","score":1,"text":"Jim and Ida Hall buried their daughter Jerra in a family plot at the bottom of a grassy rise. Several times a year, Jim Hall drives just over a mile from his home on North Main Street in the town of St. Louis, Michigan to Jerra\u2019s headstone in the back corner of Oak Grove Cemetery in his 1997 Chevy pickup. In the 12 years since complications from a rare heart defect claimed the life of their brown-haired toddler, her family continues to cover her grave with stuffed animals (frogs were her favorite). Hall gently sweeps off the leaves and debris covering the childhood paraphernalia and wipes his callused hands on a pair of worn jeans, his tall frame stooped by grief. He stops and stares at the inscription: \u201cTwo years, two months, too little.\u201d   \u201cWe didn\u2019t know what else to write,\u201d he said.  \u201cWhen your daughter is born with a heart condition and doesn\u2019t survive, you just wonder.\u201d Jim Hall\u2019s exposure to PBB as a child makes him valuable in the hunt for the answer to a burning scientific question:  Can a father\u2019s exposure to environmental toxins impact the health of his progeny?  Visual: Jeffrey Sauger for Undark   Jerra\u2019s headstone sits where an umbrella of majestic oaks gives way to the dreadlocks of vines and grasses of a small wetland in the geographic center of Michigan\u2019s Lower Peninsula, a little more than a mile from the chemical plant that once produced a toxic flame retardant called PBB, short for polybrominated biphenyl. Hall can\u2019t help but think it may have killed his little girl.   \u201cWhen your daughter is born with a heart condition and doesn\u2019t survive,\u201d he said, \u201cyou just wonder.\u201d The problem: Baby Jerra herself was never exposed to PBB. In fact, she was born some 30 years after the Michigan Chemical Corporation, headquartered in St. Louis, accidentally mixed several hundred pounds of PBB into livestock feed and subsequently stopped producing the chemical. Until a few years ago, a granite tombstone marked the shuttered site.    Still, in the early 1970s, virtually all of the Lower Peninsula\u2019s 8.5 million residents consumed meat, milk, and eggs contaminated with PBB before the mistake was discovered. The Michigan Department of Agriculture called it \u201cthe most costly and disastrous accidental contamination ever to occur in United States agriculture.\u201d Hall, who had just turned 11 in 1974 when the mix-up was uncovered, was among those residents \u2014 and because he lived just a few blocks from the plant, he likely got an extra dose. His grandparents\u2019 home, where he often stayed, was even closer: just three houses away and directly downwind.  The exposure almost certainly caused chemical changes to his genes, which affected not the sequence of letters in his genome, but rather how those genes are switched on and off through epigenetic information that sits on top of DNA. Recent animal studies have suggested that epigenetic changes could be passed down through generations, potentially causing inherited health problems in children, grandchildren, and great-grandchildren. And yet, the science behind transgenerational epigenetic inheritance, as such a transfer of maladies would be known, remains highly controversial, and researchers have never been able to prove its existence in humans. That\u2019s not for want of trying, however, and Michele Marcus, an epidemiologist at Emory University, believes the PBB mix-up in Michigan could prove crucial to that effort. Her preliminary work investigating this macabre, natural experiment has already suggested that the children of exposed mothers have many health problems possibly linked to PBB exposure. But the real question mark remains men like Hall, who were exposed to the chemical as children, grew up and married women who weren\u2019t, and then had children themselves.  Could those offspring \u2014 who were never exposed to PBB in the 1970s, and who were never exposed to even residual PBB while developing in the wombs of exposed mothers \u2014 nonetheless experience health impacts linked to the chemical? It\u2019s a question with profound implications.  \u201cIf you look at it through the lens of environmental justice, it changes the debate,\u201d Marcus said, \u201cbecause it\u2019s not just, \u2018Okay. I can make a decision to work in an industry that might be hazardous to my health,\u2019 but \u2018Am I making that choice for my grandchildren?\u2019\u201d There are people who don\u2019t have a choice about being exposed, she added. At the time of the incident, little was known about the biological effects of PBB exposure, and even today, only a handful of the 84,000 chemicals currently approved for use in manufacturing and industry have ever undergone real regulatory scrutiny. Even in the small subset of chemicals that have, government toxicologists mainly check for acute health effects on human populations, and while there are some testing procedures for chronic exposures, the potential impacts three generations on is not even a consideration. Now, with President Donald Trump\u2019s proposed budget cuts at the Environmental Protection Agency, the chemical industry will likely enjoy only more freedom, and less scrutiny \u2014 a prospect that a growing number of scientists worry could reverberate across generations in ways that we are only at the earliest stages of understanding.  Visual: Undark For researchers like Marcus, this makes grappling with the potential echoes of Michigan\u2019s PBB exposure all the more urgent \u2014 and not least because kindred chemicals remain in wide circulation. Although PBB itself fell out of use in the years following the Michigan incident, other brominated flame retardants, including polybrominated diphenyl ether (PBDE), are ubiquitous in everything from electronics and vehicles to furniture and textiles. Both PBB and PBDE are persistent organic pollutants, a large class of compounds that includes chemicals like DDT, polychlorinated biphenyls (PCBs), and dioxins that resist being broken down in the body and the environment. And if Marcus finds that PBB can affect multiple generations \u2014 even those with no direct exposure to the chemical \u2014 \u201cit\u2019s probably going to be true for a lot of other chemicals,\u201d she said. If Marcus is right, it could upend not just medicine, but whole strata of legal, regulatory, and even ethical bedrock. If Marcus is right, it could upend not just medicine, but whole strata of legal, regulatory, and even ethical bedrock. Could insurers refuse coverage to the great-grandchildren of people exposed to a chemical toxin? Where would liabilities end? What are the real-world implications of personal health problems linked not to some chemical exposure that unfolded in our lifetimes, but at some distant point in our past lineage? \u201cIt changes the rules,\u201d said Joe Nadeau, a geneticist at the Pacific Northwest Research Institute, a nonprofit biomedical research facility based in Seattle. \u201cIt\u2019s one thing to have the direct exposure. We all know that, we all worry about it,\u201d Nadeau said. But if the consequences of those environmental exposures can be delivered to our children or grandchildren, then we don\u2019t just have ourselves to consider. \u201cWe have a responsibility to do better,\u201d he said.  Hall feels that responsibility more deeply than most. When his daughter was sick, doctors told him that her heart defect was just dumb luck, and Hall had no reason to search for alternate explanations. But when he joined a community group working to clean up the contaminated Michigan Chemical site, Hall began to connect the dots between Jerra\u2019s illness and all the PBB he had inhaled and ingested as a child.     \u201cI hope that we can get research on [those of] us that have been affected,\u201d he said, \u201cso that this can be figured out.\u201d  Hall lost his daughter Jerra when she was just two years old.  He is not alone in wondering whether his exposure to a chemical toxin as a child can be linked to daughter\u2019s  death.  Visual: Jeffrey Sauger for Undark That the legacy of PBB exposure hangs like a dark cloud over generations of families \u2014 and over vast areas of rural Michigan \u2014 is a profound understatement. The stories vary in their details, but most begin with a slow awakening to the idea that something wasn\u2019t right. Maybe the cows were acting funny, or the chickens seemed off. And then, as livestock began to die, the awakening tended to grow into a certainty that was typically met, at least at the outset, with patronizing dismissal by authorities.  \u201cHe knew something was wrong, and he felt that no one was giving him any answers.\u201d   That\u2019s how it went down for dairy farmer Frederic Halbert. Dawn broke with hesitation on Thursday September 20, 1973 through overcast skies as Halbert traveled the five miles from the family\u2019s farmhouse to the milking parlor. As the first group of black-and-white cows ambled in, they seemed lethargic, and they didn\u2019t walk so much as stagger unevenly across the parlor.    Despite weighing in at 1,500 pounds, Holsteins have delicate stomachs. Since the quality of a cow\u2019s diet directly impacts the quality and quantity of milk it produces, Halbert paid close attention to what his cows ate. It\u2019s why he ordered 65 tons of a new cattle feed \u2014 called Dairy Ration 402 \u2014 from the Michigan Farm Bureau between the end of August and the first week of October 1973. Halbert, who had worked as a chemical engineer before returning to dairy farming, had grown interested in the new cow chow \u2014 a mix of grains and vitamins \u2014 because it contained small, silvery pellets of magnesium oxide to improve digestion and boost milk production. Halbert used DR402 in the fall and winter to supplement the farm\u2019s home-grown fodder. Not long after, the cows stopped eating and grew listless in the spacious, open-air barn. Milk production plummeted. Halbert\u2019s vet ran countless tests and submitted samples and entire animals for analysis by the state, none of which provided any concrete answers.   \u201cMy dad\u2019s herd was his life,\u201d said Lisa Halbert, Frederic\u2019s daughter, who was only a toddler at the time. \u201cHe knew something was wrong, and he felt that no one was giving him any answers.\u201d (Both journalist Joyce Egginton and Halbert himself would chronicle his long search for answers in books, \u201cThe Poisoning of Michigan\u201d and \u201cBitter Harvest.\u201d)   Across the state, dairy farmers were noticing similar behaviors in their herds. Cows began falling ill. But with no diagnosis \u2014 no infectious disease or concrete factor to blame \u2014 veterinarians and the Farm Bureau suggested the illnesses may be caused by fungus-contaminated feed from a cold, wet summer, or by bad animal husbandry. Halbert knew that wasn\u2019t the case, and after months of testing turned up nothing, he turned to DR402 as the culprit, since it was the only recent change to his cow\u2019s diets.    He sent samples to labs around the country, including to the U.S. Department of Agriculture\u2019s National Animal Disease Center in Ames, Iowa. In January 1974, their tests revealed an unknown chemical contaminant: an unusually heavy compound that no one could discern. Analysis from the Wisconsin Alumni Research Foundation (WARF) also showed that DR402 from Halbert\u2019s farm contained very little of the magnesium oxide that had been listed among the feed\u2019s ingredients. When the Ames lab ran out of funding to identify the contaminant in DR402, feed was also sent to USDA scientist George Fries in Maryland, who identified the mystery chemical as a type of polybrominated biphenyl (PBB). Halbert immediately realized what had happened, since Michigan Chemical made both magnesium oxide and PBB.   Instead of shipping magnesium oxide to the Farm Bureau, Michigan Chemical had accidentally shipped several thousand pounds of a flame retardant marketed under the brand name Firemaster. To the naked eye, the Firemaster pellets looked for all the world like harmless magnesium oxide mixed in with the DR402 feed \u2014 but it wasn\u2019t harmless.  \n\n \u25b2 The contamination affected virtually every resident of Michigan\u2019s Lower Peninsula, and led to the slaughter of thousands of animals.  Visual: Undark Lisa Halbert was just a toddler when her father Frederic began unwittingly feeding the family\u2019s dairy cows food contaminated with PBB. Not long after, the cows stopped eating and grew listless.  Visual: Jeffrey Sauger for Undark Newborn calves inside the family farm in Battle Creek, Michigan. Halbert says she sometimes gets shivers when she remembers being in the same spot as a little girl and realizing all the cows were gone.  Visual: Jeffrey Sauger for Undark Lisa Halbert was just a toddler when her father Frederic began unwittingly feeding the family\u2019s dairy cows food contaminated with PBB. Not long after, the cows stopped eating and grew listless.  Newborn calves inside the family farm in Battle Creek, Michigan. Halbert says she sometimes gets shivers when she remembers being in the same spot as a little girl and realizing all the cows were gone.  Visuals: Jeffrey Sauger for Undark   PBBs are dumbbell-shaped compounds consisting of two attached carbon rings (the phenyl groups) with varying numbers of bromine atoms linked to either end. When added to children\u2019s clothing, upholstery fabric, or electronics, they interfere with the item\u2019s ability to catch fire, making them one of the first commercial flame retardants and providing Michigan Chemical with a veritable goldmine. Still, an internal memo revealed that corporate chemists had expressed safety concerns if PBBs were ingested or inhaled. In 1968, people in Japan ate rice bran oil contaminated with chemically similar compounds including polychlorinated biphenyls (PCBs). After 14,000 people in Fukuoka Prefecture reported difficulty breathing, skin rash, weakness, and skin discoloration, public health officials discovered PCBs flowing through coils used to heat the oil.    PBBs, PCBs, and related chemicals are stored in the body fat and not excreted out, which gives them decades to cause health effects. Because PBBs were never meant for the food supply of humans or animals, however \u2014 and given the industrial bravado of the 1970s \u2014 Firemaster was marketed as safe. When Michigan Chemical learned that the PBB issue had been traced back to their plant, they disposed of PBBs in the county landfill and in a nearby burn pit that now sits on a golf course. Both are now designated as Superfund sites. The company stopped producing PBBs for good on November 20, 1974. Farmers felt they had little option but to allow the state to collect their animals, truck them to the northern tip of the Lower Peninsula, and bury them in mass graves.   Six months before that, the Michigan Department of Agriculture began quarantining dairy farms whose milk products were found to have PBB levels greater than 1 part per million. By November, that level had been lowered to .3 parts per million. Although the state did not mandate that farmers have their quarantined animals killed (supposedly to prevent them from taking legal action), farmers felt they had little option but to allow the state to collect their animals, truck them to the northern tip of the Lower Peninsula in Kalkaska, shoot them, and bury them in mass graves.  Other livestock, including chicken (and their eggs) and pigs, were implicated in the feed contamination, multiplying the exposures \u2014 and the animal executions \u2014 across the state. The exposures included not just farm families, but consumers who ate contaminated beef or chicken or pork, or who consumed contaminated milk, cheese, or eggs, as well as workers who produced the chemical in factories \u2014 and their spouses and offspring.  Direct exposure to PBBs, whether through consumption or absorbed through skin or inhalation, has been linked to an increased risk for a variety of ailments, from short-term incidence of skin rashes, hair loss, and muscle and joint issues, to longer term thyroid problems and changes in hormone levels.   Lisa Halbert is circumspect about her own health issues as an adult and the PBBs that her father unwittingly imported to their dairy cow operation when she was a child. She\u2019s blonde and broad-shouldered, and she greeted me in knee-high rubber boots and a forest green Lorax shirt that she wore to the March for Science in Washington earlier this year. Now in her mid-40s, Halbert had taken a medical leave from her work as a dairy veterinarian to recover from hip replacement surgery and a hysterectomy.  \u201cI got spayed,\u201d she quipped.  Her memories of the PBB disaster remain vivid. When she learned as a child that her favorite cow, named Flopsy, was to be included in the state\u2019s quarantine-and-slaughter action, she says she and her sisters discussed ways to save her, including sneaking her into the woods and then   borrowing a truck to move her somewhere safe. But the oldest Halbert sister was a mere seven years old, and Flopsy was quickly collected and dispatched to Kalkaska.  \n\t\t\t\t\n\t\t\t\t\t\tA documentary film shot shortly after the PBB contamination was discovered captured the gruesome process by which thousands of cattle had to be destroyed.\n\t\t\t\t\t\t\t\t\t The family eventually removed the feeding station where their doomed cows ate the contaminated DR402. Today the parlor still stands, but is not used to collect any milk sold for human consumption. Lisa Halbert recalls the family bulldozed one of the barns used to house cows during the quarantine phase, but many of the other outbuildings still stand.  The elder Halbert, meanwhile, is now a terse, moon-faced man in his 70s. He lives about nine miles from the dairy, situated just north of Battle Creek, and a tour of the grounds today reveals no signs, overt or covert, of the contamination that forever altered the family. By comparison to these sorts of experiences,  Jim Hall and his family were far removed from the PBB drama as it was initially unfolding. And because he didn\u2019t live on a contaminated farm or work in the factory, epidemiologists at the Michigan Department of Community Health, which established the Michigan PBB registry in 1976 to gather and analyze data on exposed residents, didn\u2019t recruit Hall to be part of their study cohort.    Still, Hall\u2019s brother developed Hodgkin\u2019s lymphoma in the early 1980s, not far from a cluster of cancer cases around the town of Breckenridge, five miles east of St. Louis. In 2008, five years after the birth of his daughter \u2014 and three years after the family buried her \u2014 Hall found a lump in his throat while driving. When he went to the doctor complaining of fatigue and difficulty swallowing, he found out that precancerous nodules adorned his thyroid gland like ornaments on a Christmas tree. Doctors removed it. PBB is classified as an endocrine disruptor, meaning that it interferes with the body\u2019s array of natural hormones.  Initial data gathered from the PBB cohort study revealed other health problems among the hundreds of families and thousands of individuals recruited. Like other infamous toxicants such as bisphenol A and dioxin, PBB is classified as an endocrine disruptor, meaning that it interferes with the body\u2019s array of natural hormones. Growing concern about endocrine disruptors since the 1990s has spurred an avalanche of research linking these chemicals to thyroid problems, diabetes, obesity, fertility problems, changes in pubertal development, and hormone-sensitive cancers such as breast and prostate cancer. \u201cWe can detect a very large number of chemicals in essentially everybody\u2019s bodies,\u201d said Jonathan Chevrier, an environmental health scientist at McGill University. \u201cChemicals that we have reason to believe may interfere with hormones, for instance. That\u2019s complete news to most people.\u201d  Studies launched from the Michigan Department of Community Health\u2019s long-term monitoring of affected residents revealed a variety of findings. Some research has shown an elevated prevalence of liver, neurological, and immunological problems among Michigan farm families, but these issues have not been consistently linked to PBB blood levels. A 2011 study by Marcus and others at Emory University found that women exposed to PBB in utero were more likely to experience miscarriages. A few other early studies \u2014 all on small groups of children \u2014 suggested that those exposed to PBBs were more likely to have developmental delays. But for all this work, in 2011, the study hit a crisis point. The state had run out of funding and, with the disaster increasingly confined to the dusty archives of history, it seemed like little more could be learned.  Michele Marcus, though, disagreed. Trained as a reproductive and environmental epidemiologist, Marcus had worked alongside Irving Selikoff at the Mount Sinai School of Medicine in New York, years after he conducted some of the first investigations into the human health effects of PBB.  As a young professor at Emory University working with the CDC in the mid-1990s, she began investigating how PBB exposure continued to affect health after 20 years. As someone interested in how early life experiences shaped a person\u2019s long-term health, Marcus knew that the decades-long study provided an invaluable set of data on the long-term health impacts of a chemical exposure. She eventually took over the research and the Michigan PBB Registry now resides at the Rollins School of Public Health at Emory University.  \n\n Michele Marcus, an epidemiologist at Emory University, was convinced that there was much more to be learned about the long-term impacts of PBB exposure. Here, she discusses her research with community members in Michigan in 2014.  Visual: Great Lakes Echo\/CC 3.0   \u201cBecause I knew about the PBB incident, I thought: Wow, these people were exposed to a chemical we think is an endocrine disruptor. Maybe they might be experiencing these problems, and this would be a much stronger type of evidence, because we could measure individual exposure levels and look at the relationship to individual health outcomes. It\u2019s much stronger evidence than just looking at trends over time, because over time, lots of things are changing,\u201d Marcus said. Given her training, Marcus was also well aware of the sensitivity of developing organisms. In 1995, she proposed to study the health issues affecting daughters of women who had been exposed to PBB. But it wasn\u2019t until 2008 that her participation in a meeting in Washington, D.C. for the Institute of Medicine (now the National Academy of Medicine) introduced her to the idea that a father\u2019s exposure to PBB could have an effect on his children. No one had verified the idea in humans, but Marcus\u2019s PBB cohort, she was convinced, provided the perfect case study.  To understand why, it helps to cast back nine years before that 2008 meeting, to the spring of 1999. It was then that Michael Skinner, a reproductive biologist at Washington State University in Pullman, recalls one of his postdocs, Andrea Cupp, bursting through his office door. Skinner and Cupp had been working to understand the process of how a fetus became male or female, and injected pregnant rats with a variety of hormone mimics, including methoxychlor (a pesticide) and vinclozolin (a fungicide), to study how the process was disrupted. The resulting adult males had low sperm counts and reduced fertility, but these were not the type of groundbreaking results either scientist was looking for. Cupp\u2019s next task was to breed the males exposed to vinclozolin in the womb (the F0 generation) to check for effects in their offspring \u2014 rats that hadn\u2019t been directly exposed to the chemical.  \u201cThis changes how you think about toxicology,\u201d Skinner said. \u201cThe whole system changes.\u201d   As it would turn out, however, Cupp had accidentally bred the children of the F1 rats by mistake, creating what would be considered an F2 generation. Visibly upset, she had come to Skinner\u2019s office to explain the mix-up. He told her to analyze the rats anyway, figuring that the task would help take her mind off the error. Several weeks later, Cupp hustled into Skinner\u2019s lab again, this time brimming with excitement. The testes on the F2 rats, she had discovered, looked exactly like those on the F1.   \u201cOf course, I didn\u2019t believe her and I made her go back and repeat it 15 times,\u201d Skinner now recalls. But no matter how many times Cupp bred the rats, out to F4, 90 percent of the male offspring had reduced sperm counts and lowered sperm motility \u2014 even though they themselves had never been exposed to vinclozolin. \u201cThen I knew that we had stumbled on something that was important,\u201d he said.   Cupp and Skinner now had to figure out how this was happening. The high rate of testicular abnormalities ruled out direct DNA mutations, which were more random and happened at a lower frequency. That left the duo with a type of genetic change that decades of scientific dogma told them wasn\u2019t possible. British biologist Conrad Waddington first coined the term epigenetics after performing an experiment involving fruit flies and heat. A horizontal vein bisects the miniscule wings on the poppy seed-sized insect, controlled by a gene named crossveinless. Waddington made this vein disappear in newly bred generations of fruit flies by repeatedly heating the pupae. In breeding those flies that lacked the bisecting vein, he discovered that their offspring had the same trait, despite never being exposed to heat themselves. His analysis didn\u2019t reveal any mutations in the genetic sequence itself, so he called it an \u201cepigenetic\u201d phenomenon \u2014 a way of describing genetic changes occurring above and beyond the ordering of genes.  After the discovery of DNA\u2019s double helix, biologists had already shown that small chemical tags called methyl groups could act as epigenetic markers on the genome \u2014 punctuation marks, in a sense, on the cascading paragraphs of A-C-G-T that make up our genetic code. Another analogy used by researchers at Harvard Medical School: \u201cIf DNA is like a book, epigenetic marks are like sticky notes. Epigenetic marks tell our cells whether and how to read the genes.\u201d   Carrie Breton, an epigeneticist and environmental health scientist at the University of Southern California, adds the key insight that folks like Skinner and Cupp and Waddington were uncovering: \u201cThese marks are fluid, not static,\u201d she told me. \u201cIt\u2019s yet another layer of complexity on top of the genetic code.\u201d    That complexity came as something of a surprise. Work by developmental biologists in the 1980s had long suggested that newly fertilized embryos appeared to strip all the methyl groups off their genomes, essentially creating a blank slate from which to work. By this understanding, the multigenerational epigenetic inheritance observed by Cupp and Skinner was technically impossible. In the intervening quarter century, however, it had become more and more evident that an embryo\u2019s epigenetic slate wasn\u2019t nearly as blank as researchers thought.    Angelman syndrome, characterized by a small head, seizures, and developmental disabilities, for example, is caused by a deletion in chromosome 15 in the mother. The exact same mutation inherited from the father causes Prader-Willi syndrome, which causes an insatiable appetite. (Both genetic disorders can also be caused if a child inherits two copies of a section of the chromosome from the father and mother, respectively, rather than one from each.) Some type of marker had to exist that distinguished the parent from which the mutation arose. To date, scientists have discovered more than 100 different imprinted conditions in humans. This type of genomic imprinting provided the first indication that epigenetic markers could be inherited.    Skinner\u2019s study provided more conclusive evidence, and when he and Cupp finally published their work in Science in 2005, the study made an immediate splash. Skinner\u2019s phone rang off the hook with calls from reporters from around the world. Six months later, scientific and industry pushback began. Critics pointed out that Skinner injected the animals with vinclozolin rather than lacing their food. Since humans were exposed to the fungicide by ingestion or inhalation, not injection, the harmful effects to the rats may not mimic human issues. A postdoc found guilty of scientific misconduct in 2010 only fueled the fire.    Still, when other labs conducted related experiments with different chemicals, they found broadly similar results. \u201cI think it sank in that what I was telling them was going to affect lots of different things,\u201d Skinner said.    \u201cThis changes how you think about toxicology,\u201d he later added. \u201cThe whole system changes.\u201d  The work of biologist Michael Skinner has upended how we think about toxic exposures. It has also helped to shape Michele Marcus\u2019s investigation into Michigan\u2019s PBB legacy and its effects on current generations of Michiganders. Visual: Washington State University   Skinner began testing other environmental compounds, including DDT, bisphenol A, and dioxin (including Agent Orange), for their ability to induce multigenerational epigenetic changes in rats. In almost every case, he found them. Importantly, each chemical created its own unique epigenetic fingerprint on the rat\u2019s DNA, giving scientists the opportunity to potentially investigate what specific compound an individual was dosed with. Could the great-grandchildren of exposed populations manifest with PBB-related health issues even though they had never come in contact with the chemical?   His work in this area placed him in high demand at universities and scientific conferences. In 2008, he spoke at the Institute of Medicine meeting in Washington, D.C., where Michele Marcus first learned of his work. As part of a committee looking into the health effects of Agent Orange exposure among Vietnam veterans, Marcus attended as an expert in the long-term health effects of early life environmental exposures, and as Skinner talked about his research,  she immediately began thinking about the PBBs in Michigan.    By that time, Marcus had already tracked a series of health issues in the children of women exposed to PBB. In one study, she and her co-authors found that daughters who were breastfed and exposed to high levels of PBB in the womb started their first periods an average of one year earlier than their counterparts who were exposed to lower levels. They were much more likely to have difficulties carrying a pregnancy to term. Sons were more likely to have urogenital birth defects. Both genders showed a high prevalence of thyroid problems associated with PBB exposure. Overall, six in ten Michiganders in the 2000s had PBB blood levels that were higher than 95 percent of the non-exposed population.   But poor health outcomes in the offspring of Michiganders first exposed to PBBs \u2014 if those outcomes were related to the initial contamination at all \u2014 would have been the result of direct exposures, too. To show multigenerational effects, Marcus would need to find grandchildren of those exposed to PBB in the 1970s.   As soon as Marcus heard Skinner\u2019s talk on dioxin epigenetics in rats, however, she began to wonder whether epigenetics could also be playing a role in causing poor health outcomes down the generational lines of these Michigan families. Could the children and grandchildren and great-grandchildren of these exposed populations manifest with PBB-related health issues even though they themselves had never come in contact with the chemical?    Marcus teamed up with Alicia Smith, a geneticist at Emory University, to begin investigating whether PBB affected an individual\u2019s epigenetic regulation, and whether those changes could be delivered across generations. After repeatedly being asked in community meetings about the effects a father\u2019s exposure could have on his offspring, she and Smith secured a grant from the NIH and headed up to Michigan to start collecting fresh blood samples. Marcus wasn\u2019t the only one paying close attention to Skinner\u2019s work in those years. Hearing him speak at an earlier National Academy of Sciences workshop, bioethicists Mark Rothstein of the University of Louisville and Gary Marchant of Arizona State University both say they immediately grasped the immense implications of this line of research. Rothstein recalled asking an industry representative in attendance about the potential legal impacts of epigenetic inheritance, and whether chemical manufacturers could one day be held liable for impacts on individuals removed from exposures not just by years, but by whole generations.    \u201cAll the blood seemed to drain from his face when I asked that question,\u201d Rothstein said, \u201cbecause now I\u2019m suggesting that there\u2019s kind of unlimited liability for manufacturers to as yet unborn generations.\u201d A 2009 paper co-authored by bioethicists Mark Rothstein and Gary Marchant raised numerous pressing questions with regard to transgenerational epigenetic transfer. But the  question that loomed over everything was how this issue would be handled by the courts. \n   After the meeting, Marchant and Rothstein delved through the scientific literature to find other writing on the bioethics of epigenetics. When they couldn\u2019t find anything, they decided to write one of their own. Their resulting paper, co-authored with Marchant\u2019s student Yu Cai and published in Health Matrix: Journal of Law-Medicine in the winter of 2009, proposed more questions than it answered. Were epigenetic test results protected under the Genetic Information Nondiscrimination Act? (Probably not). How did multigenerational epigenetic inheritance affect the environmental justice movement? How could scientists use these findings without stepping over the line into eugenics?    The question that loomed over everything, however, was how this issue would be handled by the courts. To try and answer that issue, legal experts have often turned to another toxic chemical that seemed to produce multigenerational effects that, while not epigenetic in nature, might provide a model for tracing liability in epigenetic cases.  \u201cIt\u2019s clear that the fetus doesn\u2019t react like a mature adult. The growing embryo and fetus are sensitive to things that sometimes we don\u2019t know about.\u201d   In 1966, pathologist Robert Scully asked 35-year-old Arthur Herbst, then a gynecological oncologist at Massachusetts General Hospital, about some bizarre tumors he had seen. A handful of girls, ranging in age from 15 to 22, had been diagnosed with a rare type of vaginal cancer called clear-cell adenocarcinoma that had, until this point, only been identified in post-menopausal women at MGH. Scully, Herbst, and some of their colleagues assembled these cases into a formal study to figure out what was going on. The answer came not from some dusty medical journal but rather a question from one of the girls\u2019 mothers. She asked Herbst if the cancer could be associated with the DES she took while pregnant.    DES is short for diethylstilbestrol, a synthetic form of estrogen first developed by British chemists in 1938. Three years later, the FDA approved it for the treatment of \u201cestrogen-deficient conditions\u201d in both humans and livestock. After a small animal study hinted that low estrogen might cause miscarriages, obstetricians began prescribing it to their female patients with high-risk pregnancies. Marketing campaigns by manufacturers like Wyeth and Eli Lilly and Company touted DES as a new wonder drug, and women with normal, healthy pregnancies soon began taking it. A 1953 study seemed to suggest that DES didn\u2019t prevent miscarriage, but it seemed like such a benign drug that doctors continued to prescribe it and women continued taking it into the 1970s.   Many of the studies that look at DES ask daughters if their mother took the drug. \u201cHalf the time the mom didn\u2019t even know what she was given,\u201d said Linda Titus, a cancer researcher at Dartmouth College who is heading up much of the research. \u201cThat was back in an era, roughly 1940 to 1970, when women didn\u2019t ask too many questions.\u201d   When Herbst investigated the mother\u2019s question, he found that a small group of daughters of women who took DES while pregnant were astronomically more likely to develop clear-cell adenocarcinoma. The resulting paper, published on April 22, 1971 in the New England Journal of Medicine with the bland title \u201cAdenocarcinoma of the Vagina,\u201d became a landmark in the scientific literature.     \u201cThis is the first time a drug given to the mother was associated with the subsequent development of the malignancy in her offspring, in humans,\u201d said Herbst, now in his mid-80s, who continues his work on DES at the University of Chicago. \u201cIt\u2019s clear that the fetus doesn\u2019t react like a mature adult. The growing embryo and fetus are sensitive to things that sometimes we don\u2019t know about.\u201d   Further studies revealed an increased risk of urogenital abnormalities in both the sons and daughters of women who took DES, as well as an increased risk of breast cancer in the DES mothers. To win a legal judgement, however, the plaintiffs needed to prove that DES caused their health problems. Apart from clear-cell adenocarcinoma, many of the harms reported by DES mothers and their children have a range of causes.    Epidemiological studies, according to Titus, have been able to show a strong association between prenatal exposure to DES and certain negative health outcomes. This work allows researchers to infer causation, but it generally cannot prove the exact cause of a disease in a particular individual. Titus points out that because some DES-related conditions are so rare and so strongly associated with prenatal exposure to the drug, in some cases epidemiologists can be \u201creasonably confident\u201d of the link. To hold up from a legal standpoint, the courts ruled that epidemiological studies must show that exposure to a toxin more than doubles the risk of the resulting health problem \u2014 and that\u2019s no easy task.   There are few \u201crelative risks higher than 1.5 in many disease areas,\u201d Titus said.  The success of DES mothers and daughters in winning lawsuits, along with investigations into possible harms to grandchildren, spurred interest in lawsuits by the third generation.   DES mothers and children who wanted to sue the manufacturers for damages faced other legal hurdles. Many times, several decades had elapsed between when the mothers took DES and when they filed their claims. As well, many companies sold DES between 1941 and 1971, and women rarely knew who manufactured the specific pills they took. In the 1980 case Sindell v. Abbott Labs, the California courts settled this issue by holding DES manufacturers liable for their proportion of the market share of the drug. If Company A made 40 percent of the DES sold in a particular area, they would pay 40 percent of the judgement awarded.    The success of DES mothers and daughters in winning tens of millions of dollars in class-action lawsuits, along with an opening of investigations into the possible harm to DES grandchildren, spurred interest in lawsuits by the third generation. In 1991, lawyers for nine-year-old Karen Enright, who was born prematurely and subsequently developed cerebral palsy, brought a case against DES manufacturer Eli Lilly. The grandmother took DES in 1959 while she was pregnant with her daughter, born in 1960.    Karen herself was born in 1981, and the suit alleged that the daughter\u2019s reproductive tract abnormalities caused the granddaughter\u2019s condition. In a six-to-one ruling in 1991, the New York Court of Appeals rejected the suit on philosophical grounds. Writing the majority opinion, Chief Judge Sol Wachtler noted that, \u201cFor all we know, the rippling effects of DES exposure may extend for generations. It is our duty to confine liability within manageable limits. Limiting liability to those who ingested the drug or who were exposed to it in utero serves this purpose.\u201d  \n\n \u25b2 Entrance to the EPA Superfund Cleanup Site at the former Velsicol Chemical Plant in St. Louis, Michigan. The bench declares the town\u2019s commitment \u201cthat our river and land be restored to their natural condition, safe for any use.\u201d Visual: Jeffrey Sauger for Undark Michigan Chemical Corporation, which was purchased by Velsicol, produced numerous chemical compounds during its operation from 1936 to 1978, including PBB and the pesticide DDT.  Visual: Jeffrey Sauger for Undark Penny Park is just across the Pine River from the site of the former Velsicol plant. The white cement cylinders on the far shore are helping to draw toxic substances still leeching from the site. Visual: Jeffrey Sauger for Undark Michigan Chemical Corporation, which was purchased by Velsicol, produced numerous chemical compounds during its operation from 1936 to 1978, including PBB and the pesticide DDT.  Penny Park is just across the Pine River from the site of the former Velsicol plant. The white cement cylinders on the far shore are helping to draw toxic substances still leeching from the site. Visuals: Jeffrey Sauger for Undark   The idea of proximate causation is implicit in the Enright ruling, according to Steve Gold, a professor of environmental law at of Rutgers Law School. Tort law was developed to address typically immediate harms from actions, dealing well with cases such as a broken arm from playground equipment or severe burns from hot coffee. Cause occurs immediately before effect. Many environmental harms, even those that affect those directly exposed, often take decades to show up. Injury to the children and grandchildren of these individuals can be too far removed for courts to agree that the manufacturer should be liable. Multigenerational suits \u201coffend many people\u2019s sense of justice. It\u2019s valuable [to the courts] to make liability finite,\u201d Gold said.   Third-generation DES lawsuits aren\u2019t based on epigenetics, but the courts are likely to apply similar principles to plaintiffs in any multigenerational epigenetics cases. \u201cThere\u2019s no fundamental legal issue that would interfere with a lawsuit,\u201d Gold said. \u201c[But] by the time you\u2019re three generations on, finding proof that an exposure caused harm is going to be difficult to find or show.\u201d   Still, Texas-based attorney Andrew Lipton points out that the revolution in the scientific, popular, and legal understanding of genetics could alter a judge\u2019s willingness to hear a case based on epigenetic evidence. A solid case depends on demonstrating evidence of exposure both via historic records and via every chemical\u2019s unique epigenetic fingerprint. Epidemiologists also need to provide proof that this fingerprint isn\u2019t caused by other environmental exposures and that it leads to harm.   Lipton believes that lawsuits based on multi-generational epigenetic evidence are coming down the road. \u201cThe problem that you\u2019re going to keep running into, though, is finding statistical significance for second generation or third generation injuries, and linking it back to a particular genetic or epigenetic defect,\u201d he said. \u201cScientists have only been able to show multigenerational epigenetic inheritance in animal models, never in humans. But that isn\u2019t to say that it can\u2019t or won\u2019t be done.\u201d   For the Michigan victims of PBB, it\u2019s something of a moot point. In the 1960s, the multinational corporation Velsicol purchased a controlling interest in Michigan Chemical before dissolving the company after the PBB disaster. The plant was eventually torn down, and in a bargain struck with state officials and the Environmental Protection Agency, Velsicol was permitted to escape blame for any human damages outside the plant in exchange for  paying the state of Michigan $38.5 million to clean up and construct a concrete cap over the site in St. Louis. To the best of his knowledge, Carl Cranor, a bioethicist at the University of California, Riverside, said no one has yet won a lawsuit on the grounds of multigenerational epigenetic harm, nor is the science yet ready for such a suit. In a legal case that may well define epigentic tort law, a New York district court judge ruled that \u201cIt is our duty to confine liability within manageable limit. Limiting liability to those who ingested the drug or who were exposed to it in utero serves this purpose.\u201d   Taking regulatory action may prove no easier, despite Wachtler\u2019s opinion in the Enright case that it was the FDA\u2019s role to promote safe medications, not the court\u2019s. Although contaminants like PBB fall under the purview of the EPA, the principle remains the same. The problem, according to Mustafa Ali, former senior advisor of environmental justice and community revitalization at the EPA, is that testing chemicals even for acute exposures in single generations is expensive, often costing upwards of $330,000 per compound. And the anti-science, anti-regulatory climate in Washington makes it profoundly unlikely that multigenerational toxicological testing will begin as long as the current administration is in power.   \u201cWhen you\u2019re trying to develop policy, you need to have strong science in place. Many of the chemicals haven\u2019t been evaluated for that level yet. That\u2019s where one of my great concerns comes in with the cutting of budgets and sort of taking a step back from this needed science,\u201d Ali said.   Still, many experts believe it is only a matter of time before human epigenetic inheritance moves, legitimately, from the murky realm of speculative and uncertain science to something more concrete and culturally \u2014 and perhaps even legally \u2014speaking, more consequential. In that sense, should the work of Marcus and other researchers ultimately yield fruit, it can seem impossible to overstate the potential impacts. The early morning of December 14, 2013 revealed a snowscape in St. Louis, Michigan. Temperatures hovered right around freezing at the town\u2019s four Superfund sites, including the site of the former Michigan Chemical Corporation plant. Murray Borrello and Ed Lorenz, both professors at nearby Alma College were assisting Marcus with a blood draw event at the St. Louis Town Hall. They knew her team was counting on a large turnout to get the samples she needed for her coming years of research. Bad weather was the last thing they needed. By 8 a.m., two hours before the blood draws were scheduled to begin, they realized they had the opposite problem \u2014 a line of people wrapped around the building, shivering and stamping their feet to stay warm.   The team ran out of chairs for everyone and by noon, they had run out of needles and Vacutainer tubes for blood. Marcus\u2019s assistants raided local hospitals and health departments for supplies, but even with their scavenging, the Emory team had to turn people away. Nearly 200 people showed up to provide samples and fill out questionnaires. Though he wasn\u2019t part of the initial Michigan Department of Community Health study, Jim Hall was among them \u2014 in part because Marcus wanted to cast a wider net, a hunch that paid off. When Hall\u2019s results arrived in the mail several months later, he learned his body still contained massive amounts of PBB \u2014 5.5 parts per billion, 16 times more than many farm families in the 1970s \u2014 and fathers like Hall were precisely the individuals Marcus needed for her study. \n\n \u25b2 Jim Hall\u2019s deceased toddler is memorialized on his truck. His nagging question: Can his exposure to PBB as a child be linked to Jerra\u2019s illness and death?  Visual: Jeffrey Sauger for Undark Because a developing fetus can experience direct exposure to chemicals carried in a mother\u2019s body, the key to proving transgenerational epigenetic inheritance in humans lay with fathers like Jim, whose PBB levels are high, while his wife\u2019s are nonexistent.  Visual: Jeffrey Sauger for Undark   Because PBB persists in the body\u2019s fat stores potentially indefinitely, children of exposed mothers had direct exposure to PBB in the womb and during breastfeeding. In daughters, egg cells destined to become grandchildren form during fetal development, giving even maternal grandchildren a direct PBB exposure. To prove transgenerational epigenetic inheritance, Marcus would have to wait until the arrival of the great-grandchildren of exposed mothers, the equivalent of Skinner\u2019s F2 rats.    Fathers, however, only provided their DNA. If, like Jim\u2019s wife Ida, the mother was not exposed to PBB (Ida grew up in Middleton, Michigan, 20 miles from the PBB mix-up but has no PBB in her blood), Marcus and Smith could start seeing effects in the grandchildren of these fathers.   \u201cWe need three generations, and the first generation has to have an unexposed mom, so that we know that we know that the second generation was not directly exposed in the womb and that they only get the exposure information that\u2019s contained in the father\u2019s epigenome,\u201d Marcus said.    \u201cSince the exposure was in the early 1970s,\u201d she continued, \u201cwe know quite a number of families that have three generations, although, it\u2019s going to be difficult to identify families that have that exposure pattern.\u201d   Marcus and Smith are trying to find 20 to 25 of these families, and hope to start a small pilot study by 2019. The hardest part has been finding unexposed mothers, since nearly 90 percent of Michiganders were thought to have been exposed to PBB in the 1970s, and upwards of 80 percent of them still have elevated blood levels of PBB. As Hall and others see it, the one thing they could give their children \u2014 a healthy start in life \u2014 was taken from them by a chemical exposure nearly half a century before.   Diving more deeply into PBB\u2019s toxic legacy, including finding evidence that could provide conclusive proof about multigenerational epigenetic inheritance, has hit a major roadblock. In the years before the Michigan Department of Community Health handed over control of the study to Marcus, they transferred the paper and electronic study files to the Michigan Public Health Institute (MPHI) for digitization and storage. When individuals signed consent forms to have their information transferred to Emory, MPHI was unable to locate many of them due to missing data. For once, Marcus\u2019s broad smile and easygoing manner slipped from view as she discussed the issue, though she now says that the Michigan Department of Public Health is committed to searching for the additional historic data. Without it, her studies won\u2019t have the critical number of people needed to show the potential multi-generational effects of PBB.   Community members are even more frustrated at the delays from MPHI, which have been hindering research that could finally answer health questions that have loomed over them for nearly half a century. \u201cI think it\u2019s that they don\u2019t care,\u201d Lorenz said bitterly.   Perhaps so, but John Greally, a geneticist at the Albert Einstein College of Medicine in New York suggests that no matter the data and no matter the cohort, proving epigenetic inheritance in humans will be a tall order. It doesn\u2019t help, he adds, that epigenetics has become a scientific buzzword that has different meanings to different people. \u201cWe rarely clarify what we mean when we use the word,\u201d he said in an email.   To make her case, Marcus will not only have to show changes in regulation to specific genes caused only by exposure to PBB, her team will ultimately have to outline a mechanism for how those changes lead to disease. The list of variables that can cause changes to DNA methylation seems endless \u2014 different cell types have different patterns of regulation, and natural variations in the genetic code can also affect DNA methylation patterns. Cross-sectional studies that use individuals already diagnosed with a specific condition can\u2019t determine whether epigenetic changes caused the disease or are the result of the disease \u2014 although researchers are actively exploring ways to do this. Peel back one layer of complexity, Greally said, and you find another \u2014 Russian nesting dolls made of As, Ts, Cs, and Gs.   \u201cTo my knowledge, scientists have only been able to show multigenerational epigenetic inheritance in animal models, never in humans,\u201d he said. \u201cBut that isn\u2019t to say that it can\u2019t or won\u2019t be done.\u201d     In the meantime, families in St. Louis, Michigan struggle to leave because their homes are so close to the old Michigan Chemical plant. Jim Hall can\u2019t get ahead because his thyroid problems and his daughter\u2019s illness drove him into severe debt. As Hall and others see it, the one thing they could give their children \u2014 a healthy start in life \u2014 was taken from them by a chemical exposure nearly half a century before. No lawsuit or regulation will return these stolen inheritances, but Hall \u2014 and Marcus \u2014 both said they hope that the research now underway will one day mean that fewer communities are poisoned, and perhaps that fewer genomes will march forward into the future scarred by the exposures of generations past.   \u201cThis is our home,\u201d Hall said. \u201cThere\u2019s no reason this should have happened, and I don\u2019t know if they\u2019ll ever be able to really clean it up.\u201d CORRECTION: An earlier version of this piece incorrectly identified one of the organizations that analyzed cattle feed for Frederic Halbert. It was the Wisconsin Alumni Research Foundation (WARF), not the Wisconsin Animal Research Foundation. Carrie Arnold is a freelance science writer from Virginia. She covers all aspects of the living world and has written for a variety of publications including Mosaic, Aeon, Scientific American, Discover, National Geographic, and Women\u2019s Health.  Carrie Arnold, Curiosities, epigenetic transfer, epigenetics, Michele Marcus, Michigan, Michigan Chemical Corporation, PBB, polybrominated biphenyl, Velsicol \n\t\t\t17 comments \/ Join the Discussion  Hi Carrie, I\u2019m a professor at Central Michigan University, working with community members and research partners to develop an oral history project and identify research collections related to this episode of contamination. I wondered if you would be available and\/or willing to chat?  Many thanks, Brittany Fremion It is important to not trade one problem for another. Chemical exposure has its dangers.  So does starvation. And starvation is more immediate. ROUND UP  \u2013  Glyphosate the active ingredient in Monsanto\u2019s herbicide. Hundreds of tests and independent, reliable studies have been conducted that expose Glyphosate as a major threat to human health, but since the FDA and EPA rely solely on safety tests done by the manufacturer, no regulations are put in place to protect humans, animals or the environment from this known cancer-causing weed killer.  We guinea pigs consume ROUNF-UP thru Corn, soy, cotton, sugar, wheat, and other grains.  It is unavoidable unless you grow your own food. We guinea pigs ingest an array of other pesticides and insecticides applied to fruits, veggies, berries, nuts and when we are ripe, the drug companies step in to help us with our pains and diseases.   The fight on cancer is all backwards,  the fight should be PREVENTION of cancer.  If we tread lightly upon this planet and return the ecological system back to its natural healthy status and eliminate the toxins and chemicals we humans have created, perhaps we could enjoy good health too.   It is clear a negative feedback loop is currently underway, the mental and physical health of our youth is and will continue to be severely compromised by our current farming practices of lacing poisons and toxins on our food ,  water,  air\u2026   I recall driving thru Green Bay Wi, in the front yard was a young dad spraying dandelions with Round up and in the other hand he was carrying a infant boy.   Why wait til the kid is 10 or 11 yrs old, teach them young, the result is the same.  We are willing participants in an extinction level event, the cause of which is not a great volcano , asteroid or alien or tidal wave, its simply us in Ludicrous mode.    WHAT CAN YOU DO..  Ban all pesticides, herbicides, insecticides in your local communities.  Educate your children about these deadly toxic mixes on all your food.    Go to your local community and ban application in you village and towns. Very iportant and well-done article. Hmm. Have to prove in great detsil exactly why and how the defects ocurred. The law doesn\u2019t even require that any drug mfr show exactly how and why a drug works. Remarkable things here. I am very happy to look your post.\nThanks a lot and I am looking ahead to contact you. Will you please drop me a mail? Great article that should be viral.  Just as the SC resident posted, several people that were exposed moved to other states and are suffering from the generational chemical exposure.  I believe this issue is far larger tban to One state.  Did anyone trace all the distribution locations prior to the chemical exposure being disclosed?  How many millions were exposed consuming the products prior to exposing the problem and once it was discovered?  I don\u2019t see where it has been documented the locations that beef, eggs , milk, cheese, etc.. landed while being contaminated before and after discovery.  Additionally, killing and burying in the ground, doesn\u2019t this contaminate the soil?  Just a few questions I stumbled upon while reading the article.   Thanks for publishing very informative as well as, frightening for those of us 40yrs later. I was 16 in 1973.  My family lived 7 miles from St. Louis.  If you are looking for subjects, some of us might be appropriate.  There were 8 of us (3 boys and 5 girls) who were (in 1973) between 10 and 20 years old. This article is very well written. Thank you to the author and to the scientists and researchers who are involved in discovering and publicizing this important information.  I am a lifetime Michigander and would be interested in participating in the study. Born in 1961, near Detroit. I have three male offspring. No grandchildren, yet.   Please let me know any info on whether there is a need for additional study participants. Makes me sick at heart. Read the entire article and believe it. My grandson was born with a hole in his heart. There is still a small leak and he is also extremely anxious, depressed has other health issues. We grew up in this area mentioned, ate the eggs, meat, milk etc. I have SLE which I have always maintained is environmentally caused. My daughter and son have Reynaud\u2019s. Now we live in SC which has been the dumping site for nuclear waste for years. Why? How stupid and greedy are people? And there are people in Michigan who vote for the gop. They stand for allowing industry full and complete rights to pollute at will. Some folks are too dumb to tie their shoes. The gop stands for the murder of kids by denying them health care including vaccines. With no herd immunity horrible deadly epidemics are in our future but the gop lovers don\u2019t believe in science so they don\u2019t comprehend the risk. The gop stands for starving thru taking their SS and murdering the elderly by denying them Medicare. But the gop voters are happy to see their parents and grandparents die early so the corporations can save billions on pension costs. Much of Michigan supports the gop with religious fury. Of course many of their religions love the bigoted hate filled woman hating control freak biblical horseshit. This religious myth is too insulate them from their own fear of death, but this does not extend to any others. They are happy to see everyone but the the fetus in the womb murdered with impunity. The children out of the womb can be merrily murdered by the gop and many people in Michigan will still vote for their gop, like an unchangable bad habit without thought or consideration. I read how these researchers are investigating clues about chemical affects, but I am a human \u201cguinea pig\u201d suffering from intense chemical sensitivities. As soon as I smell these chemicals I suffer reactions. I cannot sleep on conventional mattresses or use laptops. These make me ill, immediately.  We do not realize that we daily inhale a barrage of chemicals which Immediately enter our blood, interrupting immunity and hormonal balance. Everyone is suffering from the toxic affects of their many fragrances, formaldehyde in furniture, flooring, cardboard, clothing. But no one perceives it, so we beseech our doctor for a cure, which ensues in imbibing prescribed substances usually unnatural to our bodies. Our bodies just need us to get back to what is natural. In my reading about epigenetics, I learned about a study, I believe in Denmark or Norway, decades ago that for any woman who had been malnourished in childhood (a nation-wide problem at the time) any of her male children were affected genetically, while any daughters where not although their daughters were. That would be the Dutch \u201cHunger Winter\u201d at the end of World War II, which should be enough information to search for much more about it if you like. It is often presented as a case of epigenetic inheritance in humans. WARF is the acronym for Wisconsin Alumni Research Foundation. NOT Wisconsin Animal Research Foundation. Warfarin is one of the products from this research entity at my alma mater. Thanks for pointing out this error, Beverly. We\u2019ve updated the story. You people need to talk to John Berger owner of NWFF in Philomath Oregon. He was a Firefighter that was exposed to some chemical while fighting a fire in California. California fire fighting law was changed due to his case. After he was exposed he would become drunk after drinking orange juice and that was one of the more mundane changes. He has children and they have obvious genetic damage. At this point we are essentially concentrating poisons in the environment. Pollution is the greatest killer at this point and we have affected our genetics probably permanently because of it. More practical mechanism search involves anti-depressants.\nSee Dentate Gyrus and BDNF.\nDifficult\u2013\nHolocaust work remains controversial, involving Fkbp5.\nNew protocols are emerging. Your email address will not be published. Required fields are marked *  Name *  Email *  Website   \n\n   Previous Article Next Article","time":1525805046,"title":"Epigenetics and the Poisoning of Michigan","type":"story","url":"https:\/\/undark.org\/article\/pbb-michigan-epigenetics\/","label":7,"label_name":"random"},{"by":"rajs123","descendants":42,"id":17023551,"kids":"[17024072, 17024342, 17023821, 17024623, 17024035, 17024106, 17023840, 17024333, 17024438, 17023999, 17024016, 17023924, 17024025, 17024073, 17024569]","score":43,"text":"Google  will soon launch a new version of Google Maps that will give you more personalized recommendations than before. Google has long worked to make Maps seem more personalized, but since Maps is now about far more than just directions, the company is introducing new features to give you better recommendations for local places. \u201cToday, our users aren\u2019t just asking for the fastest route to a place but also what\u2019s happening around them, what the new places are and what the locals are doing in their neighborhood,\u201d Google VP for engineering and product management Jen Fitzpatrick noted in today\u2019s keynote.  The first new feature to enable this is the \u2018for you\u2019 tab. This new part of Google Maps will learn from your personal preferences and tell you about what\u2019s new in your neighborhood (or other neighborhoods you are watching). Maybe there\u2019s a new cafe or restaurant. Over the course of the last few years, reviews in Google Maps have also become increasingly important. But what does a four-star review really mean? So going forward, Google Maps will take those reviews and mash them up with what it knows about you to give you a more personalized score based on your context and interests. Another \u2014 not AI-related \u2014 feature Google is adding to Maps is a new Group Planning feature that\u2019ll allow you to long press on a place and then add them to a shareable list.  ","time":1525805041,"title":"Google Maps will soon give you better recommendations","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/google-maps-will-soon-give-you-better-recommendations\/","label":9,"label_name":"tech"},{"by":"DaveYVR","descendants":0,"id":17023544,"kids":"None","score":1,"text":"by\n          \n            Biniam Admikew\n May 8, 2018\n          \/\n            \n            \n              \n                Community, \n              \n            \n              \n                LoopBack\n In April, our team was focused on delivering \u201cDeveloper Preview 2\u201d and conducting spikes which help us shape features in LoopBack 4 and create backlog tasks for our next release. Getting Developer Preview 2 successfully out the door took up our first half of the month. For the second half of April, we took up a number of stretch goals which mainly comprised of spikes, but also had time to work on PRs which improved our build, tests, development processes, and docs. Check out May Milestone if you\u2019d like to know what\u2019s on the horizon for May. If you\u2019d like to know details of our accomplishments in April, continue reading below. As part of our DP2 release, we worked on adding test infrastructure and unit tests to our recent openapi-v3-types package. This allows us to make sure our type guard functions worked as intended and so that future changes to the package won\u2019t change the behaviour unintentionally. Check out the PR for more details. A major factors driving our design for LoopBack 4 is user\/developer experience. One of the tasks for DP2 was to make RepositoryMixin easier to use by providing a getter function for Repository instances registered to the application. See what has changed with an example code snippet below: before: after: Another loosely related feature we worked on was the ability to let users pass in Repository class constructors to the @repository decorator on top of using the Repository name. This allows for changes to be made easily after refactoring repositories in your application. See our docs on configuring controllers with repositories to learn how to achieve this now. One of the challenges of a growing codebase is having a standard to follow for file names. We had decided to adopt Angular\u2019s file naming convention, so in April, we went through our monorepo and other related LoopBack 4 repositores to enforce this naming convention. This makes the development process for us and our community contributors smoother. Check out our docs to learn more about our file naming convention. In April, we continued to look at a new way of defining relations among models in LoopBack 4 in the spike that started in March. There\u2019s still a lot of conversation around it, but it is shaping up to show how we intend to define and configure relations in LoopBack 4, starting with the hasMany relation. The spike took longer than expected because we are also trying to lay down the ground work neccessary to easily add other relation types. You can track the progress and discussion in its PR and Issue. Stay tuned for a blog post about the findings and further work on concrete implementation for our supported relations in LoopBack. On top of our main planned items in April, we focused on spikes which help us investigate implementation for features we\u2019ve prioritized for future releases. Read on to find out what we\u2019ve been up to and to see what lies ahead. This spike aimed to figure out how we could integrate service oriented backends like REST APIs and SOAP Web Services with LoopBack 4 controllers. We had a PoC which explored creating common interfaces around our service oriented connectors such as loopback-connector-rest and loopback-connector-soap and expose their DataAccessObject (DAO) methods to controller methods via a @serviceProxy decorator which can be used with constructor parameters or properties to inject a service proxy for the associated service. The PoC for the spike has given life to a new package @loopback\/service-proxy. Check it out and stay tuned for more work on this area. You can find out about the tasks that have come out of it from the issue for it. Based on a spike done to rework our @loopback\/rest package internals to support pluggable HTTP transports like Express and Koa and more, we needed to figure out which framework we should choose to use with LoopBack 4. Thus, we worked on a spike which explores the integration between those frameworks and LoopBack 4 as well as the benefits and drawbacks of using one over the other. After some investigation, we decided to pursue using Express as our low level HTTP transport provider for the time being. One of the main reasons for that is the fact that Express has more maturity, adoption, and a rich middleware ecosystem. It also offers us flexibility to work with Express-compatible frameworks like fastify. You can find out more information and discussion around this topic in its issue. Stay tuned for work around integration with Express and use of its capabilities like compression and static file serving in LoopBack 4! JSON files make it easy to configure models, datasources, and so on manually or using tools such as the LoopBack CLI. This spike explored creating a Class in memory from a JSON file as well as generating a Base Class based on the JSON file. The spike is still under way to test some complete end-to-end user flows so the team can pick the best way forward. Stay tuned for more details on this task in May. With the current implementation of routing in @loopback\/rest, no work is done to coerce string inputs from HTTP requests into their correct JavaScript type representation. From the work of validation\/coercion spike, which explored ideas to address this elegantly, we\u2019ve decided on building a type conversion system for LoopBack; this will allow us to reuse or leverage the same logic used to convert between the HTTP layer and the JavaScript layer in other layers such as databases. We\u2019ve also decided on having simple data validation done with AJV via constraints given in an OpenAPI Schema Object. Asynchronous validation at the database level is also planned to be supported through the usage of decorators. As an open source project, LoopBack depends on its community members to thrive and flourish. We\u2019re grateful to have an awesome community where members help us and themselves by answering questions and opening pull requests. In the month of\nApril, we\u2019ve seen some great community participation for LoopBack 4. We\u2019d like to say thank you for all our contributers who have taken an interest in the project and the time to fix issues or raise concerns. Shout outs to @AliMirlou, @cajoy, @cblazo, @joeytwiddle, @jwooning, @thinusn, and\n@Vandivier for your contributions to LoopBack 4 in April! If you\u2019d like to get involved, check out our Call for Action section below. On top of our planned goals and stretch goals, we\u2019ve managed to land numerous PRs which have, among other things, made us compatible with Node.js v10, improved our developer experience, and slimmed down our dependencies. Check out the list below for more details: LoopBack\u2019s future success counts on you. We appreciate your continuous support and engagement to make LoopBack even better and meaningful for your API creation experience. Please join us and help the project by:","time":1525805010,"title":"LoopBack 4 April 2018 Milestone Update","type":"story","url":"https:\/\/strongloop.com\/strongblog\/april-2018-milestone\/","label":3,"label_name":"dev"},{"by":"amluto","descendants":0,"id":17023543,"kids":"None","score":1,"text":"Powered by blists - more mailing lists\n \n \nPlease check out the\n\nOpen Source Software Security Wiki, which is counterpart to this\nmailing list.\n \n\n\n\n\nvar addthis_config =\n{ data_use_flash: false, data_use_cookies: false, data_track_clickback: true }\n\n\n","time":1525804995,"title":"CVE-2018-1087: KVM Incorrectly Handles #DB Exceptions Deferred by MOV SS\/POP SS","type":"story","url":"http:\/\/www.openwall.com\/lists\/oss-security\/2018\/05\/08\/5","label":7,"label_name":"random"},{"by":"ngoldbaum","descendants":0,"id":17023541,"kids":"None","score":1,"text":"Dear Friends, Community Members, and Colleagues, Today, I am writing to all of you to announce the launch of both a Community Edition (CE) and a subscription-based Enterprise Support Edition (ESE) for HDF5. This model is similar to Red Hat, Lustre, and other open source projects. We are moving down this pathway to address the challenges that continuously face us in achieving sustainability and increasing community involvement. Since I joined The HDF Group in April 2016, I made it my mission as the CEO to pursue a strategy that not only enables us to survive but thrive as an organization. I truly believe this new model will take us into the future and keep HDF technologies relevant for another 30 years. As this is a major announcement and change to the community, I want to personally explain why we\u2019re doing this and how this might impact your use of HDF5: We believe this new approach will ensure stable code while stimulating community-driven innovation and direct participation. The ESE subscriptions will provide The HDF Group the sustained funding that will allow us to better support our mission and the community that depends on HDF technologies every day. You can learn more about Community Edition and Enterprise Support Edition on our website. We welcome you to join the conversation on our forum. Over the coming weeks, we will be sharing more details of the coming launch to make sure we involve our community each step of the way. Sincerely, David Pearah, CEO\nThe HDF Group You must be logged in to post a comment. The HDF Group provides a unique suite of technologies and supporting services that make possible the management of extremely large and complex data collections. \u00a9 The HDF Group, 2006-2018 Privacy Policy | Terms of Service \r\n                                [ Placeholder content for popup link ]\r\n                                WordPress Download Manager - Best Download Management Plugin\n","time":1525804990,"title":"Announcing the Launch of the Enterprise Support Edition of HDF5","type":"story","url":"https:\/\/www.hdfgroup.org\/2018\/05\/announcing-development-of-the-enterprise-support-edition-of-hdf5\/","label":7,"label_name":"random"},{"by":"amluto","descendants":0,"id":17023531,"kids":"None","score":1,"text":"Powered by blists - more mailing lists\n \n \nPlease check out the\n\nOpen Source Software Security Wiki, which is counterpart to this\nmailing list.\n \n\n\n\n\nvar addthis_config =\n{ data_use_flash: false, data_use_cookies: false, data_track_clickback: true }\n\n\n","time":1525804946,"title":"CVE-2018-8897: #DB exceptions [due to] MOV SS may cause unexpected behavior","type":"story","url":"http:\/\/www.openwall.com\/lists\/oss-security\/2018\/05\/08\/4","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17023513,"kids":"None","score":2,"text":"\n                            TNW uses cookies to personalize content and ads to\n                            make our site easier for you to use.\n                            We do also share that information with third parties for\n                            advertising & analytics.\n                         \n            by Matthew Hughes\n            \u2014 \n                        in Google\n Google News is one of Alphabet\u2019s oldest and most instantly recognizable products. It was built shortly after the 9\/11 attacks in New York by a Google engineer as a 20-percent project, in an attempt to better understand the terrorist atrocity that struck the US. It\u2019s also arguably one of its most important. Millions head to Google News in order to learn about current events, and to see what\u2019s happening in their community. Without thinking about it, Google has created journalism\u2019s biggest gatekeeper. With that in mind, Mountain View has thought about how Google News can be improved, and has made some pretty drastic (but welcome) adjustments. Google News now uses AI (along with the vast quantities of behavioral and search data it owns on us all) to show us more relevant content. When you open the Google News app, the first five stories in your briefing will be custom tailored to your personal tastes and interests. The nouveau Google News is able to make these determinations from day one, but will improve over time. If there\u2019s one news organization you frequently avoid \u2014 like Fox News or the BBC \u2014 or if you\u2019re not particularly interested in a specific topic, Google News will adjust what it shows you accordingly. You can\u2019t help but think Google is being a little bit brave here. As shown by Facebook\u2019s trending stories, aggregating stories is a bit of a political minefield, and can earn accusations of bias from both sides of the partisan divide. According to Google News engineer Trystan Upstill, the new update also emphasizes depth and quality of reporting, and aims to uplift local voices.  Local journalism often takes a backseat to the larger, better-funded national and international news organizations, so it\u2019s pretty refreshing to see how Google News is able to amplify these smaller (but vitally important) outlets. Google News also has a refreshed visual aesthetic that plays greater emphasis to featured images and videos. Pictures are bigger and bolder, and allow readers to better engage with stories. Another feature introduced to Google News is called Newscast, which feels a little bit like Twitter\u2019s Moments. It allows you to explore a particular event or story by swiping through coverage and commentary. Given how popular Moments are for Twitter, you can\u2019t help but feel this is a smart move for Google. Another feature brought into Google News is called Full Coverage. Essentially, this is a more thorough exploration of a story. Just think of Moments, and then multiply it by a factor of ten.  Not only does Google News show you detailed, rich perspectives on an event, but it also tries to but it within a chronological context, so you can explore how things transpired. It also tries to understand the news for you, and to answer questions before you even ask them. Ultimately, the one thing that\u2019ll get news organizations chomping at the bit is subscriptions. It\u2019s now possible to subscribe to paywalled news sites, like the Washington Post, within Google News. This addresses two of the biggest pain points within paywalls, which is the fact that you\u2019ve got to create and manage a subscription for each site, and if you use multiple devices, you\u2019ve got to log in on each one. With Google News, you don\u2019t have to type in your credit card number each time, as Google takes the payment on the news site\u2019s behalf. And if you\u2019re signed into Google, you\u2019re signed into the paywalled news site. By addressing these two issues, hopefully Google will make it more likely people will be willing to pay for the journalism that\u2019s so fundamentally essential to our democracy. The updated Google News is rolling out today on iOS, Android, and the web, and will be available in 127 countries. \nRead next:\n\n        Google now uses the camera to help figure out which direction you're facing    \n Stay tuned with our weekly recap of what\u2019s hot & cool by our CEO Boris. \n        Join over 260,000 subscribers!\n     \n                Sit back and let the hottest tech news come to you by the magic of electronic mail.\n             \n                Prefer to get the news as it happens? Follow us on social media.\n             \n1.76M followers\n                         \n1M likes\n                         \n                Got two minutes to spare? We'd love to know a bit more about our readers.\nStart!\n \n                All data collected in the survey is anonymous.\n            ","time":1525804855,"title":"Google News aims to fix journalism by offering an easy way to pay for it","type":"story","url":"https:\/\/thenextweb.com\/google\/2018\/05\/08\/google-news-aims-to-fix-journalism-by-offering-an-easy-way-to-pay-for-it\/","label":7,"label_name":"random"},{"by":"edroche","descendants":0,"id":17023495,"kids":"None","score":1,"text":"Creative abrasion is Jerry Hirshberg\u2019s term for the kinds of friction that helps develop better ideas. Hirshberg,\u00a0a former design manager at Nissan, realized that some kinds of resistance are useful in the creative process and should be deliberately created by the leader of a team. This could be a timely critique, a barrage of difficult questions or even a temporary reworking of team processes and roles. This works against the\u00a0romantic fantasy many have for an ideal creative workplace. Somehow we all have a latent desire for a workplace free of annoying meetings and frustrating bureaucratic processes, where we could just show up at the office, pronounce our epiphany, and have the entire organization immediately swirl around us in support. But Hirshberg suggests good ideas, and good people, need to be challenged to develop and grow. A good idea can withstand critique and hold up, or improve in quality, when\u00a0compared against other good ideas,\u00a0while weaker ones will be revealed and fade away. In his book, The Creative Priority, Hirshberg documents various kinds of abrasions, including: He also noticed how his choices as manager would sometime generate dual responses from his team. It both made them uncomfortable and had effects that they appreciated, and this duality signified to him that his abrasions were having the desired effect. For example, when Hirshberg made his team\u2019s\u00a0 prototyping process much faster: Jim McJunkin, a meticulous designer from Texas, felt that \u201cGod is in the details and the nuances, and these take time to resolve.\u201d\u2026 But McJunkin then countered himself with the observation, \u201cI like the imposed haste. I\u2019m a perfectionist, and it\u2019s a nice counterbalance to my workstyle\u2026 there\u2019s something provocative in the unfinished-ness of the models,\u201d a statement that implicitly acknowledged the added value of the modelers\u2019 creative instincts in these spontaneous interpretations. (p. 44) Jim McJunkin noted that \u201cit is the abrasion of tiny air molecules that creates the beauty of a shooting star, without which it would be just another rapidly moving, cold and anonymous piece of rock.\u201d However, too much friction, or friction of the wrong kind or at the wrong time, can be just as bad, or worse, as not having enough. Hirshberg is clear the goal isn\u2019t to force heated debates or make people upset (although that may happen at times). Instead, it\u2019s the deliberate use of energy to make a kind of forcing function, that pushes people to dig deeper, rethink harder and explore alternatives they would be unlikely to choose to otherwise. Deciding the right kind of friction to apply is a subtle skill that many managers never master. It depends heavily on understanding the culture of the team, the personality of each individual, and the ability to make friction something interesting and that raises curiosity, rather than feeling like a penalty. It\u2019s also heavily dependent on timing: much like working a campfire, you have to use different kinds of friction and fuel to start it, grow it, or to just keep it going. The legendary research lab at Xerox Parc, where the GUI, Ethernet and the laser printer were invented, was led by Bob Taylor, and his approach to management might be one of the labs greatest creations. Alan Kay, who worked for him, said about Taylor: \u201cHis attitude kept it safe for others to put aside fears and ego and concentrate objectively on the problem at hand.\u201d Taylor encouraged open criticism and debate, in a weekly meeting in a room filled with beanbag chairs. The goal wasn\u2019t to tear other people down, but to push, inspire, and challenge everyone to explore their ideas deeply. Taylor put the ideas, and ideas about ideas, at the center, and moved politics, posturing, and hierarchy to the perimeter. Taylor was likely an excellent facilitator of discussions, helping make sure there was just the right amount of friction. All too often managers hear about a concept like creative abrasion and rush to apply it, without fully understanding how it works. Hirshberg shares this story: After hearing about [creative abrasion] at a meeting at NDI, a group of executives from Salomon, the great French ski equipment manufacturer, attempted to apply it. When they returned to San Diego from France a few months later for a design review of the ski boot concepts we were developing for them, one of the vice presidents said, \u201cWell, we have the abrasion part down pat!\u201d This reveals that the notion of friction applies to the manager\u2019s own work as well. It\u2019s inevitable that the use of friction as a tool will force questions about how a company or team are organized and the process that managers use. This is healthy and can lead to progress, but for insecure managers\u00a0who fear change, it\u2019s also terrifying. Creative abrasion can be seen as slowing things down or working inefficiently when it should instead be seen as one of the few ways to provoke better and more original thinking to happen. But even if all the strategies suggested in this book were invoked and followed religiously, creativity would still sit uneasily within bureaucratic bounds\u2026 None of the procedures is designed to make it a comfortable, obeisant, timely, well-oiled cog in traditional or enlightened bureaucratic machinery. Instead, the strategies were conceived to help overcome the knee-jerk resistance that inevitably accompanies the creative process, and to recognize the unease as a sign of its probable health. Hirshberg\u2019s book is a worthy read, especially for design managers or R&D lab leaders, as many of the stories he uses to illustrate his ideas come directly from his management experience.  If you sign up to receive his best posts via email, you\u2019ll get a FREE copy of a preview edition of Mindfire plus free chapters from all of his bestselling books. Over 19,000 people have signed up. Privacy policy enforced by my Rotweiller. \nName*\n\n \nEmail*\n\n \nWebsite\n\n \nComment*\n\n  * Required Click here to cancel reply.   Notify me of follow-up comments by email.  Notify me of new posts by email.  Scott Berkun is the author of seven popular books on creativity, leadership, philosophy and speaking. You can hire him to speak, ask him a question or follow him on Twitter and Facebook. Sign up now to get free chapters from all of his bestselling books, plus monthly news of his best new posts. Join over 19,000 fellow subscribers. Privacy policy enforced by my Rotweiller. Welcome to the best blog you\u2019ve ever seen at this URL: here are 7 reasons you\u2019ll want to come back. Instant delight awaits in trying one of Berkun\u2019s best posts of all space and time. Dare Berkun to answer a question on any topic, and vote on which one he answers this week. 5 Stars \"You'll find a lot to steal from this short, inspiring guide to being creative. Made me want to get up and make stuff!\" - Austin Kleon, author of How To Steal Like An Artist 4_5 Stars \u201cThought-provoking read, and highly recommended\u2026\u201d \u2013 Thomas Duff 4_5 Stars \u201cIf you want to think differently about entrepreneurship, management, or life in general, read this book.\u201d \u2014 Tim Ferriss 4_5 Stars \u201cThe ideas contained in the essays are persuasive and it\u2019s a fun, well focused read. \u201d \u2014 Simon Moore 4_5 Stars \u201cHighly recommended for CEOs, project managers, and hackers alike.\u201d \u2014 Matt Mullenweg, Founder WordPress.org 4_5 Stars \u201cBerkun sets us free to try and change the world.\u201d \u2014 Guy Kawasaki 4_5 Stars \u201cBerkun tells it like it is\u2026 you\u2019ll gain insights to take your skills to the next level.\u201d \u2014 Tony Hsieh, CEO Zappos.com Scott Berkun is the author of six popular books on creativity, leadership, philosophy and speaking. You can hire him to speak, ask him a question or follow him on Twitter and Facebook. These organizations have written about his work or invited him to speak to them: ","time":1525804759,"title":"How Creative Abrasion Can Help Your Team\u2019s Ideas","type":"story","url":"http:\/\/scottberkun.com\/2018\/how-creative-abrasion-helps-good-ideas\/","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17023490,"kids":"None","score":1,"text":"A growing body of research shows that innovative businesses are common in rural areas, and rural innovation gets a boost from the arts. One of the most persistent myths in America today is that urban areas are innovative and rural areas are not. While it is overwhelmingly clear that innovation and creativity tend to cluster in a small number of cities and metropolitan areas, it\u2019s a big mistake to think that they somehow skip over rural America. A series of studies from Tim Wojan and his colleagues at the U.S. Department of Agriculture\u2019s Economic Research Service documents the drivers of rural innovation. Their findings draw on a variety of data sets, including a large-scale survey that compares innovation in urban and rural areas called the Rural Establishment Innovation Survey (REIS). This is based on some 11,000 business establishments with at least five paid employees in tradable industries\u2014that is, sectors that produce goods and services that are or could be traded internationally\u2014in rural (or non-metro) and urban (metro) areas. The survey divides businesses into three main groups. Roughly 30 percent of firms are substantive innovators, launching new products and services, making data-driven decisions, and creating intellectual property worth protecting; another 33 percent are nominal innovators who engage in more incremental improvement of their products and processes; and 38 percent show little or no evidence of innovation, so are considered to be non-innovators. The first table below charts this breakdown for rural and urban areas. Establishments in urban areas are more innovative, but not by much. Roughly 20 percent of rural firms are substantive innovators, compared to 30 percent of firms in urban areas. In fact, the urban-rural divide in innovation may be more a product of the relative size of firms than of geography. The next chart shows this: Rural areas actually have a slightly overall higher rate of substantive innovation for large firms (those with 100 employees or more), while urban areas win out in their rate of substantive innovation by small and medium-size firms. [Subscribe to CityLab\u2019s newsletters] Rural areas also have a slight advantage over their metro counterparts in the rate of substantive innovation by the most innovative firms (those that are patent-intensive). That\u2019s because innovation in rural areas tends to be a product of patent-intensive manufacturing in industries like chemicals, electronics, and automotive or medical equipment, while urban areas have higher rates of innovation in services. Of course, innovation concentrates and clusters in certain rural areas, just as it does in cities and metros. In a 2007 study, Wojan and his collaborators identified 100 or so rural creative havens, such as Woodstock, New York, and the area around Telluride and Silverton, Colorado. These rural creative centers tend to be in relatively close proximity to and have good connections to major metro areas; are home to a major university or college; or have considerable natural amenities which draw people to them. Whereas arts districts in urban areas are typically in high-density, mixed-use neighborhoods with lots of foot traffic, rural artistic districts tend to crop up around natural, physical, and recreational amenities. According to a 2017 study by the National Endowment for the Arts (NEA), the likelihood that a rural county will contain a performing arts organization is nearly 60 percent higher if the county overlaps with a forest or national park. The most notable difference between the rural and urban arts districts is the distance one must travel: Rural arts organizations reported that 31 percent of their visitors travel \u201cbeyond a reasonable distance\u201d to visit, compared to 19 percent in urban areas. If anything, the arts may be even more important to rural innovation than they are to urban innovation. While my own research has drawn a connection between the arts and clusters of innovative high-tech startups in urban areas, Wojan and his colleague Bonnie Nichols\u2019 data suggests an even stronger connection between arts and innovation in rural areas. And according to the NEA paper, probability that a rural firm will be a substantive innovator rises from 60 percent in rural counties with no performing arts organizations to nearly 70 percent for those that host two or three, to as high as 85 percent if a rural county hosts four or more. Furthermore, the share of firms that are highly innovative rises sharply alongside performing arts organizations in rural areas. The probability that a rural business will be highly innovative increases from 17 percent to 44 percent as the number of performing arts organizations in a rural county increases from zero to one. When that number rises to two, the probability that a business will be highly innovative grows to 70 percent or higher. Ultimately, Wojan and company\u2019s analysis find a strong statistical association between the arts, innovation, and economic dynamism in rural areas. And this leads them to conclude that the arts are a direct force in rural innovation, not just an indirect factor that helps to attract and retain talent. Artists and creatives in America have long sought out rural places to fuel their creativity, from the Hudson River School painters to Bob Dylan and The Band developing their music in Woodstock. But the arts in rural places are not just a byproduct of the scenery; they play a key role in spurring the innovation that ultimately leads to economic development and rising living standards. The myth that urban areas are creative and rural areas are not is just that: a myth. Richard Florida is a co-founder and editor at large of CityLab and a senior editor at The Atlantic. He is a University Professor and Director of Cities at the University of Toronto\u2019s Martin Prosperity Institute, and a Distinguished Fellow at New York University\u2019s Schack Institute of Real Estate. A growing body of research shows that innovative businesses are common in rural areas, and rural innovation gets a boost from the arts. Talk of the transportation future is focused on the next shiny thing. But one old technology is central to real transformation. In the suburb of Waukegan, Illinois, the design firm JGMA has turned an abandoned Kmart into a bright new home for Cristo St. Rey Martin College Prep. In just eight years, Shenzhen became the first city to electrify 100 percent of its public buses\u201416,359, to be exact. In cities that gain college graduates, wages rise but so do rents, resulting in a cost burden for the least advantaged. Subscribe to our other newsletters here. CityLab is committed to telling the story of the world\u2019s cities: how they work, the challenges they face, and the solutions they need. Citylab.com \u00a9 2018 The Atlantic Monthly Group","time":1525804734,"title":"The Rise of the Rural Creative Class","type":"story","url":"https:\/\/www.citylab.com\/life\/2018\/05\/the-rise-of-the-rural-creative-class\/559319\/","label":7,"label_name":"random"},{"by":"clumsysmurf","descendants":0,"id":17023485,"kids":"None","score":1,"text":"The Supreme Court will soon decide whether employers can lawfully require workers to sign mandatory arbitration agreements that include class and collective action waivers. A ruling in NLRB v. Murphy Oil USA, Inc., Epic Systems Corp. v. Lewis, and Ernst & Young LLP v. Morris will have significant impacts on working people. If the Court sides with employers and the Trump administration, it is likely that the majority of workers in this country will be required, as a condition of employment, to sign away their right to pursue workplace disputes on a collective or class basis. In fact, available data suggest that it may take only six years for more than 80 percent of workplaces to adopt mandatory arbitration with class and collective action waivers. Last year, EPI commissioned a survey that found that 53.9 percent of nonunion private-sector employers already have mandatory arbitration procedures. Prior to that study, the one major governmental effort to investigate the extent of mandatory arbitration was a 1995 GAO survey. That survey, conducted between April 1994 and April 1995, found that just 7.6 percent of employers had mandatory arbitration agreements. In other words, the use of mandatory arbitration agreements grew by more than 600 percent between 1994 and 2017. Using the growth rates between the two surveys to forecast future expansion suggests that by 2024, more than 80 percent of private sector, non-union establishments will adopt mandatory arbitration with class and collective action waiver of employment disputes, if the Court finds that such agreements are lawful.1 That will leave more than 85 million workers subject to mandatory arbitration agreements with class and collective action waivers. This means that the vast majority of workers will be forced to sign away their right to act with their colleagues to resolve workplace disputes\u2014as well as their right to go to court for these matters. As a result, even if many workers face the same type of issue at work, each individual worker will be forced to hire their own lawyer, and resolve their dispute out of court, behind closed doors, with only their employer and a private arbitrator.  Workers depend on collective and class actions to enforce many workplace rights. Employment class actions have helped to combat race and sex discrimination and are fundamental to the enforcement of wage and hour standards. Without the ability to aggregate claims, it is very difficult, if not impossible, for workers to find legal representation in these matters. This is particularly true for low-wage workers, whose cases are unlikely to involve large enough awards to attract attorneys to invest time in the case. Class and collective action suits allow workers to pool their claims, making it possible for an attorney to earn enough to make the case worth pursuing. But NLRB v. Murphy Oil USA, Inc., Epic Systems Corp. v. Lewis, and Ernst & Young LLP v. Morris have implications beyond class action suits. If the Court is persuaded by the Trump administration, the decision could prohibit a broad category of workers\u2019 collective action guaranteed to U.S. workers since 1935 with the passage of the National Labor Relations Act. The right of working people to join together\u2014whether through a union or not\u2014to improve their wages and working conditions is at the heart of the NLRA. This right is as important today as it was when the Act was passed. We are at a critical moment as a country. If we are going to address economic inequality, combat employer practices that perpetuate race and sex discrimination, and change the epidemic of sexual harassment that has been exposed by the #metoo and #timesup movements, we must be able to use our collective voice and our collective power to do so. It is only when we act together, whether as working people demanding fair pay\u2014as the workers did in Murphy Oil\u2014or as citizens speaking out collectively against injustice, that we are able to produce meaningful change. If the Court issues a decision that erodes our right to collective action, we must join together to demand Congress act to protect this right. After all, it was collective action that convinced Congress to pass the NLRA over eighty years ago. 1.  The rate of growth of mandatory arbitration between 1994 and 2017 in percent terms was 8.9 percent per year on average, and in percentage point terms it was 2.0 percentage points per year on average. Lacking additional information to about the true growth function, we simply projected the incidence of mandatory arbitration going forward using both rates and took the average of the two. It should be noted that the 2017 survey commissioned by EPI showed that just 30.1 percent of employers who required mandatory arbitration also explicitly included class and collective action waivers in their procedures. It is likely that the lack of explicit class and collective waivers within many mandatory arbitration agreements was due to uncertainty about their legality. If the Court finds that such waivers are lawful, we expect that going forward, mandatory arbitration agreements will be highly unlikely not to include explicit class and collective action waivers. \n\n\n\n EPI is an independent, nonprofit think tank that researches the impact of economic trends and policies on working people in the United States. EPI\u2019s research helps policymakers, opinion leaders, advocates, journalists, and the public understand the bread-and-butter issues affecting ordinary Americans. \n\n\n\n \n                        1225 Eye St. NW, Suite 600\n                        Washington, DC 20005\n                        Phone: 202-775-8810 \u2022 epi@epi.org\n                        \u00a9 2018 Economic Policy Institute\nPrivacy Policy \u2022 Contact Us\n Tracking the wage and employment policies coming out of the White House, Congress, and the courts. Authoritative, up-to-date data on the living standards of American workers. Interactive tools and videos bringing clarity to the national dialogue on economic inequality. A network of state and local organizations improving workers' lives through research and advocacy. \nStaff\nBoard of Directors\nJobs at EPI\nContact us\nWhy give to EPI\nNewsroom\nNewsletter\nEvents\nDonate\n","time":1525804687,"title":"The Supreme Court is poised to make forced arbitration nearly inescapable","type":"story","url":"https:\/\/www.epi.org\/blog\/the-supreme-court-is-poised-to-make-forced-arbitration-nearly-inescapable\/","label":7,"label_name":"random"},{"by":"rbanffy","descendants":0,"id":17023481,"kids":"None","score":2,"text":"\n\n\n\n Google AR and VR There\u2019s so much information available online, but many of the questions we have are about the world right in front of us. That\u2019s why we started working on Google Lens, to put the answers right where the questions are, and let you do more with what you see. Last year, we introduced Lens in Google Photos and the Assistant. People are already using it to answer all kinds of questions\u2014especially when they\u2019re difficult to describe in a search box, like \u201cwhat type of dog is that?\u201d or \u201cwhat\u2019s that building called?\u201d Today at Google I\/O, we announced that Lens will now be available directly in the camera app on supported devices from LGE, Motorola, Xiaomi, Sony Mobile, HMD\/Nokia, Transsion, TCL, OnePlus, BQ, Asus, and of course the Google Pixel. We also announced three updates that enable Lens to answer more questions, about more things, more quickly: First, smart text selection connects the words you see with the answers and actions you need. You can copy and paste text from the real world\u2014like recipes, gift card codes, or Wi-Fi passwords\u2014to your phone. Lens helps you make sense of a page of words by showing you relevant information and photos. Say you\u2019re at a restaurant and see the name of a dish you don\u2019t recognize\u2014Lens will show you a picture to give you a better idea. \u00a0This requires not just recognizing shapes of letters, but also the meaning and context behind the words. This is where all our years of language understanding in Search help. Second, sometimes your question is not, \u201cwhat is that exact thing?\u201d but instead, \"what are things like it?\" Now, with style match, if an outfit or home decor item catch your eye, you can open Lens and not only get info on that specific item\u2014like reviews\u2014but see things in a similar style that fit the look you like. Third, Lens now works in real time. It\u2019s able to proactively surface information instantly\u2014and anchor it to the things you see. Now you\u2019ll be able to browse the world around you, just by pointing your camera. This is only possible with state-of-the-art machine learning, using both on-device intelligence and cloud TPUs, to identify billions of words, phrases, places, and things in a split second. Much like voice, we see vision as a fundamental shift in computing and a multi-year journey. We\u2019re excited about the progress we\u2019re making with Google Lens features that will start rolling out over the next few weeks. \n              Follow Us\n            ","time":1525804660,"title":"Google Lens: real time answers to questions about the world around you","type":"story","url":"https:\/\/blog.google\/products\/google-vr\/google-lens-real-time-answers-questions-about-world-around-you\/","label":7,"label_name":"random"},{"by":"danroseman","descendants":3,"id":17023465,"kids":"[17023470]","score":2,"text":"One of the great promises of blockchain technology (specifically, Turing complete blockchains such as Ethereum) is the concept of a Decentralized Application (AKA \u201c\u0110App\u201d)\u200a\u2014\u200aan Internet service completely owned and operated by computer code with no human owners\/operators. The idea of a Decentralized Internet has achieved some level of mainstream interest in the sense that is a central theme of the popular HBO series, Silicon Valley. This article discusses a few popular critiques of today\u2019s Centralized Applications (which I\u2019ll call \u201cCApps\u201d), how \u0110Apps propose to remedy the shortcomings of CApps, and highlight several significant roadblocks which threaten the viability of a mainstream \u0110App ecosystem. Today\u2019s CApps are primarily criticized on three solid grounds. First is the fact that CApps have consolidated into Internet behemoths such as Google, Facebook, Twitter, etc.; these centralized capitalist ventures rely on humans (and their biases) to control the content on its platform, with little to no regulatory oversight. This enables (in fact, requires) Facebook to apply subjective standards to regulate the content on its platform. For example, Facebook does not allow certain content considered \u201cCruel and Insensitive\u201d or \u201cFalse News\u201d and cannot (by law) allow certain content, such as that deemed to be related to \u201cDangerous Individuals and Organizations.\u201d This is flirting with the idea that \u201cfree speech gives us the right to offend,\u201d but that\u2019s a separate topic. It also enables Google to provide personalized search results based on a plethora of data associated with your Google account and CApp browsing history. This means that Google search results are no longer the same for everyone, and is especially true if you are logged into your Google account while browsing. The second critique of CApps is that these Internet conglomerates extract and sell personal identifying information to third parties without the full knowledge and consent of its users. This is the idea that \u201cYou\u2019re the Product, Not the Consumer.\u201d Facebook, Twitter, Instagram, and even \u201csharing economy\u201d services such as AirBnB and Uber all rely on its users to produce content for their respective platforms. The value these social media platforms offer their users comes from content produced by other users, not by the host platform. As TechCrunch contributor Tom Goodwin observed, \u201cUber, the world\u2019s largest taxi company, owns no vehicles. Facebook, the world\u2019s most popular media owner, creates no content. Alibaba, the most valuable retailer, has no inventory. And Airbnb, the world\u2019s largest accommodation provider, owns no real estate. Something interesting is happening.\u201d Truth be told, the real, underlying consumers of social media platforms are advertisers and data miners\u200a\u2014\u200anot end-users. Users get free access to these platforms, but only in exchange for producing both content and advertising opportunities. This arrangement enables abuses like Cambridge Analytica. The third critique of CApps is that they rely on a centralized Internet infrastructure (ISPs) which are vulnerable to the whims of politics and special interests. This results in harmful regulatory actions such as the FCC\u2019s recent decision to repeal Net Neutrality. Blockchains are effective tools for censorship resistance. With \u0110Apps, \u201ccode is law\u201d (as was the tagline of the defunct TheDAO project). The idea is that smart contracts are controlled by math, and we can trust the laws of math more than the judgement of humans. Thus, a Decentralized Facebook would not need a 10,000 person \u201ccontent review team.\u201d to determine which content is appropriate for its platform; it would publish all posts from all users with no filter. \u0110Apps require users to completely own and control their own data. This has mixed implications, which I address later. However, Governments are starting to catch up with consumer\u2019s privacy demands and are beginning to enforce consumer data controls with regulations such as GDPR. Lastly, whether or not Ethereum \u0110Apps would be affected by Net Neutrality is still very much up for debate. Time will tell. Despite the hype, there are serious roadblocks which threaten the viability of a healthy, mainstream Decentralized Internet. One of the biggest issues is fairness. There already is an elite class of technocrats; current blockchain participants (digital asset holders) own 100% of the economic and utility token supply and will therefore have the most control and influence over the design and governance of a Decentralized Internet and the \u0110Apps built on it. This is especially true if the new decentralized Google\/Facebook\/Twitter\/etc. adopt Proof of Stake as its consensus mechanism. Then there is the governance issue. Governance requires humans; designing, building, and maintaining \u0110Apps will require at least some human input, which creates a risk of centralization. Related to this is the difficulty in achieving near-perfect consensus for all \u0110App upgrades. A Decentralized Internet requires a near-perfect majority consensus on decisions affecting \u0110App development and maintenance. In order for a decentralized Facebook to stand a chance against its centralized counterpart, there can never be a chain split. Getting users to agree on the design and function of an app is extremely difficult, as Snapchat recently realized. Identity and reputation are incredibly difficult challenges for \u0110Apps. A Decentralized Internet requires \u201ctokenized identity\u201d\u200a\u2014\u200athe ability to encode\/associate your online and\/or real-world identity with a hash output on a Blockchain. The problem is that identity and reputation are by their nature very mutable\u200a\u2014\u200apeople change addresses, names, careers, marital statuses, die, etc. Thus, maintaining a blockchain record which is truly reflective of your online\/real-world identity status requires a centralized data oracle, which again creates a risk of centralization. There are pragmatic problems with how \u0110Apps actually work. Every state change in a \u0110App is an on-chain transaction which costs time and money. For example, users of centralized exchanges are able to create, modify, and delete market orders instantly and freely, thanks to centralized databases. The same is not true for decentralized exchanges, where each order creation, modification and deletion requires the user to send a transaction to the blockchain in order to update the state of the ledger and execute the command. An interesting side effect of this technical inefficiency is that it renders high-frequency trading cost-prohibitive on decentralized exchanges. Lastly, \u0110Apps require users to create, own, and manage their own private keys. Unfortunately, this requirement would exclude a significant population of today\u2019s Internet users (nontechnical, young, elderly, disabled) who may be unable to effectively control this vital piece of data. I do not intend to suggest that blockchain development is not a worthwhile pursuit; it is, and scaling solutions as well as other important innovations will likely emerge in the process. When the dust settles, it is unlikely that we will see Google, Facebook, Twitter, etc. dissolve their corporate entities and surrender to the New Decentralized Overlords. Instead, they will likely integrate elements of their applications on the blockchain where it makes the most economic sense to do so. That would be a boring outcome and I\u2019d love to be proven wrong. -DR By clapping more or less, you can signal to us which stories really stand out. Public Speaker, Entrepreneur, Technical Project Lead @Coinbase.","time":1525804558,"title":"Roadblocks to a Decentralized Internet","type":"story","url":"https:\/\/medium.com\/@danielroseman\/roadblocks-to-a-decentralized-internet-84e47c1bcb15","label":2,"label_name":"crypto"},{"by":"gk1","descendants":0,"id":17023460,"kids":"None","score":6,"text":" It's really easy to make your own site these days. Just watch a tutorial on youtube3 for 90 minutes, spend another 30 on setting things up and voila, you're done. Easy Peasy.\nAlright dude, but where's the catch? For almost two years 2.8 million websites built on Joomla were susceptible to dangerous SQL injection. The bug has been patched by Joomla with 3.4.5 version4\n2 days after the release there was a step-by-step tutorial on youtube how to exploit Joomla >3.4.5.5\nWhat if you didn't update your website immediately after the release? Based on research made by WP White Security6 around 73% of the most popular WordPress installations can be hacked. To find out which can be, you can use free automated tools. Drupal also found its place in the ungrateful history of CMS exploits. According to Drupal announcement from 29th October 2014:\n\"You should proceed under the assumption that every Drupal 7 website was compromised unless updated or patched before Oct 15th, 11 pm UTC, that is 7 hours after the announcement.\"7\nWhat if you didn't act that fast? Taking into account all of the above factors you should always update your CMS to sleep peacefully. Sounds pretty exhaustive, especially if you're using a plethora of add-ons.\nWell, chances are you're using a plethora of add-ons.8 Fortunately for people who don't want to mess with all this updates stuff (like me and the rest of Bejamas team), there're plenty of great Static Site Generator projects, which are, by the way. open-source.9 Static Site Generators aka. Static Site Engines are definitely not an ephemeral novelty. They've been around quite a while, but nobody really paid attention to them as it takes place right now.\nThe first known SSG is called tclog and has been written in Tcl\/Tk. The first release took place in 2003. You can check this project out at https:\/\/web.archive.org\/web\/20060819194024\/http:\/\/tclog.sourceforge.jp\/  The idea behind Static Site Engines is pretty simple, yet powerful: take dynamic content and build it into raw HTML\/CSS\/Javascript files, then deploy them as static files to the server. No server horsepower needed, what takes us to the first of many advantages of this approach. Hence the website is always completely built on your production machine when you use a Static Site Generator the only thing you have to pay for is basically the storage space. If you're not tech savvy, please let me explain it to you shortly:\nWhen you build your website on a production machine and serve the raw files on your hosting they're ready to view - basically nothing happens on the server-side. If someone's visiting your website he's seeing the files as they were built on your machine, nothing's being built on the server.  On the other side, things are getting a bit more sophisticated when you consider how a dynamic site, build on WordPress, Joomla etc. works.\nWhen a visitor hits your website the server-side script is being run.\nA server-side script's querying one or many databases to get data (content), which have to be displayed to the visitor.\nData is being passed to templating engine and templating engine sets up the HTML file which is then seen by the visitor.\nNow multiply that process by hundreds or thousands of visitors who have the interest to get your website's content.\nSounds laborious.\nSounds pricey.  I think the conclusion here is plain and simple: going static means to be better prepared for handling large amounts of web traffic, as, comparing to dynamic websites, it consumes a small fraction of server horsepower.  In the age of impatience and information overload, each millisecond of your website loading time is literally equal to a certain amount of dollars. I'm dead serious.\nBased on research made by www.hobo-web.co.uk10  if your website is loading for 4 seconds or more it'll cause more than 25% of visitors to abandon viewing it.   If you read my quick explanation above where I described the differences between hosting static site and dynamic one you probably can get to this conclusion yourself: static eats dynamic when it comes down to performance. Period.\nBut wait! You won't show us any research to make your thesis more credible!?\nHey, of course, I do! Follow me. At the end of 2016, https:\/\/gettingthingstech.com moved from their WordPress platform to Hugo. Part of their motivation was to improve the page load performance. They measured, of course, the page loading speed on WordPress and after moving to Hugo and made a great comparison.11\nIn this case, they've shortened the time server needs to send the HTML to the user from about 2.5 seconds to 0.3 seconds (it's pretty tough to say precisely watching this chart, what you can say with no doubt though is that the difference is tremendous).\nhttps:\/\/gettingthingstech.com\/img\/speed6mo-zoom.png  At the time they changed their hosting provider, so it has an impact on the time as well.  Here another example of simple comparison between a static site generated with Jekyll and a WordPress one: https:\/\/bradonomics.com\/jekyll-wordpress-speed\/\nLoading time improved by about 164%. As I've already described in The Catch section of this post - you're exposed to several threats if you'll go with \u2018the standard CMS way'. On the other side, what can possibly go wrong if you just serve flat HTML files with CSS and JavaScript? You don't have any database which can be a victim of SQL injection. Everything's static & safe so you don't have to care much about the security of your website. Guess what you have to do to backup your WordPress site? Bingo! Install another add-on12 \"Remember, keep at least three backups on file, just in case one is corrupted or lost, and store them in different places and on different mediums, like CD, DVD, different hard drives, etc.\" You have to remember to make the backup manually once in a while. If your database is full of statistics from WordPress plugins (because, as I wrote above, chances are your theme is packed with a hell lot of plugins) you've to choose only those tables which you want to backup. Well, that sounds like a lot of work. You can do all that, but you still don't have control over each version of your site. To gain it you'll have to backup your site each time you're making changes. Have fun. With the emergence of Git-based distributed revision control systems as f.e. Bitbucket or Github this is not an issue anymore. Each time you commit & push new changes to your site the previous version is backed up automatically!\nIt takes literally one line of code in your terminal to get back to the previous version. How cool is that! And, just so you know, with Netlify13 and BitBucket14 it's totally free. I'm not trying to say in this post that there are no pros of using WP\/Joomla\/Drupal or similar solutions. It's obviously easier to set everything up, at least in the beginning, but you'll probably struggle with this setup as you go further. On the other side, if you want to go static and your website isn't a one-page you'll probably have to hire a professional who has the technical knowledge. You would have to do it anyway even with a WordPress site if you're thinking seriously about it, wouldn't you? I just wanted to underline in this post that there are a lot better options available and, honestly, everyone suggests you WordPress because it's the most popular, not the best, option. You didn\u2019t mention anything about managing content or adding posts on a static site! Is it even possible? Yup static site doesn\u2019t mean static content.\nRead more about CMS + Static Site Generators setup in our next posts.\nStay tuned!","time":1525804542,"title":"Today's Web is Tedious","type":"story","url":"https:\/\/bejamas.io\/blog\/web-is-tedious","label":3,"label_name":"dev"},{"by":"pranoy","descendants":0,"id":17023448,"kids":"None","score":1,"text":"Generative models aim at learning the true data distribution of the training data so as to generate new data points with some variations. To train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. GAN\u2019s pose the training process as a game between two separate networks: a Generator network and a second discriminative network that tries to classify samples as either coming from the true distribution (training data) or the model distribution (generated data)\u00a0. Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away, until at the end the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference. In our function V(D, G), the weights in the discriminator are adjusted so that when real data distribution (pdata(x)) passes through the discriminator, the output of the discriminator is equal to 1. (Increasing V) The weights in the discriminator are adjusted so that when generated data from the output of the generator is passed through the discriminator, the output of the discriminator is equal to 0. (Increasing V) The weights in the generator are adjusted so that when generated data from the output of the generator is passed through the discriminator, discriminator output is equal to 1. (Decreasing V) By clapping more or less, you can signal to us which stories really stand out. Deep Learning Enthusiast, Entrepreneur","time":1525804502,"title":"Generative Adverserial Networks","type":"story","url":"https:\/\/medium.com\/@pranoyradhakrishnan\/generative-adverserial-networks-bef215ec4e7c","label":5,"label_name":"ml"},{"by":"rbanffy","descendants":0,"id":17023434,"kids":"None","score":2,"text":"\n\n\n\n Maps Google Maps has always helped you get where you need to go as quickly as possible, and soon, it\u2019ll help you do even more. In the coming months, Google Maps will become more assistive and personal with new features that help you figure out what to eat, drink, and do\u2013no matter what part of the world you\u2019re in. So say goodbye to endless scrolling through lists of recommended restaurants or group texts with friends that never end in a decision on where to go. The next time you\u2019re exploring somewhere new, getting together with friends, or hosting out-of-towners in your own city, you can use Google Maps to make quick decisions and find the best spots. The redesigned Explore tab We\u2019ll even help you track your progress against each list, so if you\u2019ve crossed four of the top restaurants in the Meatpacking District off your list, you\u2019ll know that you have six more to try.  Events and activities, options around you,\u00a0and top lists Tapping on any food or drink venue will display your \u201cmatch\u201d\u2014a number that suggests how likely you are to enjoy a place and reasons explaining why. We use machine learning to generate this number, based on a few factors: what we know about a business, the food and drink preferences you\u2019ve selected in Google Maps, places you\u2019ve been to, and whether you\u2019ve rated a restaurant or added it to a list. Your matches change as your own tastes and preferences evolve over time\u2014it\u2019s like your own expert sidekick, helping you quickly assess your options and confidently make a decision. Your match When you need to corral a group for a meal or activity, there\u2019s a new feature that makes it easier to coordinate. Long press on the places you\u2019re interested in to add it to a shareable shortlist that your friends and family can add more places to and vote on. Once you\u2019ve made a decision together, you can use Google Maps to book a reservation and find a ride. Add places to a shortlist Vote on where to go The new \u201cFor you\u201d tab is the best way to stay on top of the latest and greatest happening in the areas you\u2019re into. You can choose to follow neighborhoods and dining spots you want to try so you\u2019ll always have an idea for your next outing. Information about that new sandwich spot downtown, the surprise pop-up from your favorite chef, or that new bakery shaking up the pastry scene in Paris will now come straight to you. The For you tab You\u2019ll start to see these features rolling out globally on Android and iOS in the coming months. Get ready to rediscover your world with Google Maps. \n              Follow Us\n            ","time":1525804392,"title":"Explore and eat your way around town with Google Maps","type":"story","url":"https:\/\/blog.google\/products\/maps\/explore-around-town-google-maps\/","label":7,"label_name":"random"},{"by":"rbanffy","descendants":0,"id":17023420,"kids":"None","score":3,"text":"Above: ML Kit logo Google today announced the debut in beta of ML Kit, a software development kit optimized for deploying AI for mobile apps on app development platform Firebase. ML Kit, available for both Android and iOS developers, can call on APIs both on-device and in the cloud. AI in mobile apps can do a range of things, such as extract the nutrition information from a product label or add style transfers, masks, and other effects to a photo. The news was announced today at Google\u2019s I\/O developer conference, held May 8-10, 2018, at the Shoreline Amphitheater in Mountain View, California. The kit is designed to be easy for both novice and advanced developers to employ.\u00a0AI for text recognition,\u00a0face detection,\u00a0barcode scanning, image labeling, and landmark recognition will be available for free on the Firebase console. On-device APIs don\u2019t require a network connection to work.  APIs for smart reply and high-density face contouring will be made available in the coming months. Also available in the cloud only are image labeling and optical character recognition (OCR) for recognizing text on an ad or billboard. Landmark recognition is only available in the cloud. Custom TensorFlow Lite models can also be uploaded through the Firebase console.\u00a0TensorFlow Lite debuted at I\/O last year and launched in developer preview in November. Google hosts the custom TensorFlow Lite models and serves them to your app\u2019s users in order to eliminate the size of AI models from an app\u2019s APK. \u201cIt\u2019s very important,\u201d\u00a0Google product manager Brahim Elbouchikhi told VentureBeat in an interview. \u201cThe ability to dynamically download the model and not have to bundle it into the app, which could be literally it could be 25 MBs of data.\u201d Google is currently experimenting with model compression that would allow a user to upload a full TensorFlow model with training data and receive back a TensorFlow Lite model. The goal of ML Kit, Elbouchikhi said, is to make AI services easy to add to apps. \u201cWe want ML to become just another tool in a developer\u2019s toolkit just like we use cloud storage or analytics or A\/B testing. That\u2019s why it was important to deploy this through Firebase,\u201d he said.  The ML Kit provides app developers with features similar to those Android device users get through Google Assistant. Google\u2019s Lens computer vision began to roll out to Pixel users last fall and has since learned to pull text, email addresses, and phone numbers from images. Lens can also identify landmarks, species of plants, specific animals, and even famous people. Smart Reply, available through ML Kit, is comparable to the suggested reply messages available in Android Messages and Wear OS. The face detection base API is able to determine the size of a smile in an image, a Google spokesperson told VentureBeat in an email. Additional emotion recognition is also in the works,\u00a0Elbouchikhi said. \u201cWe have some stuff down the road that will get into a bit more fine-grained emotion, but that\u2019s not something we\u2019re releasing right now,\u201d\u00a0he said.","time":1525804313,"title":"Google Launches ML Kit for Android and iOS Developers","type":"story","url":"https:\/\/venturebeat.com\/2018\/05\/08\/google-launches-ml-kit-for-android-and-ios-developers\/","label":9,"label_name":"tech"},{"by":"smacktoward","descendants":0,"id":17023416,"kids":"None","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 The Gerald R. Ford, the U.S. Navy\u2019s costliest warship, suffered a new failure at sea that forced it back to port and raised fresh questions about the new class of aircraft carriers. The previously undisclosed problem with a propulsion system bearing, which occurred in January but has yet to be remedied, comes as the Navy is poised to request approval from a supportive Congress to expedite a contract for a fourth carrier in what was to have been a three-ship class. It\u2019s part of a push to expand the Navy\u2019s 284-ship fleet to 355 as soon as the mid-2030s. The U.S.S. Gerald R. Ford It was the second failure in less than a year with a \u201cmain thrust bearing\u201d that\u2019s part of the $12.9 billion carrier\u2019s propulsion system. The first occurred in April 2017, during sea trials a month before the vessel\u2019s delivery. The ship, built by Huntington Ingalls Industries Inc., has been sailing in a shakedown period to test systems and work out bugs. It\u2019s now scheduled to be ready for initial combat duty in 2022. The Naval Sea Systems Command said the Ford experienced \u201can out of specification condition\u201d with a propulsion system component. Huntington Ingalls determined it was due to a \u201cmanufacturing defect,\u201d the command said, and \u201cnot improper operation\u201d by sailors. The defect \u201caffects the same component\u201d located in other parts of the propulsion system, the Navy added. Navy officials didn\u2019t disclose the problem during budget hearings before Congress in recent weeks, and House and Senate lawmakers didn\u2019t ask about it. Shelby Oakley, a director with the U.S. Government Accountability Office who monitors Navy shipbuilding, said the latest part failure was \u201cunfortunate, but this and other ship quality issues are not surprising. The Navy has had issues with the extent of its inspections prior to delivery from the shipbuilder.\u201d The Navy is seeking approval in the fiscal 2019 defense request to accelerate purchase of the fourth Ford-class carrier by bundling it in a contract with the third. It expects to request congressional support over the next month or two for what\u2019s now an estimated $58 billion program. President Donald Trump promised the \u201c12-carrier Navy we need,\u201d up from 11 today, when he stood on the Ford\u2019s vast deck during a visit in March 2017 to Newport News, Virginia, where Huntington Ingalls built the ship and is headquartered. The Ford\u2019s propulsion system flaws are separate from reliability issues on its troubled aircraft launch and recovery system and less publicized delays with its 11 advanced weapons elevators for moving munitions, which are not yet operational. In the January incident, the bearing overheated to what a March 8 Navy memo described as \"92 degrees Fahrenheit above the bearing temperature setpoint\u201d and \u201cafter securing the equipment to prevent damage, the ship safely returned to port.\" A failure review board is identifying \u201cmodifications required to preclude recurrence,\u201d it said. The bearing is one of four that transfers thrust from the ship\u2019s four propeller shafts. The Navy and Huntington Ingalls \u201care evaluating the case for a claim against the manufacturer,\u201d so the amount of repair costs to be paid by \u201cthe manufacturer has not yet been determined,\u201d William Couch, a spokesman for the Sea Systems Command, said in the statement. It\u2019s \u201cencouraging that the Navy wants to hold the manufacturer accountable, however, it is unclear what warranty provisions the Navy has,\u201d Oakley said. \u201cThe Navy has a cost-reimbursement contract with the shipbuilder, where the Navy pays the shipbuilder\u2019s costs in exchange for its best efforts to build the ship, and also did not have a warranty with the shipbuilder.\u201d Couch and Huntington Ingalls spokesman Beci Brenton declined to say who made the bearing that failed. But General Electric Co. is responsible for the propulsion system part, and the Navy program office said in an assessment that an inspection of the carrier\u2019s four main thrust bearings after the January failure revealed \u201cmachining errors\u201d by GE workers at a Lynn, Massachusetts, facility \u201cduring the original manufacturing\u201d as \u201cthe actual root cause.\u201d Deborah Case, a GE spokeswoman, said in an email that \u201cGE did produce the gears for the CVN-78. However, we are no longer producing gears for CVN-78\u201d and \u201cwe cannot comment on the investigation.\u201d The CVN-78 is the official name of the Gerald R. Ford. Couch said defects \u201cwill be fully corrected\u201d during the ship\u2019s upcoming \u201cpost-shakedown availability\u201d phase. All vessels go through the phase intended for correcting deficiencies discovered during the post-delivery sea trial conducted by sailors. The post-shakedown availability was supposed to start last month and end in December. Its start is now delayed until this summer in part because of the failure, with completion about a year later, according to Couch.","time":1525804291,"title":"Carrier Suffers New Failure at Sea as U.S. Navy Seeks More Funds","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-08\/carrier-suffers-new-failure-at-sea-as-u-s-navy-seeks-more-funds","label":7,"label_name":"random"},{"by":"rbanffy","descendants":1,"id":17023415,"kids":"[17023498]","score":1,"text":"\n\n\n\n Gmail Email makes it easy to share information with just about anyone\u2014friends, colleagues and family\u2014but drafting a message can take some time. Last year, we introduced Smart Reply in Gmail to help you quickly reply to incoming emails. Today, we're announcing Smart Compose, a new feature powered by artificial intelligence, to help you draft emails from scratch, faster. From your greeting to your closing (and common phrases in between), Smart Compose suggests complete sentences in your emails so that you can draft them with ease. Because it operates in the background, you can write an email like you normally would, and Smart Compose will offer suggestions as you type. When you see a suggestion that you like, click the \u201ctab\u201d button to use it. Smart Compose helps save you time by cutting back on repetitive writing, while reducing the chance of spelling and grammatical errors. It can even suggest relevant contextual phrases. For example, if it's Friday it may suggest \"Have a great weekend!\" as a closing phrase. Over the next few weeks, Smart Compose will appear in the new Gmail for consumers, and will be made available for G Suite customers in the workplace in the coming months.     To get started, make sure you\u2019ve enabled the new Gmail by going to Settings > \u201cTry the new Gmail.\u201d Next, go to the general tab in your settings, scroll down and enable \u201cexperimental access.\u201d If you want to switch back, you can always uncheck the box. \n              Follow Us\n            ","time":1525804286,"title":"Write Emails Faster with Smart Compose in Gmail","type":"story","url":"https:\/\/blog.google\/products\/gmail\/subject-write-emails-faster-smart-compose-gmail\/","label":7,"label_name":"random"},{"by":"jdtabish","descendants":0,"id":17023390,"kids":"None","score":3,"text":"TL;DR the Senate is about to vote on a resolution to save net neutrality, so put your websites, subreddits, apps, and social accounts on Red Alert starting on May 9th. Look, I know everyone is tired of hearing about this. I\u2019m tired of writing posts about it. But the fight for net neutrality is NOT over, and this upcoming Senate vote on the Congressional Review Act (CRA) resolution to block the Ajit Pai\u2019s repeal is going to define the battle field that we\u2019re fighting on possibly for the next decade. The ISPs army of lobbyists are hoping that we\u2019re tired. They\u2019re hoping that we\u2019ve become apathetic and have heard too many times that this is the \u201clast chance\u201d to save net neutrality. The media isn\u2019t helping. They ran incorrect headlines on April 23rd saying \u201cNet neutrality is officially dead.\u201d Well, it isn\u2019t. And there is a real path to saving it. On May 9th, net neutrality supporters in the Senate will file a \u201cdischarge petition\u201d that will force a vote to the Senate floor, likely a few days later. WE NEED TO WIN THAT VOTE. It only takes a simple majority, 51 Senators, and we already have 50 committed. But we don\u2019t need to just win, we need to win with a landslide and pick up more Republican support, so we can show the world that net neutrality is NOT a partisan issue as we take the fight to the House. We\u2019re close to making that happen. A half a dozen GOP Senators have indicated that they\u2019re considering it. Look, we all know we can\u2019t trust politicians of either party. We all know that yes, they\u2019re beholden to corporate Interests. But we also know that they\u2019re not immune to public pressure. They are ELECTED officials after all. And the polls show that the overwhelming majority of voters, including Republicans and Independents, support net neutrality and want them to vote to bring it back. So if we can harness the power of the Internet to sound the alarm and get EVERYONE to flood lawmakers with real phone calls and emails from their constituents right before the vote, we can win. That\u2019s why starting May 9th we\u2019re calling on the Internet to \u201cgo red\u201d to sound the alarm. Put your website, subreddit, or social media profile on \u201cred alert\u201d using the tools here. Reddit, Etsy, Vimeo, Imgur, Ok Cupid, Foursquare, Pornhub, Mozilla, Private Internet Access, Bittorrent, Fark, BoingBoing, Namecheap, Gandi, DuckDuckGo, FreeMusicArchive, and a ton of other big sites are participating. But this isn\u2019t about the big companies. It\u2019s about the entire Internet coming together. Are you with us? Let\u2019s fight like we mean it! By clapping more or less, you can signal to us which stories really stand out. We believe there's hardly anything as important as ensuring that our shared future has freedom of expression and creativity at its core.","time":1525804134,"title":"The Senate is about to vote on net neutrality and it\u2019s time to pay attention","type":"story","url":"https:\/\/medium.com\/@fightfortheftr\/i-know-youre-tired-of-hearing-about-net-neutrality-ba2ef1c51939","label":7,"label_name":"random"},{"by":"el_duderino","descendants":15,"id":17023377,"kids":"[17023663, 17023545, 17023867, 17024118, 17023727, 17023558]","score":36,"text":"If you want to get an early look at the next version of Android, now you have the chance. Google is launching the public beta for Android P today, which you\u2019ll be able to sign up for at this link. And in a nice twist, the beta is open to more than just Google-made phones this year: you\u2019ll be able to get it on the Essential Phone, Oppo\u2019s R15 Pro, Nokia\u2019s 7 Plus, Sony\u2019s Xperia XZ2, Xiaomi\u2019s Mi Mix 2S, Vivo\u2019s X21, the still-unreleased OnePlus 6, and of course the Pixel and Pixel 2. Depending on what phone you have, you\u2019ll have to take different steps to get started with the beta; it\u2019ll be easiest on a Pixel, which just lets you download it through the existing software update feature. The downside of testing is \u2014 and this is a big warning \u2014 once you\u2019re in the beta program, there\u2019s no going back. Not easily, at least: you\u2019ll have to wipe your phone if you want to opt out and return to stable releases of Android Oreo. Keep in mind that you\u2019re likely to encounter some bugs and probably shouldn\u2019t try this on the phone you use daily. \n    Related\n   Android P includes some major new features that ought to make your phone a little bit less stressful to use. The update allows you to set time limits on apps, switch your phone to grayscale at night, and easily hide all notifications when you\u2019re on vacation, going to bed, or just don\u2019t want to see them. On top of that, the update includes a rethought control scheme, which meaningfully updates Android\u2019s back \/ home \/ multitasking buttons for the first time in years. Command Line delivers daily updates from the near-future.","time":1525804086,"title":"Android P launches today in public beta","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17320826\/android-p-beta-public-download-link-opt-in-sign-up-io-2018","label":9,"label_name":"tech"},{"by":"TheAvengers","descendants":0,"id":17023372,"kids":"None","score":1,"text":"","time":1525804067,"title":"UTRUST and Trustroot team up to increase confidence in cryptocurrency payments","type":"story","url":"https:\/\/medium.com\/@UTRUST\/utrust-announces-partnership-with-leading-blockchain-security-solution-trustroot-to-secure-4406019b6cd4","label":7,"label_name":"random"},{"by":"rrauenza","descendants":0,"id":17023365,"kids":"None","score":2,"text":"After more than 40 years of widespread use, new scientific tests show formulated weedkillers have higher rates of toxicity to human cells  \nCarey Gillam \n\nTue 8 May 2018 01.00\u00a0EDT\n\n\nLast modified on Tue 8 May 2018 16.11\u00a0EDT\n\n US government researchers have uncovered evidence that some popular weedkilling products, like Monsanto\u2019s widely-used Roundup, are potentially more toxic to human cells than their active ingredient is by itself. These \u201cformulated\u201d weedkillers are commonly used in agriculture, leaving residues in food and water, as well as public spaces such as golf courses, parks and children\u2019s playgrounds.  The tests are part of the US National Toxicology Program\u2019s (NTP) first-ever examination of herbicide formulations made with the active ingredient glyphosate, but that also include other chemicals. While regulators have previously required extensive testing of glyphosate in isolation, government scientists have not fully examined the toxicity of the more complex products sold to consumers, farmers and others.  Monsanto introduced its glyphosate-based Roundup brand in 1974. But it is only now, after more than 40 years of widespread use, that the government is investigating the toxicity of \u201cglyphosate-based herbicides\u201d on human cells. The NTP tests were requested by the Environmental Protection Agency (EPA) after the International Agency for Research on Cancer (IARC) in 2015 classified glyphosate as a probable human carcinogen. The IARC also highlighted concerns about formulations which combine glyphosate with other ingredients to enhance weed killing effectiveness. Monsanto and rivals sell hundreds of these products around the world in a market valued at roughly $9bn. Mike DeVito, acting chief of the National Toxicology Program Laboratory, told the Guardian the agency\u2019s work is ongoing but its early findings are clear on one key point. \u201cWe see the formulations are much more toxic. The formulations were killing the cells. The glyphosate really didn\u2019t do it,\u201d DeVito said.  A summary of the NTP work stated that glyphosate formulations decreased human cell \u201cviability\u201d, disrupting cell membranes. Cell viability was \u201csignificantly altered\u201d by the formulations, it stated. DeVito said the NTP first-phase results do not mean the formulations are causing cancer or any other disease. While the work does show enhanced toxicity from the formulations, and show they kill human cells, the NTP appears to contradict an IARC finding that glyphosate and\/or its formulations induce oxidative stress, one potential pathway toward cancer. The government still must do other testing, including examining any toxic impact on a cell\u2019s genetic material, to help add to the understanding of risks, according to DeVito.  The NTP work informs a global debate over whether or not these glyphosate-based weedkilling chemical combinations are endangering people who are exposed. More than 4,000 people are currently suing Monsanto alleging they developed cancer from using Roundup, and several European countries are moving to limit the use of these herbicides. \u201cThis testing is important, because the EPA has only been looking at the active ingredient. But it\u2019s the formulations that people are exposed to on their lawns and gardens, where they play and in their food,\u201d said Jennifer Sass, a scientist with the Natural Resources Defense Council.  One problem government scientists have run into is corporate secrecy about the ingredients mixed with glyphosate in their products. Documents obtained through Freedom of Information Act requests show uncertainty within the EPA over Roundup formulations and how those formulations have changed over the last three decades.  That confusion has continued with the NTP testing. \u201cWe don\u2019t know what the formulation is. That is confidential business information,\u201d DeVito said. NTP scientists sourced some samples from store shelves, picking up products the EPA told them were the top sellers, he said. It is not clear how much Monsanto itself knows about the toxicity of the full formulations it sells. But internal company emails dating back 16 years, which emerged in a court case last year, offer a glimpse into the company\u2019s view. In one 2003 internal company email, a Monsanto scientist stated: \u201cYou cannot say that Roundup is not a carcinogen \u2026 we have not done the necessary testing on the formulation to make that statement. The testing on the formulations are not anywhere near the level of the active ingredient.\u201d  Another internal email, written in 2010, said: \u201cWith regards to the carcinogenicity of our formulations we don\u2019t have such testing on them directly.\u201d And an internal Monsanto email from 2002 stated: \u201cGlyphosate is OK but the formulated product \u2026 does the damage.\u201d Monsanto did not respond to a request for comment. But in a 43-page report, the company says the safety of its herbicides is supported by \u201cone of the most extensive worldwide human health and environmental databases ever compiled for a pesticide product\u201d. Carey Gillam is a journalist and author, and a public interest researcher for US Right to Know, a non-profit food industry research group.","time":1525804032,"title":"Weedkiller products more toxic than their active ingredient, tests show","type":"story","url":"https:\/\/www.theguardian.com\/us-news\/2018\/may\/08\/weedkiller-tests-monsanto-health-dangers-active-ingredient","label":7,"label_name":"random"},{"by":"lobo_tuerto","descendants":0,"id":17023333,"kids":"None","score":2,"text":"Adventures of Huckleberry Finn, The\nby Mark Twain Adventures of Tom Sawyer, The\nby Mark Twain Aesop\u2019s Fables \nby Aesop Agnes Grey \nby Anne Bront\u00eb Alice\u2019s Adventures in Wonderland \nby Lewis Caroll Andersen\u2019s Fairy Tales \nby Hans Christian Andersen Anna Karenina \nby Leo Tolstoy Anne of Green Gables\nby Lucy Maud Montgomery Around the World in 80 Days \nby Jules Verne Beyond Good and Evil \nby Friedrich Nietzsche Bleak House \nby Charles Dickens Brothers Karamazov, The\nby Fyodor Dostoevsky Christmas Carol, A \nby Charles Dickens Crime and Punishment\nby Fyodor Dostoevsky David Copperfield \nby Charles Dickens Down and Out in Paris and London \nby George Orwell Dracula \nby Bram Stoker Dubliners \nby James Joyce Emma \nby Jane Austen Erewhon \nby Samuel Butler For the Term of His Natural Life \nby Marcus Clarke Frankenstein \nby Mary Shelley Great Expectations\nby Charles Dickens Great Gatsby, The\nby F. Scott Fitzgerald Grimms Fairy Tales \nby The Brothers Grimm Gulliver\u2019s Travels \nby Jonathan Swift Heart of Darkness \nby Joseph Conrad Hound of the Baskervilles, The\nby Arthur Conan Doyle Idiot, The\nby Fyodor Dostoevsky Iliad, The\nby Homer Island of Doctor Moreau, The\nby H. G. Wells Jane Eyre \nby Charlotte Bront\u00eb Jungle Book, The\nby Rudyard Kipling Kidnapped\nby Robert Louis Stevenson Lady Chatterly\u2019s Lover \nby D. H. Lawrence Last of the Mohicans, The\nby James Fenimore Cooper Legend of Sleepy Hollow, The\nby Washington Irving Les Miserables \nby Victor Hugo Little Women \nby Louisa May Alcott Madame Bovary \nby Gustave Flaubert Merry Adventures of Robin Hood, The\nby Howard Pyle Metamorphosis, The\nby Franz Kafka Middlemarch \nby George Eliot Moby Dick \nby Herman Melville 1984\nby George Orwell Northanger Abbey \nby Jane Austen Nostromo: A Tale of the Seaboard \nby Joseph Conrad Notes from the Underground \nby Fyodor Dostoevsky Odyssey, The\nby Homer Of Human Bondage \nby W. Somerset Maugham Oliver Twist \nby Charles Dickens Paradise Lost \nby John Milton Persuasion \nby Jane Austen Picture of Dorian Gray, The\nby Oscar Wilde Pollyanna \nby Eleanor H. Porter Portrait of a Lady, The\nby Henry James Portrait of the Artist as a Young Man, A\nby James Joyce Pride and Prejudice \nby Jane Austen Prince, The\nby Nicolo Machiavelli Robinson Crusoe \nby Daniel Defoe Scarlet Pimpernel, The\nby Baroness Orczy Sense and Sensibility\nby Jane Austen Sons and Lovers \nby D. H. Lawrence Strange Case of Dr Jekyll and Mr Hyde, The\nby Robert Louis Stevenson Swanns Way \nby Marcel Proust Tale of Two Cities, A\nby Charles Dickens Tales of Mother Goose, The\nby Charles Perrault Tarzan of the Apes \nby Edgar Rice Burroughs Tender is the Night \nby F. Scott Fitzgerald Tess of the d\u2019Urbervilles \nby Thomas Hardy Thirty Nine Steps, The\nby John Buchan Three Musketeers, The\nby Alexandre Duma Time Machine, The\nby H. G. Wells Treasure Island \nby Robert Louis Stevenson Trial, The\nby Franz Kafka Ulysses \nby James Joyce Utopia \nby Sir Thomas More Vanity Fair \nby William Makepeace Thackeray War and Peace\nby Leo Tolstoy Within A Budding Grove \nby Marcel Proust Women In Love \nby D. H. Lawrence Wuthering Heights \nby Emily Bront\u00eb \n\n\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n ","time":1525803844,"title":"Planet EBook \u2013 100% Free Literature for Win, Mac, iOS, Android and Kindle","type":"story","url":"https:\/\/www.planetebook.com\/","label":7,"label_name":"random"},{"by":"lhenry","descendants":0,"id":17023329,"kids":"None","score":2,"text":"The official source of product insight from the Visual Studio Engineering Team  Today, at Build 2018, we announced a preview of the Google Android emulator that\u2019s compatible with Hyper-V, available on the Windows 10 April 2018 Update. This enables developers with Hyper-V enabled on their machines to use a hardware accelerated Android emulator, without needing to switch to Intel\u2019s HAXM hypervisor. Amazing work was done by the Windows Hyper-V team, with help from the Xamarin team, to make to this happen. Today\u2019s preview means you can use Google\u2019s Android emulator side-by-side with other Hyper-V based technologies, including Hyper-V VMs, Docker tooling, the HoloLens emulator, and more. This means that any Android developer on Windows, who also uses Hyper-V, can use a fast Android emulator that will always support the latest Android APIs, works with Google Play Services out of the box, and works with all features in the Android emulator, including camera, geolocation, and Quick Boot. The Windows Hypervisor Platform was introduced in the Windows 10 April 2018 Update and enables third-party virtualization stacks to utilize the Windows Hypervisor for hardware acceleration. If you are using Hyper-V, this stack replaces Intel HAXM as the hypervisor for the Android emulator. Support for using the Windows Hypervisor as an accelerator for the Android emulator is currently in preview and requires the Windows 10 April 2018 Update. Here are the steps to get it installed. Open Turn Windows features on or off and select Hyper-V and the Windows Hypervisor Platform checkboxes. For these features to take effect, you will need to restart your machine.  To enable IDE support for the Android emulator, such as debugging, you must install an updated preview of the Visual Studio Tools for Xamarin. First, ensure you have Visual Studio 2017 version 15.8 Preview 1 or higher with the Mobile development with .NET (Xamarin) workload installed. Download and open the preview installer. Click Install. In Visual Studio, select Tools > Options > Android > Android SDK Manager. Click the Tools tab, select the Android Emulator component, and select Apply Changes.  Now when you debug your Android apps, you can use the latest Android SDKs in a fast emulator, right from Visual Studio alongside Hyper-V based technologies like Docker. We need your help to make using the Google Android emulator with Hyper-V an amazing experience. Be sure to share your feedback in Visual Studio by going to Help > Send Feedback > Report a Problem if you experience any problems or strange behavior. Please provide the following information in your bug report: For more information on configuring the Hyper-V emulator, and for a listing of known issues, visit our documentation. Miguel Miguel is a Distinguished Engineer at Microsoft, focused on the mobile platform and creating delightful developer tools. With Nat Friedman, he co-founded both Xamarin in 2011 and Ximian in 1999. Before that, Miguel co-founded the GNOME project in 1997 and has directed the Mono project since its creation in 2001, including multiple Mono releases at Novell. Miguel has received the Free Software Foundation 1999 Free Software Award, the MIT Technology Review Innovator of the Year Award in 1999, and was named one of Time Magazine\u2019s 100 innovators for the new century in September 2000.  Name *  Email *  Website   \n\n   Woohoo!  This is great to see.  I have had soooooo many problems trying to install Xamarin\/Droid workloads.  This sounds like the answer to the problem I have been having here:\nhttps:\/\/forums.xamarin.com\/discussion\/121476\/android-application-does-not-deploy-to-emulator Cool !!! Feature have been waited for long time Does it also work in Windows Server 2016 and beyond? Thank you, this is so awaited! I dont get it. Xamarin gave up on its Android emulator but now emulator it\u2019s back?\nWhy there\u2019s nothing mentioned about this? does this also Support the \u201cwarm\/quick boot\u201d of the Emulator? I never got this working with the normal Emulator even if I select it the Emulator always does a col boot, is slow to start and so deployment fails. \u00a9 2018 Microsoft Corporation.","time":1525803819,"title":"Microsoft Hyper-V Support for Android Emulator","type":"story","url":"https:\/\/blogs.msdn.microsoft.com\/visualstudio\/2018\/05\/08\/hyper-v-android-emulator-support\/","label":9,"label_name":"tech"},{"by":"philipashlock","descendants":0,"id":17023325,"kids":"None","score":1,"text":"The White House on Thursday plans to convene\u00a0 executives from Amazon, Facebook, Google, Intel and 34 other major U.S. companies as it seeks to supercharge the deployment of powerful robots, algorithms and the broader field of artificial intelligence. The Trump administration intends to ask academics, government officials and AI developers about ways to adapt regulations to advance AI in such fields as agriculture, health care and\u00a0transportation, according to a draft schedule of the event. And they\u2019re set to discuss the U.S. government\u2019s power to fund cutting-edge research into such technologies as machine learning. For the White House, the challenge is to strike a balance between the benefits of computers that can spot disease or drive cars and the reality that jobs \u2013 or lives \u2013 are at stake in the age of AI. \u201cWhether you\u2019re a farmer in Iowa, an energy producer in Texas, a drug manufacturer in Boston, you are going to be using these techniques to drive your business going forward,\u201d Michael Kratsios, deputy chief technology officer at the White House, said in a recent interview. Among those expected to be in the room for that private gathering Thursday will be representatives from tech giants like Microsoft, Nvidia and Oracle, as well as other businesses like Ford, Land O\u2019Lakes, MasterCard, Pfizer and United Airlines, according to the White House. Slated to represent Facebook is Jerome Pesenti, its vice president of AI, the company confirmed. Amazon plans to send Rohit Prasad, the head scientist for its voice-assistant Alexa. Intel chief executive Brian Krzanich is also expected to attend. By the Trump administration\u2019s own estimate, the U.S. government spent more than $2 billion in unclassified programs alone during the 2017 fiscal year to research and develop AI technology, according to data furnished this week by the White House\u2019s Office of Science and Technology Policy. That doesn\u2019t include spending at the Pentagon and key intelligence agencies far removed from public view, or additional boosts that the White House has sought for 2019. Still, many experts said they would ask the Trump administration this week to dedicate new federal dollars to fuel the field and help them compete with firms in other countries, particularly China, now seeking to incubate their own advancements in AI. A key focus is jobs \u2013 training workers for new tech-heavy roles, as well as helping those who may eventually be displaced because of automation. \u201cWe do believe in the short and medium term there will be job losses,\u201d said Paul Daugherty, the chief technology\u00a0and innovation officer of management-consulting firm Accenture, who is scheduled to attend the Thursday gathering. \u201cThere will be new jobs available, but the real challenge: if we can match people up and train them in an appropriate way.\u201d AI encompasses powerful technologies. It guides the self-driving cars offered by Uber and Google-owned Waymo. It\u2019s the software behind smartphone voice assistants like Apple\u2019s Siri and the facial-recognition tools on Facebook. And its ability to interpret large amounts of data is also being tapped in new, unconventional ways, including efforts to reshape workforce recruiting, farming and food production. About\u00a0a year ago, the Trump administration publicly had sounded a different note on AI. Steven Mnuchin, who had just been tapped as treasury secretary, told reporters that artificial intelligence was \u201cnot even on my radar screen,\u201d stressing that the real use and implication of those tools were \u201c50 to 100 more years\u201d on the horizon. The remark drew swift condemnation throughout the tech industry just months after the outgoing Obama administration had published two papers on AI policy. Tech companies had \u201ca lot of bad blood\u201d with the Trump administration in the early months, said Pedro Domingos, an AI researcher and University of Washington professor who wrote \"The Master Algorithm.\" President Trump\u2019s focus on job loss in non-tech industries \u2014 and his attacks on companies like Amazon \u2014 didn\u2019t help, Domingos said. (Amazon chief executive Jeffrey P. Bezos owns The Washington Post.) Over the past year, however, the science and tech aides in the White House\u2019s ranks have sought to chart their own course on AI. They last hosted leading executives from Apple, Amazon and Google at the White House in June, when Trump committed generally to removing regulatory barriers facing emerging technology. At the White House, Trump in September signed a policy directive carving out $200 million for coding education grants that could help prepare students for jobs in science, tech and engineering fields. Federal agencies like the Department of Transportation, meanwhile, have sought in recent rules to help deploy more self-driving cars on U.S. roads. Later this week, the agency is also expected to grant permission to as many as 10 cities and states to test more autonomous drones, potentially including those from Amazon and Google that would deliver packages. The move comes a week after another agency, the Food and Drug Administration, approved an AI device to scan diabetic patients for a related eye disease \u2014 the first such approval of its kind. \u201cI think it\u2019s been a slow ramp [up], but they\u2019re now focused on the right things,\u201d said Dean Garfield, the president of the Information Technology Industry Council, which represents companies like Apple, Facebook and Google. He said the White House\u2019s upcoming meeting \u201cis an opportunity to accelerate toward success and leave that slow start in the rearview mirror.\u201d Beyond public research funding, tech firms believe Washington could play a critical role in helping educate American workers on the ways AI will reshape jobs and create new ones \u2014 including in factories, where AI and robotics have helped drive a resurgence of domestic manufacturing. \u201cThere are a lot of people who say that AI is going to be about job destruction. It is not. It is going to be about job movement,\u201d said ITI\u2019s Garfield. \u201cSo I think the White House is actually in a unique place to mobilize a movement that\u2019s necessary to prepare the American workforce for what will be here in the next 20 years, and that needs to be comprehensive and strategic.\u201d For many in the industry, though, AI sits at the center of a high-stakes technological duel between the United States and China. In July, leaders in Beijing announced a plan to incubate a local AI industry valued at about $150 billion by 2030, relying in no small part on government investment. To that end, recent reports have suggested the White House is exploring policy options to restrict U.S. firms from partnering with China on research in artificial intelligence. Some companies are emboldened by the assertive steps the Trump administration has taken toward China in criticizing intellectual-property theft and the restrictive rules tech companies must follow to operate in the country. \"They think, 'Hey, we are competing with these Chinese companies, and they're getting an unfair kind of help from the government that we're not getting,\u2019 \u201d Domingos said. A spokesman for the administration\u2019s Office of Science and Technology Policy declined to comment on the reports. In December, though, the president\u2019s official National Security Strategy warned that \"risks to U.S. national security will grow as competitors integrate information derived from personal and commercial sources with intelligence collection and data analytic capabilities based on artificial intelligence and machine learning.\" For now, a source familiar with the White House\u2019s event said the administration is considering a different set of proposals, including efforts to make more data available for AI research. Many companies long have asked the government to make more of its information available. \u201cOur free-market approach to scientific discovery harnesses the combined strengths of government, industry and academia, resulting in the most successful innovation machine the world has ever known,\u201d Kratsios said.","time":1525803778,"title":"White House to host major companies for summit on AI","type":"story","url":"https:\/\/www.washingtonpost.com\/news\/the-switch\/wp\/2018\/05\/08\/white-house-will-host-amazon-facebook-ford-and-other-major-companies-for-summit-on-ai\/","label":5,"label_name":"ml"},{"by":"hbcondo714","descendants":0,"id":17023321,"kids":"None","score":1,"text":"\u201cWhat are some cool websites where you can download free stuff?\u201d asked redditor howtoadvanced. He got over 5,000 answers. We\u2019ve gone through and categorized the best ones, identifying each resource. You could spend the rest of your life just entertaining and educating yourself with the free books, music, games, apps, and other freebies available on the sites listed here. (Don\u2019t go cancelling your Netflix, but maybe stop paying Amazon for books that Project Gutenberg hands out for free.) Unless noted (and with the possible exception of some \u201cabandonware\u201d sites) these are legal and legitimate resources, but some are only for personal use. Always check before you use a resource commercially or publicly. Advertisement  I\u2019m always trying to save a few bucks when stocking up on books for my beat-up Kindle or my iPad.\u2026 Advertisement  (special thanks to redditor chinchalinchin) You know how to tell if something controversial is actually true, but what if you want to read up\u2026 Advertisement  Advertisement  Windows\/Mac\/Linux: While Steam has its share of indie titles, it doesn\u2019t have everything. If you\u2019re \u2026 Finding decent, non-stock-y images that are free to use can sometimes be a frustrating search. \u2026 Advertisement  Dear Lifehacker,\nI've learned to code and want to start using GitHub to manage my projects.\u2026 Advertisement  Much to Taylor Swift\u2019s chagrin, music streaming subscriptions are getting cheaper and cheaper these \u2026 Advertisement  Some people avoid any and all update notices\u2014and you can't always blame them, given how many\u2026 Advertisement  Advertisement  Libraries are a pretty awesome resource. They\u2019re full of tools, information, and materials anyone\u2026 Advertisement  3D printing is a great way to create something truly personal, make your awesome ideas reality, or\u2026 Probably illegal. Proceed with caution and\/or guilt. There are more BitTorrent clients than we could possibly compare, but some of the most popular\u2014and\u2026 Advertisement  Find more free (and legal) resources at allOPEN.org, a collection curated by redditor corydave. What are some cool websites where you can download free stuff? | AskReddit Staff Writer, Lifehacker | Nick has been writing online for 11 years at sites like Urlesque, Gawker, the Daily Dot, and Slacktory. He lives in Park Slope with his wife and their books.","time":1525803766,"title":"Download Free Stuff from Reddit's Favorite Websites","type":"story","url":"https:\/\/lifehacker.com\/download-free-stuff-from-reddits-favorite-websites-1825781590","label":7,"label_name":"random"},{"by":"rdslw","descendants":6,"id":17023314,"kids":"[17023621, 17023578, 17023610, 17023618]","score":21,"text":"Ten years ago, at the 2008 RSA Conference, Yubico launched the first YubiKey with the goal of making secure login easy and accessible for everyone. The vision was one single security key to work across any number of services, with great user experience, security, and privacy.  On this anniversary, Yubico has taken another major leap forward toward this vision with the announcement that the recently-launched Security Key by Yubico, with FIDO2, will be supported in Windows 10 devices and Microsoft Azure Active Directory (Azure AD). The feature is currently in limited preview for Microsoft Technology Adoption Program (TAP) customers.  FIDO2 is the passwordless evolution of the FIDO Universal 2nd Factor (U2F) standard, created by Yubico and Google. While U2F included a username and password, FIDO2 supports more use cases, including passwordless authentication. Yubico has worked in close collaboration with Microsoft on developing the FIDO2 technical specifications, and the Security Key by Yubico is the first FIDO2 authentication device on the market. What Does This Mean?  Organizations will soon have the option to enable employees and customers to sign in to an Azure AD joined device with no password, by simply using a Security Key to get single sign-on to all Azure AD based applications and services. This is just the beginning; Google and Mozilla also announced Chrome and Firefox support for the Web Authentication API (WebAuthn) developed by Yubico and members of the World Wide Web Consortium (W3C) and included in the FIDO2 specification. \n  Why Is This Important?  Nearly every digital experience today requires passwords, an increasingly frustrating fact of life for businesses and users. For any one person there can be hundreds of sites and devices \u2014 both personal and business related \u2014 that require memorized passwords. This leads to poor password hygiene: shared and reused passwords. And it is a real cost for businesses managing, storing and resetting passwords for employees and end-users.  Working in conjunction with Windows and Microsoft cloud services, the new Security Key by Yubico offers a secure, seamless and passwordless login experience with one of the world\u2019s largest computer operating systems. Use cases include retail, healthcare, transportation, finance, manufacturing, and more.  How Does It Work?  FIDO2 is built on the same security and privacy features of FIDO U2F: strong public key cryptography, no drivers or client software and one key for unlimited account access with no shared secrets. With FIDO U2F, the user entered a username and password, inserted a \u00a0security key in the USB-port, and touched the gold area. FIDO2 adds more options to the login process:  Who Can Get Involved? Everyone is encouraged to get involved, and accelerate progress to a secure and passwordless world. As with any open standard, advancement will be a collective industry effort and a process of global adoption. Yubico helped the majority of services in making support for FIDO U2F by providing open source code and support. Together with W3C and FIDO Alliance we have made the FIDO2 open authentication standard available, and we are helping support its rapid integration into services and applications through our new Yubico Developer Program. Enterprises \u2192 Learn about using FIDO2 with Windows 10 devices and Microsoft Azure Active Directory in your enterprise environment. Explore the benefits of FIDO2.  Developers \u2192 Implement early support for FIDO2 by signing up for updates from Yubico\u2019s Developer Program. Members will have first access to resources to implement FIDO2 within their applications and services.  Individuals \u2192 Are you tired of passwords? If you had a choice to securely and easily login to any device or online service without them, would you? Ask for it! Visit your favorite service or businesses on Twitter and tell them you want to securely login to your account without a password by using FIDO2 and the Security Key by @Yubico! Are you interested in learning more about a life without passwords? Learn more about the Security Key by Yubico and benefits of FIDO 2.  Comments are closed. Subscribe to the  Yubico Blog RSS feed Find\nTake Product Finder Quiz Set Up\nFind Set-Up Guides Buy\nBuy Online\nContact Sales\nFind Resellers Stay Connected\nSign up for email  Cookies Legal Privacy Terms of Use EnglishSwedishGermanFrench Yubico \u00a9 2018. All Rights Reserved.","time":1525803744,"title":"Passwordless authentication is here: new Yubico FIDO2 key","type":"story","url":"https:\/\/www.yubico.com\/2018\/04\/yubico-and-microsoft-introduce-passwordless-login\/?source=me","label":3,"label_name":"dev"},{"by":"m1","descendants":10,"id":17023306,"kids":"[17024338, 17024484, 17024263, 17024098, 17024764]","score":19,"text":"The president said the US will \u2018exit\u2019 the nuclear agreement in violation of the landmark deal, describing the move as a \u2018withdrawal\u2019 \nAmanda Holpuch in New York \n\nTue 8 May 2018 16.36\u00a0EDT\n\n\n\nFirst published on Tue 8 May 2018 12.02\u00a0EDT\n\n \n\n4.32pm EDT\n16:32\n\n Oliver Holmes Syrian air defences downed two Israeli missiles near Damascus, state media reported, after explosions were heard at a military base south of the capital. State news agency, SANA, cited a military source as saying: \u201cSyrian air defences fired at two Israeli missiles and destroyed them in Kisweh\u201d. The Israeli Defense Forces said earlier on Tuesday evening that it had identified \u201cirregular activity of Iranian forces in Syria\u201d and had decided to unlock and ready bomb shelters in the north, where it shares a frontier with Syria.  \u201cAdditionally, defence systems have been deployed and IDF troops are on high alert for an attack,\u201d it said in a statement. \u201cThe IDF is prepared for various scenarios and warns that any aggression against Israel will be met with a severe response.\u201d  Israel has struck Iranian forces operating in Syria several times this year while Tehran has vowed to retaliate. Trump\u2019s decision to withdraw from the Iran nuclear agreement has put the region on edge. Reuters news agency cited a commander loyal to Syrian President Bashar al-Assad as saying the Israeli strike on Tuesday evening caused no casualties. Israel did not immediately comment on the reported strike. \n\n4.22pm EDT\n16:22\n\n Here\u2019s video from Trump\u2019s announcement that the US would torpedo the Iran nuclear deal. \u201cAmerica will not be held hostage to nuclear blackmail,\u201d he said. \n\n4.16pm EDT\n16:16\n\n Trump\u2019s decision to violate the Iran agreement could potentially trigger a new crisis in the Gulf. And while European US allies have said they will stay in the agreement, but it is not clear how that will be possible in the face of the sanctions that Trump has reintroduced, targeting companies around the world for doing business with Iran. Karim Sadjadpour, a senior fellow at the Carnegie Endowment for International Peace, warned: \u201cBy withdrawing from the JCPOA, Trump hastens the possibility of three disparate but similarly cataclysmic events: an Iranian war, an Iranian bomb, or the implosion of the Iranian regime.\u201d \u201cIran looms large over major US national security concerns including Syria, Iraq, Afghanistan, cyber, energy security, terrorism, & obviously nuclear proliferation,\u201d Sadjadpour said in a tweet. \u201cThe opportunities for direct conflict are numerous.\u201d \n\n3.43pm EDT\n15:43\n\n Former US president Barack Obama, whose adminstration successfully negotiated the Iran deal, said Trump\u2019s announcement was \u201cmisguided.\u201d \u201cAt a time when we are all rooting for diplomacy with North Korea to succeed, walking away from the JCPOA risks losing a deal that accomplishes \u2013 with Iran \u2013 the very outcome that we are pursuing with the North Koreans,\u201d Obama said.  \u201cThat is why today\u2019s announcement is so misguided. Walking away from the JCPOA turns our back on America\u2019s closest allies, and an agreement that our country\u2019s leading diplomats, scientists, and intelligence professionals negotiated. In a democracy, there will always be changes in policies and priorities from one Administration to the next. But the consistent flouting of agreements that our country is a party to risks eroding America\u2019s credibility, and puts us at odds with the world\u2019s major powers.\u201d He also provided a five point rebuttal to Trump adminstration criticisms of the Iran agreement, including Trump\u2019s claim Iran was building a nuclear program in violation of the agreement. \u201cThe JCPOA does not rely on trust \u2013 it is rooted in the most far-reaching inspections and verification regime ever negotiated in an arms control deal,\u201d Obama said.  \u201cBecause of these facts, I believe that the decision to put the JCPOA at risk without any Iranian violation of the deal is a serious mistake,\u201d Obama said. \u201cWithout the JCPOA, the United States could eventually be left with a losing choice between a nuclear-armed Iran or another war in the Middle East.\u201d \n\n3.37pm EDT\n15:37\n\n Saeed Kamali Dehghan Ordinary Iranians were on tenterhooks, monitoring the developments closely, particularly any immediate impact on the country\u2019s national currency, which hit an all-time low last month, prompting panic-buying of hard-to-find dollars amid political uncertainty.  Trump\u2019s decision to reimpose sanctions is likely to exacerbate the state of the economy at the time when public discontent is rife. In January, protests over economic grievances that began by the end of last year spread in an spontaneous manner to as many as 80 cities, taking on a political dimension. The unrest resulted in the death of at least 25 protesters and jailing of more than 3000 people - many of whom remain in prison.  Sadeq Zibakalam, a prominent political commentator and professor of politics at Tehran University, struck a pessimistic tone about the consequences of Trump\u2019s decision in Iran. \u201cMany people are worried about war,\u201d he told the Guardian on phone from Tehran. \u201cWhenever the country faces a crisis in its foreign policy or economy, the situation gets better for hardliners, they\u2019d be able to exert their force more easily.\u201d  He added: \u201cAt the same time, hardliners will gain politically from this situation, because they\u2019ll attack reformists and moderates like [President] Rouhani that this is evidence of what they had been saying for years, that the US cannot be trusted, and that US is always prepared to knife you in the back.\u201d  Zibakalam, who is close to the reformists, said he did not think it would take long for Europeans and other nations to follow in the footsteps of the US, because they won\u2019t endanger their economic ties with Washington, which would outweigh the benefits of doing business with Iran.  \u201cIn the short term, the radical faction in Iran will be strengthened, this is good for the hardliners, this will boost their position in Iran\u2019s political system,\u201d he said.  Foad Izadi, a Tehran-based conservative political analyst, said Trump\u2019s aim was to confront ran with greater force. \u201cThis shows that the idea that you can negotiate with the US and reach an agreement won\u2019t bear any fruits, not only he wants to reimpose sanctions that had been suspended, he wants to impose new sanctions.\u201d Izadi said Iran can withstand the pressure. \u201cFor 40 years, we have been living under sanctions. The collapse of the nuclear deal will mean that we have to find ways to circumvent sanctions, something we have done in the past. The institutions that have been doing that now need to be reequipped.\u201d \u201cWhen relations between Iran and the US goes down from bad to worse, the risk of confrontation and war, particularly in the region, goes up,\u201d Izadi added.  Trita Parsi, president of the National Iranian American Council (NIAC), said Trump\u2019s \u201creckless decision\u201d puts the US on path to war with Iran.  \u201cDonald Trump has committed what will go down as one of the greatest acts of self-sabotage in America\u2019s modern history. He has put the United States on a path towards war with Iran and may trigger a wider regional war and nuclear arms race,\u201d he said. Updated\nat 4.36pm EDT\n \n\n3.22pm EDT\n15:22\n\n UK prime minister Theresa May, German chancellor Angela Merkel and French president Emmanuel Macron released a joint statement on the US exit from the Iran deal: They said they had \u201cregret and concern\u201d about Trump\u2019s decision and they planned to continue abiding by the agreement.  Our governments remain committed to ensuring the agreement is upheld, and will work with all the remaining parties to the deal to ensure this remains the case including through ensuring the continuing economic benefits to the Iranian people that are linked to the agreement. We urge the US to ensure that the structures of the JCPoA can remain intact, and to avoid taking action which obstructs its full implementation by all other parties to the deal. After engaging with the US Administration in a thorough manner over the past months, we call on the US to do everything possible to preserve the gains for nuclear non-proliferation brought about by the JCPoA, by allowing for a continued enforcement of its main elements. We encourage Iran to show restraint in response to the decision by the US; Iran must continue to meet its own obligations under the deal, cooperating fully and in a timely manner with IAEA inspection requirements. The IAEA must be able to continue to carry out its long-term verification and monitoring programme without restriction or hindrance. In turn, Iran should continue to receive the sanctions relief it is entitled to whilst it remains in compliance with the terms of the deal. \n\n3.15pm EDT\n15:15\n\n Saeed Kamali Dehghan Iran\u2019s president, Hassan Rouhani, speaking shortly after Donald Trump\u2019s decision to pull the US out of the landmark nuclear deal and reimpose sanctions at its highest level, said he believed the agreement could still survive if other negotiating partners defied Trump.  But the Iranian president warned that he has instructed the country\u2019s atomic energy agency to prepare to restart enrichment of uranium at an industrial level in a few weeks time should the deal collapses completely.  \u201cThis is a psychological war, we won\u2019t allow Trump to win... I\u2019m happy that the pesky being has left the Barjam,\u201d he said referring to Persian acronym for JCPOA or the nuclear deal. \u201cTonight we witnessed a new historic experience... for 40 years we\u2019ve said and repeated that Iran always abides by its commitments, and the US never complies, our 40-year history shows us Americans have been aggressive towards great people of Iran and our region .. from the [1953] coup against the legitimate government of [Mohammad] Mosaddegh Mosadeq government and their meddling in the affairs of the last regime, support for Saddam [Hussein during Iran-Iraq war] and downing or our passenger plane by a US vessel and their actions in Afghanistan, in Yemen,\u201d he said. \u201cWhat Americans announced today was a clear demonstration of what they have been doing for months. Since the nuclear deal, when did they comply? They only left a signature and made some statements, but did nothing that would benefit the people of Iran.\u201d  Rouhani said the International Atomic Energy Agency (the IAEA) has verified that Tehran has abide by its obligations under the deal. \u201cThis is not an agreement between Iran and the US... for US to announce it\u2019s pulling out, it\u2019s a multilateral agreement, endorsed by the UN security council resolution 2231, Americans officially announcement today showed that their disregard for international commitments.. We saw that in their disregard for Paris agreement..  \u201cOur people saw that the only regime that supports Trump is the illegitimate Zionist regime, the ame regime that killed our nuclear scientists\u201d \u201cFrom now on, this is an agreement between Iran and five countries... from now on the P5+1 has lost its 1... we have to wait and see how other react. If we come to the conclusion that with cooperation with the five countries we can keep what we wanted despite Israeli and American efforts, Barjam can cursive,\u201d he said referring to Persian acronym for JCPOA or the nuclear deal.  \u201cWe had already come to the conclusion that Trump will not abide by international commitments and won\u2019t respect Barjam.\u201d \n\n3.08pm EDT\n15:08\n\n The Guardian\u2019s Jerusalem correspondent Oliver Holmes sent over Israeli Prime Minister Benjamin Netanyahu\u2019s response to Trump\u2019s announcement.  Netanyahu, who has been a vocal critic of the deal and called for Trump to \u201cfix it or nix it\u201d, said on Tuesday evening: \u201cIsrael fully supports President Trump\u2019s bold decision today to reject the disastrous nuclear deal with the terrorist regime in Tehran.\u201dHe said Israel opposed the deal as it \u201cpaves Iran\u2019s path to an entire arsenal of nuclear bombs.\u201d The \u201cremoval of sanctions under the deal has already produced disastrous results,\u201d he said. \u201cIsrael thanks President Trump for his courageous leadership,\u201d he added. \n\n2.57pm EDT\n14:57\n\n US Treasury secretary Steven Mnuchin has confirmed the sanctions in a statement.  He said the sanctions will be reimposed in keeping with 90 day and 180 day \u201cwind-down\u201d periods. Mnuchin said:  President Trump has been consistent and clear that this Administration is resolved to addressing the totality of Iran\u2019s destabilizing activities. We will continue to work with our allies to build an agreement that is truly in the best interest of our long-term national security. The United States will cut off the IRGC\u2019s access to capital to fund Iranian malign activity, including its status as the world\u2019s largest state sponsor of terror, its use of ballistic missiles against our allies, its support for the brutal Assad regime in Syria, its human rights violations against its own people, and its abuses of the international financial system. The specifics of how the sanctions will work are outlined here. \n\n2.51pm EDT\n14:51\n\n Sabrina Siddiqui Republicans were quick to defend Trump\u2019s decision, signaling the party\u2019s ongoing reluctance to challenge the president even as some had expressed reservations about pulling out of the accord.Kevin McCarthy, the House majority leader, said Trump was saying was staying true to what he promised as a candidate.\u201cThis is something the president campaigned on,\u201d McCarthy told Fox News. \u201cI think President Trump understands foreign policy \u2026 If our main goal here is not to have Iran have a nuclear weapon, i would trust this president to actually get it done.\u201dMcCarthy said he had been briefed on the administration\u2019s plans to exit the agreement, but as of now there had been no immediate demands from Congress to take action.\u201cI think it\u2019s only appropriate that the world together becomes involved and gets a better agreement and makes sure Iran does not have a nuclear weapon,\u201d he added. The Republican Jewish Coalition, a group that aggressively lobbied against the Iran deal and is backed by the casino magnate Sheldon Adelson, welcomed Trump\u2019s announcement. \u201cToday\u2019s decision renewed hope for a truly long-term nuclear-free Iran,\u201d the group said in a statement. \u201cIran continues to be an existential threat to Israel, and continues to menace Israel directly and through its proxies (such as Hezbollah).\u201d \u201cWe appreciate President Trump\u2019s bold foreign policy and look forward to his leadership in dealing with the Iranian threat.\u201d Adelson is set to meet tomorrow with Trump, according to Washington Post White House reporter Ashley Parker.  Sheldon Adelson to meet with Trump in D.C. tomorrow. Described as a \u201cfriendly,\u201d long-planned meeting, not related to today\u2019s Iran news. \n\n2.49pm EDT\n14:49\n\n The White House has released its justification for violating the Iran nuclear deal and re-imposing sanctions lifted under the deal.  \u201cThe re-imposed sanctions will target critical sectors of Iran\u2019s economy, such as its energy, petrochemical, and financial sectors,\u201d according to the White House brief. \u201cThose doing business in Iran will be provided a period of time to allow them to wind down operations in or business involving Iran. \n\n2.42pm EDT\n14:42\n\n Sabrina Siddiqui Senator Bob Menendez, the top Democrat on the Senate foreign relations committee, condemned Trump\u2019s decision to withdraw from the accord as \u201ca grave mistake\u201d. \u201cWith this decision President Trump is risking US national security, recklessly upending foundational partnerships with key US allies in Europe and gambling with Israel\u2019s security,\u201d Menendez said in a statement.  \u201cToday\u2019s withdrawal from the JCPOA makes it more likely Iran will restart its nuclear weapons program in the future.\u201d Menendez also called on Trump to immediately dispatch his national security team to Capitol Hill to explain his administration\u2019s strategy toward Iran before Congress. Chris Murphy, a Democrat on the Senate foreign relations committee, dubbed Trump\u2019s announcement as \u201cterrible news\u201d.\u201cPulling out of the Iran deal is like a soccer player deliberately kicking the ball into their own team\u2019s goal,\u201d Murphy said. \u201cThere is nothing but downside for the U.S., especially since Trump has zero plan for what comes next.\u201d Murphy added that Trump\u2019s move would not only escalate nuclear crisis with Iran but also complicate negotiations with North Korea, stating: \u201cIt will make it even harder to convince Kim Jong Un to give up his nuclear weapons because we just showed that we can\u2019t be trusted to live up to our end of a bargain.\u201d \n\n2.39pm EDT\n14:39\n\n EU president Donald Tusk was also quick to release a statement promising a \u201cunited European approach\u201d to Trump\u2019s decision.  Policies of @realDonaldTrump on #IranDeal and trade will meet a united European approach. EU leaders will tackle both issues at the summit in Sofia next week.","time":1525803717,"title":"Trump withdraws from Iran nuclear agreement","type":"story","url":"https:\/\/www.theguardian.com\/world\/live\/2018\/may\/08\/iran-nuclear-deal-donald-trump-latest-live-updates","label":7,"label_name":"random"},{"by":"coloneltcb","descendants":10,"id":17023305,"kids":"[17024800, 17023927, 17023935, 17023784]","score":113,"text":"At its I\/O developer conference, Google today introduced ML Kit, a new software development kit (SDK) for app developers on iOS and Android that allows them to integrate  into their apps a number of pre-built Google-provided machine learning models. One nifty twist here is that these models \u2014 which support text recognition, face detection, barcode scanning, image labeling and landmark recognition \u2014 are available both online and offline, depending on network availability and the developer\u2019s preference. In the coming months, Google plans to extend the current base set of available APIs with two more: one for integrating the same kind of smart replies that you\u2019re probably familiar with from apps like Inbox and Gmail, and a high-density face contour feature for the face detection API. The real game-changer here are the offline models that developers can integrate into their apps and that they can use for free. Unsurprisingly, there is a trade-off here. The models that run on the device are smaller and hence offer a lower level of accuracy. In the cloud, neither model size nor available compute power are an issue, so those models are larger and hence more accurate, too. That\u2019s pretty much standard for the industry. Earlier this year, Microsoft launched its offline neural translations, for example, which can also either run online or on the device. The trade-off there is the same. Brahim Elbouchikhi, Google\u2019s group product manager for machine intelligence and the camera lead for Android, told me that a lot of developers will likely do some of the preliminary machine learning inference on the device, maybe to see if there is actually an animal in a picture, and then move to the cloud to detect what breed of dog it actually is. And that makes sense, because the on-device image labeling service features about 400 labels, while the cloud-based one features more than 10,000. To power the on-device models, ML Kit uses the standard Neural Networks API on Android and its equivalent on Apple\u2019s iOS. He also stressed that this is very much a cross-platform product. Developers don\u2019t think of machine learning models as Android- or iOS-specific, after all. For developers who want to go beyond the pre-trained models, ML Kit also supports TensorFlow Lite models.  As Google rightly notes, getting up to speed with using machine learning isn\u2019t for the faint of heart. This new SDK, which falls under Google\u2019s Firebase  brand, is clearly meant to make using machine learning easier for mobile developers. While Google Cloud already offers a number of similar pre-trained and customizable machine learning APIs, those don\u2019t work offline, and the experience isn\u2019t integrated tightly with Firebase and the Firebase Console either, which is quickly becoming Google\u2019s go-to hub for all things mobile development. Even for custom TensorFlow Lite models, Google is working on compressing the models to a more workable size. For now, this is only an experiment, but developers who want to give it a try can sign up here. Overall,\u00a0Elbouchikhi\u00a0argues, the work here is about democratizing machine learning. \u201cOur goal is to make machine learning just another tool,\u201d he said. ","time":1525803704,"title":"Google\u2019s ML Kit makes it easy to add AI smarts to iOS and Android apps","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/googles-ml-kit-makes-it-easy-to-add-ai-smart-to-ios-and-android-apps\/","label":5,"label_name":"ml"},{"by":"manidoraisamy","descendants":1,"id":17023298,"kids":"[17023385]","score":5,"text":"\n\n\n\n News In the nearly 30 years since the world wide web launched, more than 2 billion websites have been created. It can feel impossible to keep up with the hundreds of thousands of tweets, tens of thousands of pages, and hundreds of hours of video that come online every single minute. Amid this deluge of information, important new voices are constantly emerging. There\u2019s more diverse content to discover and more great journalism being produced than ever before. In order to make it easier to keep up and make sense of it all, we\u00a0set out to bring our news products into one unified experience.\u00a0\u00a0 Today we\u2019re rolling out an all new Google News, which uses the best of artificial intelligence to find the best of human intelligence\u2014the great reporting done by journalists around the globe.  When we created the original Google News 15 years ago, we simply organized news articles to make it easier to see a range of sources on the same topic.\u00a0\u00a0 The reimagined Google News uses a new set of AI techniques to take a constant flow of information as it hits the web, analyze it in real time and organize it into storylines. This approach means Google News understands the people, places and things involved in a story as it evolves, and connects how they relate to one another. At its core, this technology lets us synthesize information and put it together in a way that helps you make sense of what\u2019s happening, and what the impact or reaction has been.  For many of us, news comes from dozens of different places\u2014sports from a favorite website, politics from TV, and news about your community from your local paper. When you\u2019re in the app, \u201cFor You\u201d makes it easy to stay up to date on everything you care about all in one place. We start with a briefing of five stories that Google News has organized for you\u2014a mix of the most important headlines, local news and the latest developments on the topics you\u2019re interested in. \u00a0 And the more you use the app, the better the app gets. We\u2019ve also built easy-to-use and easy-to-access controls so you can decide if you want to see more or less of a topic or publisher. As we built the app, we focused on letting the stories speak for themselves with great images and videos from YouTube and across the web. To help you quickly get you up to speed, we\u2019re experimenting with a unique visual format called newscasts. Here, the latest developments in natural language understanding bring together a collection of articles, videos and quotes on a single topic. Newscasts make it easy to dive right into perspectives to learn more about a story\u2014plus, it\u2019s easy to read on your phone.  If you want to get a deeper insight into a story, the \u201cFull Coverage\u201d feature provides a complete picture of how that story is reported from a variety of sources. With just a tap you\u2019ll see top headlines from different sources, videos, local news reports, FAQs, social commentary, and a timeline for stories that have played out over time. Having a productive conversation or debate requires everyone to have access to the same information. That\u2019s why content in Full Coverage is the same for everyone\u2014it\u2019s an unpersonalized view of events from a range of trusted news sources. To find out what the world is reading, head over to Headlines for an unfiltered view of news from around the world. Additional sections let you dig into more on technology, business, sports, entertainment and others.  Of course Google News wouldn\u2019t exist without the great journalism being created every day. The Newsstand tab makes it easy to find and follow the sources you trust, as well as browse and discover new ones. You can also access more than 1,000 magazine titles in a mobile-optimized reading format.\u00a0\u00a0   And if you want to support your favorite news sources, we\u2019ve made it simple to subscribe with your Google account. This means no more forms, credit card numbers, or new passwords. And soon, thanks to the new Subscribe with Google platform (launched as a part of the Google News Initiative), you\u2019ll get access to your paid content everywhere\u2014on all platforms and devices, on Google News, Google Search, and on publishers\u2019 own websites.    \n              Follow Us\n            ","time":1525803680,"title":"The new Google News: AI meets human intelligence","type":"story","url":"https:\/\/www.blog.google\/products\/news\/new-google-news-ai-meets-human-intelligence\/","label":7,"label_name":"random"},{"by":"runesoerensen","descendants":0,"id":17023295,"kids":"None","score":3,"text":"Advertisement By MARK LANDLERMAY 8, 2018\n President Trump said pulling out of the Iran nuclear deal sends a message that \u201cthe United States no longer makes empty threats.\u201d WASHINGTON \u2014 President Trump declared on Tuesday that he was pulling out of the Iran nuclear deal, unraveling the signature foreign policy achievement of his predecessor, Barack Obama, and isolating the United States among its Western allies. \u201cThis was a horrible one-sided deal that should have never, ever been made,\u201d Mr. Trump said at the White House in announcing his decision. \u201cIt didn\u2019t bring calm, it didn\u2019t bring peace, and it never will.\u201d [Read a full transcript of President Trump\u2019s remarks.] Mr. Trump\u2019s announcement, while long anticipated and widely telegraphed, plunges America\u2019s relations with European allies into deep uncertainty. They have committed to staying in the deal, raising the prospect of a diplomatic and economic clash as the United States reimposes stringent sanctions on Iran. It also raises the prospect of increasing tensions with Russia and China, which also are parties to the agreement. Advertisement In a joint statement, President Emmanuel Macron of France, Prime Minister Theresa May of Britain and Chancellor Angela Merkel of Germany urged Iran to \u201ccontinue to meet its own obligations under the deal,\u201d despite the American withdrawal. The president called the nuclear agreement a \u201chorrible one-sided deal that should have never, ever been made.\u201d \u201cWe encourage Iran to show restraint in response to the decision by the U.S.,\u201d the statement said. Separately, in a post on Twitter, Mr. Macron said the European allies \u201cregret\u201d Mr. Trump\u2019s decision, adding, \u201cThe international regime against nuclear proliferation is at stake.\u201d Advertisement Mr. Obama called the withdrawal \u201cso misguided.\u201d One person familiar with negotiations to keep the accord in place said the talks collapsed over Mr. Trump\u2019s insistence that sharp limits be kept on Iran\u2019s nuclear fuel production after 2030. The deal currently lifts those limits. As a result, the United States is now preparing to reinstate all sanctions it had waived as part of the nuclear accord \u2014 and impose additional economic penalties as well, according to another person briefed on Mr. Trump\u2019s decision. The withdrawal fulfills one of Mr. Trump\u2019s oft-repeated campaign promises, and came despite intense personal lobbying by European leaders and frantic attempts to craft fixes to the deal that would satisfy him. In part, Mr. Trump was driven by the conviction that taking a tough line with Iran would help an upcoming negotiation with the North Korean leader, Kim Jong-un, whom he plans to meet in the next several weeks. He also announced that Secretary of State Mike Pompeo was headed to North Korea to continue discussions with Mr. Kim about the meeting. Asked if the three Americans who are detained in North Korea would return with Mr. Pompeo, the president signaled that was a real possibility. The president\u2019s own aides had already persuaded him three times not to dismantle the Iran deal. But Mr. Trump made clear that his patience with the deal had worn thin, and with a new, more hawkish set of advisers \u2014 led by Mr. Pompeo and the national security adviser, John R. Bolton \u2014 the president faced less internal resistance this time. \n                The restrictions on Iran\u2019s nuclear program under the deal could survive.             He said the United States and its allies could not stop Iran from building a nuclear weapon \u201cunder the decaying and rotten structure of the current deal.\u201d Advertisement \u201cThe Iran deal is defective at its core,\u201d he said. Prime Minister Benjamin Netanyahu of Israel promptly hailed Mr. Trump for his \u201chistoric move\u201d and \u201ccourageous leadership\u201d in ending an agreement that he said failed its purpose. \u201cThe deal didn\u2019t push war further away, it actually brought it closer,\u201d he said. \u201cThe deal didn\u2019t reduce Iran\u2019s aggression, it dramatically increased it, and we see this across the entire Middle East.\u201d Mr. Trump\u2019s decision capped a frantic four-day period in which American and European diplomats made a last-ditch effort to bridge their differences and preserve the agreement. [Read more about President Trump\u2019s risky calculus in withdrawing from the deal.] On Friday, Mr. Pompeo called his counterparts in Europe to tell them that Mr. Trump was planning to withdraw, but that he was trying to win a two-week reprieve for the United States and Europe to continue negotiating. Mr. Pompeo, people familiar with the talks said, suggested that he favored a so-called soft withdrawal, in which Mr. Trump would pull out of the deal but hold off on reimposing some of the sanctions. Sign up to receive an email from The New York Times as soon as important news breaks around the world. Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. View all New York Times newsletters. On Saturday, the State Department\u2019s chief negotiator, Brian Hook, consulted with European diplomats to try to break a deadlock over the so-called \u201csunset provisions,\u201d under which the restrictions on Iran\u2019s ability to produce nuclear fuel for civilian use expire after 15 years. The Europeans had already agreed to a significant compromise: to reimpose sanctions if there was a determination that the Iranians were within 12 months of producing a nuclear weapon. But officials said that still did not satisfy Mr. Trump, and the Europeans were not willing to go any farther. By Monday, the White House began informing allies that Mr. Trump was going to withdraw from the deal and reimpose oil sanctions and secondary sanctions against the Central Bank of Iran. Advertisement Mr. Trump has also instructed the Treasury Department to develop additional sanctions against Iran, a process that could take several weeks. Under the financial sanctions, European companies will have between 90 days and 180 days to wind down their operations in Iran, or they will run afoul of the American banking system. The oil sanctions will require European and Asian countries to reduce their imports from Iran. For all the frenzied maneuvering, Mr. Trump seems to have made up his mind several weeks ago. When Mr. Macron visited him two weeks ago, officials said, Mr. Trump told the president that he planned to pull out of the deal on May 12, adding, \u201cI haven\u2019t told Melania yet.\u201d David E.Sanger and Nicholas Fandos contributed reporting. We\u2019re interested in your feedback on this page. Tell us what you think. See More \u00bb Go to Home Page \u00bb","time":1525803667,"title":"U.S. Withdraws From Iran Nuclear Deal","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/08\/world\/middleeast\/trump-iran-nuclear-deal.html","label":7,"label_name":"random"},{"by":"rbanffy","descendants":0,"id":17023294,"kids":"None","score":3,"text":"\n\n\n\n Android Ten years ago, when we launched the first Android phone\u2014the T-Mobile G1\u2014it was with a simple but bold idea: to build a mobile platform that\u2019s free and open to everyone. Today, that idea is thriving\u2014billions of people around the world rely on their Android phone every day.  To make Android smarter and easier to use than ever, today we\u2019re unveiling a beta version of Android P, the next release of Android. Android P makes your smartphone smarter, helping it learn from and adapt to you. Take battery life, for instance\u2014I think all of us often wish we had more of it. In Android P, we partnered with DeepMind to build Adaptive Battery, which prioritizes battery power only for the apps and services you use the most, to help you squeeze the most out of your battery. We also used machine learning to create Adaptive Brightness, which learns how you like to set the brightness slider given your surroundings. Across the platform, your phone will help you better navigate your day, using context to give you smart suggestions based on what you like to do the most and automatically anticipating your next action. App Actions, for instance, help you get to your next task more quickly by predicting what you want to do next. Say you connect your headphones to your device, Android will surface an action to resume your favorite Spotify playlist. Actions show up throughout Android in places like the Launcher, Smart Text Selection, the Play Store, the Google Search app and the Assistant.  Adaptive Battery App Actions Slices We want the entire device experience to be smarter, not just the OS, so we\u2019re bringing the power of Google\u2019s machine learning to app developers with the launch of ML Kit, a new set of cross-platform APIs available through Firebase. ML Kit offers developers on-device APIs for text recognition, face detection, image labeling and more. So mobile developers building apps like Lose It!, a nutrition tracker, can easily deploy our text recognition model to scan nutritional information and ML Kit\u2019s custom model APIs to automatically classify over 200 different foods with your phone\u2019s camera.  With Android P, we put a special emphasis on simplicity. The look and feel of Android is more approachable with a brand new system navigation. In Android P, we\u2019re extending gestures to enable navigation right from your homescreen. This is especially helpful as phones grow taller and it\u2019s more difficult to get things done on your phone with one hand. With a single, clean home button, you can swipe up to see a newly designed Overview, the spot where at a glance you have full-screen previews of your recently used apps. Simply tap to jump back into one of them. If you find yourself constantly switching between apps, we\u2019ve got good news for you: Smart Text Selection (which recognizes the meaning of the text you\u2019re selecting and suggests relevant actions) now works in Overview, making it easier to perform the action you want. Changing how you navigate your phone is a big deal, but small changes can make a big difference too. Android P also brings a redesigned Quick Settings, a better way to take and edit screenshots (say goodbye to the vulcan grip that was required before), simplified volume controls, an easier way to manage notifications and more. You\u2019ll notice small changes like these across the platform, to help make the things you do all the time easier than ever. \u00a0 Dashboard, App Timer, Wind Down Beyond smarts, simplicity and digital wellbeing, there are hundreds of additional improvements coming in Android P, including security and privacy improvements such as DNS over TLS, encrypted backups, Protected Confirmations and more.\u00a0\u00a0 Android P Beta is available today on Google Pixel. And thanks to work on Project Treble, an effort we introduced last year to make OS upgrades easier for partners, a number of our partners are making Android P Beta available today on their own devices, including Sony Xperia XZ2, Xiaomi Mi Mix 2S, Nokia 7 Plus, Oppo R15 Pro, Vivo X21, OnePlus 6, and Essential PH\u20111.\u00a0\u00a0 Since we first launched Android ten years ago, we\u2019ve been focused on how to build a platform for everyone. Android P is an important step toward bringing machine learning to everyone with an operating system that learns from you, adapts to you and helps you with everyday tasks. \n              Follow Us\n            ","time":1525803665,"title":"Android P: Packed with smarts and simpler than ever","type":"story","url":"https:\/\/blog.google\/products\/android\/android-p\/","label":9,"label_name":"tech"},{"by":"flipchart","descendants":1,"id":17023288,"kids":"[17023302]","score":1,"text":"A cure for baldness could be on the horizon after British scientists discovered that an osteoporosis drug stimulates hair growth three times quicker than other drugs. Around four in 10 men suffer male pattern baldness by the age of 45 and two thirds by the age of 60. At the moment only two drugs, minoxidil and finasteride, are available for the treatment of male pattern baldness (androgenetic alopecia) - the classic type of receding hair loss in men. But both have side effects and often produce disappointing results. The only other option open to patients losing their hair is transplantation surgery. To find a new treatment, scientists at Manchester University first studied a cancer drug called CsA, which has the embarrassing side-effect of substantial unwanted hair growth. They discovered the hair growth happens because the drug reduces the activity of a protein called SFRP1, which prevents the growth of hair follicles. Although CsA itself is not suitable as a baldness treatment because of its extreme side-effects, scientists found that a drug previously designed to treat osteoporosis, called WAY-316606, was even better at targeting the hair-suppressing protein. In tests, follicles donated by more than 40 patients undergoing hair transplant surgery were treated with the drug and and quickly went into the active phase of hair growth, sprouting 2mm of hair within just six days. Lead scientist Dr Nathan Hawkshaw, said: \u201cThe drug also maintained more hair follicles to stay within the growth phase of the hair cycle compared to control. Not only this, we show that this drug effectively enhances hair shaft keratin production. \u201cCollectively this suggests, that WAY-316606 could be an effective therapeutic option for treating human hair growth disorders. \u201cCsA takes at least six days to enhance human hair growth ex vivo whereas we see a significant increase in hair growth after two days with WAY-316606. \u201cThe fact this new agent, which had never even been considered in a hair loss context, promotes human hair growth is exciting because of its translational potential: It could one day make a real difference to people who suffer from hair loss.\u201d The research also explains why CsA so often induces undesired hair growth in patients as it removes an inbuilt and potent molecular brake on human hair growth. The research was published in the journal Plos Biology. \u00a9 Telegraph Media Group Limited 2018 We rely on advertising to help fund our award-winning journalism. We urge you to turn off your ad blocker for The Telegraph website so that you can continue to access our quality content in the future. Thank you for your support. Need help? Click here for instructions ","time":1525803641,"title":"Baldness cure could come from side-effect of cancer drug","type":"story","url":"https:\/\/www.telegraph.co.uk\/science\/2018\/05\/08\/baldness-cure-could-come-side-effect-cancer-drug\/","label":7,"label_name":"random"},{"by":"ryanjodonnell","descendants":1,"id":17023275,"kids":"[17023285]","score":1,"text":"In two lab experiments, nearly 800 people completed tasks designed to measure their cognitive capacity. Before completing these tasks,\u00a0the researchers asked participants to either: place their phones in front of them (face-down on their desks); keep them in their pockets or bags; or leave them in another room. The results were striking: the closer the phone to the participant, the worse she fared on the task.\u00a0The mere presence of our smartphones is like the sound of our names or a crying baby \u2014 something that\u00a0automatically\u00a0exerts a gravitational pull on our attention. Resisting that pull takes a cognitive toll. \u201cPut your phone away\u201d has become a commonplace phrase that is just as often dismissed. Despite wanting to be in the moment, we often do everything within our power to the contrary. We take out our phones to take pictures in the middle of festive family meals, and send text messages or update our social media profiles in the middle of a date or while watching a movie. At the same time, we are often interrupted passively by notifications of emails or phone calls. Clearly, interacting with our smartphones affects our experiences. But can our smartphones affect us even when we aren\u2019t interacting with them \u2014 when they are simply nearby? In recent research, we investigated whether merely having one\u2019s own smartphone nearby could influence cognitive abilities. In two lab experiments, nearly 800 people completed tasks designed to measure their cognitive capacity. In one task, participants simultaneously completed math problems and memorized random letters. This tests how well they can keep track of task-relevant information while engaging in a complex cognitive task. In the second task, participants saw a set of images that formed an incomplete pattern, and chose the image that best completed the pattern. This task measures \u201cfluid intelligence,\u201d or people\u2019s ability to reason and solve novel problems. Performance on both of these tasks is affected by individuals\u2019 available mental resources. Our intervention was simple: before completing these tasks, we asked participants to either place their phones in front of them (face-down on their desks), keep them in their pockets or bags, or leave them in another room. Importantly, all phones had sound alerts and vibration turned off, so the participants couldn\u2019t be interrupted by notifications. The results were striking: individuals who completed these tasks while their phones were in another room performed the best, followed by those who left their phones in their pockets. In last place were those whose phones were on their desks. We saw similar results when participants\u2019 phones were turned off: people performed worst when their phones were nearby, and best when they were away in a separate room. Thus, merely having their smartphones out on the desk led to a small but statistically significant impairment of individuals\u2019 cognitive capacity \u2014 on par with effects of lacking sleep. This cognitive capacity is critical for helping us learn, reason, and develop creative ideas. In this way, even a small effect on cognitive capacity can have a big impact, considering the billions of smartphone owners who have their devices present at countless moments of their lives. This means that in these moments, the mere presence of our smartphones can adversely affect our ability to think and problem-solve \u2014 even when we aren\u2019t using them. Even when we aren\u2019t looking at them. Even when they are face-down. And even when they are powered off altogether. Why are smart phones so distracting, even when they\u2019re not buzzing or chirping at us? The costs of smartphones are inextricably linked to their benefits. The immense value smartphones provide, as personal hubs connecting us to each other and to virtually all of the world\u2019s collective knowledge, necessarily positions them as important and relevant to myriad aspects of our everyday lives. Research in cognitive psychology shows that humans learn to automatically pay attention to things that are habitually relevant to them, even when they are focused on a different task. For example, even if we are actively engaged in a conversation, we will turn our heads when someone says our name across the room. Similarly, parents automatically attend to the sight or sound of a baby\u2019s cry. Our research suggests that, in a way, the mere presence of our smartphones is like the sound of our names \u2014 they are constantly calling to us, exerting a gravitational pull on our attention. If you have ever felt a \u201cphantom buzz\u201d you inherently know this. Attempts to block or resist this pull takes a toll by impairing our cognitive abilities. In a poignant twist, then, this means that when we are successful at resisting the urge to attend to our smartphones, we may actually be undermining our own cognitive performance. Are you affected? Most likely. Consider the most recent meeting or lecture you attended: did anyone have their smartphone out on the table? Think about the last time you went to the movies, or went out with friends, read a book, or played a game: was your smartphone close by? In all of these cases, merely having your smartphone present may have impaired your cognitive functioning. Our data also show that the negative impact of smartphone presence is most pronounced for individuals who rank high on a measure capturing the strength of their connection to their phones \u2014 that is, those who strongly agree with statements such as \u201cI would have trouble getting through a normal day without my cell phone\u201d and \u201cIt would be painful for me to give up my cell phone for a day.\u201d In a world where people continue to increasingly rely on their phones, it is only logical to expect this effect to become stronger and more universal. We are clearly not the first to take note of the potential costs of smartphones. Think about the number of fatalities associated with driving while talking on the phone or texting, or of texting while walking. Even hearing your phone ring while you\u2019re busy doing something else can boost your anxiety. Knowing we have missed a text message or call leads our minds to wander, which can impair performance on tasks that require sustained attention and undermine our enjoyment. Beyond these cognitive and health-related consequences, smartphones may impair our social functioning: having your smartphone out can distract you during social experiences and make them less enjoyable. With all these costs in mind, however, we must consider the immense value that smartphones provide. In the course of a day, you may use your smartphone to get in touch with friends, family, and coworkers; order products online; check the weather; trade stocks; read HBR; navigate your way to a new address, and more. Evidently, smartphones increase our efficiency, allowing us to save time and money, connect with others, become more productive, and remain entertained. So how do we resolve this tension between the costs and benefits of our smartphones? Smartphones have distinct uses. There are situations in which our smartphones provide a key value, such as when they help us get in touch with someone we\u2019re trying to meet, or when we use them to search for information that can help us make better decisions. Those are great moments to have our phones nearby. But, rather than smartphones taking over our lives, we should take back the reins: when our smartphones aren\u2019t directly necessary, and when being fully cognitively available is important, setting aside a period of time to put them away \u2014 in another room \u2014 can be quite valuable. With these findings in mind, students, employees, and CEOs alike may wish to maximize their productivity by defining windows of time during which they plan to be separated from their phones, allowing them to accomplish tasks requiring deeper thought. Moreover, asking employees not to use their phones during meetings may not be enough. Our work suggests that having meetings without phones present can be more effective, boosting focus, function, and the ability to come up with creative solutions. More broadly, we can all become more engaged and cognitively adept in our everyday lives simply by putting our smartphones (far) away. Kristen Duke is a PhD candidate in Marketing at the Rady School of Management, University of California, San Diego. She studies how uncertainty, emotional complexity, and contextual factors affect decision-making and consumer experiences. Adrian Ward is an Assistant Professor of Marketing in the McCombs School of Business, University of Texas at Austin. His research focuses on technology and cognition, consumer financial decision-making, and morality. Ayelet Gneezy is an Associate Professor of Behavioral Sciences and Marketing at the Rady School of Management, University of California, San Diego. Her research focuses on consumers\u2019 judgment and decision-making, prosocial and charitable behavior, and behavior change. Maarten Bos is a visiting scholar at the Department of Social & Decision Sciences, Carnegie Mellon University. His research interests include decision science, persuasion, and behavioral economics.","time":1525803582,"title":"HBR study: having your smartphone nearby takes its toll on your thinking","type":"story","url":"https:\/\/hbr.org\/2018\/03\/having-your-smartphone-nearby-takes-a-toll-on-your-thinking?referral=03758&cm_vc=rr_item_page.top_right","label":7,"label_name":"random"},{"by":"turrini","descendants":1,"id":17023265,"kids":"[17023389]","score":1,"text":"","time":1525803524,"title":"You spelled it wrong","type":"story","url":"http:\/\/guthib.com\/","label":7,"label_name":"random"},{"by":"etr71115","descendants":0,"id":17023250,"kids":"None","score":1,"text":"By Lisa HanMay 7, 2018 There is no conventional timeline on when you should raise money. It\u2019s whenever you have a winning recipe to scale your business with capital investment. If you have data to build a strong story around this, then you\u2019re ready to raise money. \u201cHangover drinks! Can we hand them out at our launch party?\u201d When James Wong, my attorney colleague, asked me this question, I thought he was joking. 12 hours later, an unassuming Asian guy dressed in black showed up at our Atrium launch party, hauling 20 boxes of his liver-boosting supplement straight from his lab in Los Angeles.  This was my first time meeting Sisun Lee and learning about his new company, 82 Labs\u2013which uses the latest research to help us wake up feeling great after a crazy night out.\u00a0 We watched as the company grew along us, doubling sales and headcount in a matter of months.\u00a0 When Sisun was ready to raise his first priced round, he flew up to San Francisco to attend\u00a0Atrium Scale, our hands-on Series A fundraising boot camp.\u00a0 We started Atrium Scale because founders (especially first-time founders) find it valuable to learn the mechanics of raising a sizable round of venture financing.\u00a0 Unlike a seed round consisting of SAFEs or convertible notes, raising a priced round invites higher business scrutiny and legal complexity.\u00a0 Seed rounds can be prepared with just a few pages and finished within a matter of hours.\u00a0 Priced rounds, on the other hand, require over a hundred pages of dense, heavily negotiated legalese and take one to two months from signed term sheet to money in the bank.\u00a0 As someone who leads Atrium Scale, I got a front-row seat to how Sisun raised a whopping $8 million from Altos,\u00a0Slow and other great investors.\u00a0\u00a0Glowing press releases aside, the financing process was not without frustration, patience and persistence. We each compiled key learnings from this journey. At the end of the day, fundraising is only the vehicle to help you grow your business. As Justin puts it, \u201cParticipating in the Silicon Valley beauty contest of who raised what at what valuation is a fool\u2019s errand. There are lots of examples of companies raising too much money from the wrong partner and ending up shooting themselves in the foot.\u201d  Make sure to do your homework, have a support system of friends and mentors to help guide you through the process, and take in the experience.  At Atrium, we launched Atrium Scale to help startup founders craft compelling pitches and get mentorship from top mentors and investors in the SV \u2013 Apply here!  Everything you need to know about early stage fundraising from our unique perspective helping our clients close deals everyday. By Justin KanMay 2, 2018 Many people, when faced with doing something new that they know nothing about, won\u2019t ever get started. The project seems\u2026 Read On Startup advice, war stories, and best practices.\u00a0 Found exclusively in the Atrium newsletter. First Name\nLast Name\nEmail\n \u00a9Atrium\nAtrium LLP is a registered limited liability partnership under the laws of the State of California.\nAtrium LTS is a [non-law] corporation incorporated under the laws of the State of Delaware.","time":1525803422,"title":"Behind the Scenes: How Sisun Lee Raised $8M","type":"story","url":"https:\/\/www.atrium.co\/blog\/behind-the-scenes-how-sisun-lee-fundraises\/","label":7,"label_name":"random"},{"by":"prostoalex","descendants":1,"id":17023249,"kids":"[17023373]","score":2,"text":"I\u2019ve been talking with friends about the scams that have been going on in the crypto space for a while. We watch what\u2019s happening with ICOs, some of us even get calls from relatives about tokens because we\u2019re \u201cin tech,\u201d and we wonder when the SEC will take some action in the space. It\u2019s certainly more complicated than \u201ceverything is a scam\u201d or \u201call these people are terrible,\u201d but there are many scams and many terrible people, so it is understandable that Charlie Munger might characterize the thinking amongst people piling in as \u201cSomeone else is trading turds and you decide I can\u2019t be left out.\u201d It\u2019s also understandable that folks who are true long-term investors and believers in the technology might take offense to this. It certainly doesn\u2019t apply to them. Perhaps the best, wrote a post suggesting that it\u2019s not all trading cards and beanie babies. We could debate what percentage of tokens do or do not fit this description. Suffice it to say, I think most do. But let\u2019s agree some represent real investment opportunities. So what\u2019s the problem? That gets us to the \u201cindustry\u2019s\u201d Big Lie, which is a common refrain in social media and the press, and which Fred interestingly closes with in his rebuke of Buffett. This is crypto\u2018s Big Lie. And it\u2019s seductive. You too can get rich like venture capitalists (though consider a higher bar since you don\u2019t get management fees on your crypto bets). Setting aside the fact that investing is not independent of wealth (you need money to invest), these markets are opaque, controlled by insiders, and by the time \u201canyone in the world\u201d has the opportunity to invest, the price is often an order of magnitude or three higher than the price paid by venture capitalists (first in line), and significantly higher than what accredited investors paid in pre-sales (second in line). And you hear it in private conversations with crypto teams as well. To paraphrase one statement (but not much), \u201cwe are letting value-add VCs buy equity in the company, and pushing the dumb money that got rich on bitcoin into the public sale.\u201d When money is abundant, who it is attached to matters even more, and money has never been as abundant in traditional venture as it is in crypto. So, while these may be compelling assets if you know how to pick the right ones, we need to start being honest with investors about where they sit in the pecking order. They need to understand that their cost basis is not the same as the venture capitalists, crypto funds, etc whose logos are on the ICO website and whose social proof they trade on to raise hundreds of millions of dollars. They need to understand that many of these insiders (to be clear, not USV AFAIK) are flipping into these public sales, taking an easy 10\u2013100x, taking profits by virtue of their access. They need to understand that they\u2019re playing a very different game. It may be one they can win, but it has its own rules. And they need to determine for themselves with more complete information if that asymmetry of access is really democracy. Crypto-Twitter spent the weekend reminding each other that Buffett doesn\u2019t understand the technology that underlies these innovations (and they\u2019re almost certainly right). In my opinion, this misses the point, which is why the Munger quote above on investor psychology is much more important than Buffett\u2019s (quoted by Wilson) on value. Today many crypto investors (retail and otherwise) are drunk with greed. They believe the Big Lie that this is a democracy where they can now get rich like all the venture capitalists, because they hear it wherever they go to read about investing in crypto. As I was reflecting on the hype and greed in the crypto market this weekend, I was reminded of an amazing Buffett quote (not about crypto) I think about often during hype cycles, as relayed by Brian Chesky: Chesky to Bezos: \u201cJeff, what\u2019s the best advice Warren Buffet ever gave you?\u201d Bezos: \u201c[I asked Warren,] your investment thesis is so simple\u2026you\u2019re the second richest guy in the world, and it\u2019s so simple. Why doesn\u2019t everyone just copy you?\u201d Buffett: \u201dBecause nobody wants to get rich slow.\u201d By clapping more or less, you can signal to us which stories really stand out. User Generated Discontent","time":1525803410,"title":"Crypto\u2019s Big Lie","type":"story","url":"https:\/\/medium.com\/@pt\/cryptos-big-lie-c03c474cbe6c","label":7,"label_name":"random"},{"by":"htiek","descendants":0,"id":17023232,"kids":"None","score":2,"text":"","time":1525803329,"title":"Speeding Up Binary Search Trees with Crazy Good Chocolate Pop Tarts (2012) [pdf]","type":"story","url":"https:\/\/pdfs.semanticscholar.org\/7a3f\/2381519d0d527e4abe830a39c5857a672b11.pdf","label":7,"label_name":"random"},{"by":"jimschley","descendants":76,"id":17023220,"kids":"[17023711, 17023707, 17023682, 17024079, 17024651, 17023914, 17023675, 17024032, 17023941, 17023954]","score":86,"text":"Lying or spreading \u201cfalse news\u201d was treated as a crime in colonial Massachusetts.\u00a0 In 1645 the Massachusetts Bay Colony passed a law which stated: \u201cWhereas truth in words as well as in actions is required of all men, especially of Christians, who are the professed of the God of Truth; and whereas all lying is contrary to truth, and some sort of lies are not only sinful, (as all lies are) but pernicious to the publick weal, and injurious to particular persons: It is therefore ordered by this court and authority thereof, that every person of the age of discretion (which is accounted fourteen years) who shall wittingly and willingly make, or publish any lie, which may be pernicious to the publick weal, or tending to the damage or injury of any particular person, or with the intent to deceive and abuse the people with false news and reports, \u2026 such person shall be fined for the first offense ten shillings, or if the party be unable to pay the same, then to be set in the stocks\u2026 in some open place, not exceeding two hours. For the second offense in that kind, whereof any shall be legally convicted, shall pay the sum of twenty shillings, or be whipped upon the naked body, not exceeding ten stripes. And for the third offense forty shillings, or if the party be unable to pay, then to be whipped with more stripes, not exceeding fifteen. \u201c For a fourth offense, the punishment was 10 shillings more than formerly, or if unable to pay, then 5-6 more stripes than formerly, not exceeding 40. [Bold letters are\u00a0 mine.] This law was included in the Massachusetts Bay Colony\u2019s first printed compilation of statutes\u00a0The Book of the General Lawes and Libertyes Concerning the Inhabitants of Massachusetts\u00a0(1648). Facsimiles of this book\u00a0are owned by the\u00a0Massachusetts Trial Court Law Libraries. One can also read selections of this book online at Le Projet Albion\/Puritan Studies on the Web\/Primary Sources.\u00a0\u00a0(see # 35) \u00a0 The Puritan-dominated Colony clearly hoped to instill Christian values in the community, and protect the vulnerable young colony from danger, by threatening to punish all lying, not only those lies that damaged\u00a0 a particular individual, but also any lies that were simply \u201cpernicious to the publick weal\u201d. (Plymouth Colony passed its own law against spreading false news in 1653, with very similar wording; Virginia followed in 1661; Maryland in 1671; New York in 1664; Pennsylvania in 1682; and South Carolina in 1691.) If someone lied about a particular individual, that individual could sue for slander, which was often done; but the liar could also face criminal sanctions, especially if the person lied about an important government or church official. \u00a0This had been typical in English common law for centuries. \u00a0The impetus was to protect the reputations of important men, because the stability of the government depended upon their good relations with each other (and with the crown) and the nation\u2019s trust in their integrity.\u00a0 That was, in essence, the old conception of \u201cfalse news\u201d. The new conception of \u201cfalse news\u201d, which arose in the seventeenth century, had to do with events rather than particular people.\u00a0 If one lied about a supposed event, especially in a way that endangered the public good, one could face the criminal charge of lying and spreading \u201cfalse news and reports\u201d. Lying often accompanies other crimes, so it is sometimes difficult to separate the crime of lying from other charges an individual might face simultaneously, as for example, when someone were charged with both lying and theft, lying and blasphemy, or lying in order to entrap a young woman so as to pander her. EXAMPLES of people in colonial Massachusetts convicted and punished for lying alone include the following. In 1678, Nicholas Shapleigh and Richard Naggs, mariners on a ship, were fined 10 shillings apiece for their\u00a0 \u201cpernicious lie to ye \u00a0country\u201d, for claiming that there was French contraband and brandy on a ship, and then denying their lie in court, saying it was just \u201cthe slip of the pen\u201d. \u00a0In this apparently civil case, the plaintiff was their captain Andrew Craty; they agreed to give him their wages as libel, so he withdrew his case.\u00a0 But the court fined them for lying to the court about it. In 1682, James Fuller of Springfield claimed that he had prayed to the Devil for help, and was familiar with the Devil on several occasions; in court he admitted that he had lied about this; he was found not guilty by a jury for witchcraft, but the Court, \u201cconsidering his wicked and pernicious willful lying and continuance in it till now putting the country to so great a charge\u201d sentenced him to be whipped 30 stripes and fined 5 pounds. In 1691, Josiah Littlefield (bound over to the General Court in Boston from the County Court of Salem) \u00a0was convicted of lying for reporting that he knew men who had had sold powder and shot to the Indians; he denied it in court; he was fined 10 shillings and fees of Court. [Records of the Court of Assistants of the Colony of the Massachusetts Bay 1630-1692, (1901). Vol 1, p. 147, 228, & 356 respectively.] \u00a0 Today, the Massachusetts General Laws have no proscription against spreading false news, except as it relates to libel and slander of particular individuals.\u00a0 (Slander is oral defamation, libel is written.) For further information, including laws, regulations, case law, and online and print resources, see our web page \u201cMassachusetts Law about Defamation\u201d. \u00a0  Written By: \u00a0 Words of law\u00a0is a regular feature of\u00a0Massachusetts Law Updates, highlighting a particular word or phrase and its meaning in law. Today\u2019s phrase is search\u00a0warrant. search warrant, n. (18c) Criminal law. A judge\u2019s written order authorizing a law-enforcement officer to conduct a search of a\u00a0 \u00a0\u2026Continue Reading Word(s) of the Month \u2013 search warrant Portraits in Massachusetts Law is a regular feature of Massachusetts Law Updates.\u00a0These pages provide links to biographical information abut people who have been particularly important in legal history in Massachusetts, as our government took shape in the cauldron of the American Revolution and grew and\u00a0 \u00a0\u2026Continue Reading Portraits in Massachusetts Law: Lemuel Shaw National Freedom of Information Day\u00a0is observed on March 16, 2018 to recognize the public\u2019s right to information held by government agencies. Freedom of Information Day is an annual event held on the birthday of James Madison, one of our founding fathers. The Freedom of Information\u00a0 \u00a0\u2026Continue Reading Freedom of Information Day We want to hear from you.  Connect with us. \u00a0 Words of law\u00a0is a regular feature of\u00a0Massachusetts Law Updates, highlighting a particular word or phrase and its meaning in law. Today\u2019s phrase is search\u00a0warrant. search warrant, n. (18c) Criminal law. A judge\u2019s written order authorizing a law-enforcement officer to conduct a search of a\u00a0 \u00a0\u2026Continue Reading Word(s) of the Month \u2013 search warrant Portraits in Massachusetts Law is a regular feature of Massachusetts Law Updates.\u00a0These pages provide links to biographical information abut people who have been particularly important in legal history in Massachusetts, as our government took shape in the cauldron of the American Revolution and grew and\u00a0 \u00a0\u2026Continue Reading Portraits in Massachusetts Law: Lemuel Shaw National Freedom of Information Day\u00a0is observed on March 16, 2018 to recognize the public\u2019s right to information held by government agencies. Freedom of Information Day is an annual event held on the birthday of James Madison, one of our founding fathers. The Freedom of Information\u00a0 \u00a0\u2026Continue Reading Freedom of Information Day Mass. Trial Court Law Libraries  Hours: Monday - Friday from 9:00 a.m. - 4:00 p.m.   Phone: 617-878-0336 Toll Free: 1-800-445-8989 (within Mass. only)TTY: 1-800-281-3683 (within Mass. only) \u00a9 2018 Commonwealth of Massachusetts |\r\n\t\t\t\t\t\tHOME |\r\n\t\t\t\t\t\tMass.Gov\u00ae Blog Portal Site Policies |\r\n\t\t\t\t\t\tLogin\n","time":1525803292,"title":"Fake news was illegal in 17th century colonial Massachusetts","type":"story","url":"http:\/\/blog.mass.gov\/masslawlib\/legal-history\/the-law-against-lying-and-false-news-in-colonial-massachusetts\/","label":7,"label_name":"random"},{"by":"MilnerRoute","descendants":0,"id":17023209,"kids":"None","score":1,"text":"MOUNTAIN VIEW \u2014 As Google kicked off its annual I\/O developers conference on Tuesday, women\u2019s advocacy activists protested outside its campus to demand the removal of anti-abortion \u201ccrisis pregnancy centers\u201d from search results for abortion services. Women\u2019s advocacy group UltraViolet and progressive advocacy group CREDO protested outside Google\u2019s campus \u2014 just across the street from the Shoreline Amphitheater, where the I\/O keynote speech is being held \u2014Tuesday morning. They also will deliver\u00a0a petition with more than 100,000 signatures to Google. The two groups argue that when someone searches on Google for abortion services or on Google Maps for nearby abortion centers, crisis pregnancy centers often emerge at the top of the results. Crisis pregnancy centers are non-profit centers that aim to dissuade pregnant women from having abortions. \u201cMillions of women rely on Google for honest, accurate information to find legitimate reproductive health care facilities,\u201d said UltraViolet co-founder Shaunna Thomas in a statement. \u201cGoogle is complicit in luring vulnerable, pregnant women to anti-abortion centers posing as legitimate reproductive health clinics that deceive, lie to, and shame them out of abortions. By running sham abortion clinic ads and ignoring misleading results on Search and Maps, Google fuels this industry of deceit and puts women\u2019s health and lives at risk.\u201d When reached for comment Tuesday, Google sent the following statement: \u201cWe strive for business results that are relevant, accurate and help users find what they\u2019re looking for. At the same time, we have robust policies in place against misleading and misrepresentative ads. We actively enforce those policies.\u201d Get tech news in your inbox weekday mornings. Sign up  for the free Good Morning Silicon Valley newsletter. In 2014,\u00a0Google removed crisis pregnancy center advertisements after repeated demands from pro-choice organizations. Before the 10 a.m. keynote speech, protesters from UltraViolet and CREDO headed to Shoreline Amphitheater to enter the I\/O conference to collect petition signatures. At 9 p.m., the protesters will\u00a0turn on a massive light projection in various locations around the Shoreline Amphitheater with the message \u201cGoogle, stop lying to women. No to fake, anti-abortion clinics.\u201d In 2015, California enacted a law that forces anti-abortion crisis pregnancy centers to provide information about abortion, such as the availability of contraception and abortion and pre-natal care at affordable or free prices. The law is currently under review by the Supreme Court on whether it intrudes on First Amendment rights.\u00a0Oakland and Santa Clara County submitted\u00a0friend-of-the-court briefs supporting the law in March, according to SF Weekly. A 2006 investigative report from former U.S. Rep. Henry Waxman, D-California, found that 87 percent of crisis pregnancy centers, also known as pregnancy resource centers, provided false and misleading information about abortion and its medical risks. Morning Report in your inbox!","time":1525803248,"title":"Activists protest outside Google HQ over anti-abortion search results","type":"story","url":"https:\/\/www.siliconvalley.com\/2018\/05\/08\/activists-protest-outside-google-hq-over-anti-abortion-search-results\/","label":7,"label_name":"random"},{"by":"ohjeez","descendants":0,"id":17023196,"kids":"None","score":1,"text":" +1 650 687 5817  Design, deliver, and run enterprise blockchain workloads quickly and easily. Enable unprecedented levels of automation and agility with cloud computing solutions. Take a new approach to architecture with infrastructure as code. Drive a consistent experience across IT with agility, security, and economic control for your business. Empower innovation at any scale with purpose-built HPC systems and solutions. Combine the right mix of traditional IT, private cloud, and public cloud to achieve optimal workload placement. Integrate compute and software-defined storage into a single, easy-to-manage, software-defined platform. Extract more value with machine learning, memory-driven computing, and other innovations in data and analytics. Drive efficiencies, engage customers, and develop new business insights at the Intelligent Edge. Significantly reduce complexity and the limitations of dedicated hardware. Modernize your data center with resilient, scalable rack servers. HPE Synergy lets you manage your infrastructure as code to accelerate application delivery. Get built-in convergence, tested and optimized for a variety of workloads. HPE SimpliVity is an innovative, all-in-one, software-defined platform that enables streamlined IT operations and huge performance gains. Overcome the barriers to supercomputing with purpose-built HPC systems. Scale up and scale out with servers optimized for HPC and advanced data analytics workloads. Implement edge computing that delivers control and insight from the Industrial Internet of Things. Enable real-time business with high-speed, resilient, x86 systems for high-volume, high-value workloads. HPE OneView allows you to manage your infrastructure programmatically for faster deployment and simplified operations. Add the benefits of cloud to multi-hypervisor, multi-OS, and heterogeneous environments with open servers. Optimize your server performance with additional options and upgrades. Get right-sized servers that are easy to use, cost-effective, and ready to grow. Accelerate performance, consolidate space, and meet SLAs more affordably with high-performance all-flash and hybrid storage arrays. Safeguard your business-critical information with flash-optimized data protection, availability, and retention solutions. HPE InfoSight brings AI to your data center storage by predicting and preventing problems before they can affect your business. Store data on premises and in multiple public clouds, and easily move data between them. Harness unstructured data at any size with affordable NAS to durable petabyte-scale storage. Accelerate storage provisioning from months to minutes while enabling infrastructure visibility. Storage solutions for virtualized data delivering performance, integration, management, and availability for Tier-1 and mission-critical deployments. Boost performance, efficiency, and availability with HPE storage solutions for Microsoft, SAP, and Oracle environments. Get a superior SAN experience from a broad selection of trusted HPE StoreFabric products focused on performance and resiliency solutions. Get business-grade Wi-Fi for small businesses with HPE OfficeConnect. Boost customer engagement and loyalty with location-aware mobile apps. Predict the health of your network from the access layer to the data center with Aruba AirWave. Enable workplace productivity with a mobile-first environment. Monitor and automate IT operations with HPE OneView, a management platform based on software-defined intelligence. Manage every stage of your server's lifecycle with easy-to-operate software. Build, operate, and optimize workloads running across your hybrid platform. Predict the health of your network from the access layer to the data center with Aruba AirWave and HPE IMC. HPE Proliant for Microsoft Azure Stack is co-engineered by HPE and Microsoft for a seamless hybrid cloud experience. HPE Helion OpenStack lets you build a cloud that supports your business stakeholder demands and provides a modern, agile experience for your developers. HPE OneSphere lets you manage all of your on-prem and cloud-based infrastructure from a single interface. Deliver IT as a service with on-premises and off-premises hosted apps and services. Develop IT strategies to modernize and migrate to an on-premises, automated, digital platform. Transform to modular-based, infrastructure-ready data center facilities. Eliminate complexity and modernize storage infrastructure to deliver better business outcomes. Simplify, advance, and protect your network with a complete lifecycle of networking services. Simplify hybrid IT with HPE Pointnext services. Rethink the way you acquire, pay for, and use IT. Operationalize data architectures for effective and actionable analytics. Get insights faster with HPE Pointnext services. Accelerate outcomes and bring intelligent spaces to life. Implement advanced networking capabilities within your data center and at the Intelligent Edge. Transform your workplace to deliver rich digital and mobile experiences. Securely run IoT for scalability, rich data analysis, and optimal utilization of assets. Power the Intelligent Edge with HPE Pointnext services. Implement security, protection, and continuity strategies that reduce risk and help meet compliance mandates. Identify and minimize risk with HPE Pointnext services. Obtain flexible, relationship-based support and management from a dedicated account team. Take advantage of simplified, cost-effective IT support that proactively diagnoses and resolves potential problems. Get IT infrastructure support for HPE servers, storage, and networking products, as well as third-party software. Get a ready-to-deploy, custom solution built to your specific requirements. Securely run IoT for scalability, rich data analysis, and optimal utilization of assets. Achieve exceptional levels of performance with market-leading course offerings for everyone on your team. Subscribe to our self-paced eLearning offerings and materials for key products and technologies. Receive HPE-branded credential badges for developing the technical skills needed to achieve peer performance levels. Quickly gain expertise and a competitive advantage with HPE certification-ready training courses. Prepare network professionals to become go-to experts on Aruba mobile first solutions. Bring on-demand, pay-as-you-go capacity to your on-premises infrastructure. Overcome limited IT budgets and accelerate innovation with new approaches to fund your IT initiatives. Create more opportunities for consumption-based IT and optimize hybrid cloud usage and spend. One-size-fits-all funding models aren\u2019t cut out for IT transformation. Rethink the way you acquire, pay for, and use IT.  Overcome limited IT budgets and accelerate innovation with new approaches to fund your IT initiatives. Maximize your company\u2019s IT potential when you couple your investment with value-added services. Deliver better IT economics with flexible consumption services aligned to business outcomes. The latest insights and resources to help IT pros shape the future. Predictive analytics has become a buzzword among smart city officials who are eager to use sensor technology and data to help solve entrenched social problems as varied as crime, homelessness, and illiteracy. Ambitiously, a panel discussion at a recent Smart City Connect conference in Kansas City, Missouri, was called, \u201cUsing data science to fight blight, crime, and potholes.\u201d Yeah, sure, I thought, as I took my seat.\u00a0 The cynics in the room didn\u2019t speak much. But one person who asked not to be named commented later, \u201cIt\u2019s na\u00efve to think that technology could be the panacea for social ills in cities.\u201d Ideas like tracking empty trash bins with sensors to see where homeless people scrounge for food and seek shelter sounds very nice. But those ideas won\u2019t be as effective as a broad, community-based approach that\u2019s based on civic goodwill, the attendee said. I\u2019ve been of two minds about the value of smart city tech since I began reporting and shooting videos on pilot projects four years ago in Singapore, Montreal, New York, Barcelona, and elsewhere. I\u2019ve seen firsthand that cities can achieve sizable cost savings by incorporating technology. For example,\u00a0Los Angeles installed LED streetlights that dim automatically on bright, moonlit nights, saving the city $9 million a year. What's more, streetlight poles can have Internet of Things (IoT) sensors and cameras attached in order to monitor crowds, pollution, noise, traffic, and parking spaces. In Singapore, sensors on storm water drains\u00a0activate tweets to warn residents to move their cars in the event of a flash flood. Still, isn\u2019t it a bit of a stretch to say a city can use tech to lower a city\u2019s murder rate or improve students' reading scores? That\u2019s putting a lot on the back of technology and data analysis, I thought. Are you ready for the IoT? Despite massive investment, much of the impact of this transformation is yet to be felt. Learn how to approach IT infrastructure readiness. Bob Bennett, Kansas City's chief innovation officer, has a different view.\u00a0\u201cI don\u2019t think we\u2019re putting enough stress on technology,\u201d says Bennett. \u201cWe have to figure out ways to use technology to solve big societal problems. We\u2019ve got to make the case to companies that it\u2019s in their interest as well. The profit margins for tech vendors are smaller when working with the public sector, but I need data aggregation for things like what\u2019s happening with education. I\u2019ve got to move faster, because the rest of the world is.\u201d Bennett talks quickly and passionately.\u00a0He became a city employee two years ago after a U.S. Army career that took him to Iraq working under top commanders. He has a lot to say, and there\u2019s a long way to go.\u00a0\u00a0 Mayor Sly James recently used his state of the city\u00a0address to bemoan the city\u2019s crime rate, which hit 149 murders in 2017. The mayor also praised an improvement in third-grader reading scores, though he lamented they are still below standard. Bennett takes the mayor\u2019s concerns to heart. \u201cI want to help,\u201d he says. \u201cOur murder rate is a catastrophe,\u201d says Bennett. \u201cWe\u2019ve got to get to work on it. If I could figure out 10 different factors that correlate with high crime, it would help. I don\u2019t have a hypothesis or a big data recipe. But let me see the correlation between how many kids are on the school lunch programs and crime. Kids who are hungry are probably more likely to commit crimes.\u201d \u00a0 Big data and predictive analytics are already in use in private industry, often to find consumer preferences for future products such as the next in-demand smartphone features. Bennett wants to make sure the public sector keeps up. Using existing data about shootings, Kansas City has recently correlated gun violence within the 12-square-block Westport nightlife district within certain weekend hours. That insight helped city officials document and then approve a Community Improvement District to allow restaurant and bar owners to hire private security and off-duty cops to patrol the sidewalks at those most dangerous times, calling on-duty police when needed. \u201cWe\u2019re hoping to see an impact in the next three to four months,\u201d\u00a0Bennett says. Elsewhere, the city is using pole-mounted video cameras to monitor crowds that leave the Sprint Center arena. People meander through a downtown restaurant area called the Power and Light District, where the city can dispatch added patrols when needed.\u00a0 \u201cWe don\u2019t see much violent crime near Power and Light, but there are drug- and alcohol-related problems,\u201d says Chris Crosby, CEO of Xaqt, the city\u2019s data analytics partner. \u201cWe run the video analytics right on the camera box. As we understand the pedestrian movement and flow, it helps us better forecast the crime. With more pedestrians on the street, crime is usually down.\u201d Dozens of U.S. cities, including New York and Kansas City, have relied on sophisticated ShotSpotter\u2019s gunfire-specific acoustic sensing technology\u00a0to respond quickly to the distinct sound of gunfire. Such technology won\u2019t necessarily prevent gunfire, but it can lead to quicker arrests and victim response, and possibly shorten the severity of an attack. Kansas City recently issued a request for proposal to add more ShotSpotter technology, although it won\u2019t say for which neighborhoods.\u00a0 \u201cHow likely would ShotSpotter make an impact on our murder rate? I don\u2019t know,\u201d Bennett says. The impact of ShotSpotter will surely become a data point. Improving reading scores for public school third graders sounds like a challenge that could be helped with faster broadband to every home. In San Francisco, an ambitious plan is underway to create a $2 billion public fiber utility to reach every residence, attacking a 12 percent gap in Internet access. Before Kansas City became the first city in the nation to welcome Google Fiber in 2012, it was estimated that 17 percent of its population didn\u2019t have fast Internet access and more so in poor neighborhoods. There have been obvious improvements in Internet connections over the past six years, but the city of 580,000 residents can\u2019t currently put an actual number on how big its overall digital divide is, even though Xaqt has aggregated plenty of data that can be viewed on a public dashboard. (A separate Xaqt dashboard shows real-time traffic and downtown parking along the city's 2.1-mile streetcar corridor.) The Smart KCMO digital inclusion dashboard is amazing. The Kansas City region has more than 2 million residents, on both the Missouri and Kansas sides of the river. You can zoom in on an area map to view any census tract and see what level of Internet speed is available, going in five increments from zero to 500-plus Mbps. But while the dashboard gives a good view of pockets of poor Internet speeds and shows how fast speeds are near somebody\u2019s home, there\u2019s not an overall number available for the region's Internet adoption rate. Also, carriers like Google Fiber don\u2019t release subscriber numbers. It\u2019s not particularly a problem, except when you need to compare Kansas City overall to, say, San Francisco, which says it has 100,000 residents\u201412 percent of the total\u2014without Internet. Bennett doesn\u2019t completely buy the notion that fast Internet and computerized learning will greatly improve reading scores, especially for the very young. Kansas City already provides older public school students with computers, and there are computer labs in most schools and public housing complexes. Hometown wireless carrier Sprint has a free Wi-Fi initiative to help underserved communities. However, the biggest variable in improving young student reading is parental involvement, Bennett and others believe.\u00a0\u201cUniversal broadband is nice, but there\u2019s no silver bullet,\u201d he says. \u201cGoverning is really, really hard, and there\u2019s no single software app that fixes all this. It takes a lot of blood and sweat and tears.\u201d\u00a0 Rick Usher, assistant city manager, says an updated digital inclusion survey in the city is needed, but many factors have to be considered. \u201cWe are also recognizing that wireless carriers are serving the [digital inclusion] need as mobile apps become more useful for education, banking, and employment,\u201d Usher says. \u201cIt\u2019s hard to just say that Internet connections to a computer at home is the answer anymore.\u201d One tangible city service that Kansas City has begun to tackle with data analysis is street maintenance. By using data on street conditions gathered from observers, repair records, and a comparative test along lanes on a portion of Ward Parkway, the city has determined when cracks in pavement should be repaired with a sealant instead of delaying until potholes emerge to make repairs\u2014a far more expensive process. The city expects a 20 to 40 percent savings in its road repair budget, covering 6,200 miles of roads with this new preventative maintenance process, Bennett says. \u201cThe savings are not going to be immediate,\u201d Bennett says. Road maintenance is just one area where predictive analytics will help achieve results over time.\u00a0 But road improvements aren\u2019t as complex as lowering the murder rate. \u201cWe have to have a long-range view on these matters. It\u2019s a new way of thinking about local government, a fundamental restructuring of the way we act,\u201d Bennett says. Gartner analyst Bettina Tratz-Ryan, a consultant on smart city initiatives globally, sometimes hears critics question the efficacy of using involved data analysis to solve protracted social ills. Data analysis can\u2019t replace the need for civic resolve.\u00a0 \u201cGartner believes that technology should be enabling services that support social and community objectives and approaches, including the ability to solve demographic, aspirational, or contextualized questions,\u201d says Tratz-Ryan. \u201cWith the ability to analyze and cross-reference data across urban ecosystems, government officials and startup companies have direct insights into root causes.\u00a0\u00a0 \u201cHowever, not every insight can be solved through technology, nor can it be identified,\u201d she adds. \u201cNot all citizens have the same level of digital activity or maturity. There is always the risk that technology today may only give you the view of the most digitally connected or evident issues. Only with secondary or linked analysis will we find that there are some very analog social problems that we can only solve through analog remedies\u2026like communal empathy.\u201d Which makes me wonder: Is there a really good app for building communal empathy? This article\/content was written by the individual writer identified and does not necessarily reflect the view of Hewlett Packard Enterprise Company. Matt Hamblen is a journalist covering mobile and smart city tech. He served as a reporter and senior editor at Computerworld for more than 20 years. His current site is\u00a0Smart City Scout. Matt Hamblen is a journalist covering mobile and smart city tech. He served as a reporter and senior editor at Computerworld for more than 20 years. His current site is\u00a0Smart City Scout.","time":1525803203,"title":"Kansas City is using predictive analytics to combat social ills","type":"story","url":"https:\/\/www.hpe.com\/us\/en\/insights\/articles\/cities-push-predictive-analytics-to-combat-social-ills-1805.html","label":7,"label_name":"random"},{"by":"john58","descendants":0,"id":17023183,"kids":"None","score":19,"text":"\n                            TNW uses cookies to personalize content and ads to\n                            make our site easier for you to use.\n                            We do also share that information with third parties for\n                            advertising & analytics.\n                         \n            by Tristan Greene\n            \u2014 \n                        in Artificial Intelligence\n Kicking off Google I\/O is an announcement that brings six new voices to our favorite digital assistant. Thanks to an overhaul to its Wavenet platform, Android users will soon be able to select new voices aside from its standard \u201cmale\u201d or \u201cfemale\u201d options. The big story: Grammy Award winning artist John Legend\u2019s voice will join the lineup later this year as one of the new voices. Google CEO Sundar Pichai said Assistant won\u2019t be able to answer every question you could possible ask with Legend\u2019s voice, but for fans it\u2019ll be nice to hear him doing the basics like telling you the weather. Google Assistant is getting better, smarter, and soon it\u2019ll have the velvet voice of a legend named Legend, or any of the five other currently unknown voices joining the new lineup. Check out our\u00a0event page\u00a0for more\u00a0Google I\/O\u00a0stories this week, or follow our reporters on the ground until the event wraps on Thursday:\u00a0@bryanclark\u00a0and\u00a0@mrgreene1977. \nRead next:\n\n        Google Assistant puts an end to impolite queries with 'Pretty Please' feature    \n Stay tuned with our weekly recap of what\u2019s hot & cool by our CEO Boris. \n        Join over 260,000 subscribers!\n     \n                Sit back and let the hottest tech news come to you by the magic of electronic mail.\n             \n                Prefer to get the news as it happens? Follow us on social media.\n             \n1.76M followers\n                         \n1M likes\n                         \n                Got two minutes to spare? We'd love to know a bit more about our readers.\nStart!\n \n                All data collected in the survey is anonymous.\n            ","time":1525803119,"title":"Google announces six new voices for Assistant at #IO18 \u2013 including John Legend","type":"story","url":"https:\/\/thenextweb.com\/artificial-intelligence\/2018\/05\/08\/google-announces-six-new-voices-at-io-2018-including-john-legend\/","label":7,"label_name":"random"},{"by":"johnmyleswhite","descendants":0,"id":17023179,"kids":"None","score":1,"text":"By: Oliver Zeldin, Sameer Indarapu, Damien Lefortier, Zheng Chen, Sushma Bannur The Facebook Field Guide to Machine Learning is a six-part video series developed by the Facebook ads machine learning team. The series shares best real-world practices and provides practical tips about how to apply machine-learning capabilities to real-world problems. Machine learning and artificial intelligence are in the headlines everywhere today, and there are many resources to teach you about how the algorithms work and demonstrations of the latest cutting-edge research. However, if you\u2019re interested in using machine learning to enhance your product in the real world, it\u2019s important to understand how the entire development process works. It\u2019s not only what happens during the training of your models, but everything that comes before and after, and how each step can either set you up for success or doom you to fail. The Facebook ads machine learning team has developed a series of videos to help engineers and new researchers learn to apply their machine learning skills to real-world problems. The series breaks down the machine learning process into six steps: 1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Model\n6. Experimentation The video series covers each of these steps, explaining how the decisions you make along the way can help you successfully apply machine learning to your product or use case. Each lesson highlights examples and stories of non-obvious things that can be important in an applied setting. We hope this video series will help you increase your knowledge of the machine learning process, the importance of making the right decisions at each step, and how using machine-learning models effectively can help deliver the business outcome you are trying to achieve. The first lesson, Problem definition, shares best practices about defining the problem. How the right set up is often more important than the choice of algorithm, and why a few hours spent at this stage in the process can save many weeks work further downstream, preventing you from solving the wrong problem. In this lesson, you will learn how preparing the training data is a core part of a machine learning engineer\u2019s job. It\u2019s an active not passive part of machine learning research and is one of the most powerful variables to create high-quality machine learning systems. Before jumping into developing more features and iterating on model architectures, it\u2019s important to have a clear plan for how to evaluate the performance of your model. This lesson covers how to evaluate your approach. \u00a0 Watch for lesson\u2019s 4-6 later this week. \n\t\t\tYuhang Zhao, Shaomei Wu, Lindsay Reynolds, Shiri Azenkot\t\t \n\t\t\tBenjamin Graham, Laurens van der Maaten, Martin Engelcke\t\t \n\t\t\tGao Huang, Shichen Liu, Laurens van der Maaten, Kilian Q. Weinberger\t\t \n\t\t\tDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri\t\t  \n\t\t\t\t\tFacebook \u00a9 2018\t\t\t\t","time":1525803087,"title":"Facebook\u2019s Field Guide to Machine Learning Video Series","type":"story","url":"https:\/\/research.fb.com\/the-facebook-field-guide-to-machine-learning-video-series\/","label":5,"label_name":"ml"},{"by":"rbanffy","descendants":0,"id":17023174,"kids":"None","score":3,"text":"\nPosted by Jan-Felix Schmakeit, Google Photos Developer Lead\n\n \nPeople create and consume photos and videos in many different ways, and we think it should be easier to do more with the photos you've taken, across all the apps and devices you use.\n \nThat's why we're introducing a new Google Photos partner program that gives you the tools and APIs to build photo and video experiences in your products that are smarter, faster and more helpful. \n \n\n \n \nWith the Google Photos Library API, your users can seamlessly access their photos whenever they need them. \n \nWhether you're a mobile, web, or backend developer, you can use this REST API to utilize the best of Google Photos and help people connect, upload, and share from inside your app.\n \nYour user is always in the driver's seat. Here are a few things you can help them to do:\n \nWith the Library API, you don't have to worry about maintaining your own storage and infrastructure, as photos and videos remain safely backed up in Google Photos. \n \nPutting machine intelligence to work in your app is simple too. You can use smart filters, like content categories, to narrow down or exclude certain types of photos and videos and make it easier for your users to find the ones they're looking for. \n \nWe've also aimed to take the hassle out of building a smooth user experience. Features like thumbnailing and cross-platform deep-links mean you can offload common tasks and focus on what makes your product unique.\n \n \nToday, we're launching a developer preview of the Google Photos Library API. You can start building and testing it in your own projects right now. \n \nGet started by visiting our developer documentation where you can also express your interest in joining the Google Photos partner program. Some of our early partners, including HP, Legacy Republic, NixPlay, Xero and TimeHop are already building better experiences using the API. \n \nIf you are following Google I\/O, you can also join us for our session to learn more.\n \nWe're excited for the road ahead and look forward to working with you to develop new apps that work with Google Photos.\n \n \n","time":1525803062,"title":"Google Developers Blog: Introducing the Google Photos Partner Program","type":"story","url":"https:\/\/developers.googleblog.com\/2018\/05\/introducing-google-photos-partner.html","label":9,"label_name":"tech"},{"by":"devhxinc","descendants":2,"id":17023168,"kids":"[17024757, 17024767]","score":25,"text":"\n\n\n\n News In the nearly 30 years since the world wide web launched, more than 2 billion websites have been created. It can feel impossible to keep up with the hundreds of thousands of tweets, tens of thousands of pages, and hundreds of hours of video that come online every single minute. Amid this deluge of information, important new voices are constantly emerging. There\u2019s more diverse content to discover and more great journalism being produced than ever before. In order to make it easier to keep up and make sense of it all, we\u00a0set out to bring our news products into one unified experience.\u00a0\u00a0 Today we\u2019re rolling out an all new Google News, which uses the best of artificial intelligence to find the best of human intelligence\u2014the great reporting done by journalists around the globe.  When we created the original Google News 15 years ago, we simply organized news articles to make it easier to see a range of sources on the same topic.\u00a0\u00a0 The reimagined Google News uses a new set of AI techniques to take a constant flow of information as it hits the web, analyze it in real time and organize it into storylines. This approach means Google News understands the people, places and things involved in a story as it evolves, and connects how they relate to one another. At its core, this technology lets us synthesize information and put it together in a way that helps you make sense of what\u2019s happening, and what the impact or reaction has been.  For many of us, news comes from dozens of different places\u2014sports from a favorite website, politics from TV, and news about your community from your local paper. When you\u2019re in the app, \u201cFor You\u201d makes it easy to stay up to date on everything you care about all in one place. We start with a briefing of five stories that Google News has organized for you\u2014a mix of the most important headlines, local news and the latest developments on the topics you\u2019re interested in. \u00a0 And the more you use the app, the better the app gets. We\u2019ve also built easy-to-use and easy-to-access controls so you can decide if you want to see more or less of a topic or publisher. As we built the app, we focused on letting the stories speak for themselves with great images and videos from YouTube and across the web. To help you quickly get you up to speed, we\u2019re experimenting with a unique visual format called newscasts. Here, the latest developments in natural language understanding bring together a collection of articles, videos and quotes on a single topic. Newscasts make it easy to dive right into perspectives to learn more about a story\u2014plus, it\u2019s easy to read on your phone.  If you want to get a deeper insight into a story, the \u201cFull Coverage\u201d feature provides a complete picture of how that story is reported from a variety of sources. With just a tap you\u2019ll see top headlines from different sources, videos, local news reports, FAQs, social commentary, and a timeline for stories that have played out over time. Having a productive conversation or debate requires everyone to have access to the same information. That\u2019s why content in Full Coverage is the same for everyone\u2014it\u2019s an unpersonalized view of events from a range of trusted news sources. To find out what the world is reading, head over to Headlines for an unfiltered view of news from around the world. Additional sections let you dig into more on technology, business, sports, entertainment and others.  Of course Google News wouldn\u2019t exist without the great journalism being created every day. The Newsstand tab makes it easy to find and follow the sources you trust, as well as browse and discover new ones. You can also access more than 1,000 magazine titles in a mobile-optimized reading format.\u00a0\u00a0   And if you want to support your favorite news sources, we\u2019ve made it simple to subscribe with your Google account. This means no more forms, credit card numbers, or new passwords. And soon, thanks to the new Subscribe with Google platform (launched as a part of the Google News Initiative), you\u2019ll get access to your paid content everywhere\u2014on all platforms and devices, on Google News, Google Search, and on publishers\u2019 own websites.    \n              Follow Us\n            ","time":1525803046,"title":"The new Google News: AI meets human intelligence","type":"story","url":"https:\/\/blog.google\/products\/news\/new-google-news-ai-meets-human-intelligence\/","label":7,"label_name":"random"},{"by":"Arathorn","descendants":0,"id":17023164,"kids":"None","score":2,"text":" by Matthew Hodgson | May 8, 2018 | Uncategorized | 0 comments Hi all, As the May 25th deadline looms, we\u2019ve had lots and lots of questions about how GDPR (the EU\u2019s new General Data Protection Regulation legislation) applies to Matrix and to folks running Matrix servers \u2013 and so we\u2019ve written this blog post to try to spell out what we\u2019re doing as part of maintaining the Matrix.org server (and bridges and hosted integrations etc), in case it helps folks running their own servers. The main controversial point is how to handle Article 17 of the GDPR: \u2018Right to Erasure\u2019 (aka Right to be Forgotten). \u00a0The question is particularly interesting for Matrix, because as a relatively new protocol with somewhat distinctive semantics it\u2019s not always clear how the rules apply \u2013 and there\u2019s no case law to seek inspiration from. The key question boils down to whether Matrix should be considered more like email (where people would be horrified if senders could erase their messages from your mail spool), or should it be considered more like Facebook (where people would be horrified if their posts were visible anywhere\u00a0after they avail themselves of their right to erasure). Solving this requires making a judgement call, which we\u2019ve approached from two directions: firstly, considering what the spirit of the GDPR is actually trying to achieve (in terms of empowering users to control their data and have the right to be forgotten if they regret saying something in a public setting) \u2013 and secondly, considering the concrete legal obligations which exist. \u00a0 The conclusion we\u2019ve ended up with is to (obviously) prioritise that Matrix can support all the core concrete legal obligations that GDPR imposes on it \u2013 whilst also having a detailed plan for the full \u2018spirit of the GDPR\u2019 where the legal obligations are ambiguous. \u00a0The idea is to get as much of the longer term plan into place as soon as possible, but ensure that the core stuff is in place for May 25th. Please note that we are still talking to GDPR lawyers, and we\u2019d also very much appreciate feedback from the wider Matrix community \u2013 i.e. this plan is very much subject to change. \u00a0We\u2019re sharing it now to ensure everyone sees where our understanding stands today. The current todo list breaks down into the following categories. Most of these issues have matching github IDs, which we\u2019ll track in a progress dashboard. We\u2019re opting to follow the email model, where the act of sending an event (i.e. message) into a room shares a copy of that message to everyone who is currently\u00a0in that room. \u00a0This means that in the privacy policy (see Consent below) users will have to consent to agreeing that a copy of their messages will be transferred to whoever they are addressing. \u00a0This is also the model followed by IM systems such as WhatsApp, Twitter DMs or (almost) Facebook Messenger. This means that if a user invokes their right to erasure, we will need to ensure that their events will only ever be visible to users who already have a copy \u2013 and must never be served to new users or the general public. \u00a0Meanwhile, data which is no longer accessible by any user must of course be deleted entirely. In the email analogy: this is like saying that you cannot erase emails that you have sent other people; you cannot try to rewrite history as witnessed by others\u2026 but you can erase your emails from a public mail archive or search engine and stop them from being visible to anyone else.  It is important to note that GDPR Erasure is completely separate\u00a0from the existing Matrix functionality of \u201credactions\u201d which let users remove events from the room. A \u201credaction\u201d today represents a request for the human-facing details of an event (message, join\/leave, avatar change etc) to be removed. \u00a0Technically, there is no way to enforce a redaction over federation, but there is a \u201cgentlemen\u2019s agreement\u201d that this request will be honoured.\n\nThe alternative to the \u2018email-analogue\u2019 approach would have been to facilitate users\u2019 automatically applying the existing redact function to all of the events they have ever submitted to a public room. The problem here is that defining a \u2018public room\u2019 is subtle, especially to uninformed users: for instance, if a message was sent in a private room (and so didn\u2019t get erased), what happens if that room is later made public? Conversely, if right-to-erasure removed messages from all rooms, it will end up destroying the history integrity of 1:1 conversations, which pretty much everyone agrees is abhorrent. \u00a0Hence our conclusion to protect erased users from being visible to the general public (or anyone who comes snooping around after the fact) \u2013 but preserving their history from the perspective of the people they were talking to at the time. In practice, our core to-do list for Right to Erasure is: One interesting edge case that comes out of GDPR erasure is that we need a way to stop GDPR-erased events from leaking out over federation \u2013 when in practice they are cryptographically signed into the event Directed Acyclic Graph (DAG) of a given room. \u00a0Today, we can remove the message contents (and preserve the integrity of the room\u2019s DAG) via redaction \u2013 but this still leaves personally identifying information in the form of the Matrix IDs (MXIDs) of the sender of these events. In practice, this could be quite serious: imagine that you join a public chatroom for some sensitive subject (e.g. #hiv:example.com) and then later on decide that you want to erase yourself from the room. \u00a0It would be very undesirable if any new homeserver joining that room received a copy of the DAG showing that your MXID had sent thousands of events into the room \u2013 especially if your MXID was clearly identifying (i.e. your real name). Mitigating this is a hard problem, as MXIDs are baked into the DAG for a room in many places \u2013 not least to identify which servers are participating in a room. \u00a0The problem is made even worse by the fact that in Matrix, server hostnames themselves are often personally identifying (for one-person homeservers sitting on a personal domain). We\u2019ve spent quite a lot time reasoning through how to fix this situation, and a full technical spec proposal for removing MXIDs from events can be found at https:\/\/docs.google.com\/document\/d\/1ni4LnC_vafX4h4K4sYNpmccS7QeHEFpAcYcbLS-J21Q. \u00a0The high level proposal is to switch to giving each user a different ID in the form of a cryptographic public key for every room it participates in, and maintaining a mapping of today\u2019s MXIDs to these per-user-per-room keys. \u00a0In the event of a GDPR erasure, these mappings can be discarded, pseudonymising the user and avoiding correlation across different rooms. We\u2019d also switch to using cryptographic public keys as the identifiers for Rooms, Events and Users (for cross-room APIs like presence). This is obviously a significant protocol change, and we\u2019re not going to do it lightly \u2013 we\u2019re still waiting for legal confirmation on whether we need it for May 25th (it may be covered as an intrinsic technical limitation of the system). \u00a0However, the good news is that it paves the way towards many other desirable features: the ability to migrate accounts between homeservers; the ability to solve the problem of how to handle domain names being reused (or hijacked); the ability to decouple homeservers from DNS so that they can run clientside (for p2p matrix); etc. \u00a0The chances are high that this proposal will land in the relatively near future (especially if mandated by GDPR), so input is very appreciated at this point! GDPR describes six lawful bases for processing personal data. \u00a0For those running Matrix servers, it seems the best route to compliance is the most explicit and active one: consent. Consent requires that our users are fully informed as to exactly how their data will be used, where it will be stored, and (in our case) the specific caveats associated with a decentralised, federated communication system. They are then asked to provide their explicit approval before using (or continuing to use) the service. In order to gather consent in a way that doesn\u2019t break all of the assorted Matrix clients connecting to matrix.org today, we have identified both an immediate- and a long-term approach. The (immediate-term) todo list for gathering consent is: Long-term: Account deactivation (the ability to terminate your account on your homeserver) intersects with GDPR in a number of places. Todo list for account deactivation: GDPR states that users have a right to extract their data in a structured, commonly used and machine-readable format. In the medium term we would like to develop this as a core feature of Matrix (i.e. an API for exporting your logs and other data, or for that matter account portability between Matrix servers), but in the immediate term we\u2019ll be meeting our obligations by providing a manual service. The immediate todo list for data portability is: GDPR mandates rules for all the personal data stored by a business, so there are some broader areas to bear in mind which aren\u2019t really Matrix specific, including: So, there you have it. \u00a0We\u2019ll be tracking progress in github issues and an associated dashboard over the coming weeks; for now https:\/\/github.com\/matrix-org\/synapse\/issues\/1941 (for Right to Erasure) or https:\/\/github.com\/vector-im\/riot-meta\/issues\/149 (GDPR in general) is as good as place as any to gather feedback. \u00a0Alternatively, feel free to comment on the original text of this blog post: https:\/\/docs.google.com\/document\/d\/1JTEI6RENnOlnCwcU2hwpg3P6LmTWuNS9S-ZYDdjqgzA. It\u2019s worth noting that we feel that GDPR is an excellent piece of legislation from the perspective of forcing us to think more seriously about our privacy \u2013 it has forced us to re-prioritise all sorts of long-term deficiencies in Matrix (e.g. dependence on DNS; improving User Interactive authentication; improving logout semantics etc). \u00a0There\u2019s obviously a lot of work to be done here, but hopefully it should all be worth it! \u00a0 Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n   Designed by Elegant Themes | Powered by WordPress","time":1525803006,"title":"GDPR Compliance in Matrix.org","type":"story","url":"https:\/\/matrix.org\/blog\/2018\/05\/08\/gdpr-compliance-in-matrix\/","label":7,"label_name":"random"},{"by":"digital55","descendants":0,"id":17023161,"kids":"None","score":1,"text":"We use cookies to improve your experience with our site.\n\n                            Accept and close\n                        \n | More\n                            info. Thank you for visiting nature.com. You are using a browser version with\n    limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off\n    compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site\n    without styles and JavaScript. A Nature Research Journal Instruments on NASA\u2019s Cold Atom Laboratory will use lasers and other techniques to chill atoms to near absolute zero.Credit: NASA\/JPL-Caltech Quantum physicists are about to get their own playground in space. NASA\u2019s Cold Atom Laboratory, scheduled to launch to the International Space Station on 20 May, is set to be the coldest place in the known Universe. Researchers will use the lab to probe quantum phenomena that would be impossible to observe on Earth. \n\nUniverse's first moments mimicked with ultracool atoms\n The US$83-million mission will be used to study quantum mechanics on the macroscopic scale by making a state of matter known as a Bose\u2013Einstein condensate (BEC). These are clouds of hundreds of thousands of atoms that, when chilled to just above absolute zero, behave as waves that synchronize into a single quantum object. \u201cJust being able to do these experiments in space I think is a huge accomplishment,\u201d says Kamal Oudrhiri, mission manager at the Jet Propulsion Laboratory (JPL) in Pasadena, California. On Earth, gravitational forces usually disperse these condensates within a few seconds. The closest that BECs have come to being in space-like conditions is during brief stints in a research rocket, or falling from a drop tower over 9 seconds. But, floating on the space station, they should be able to exist for at least 10 seconds. That\u2019s long enough for them to be cooled to record-low temperatures \u2014 perhaps as little as 20 trillionths of a degree above absolute zero. That would be the coldest known temperature in the Universe, says Oudrhiri. Colder and long-lasting condensates will \u201cpush the frontiers of studying fundamental physics\u201d, says Gretchen Campbell, an atomic physicist at the US National Institute of Standards and Technology in Gaithersburg, Maryland. \u201cIt\u2019s something people have hoped for for almost 15 years.\u201d Down-sized\ufeff kit Real estate on the space station is at a premium, so engineers had to crunch down atomic-physics equipment that usually fills a large room into a cool-box-sized chest. The equipment will cool rubidium and potassium atoms by scattering laser light off the particles in all directions to slow them to almost a standstill. It will then use magnetic fields to trap the cloud. To create the condensate, other cooling techniques are used to push the cloud even closer to absolute zero \u2014 including creaming off the most energetic atoms using a radio-wave \u2018knife\u2019 and widening the trap to let the cloud expand.  Engineers also had to design shielding to protect the delicate condensates from interference from densely packed components and from Earth\u2019s varying magnetic field. Although astronauts will unpack and install the equipment, the experiments will run only while the team is asleep to minimize disruption from any movements. The Cold Atom Laboratory is designed to study ultracold quantum fluids in the microgravity environment of the International Space Station.Credit: NASA\/JPL-Caltech The technology is simpler than initially intended, after a more-complex version of the lab developed a leak that affected the vacuum chamber and threatened to delay the project. So physicists will not yet be able to achieve their ultimate goal of performing space-based atom interferometry \u2014 a process that involves splitting the condensate\u2019s quantum wave in two and recombining the waves. The resulting interference patterns would allow scientists to analyse the effects of gravity with exquisite precision, as well as test whether condensates could be used as very sensitive rotation and gravity sensors. But the more-advanced kit should arrive by the end of 2019, says Robert Thompson, project scientist for the mission at JPL.  Bubbles, rings and whirlpools The current iteration will still allow new physics, says Thompson. Five teams are set to experiment with the lab; one plans to use radio waves and magnetic fields to trap the condensate in a bubble shape about 30 micrometres across \u2014 roughly half the width of a human hair. Quantum mechanics suggests that because the bubble is both thin and edgeless, the condensate should behave differently from when it is shaped as a disc or sphere on Earth. For instance, it might more readily form whirlpools known as vortices, says Courtney Lannert, a theoretical physicist at Smith College in Northampton, Massachusetts. On Earth, attempts to create bubbles always end up with bowl shapes as the fluid falls. \u201cWe can\u2019t get this shape at all unless we can get rid of gravity,\u201d she says.  A group led by Eric Cornell of the University of Colorado, Boulder, who shared the 2001 Nobel physics prize for co-discovering BECs, will try to create exotic loosely bound systems known as Efimov states. Named after Russian theoretical physicist Vitaly Efimov, who proposed their existence in 1970, these quantum states crop up where atoms would bind too weakly to join in pairs, but can form threesomes. These are similar to Borromean rings \u2014 rings linked in such a way that the system falls apart if any one ring is removed \u2014 and are of interest to nuclear physicists because they have parallels with rare and poorly understood three-particle nuclei made up of neutrons and protons. The team hopes to create the simplest Efimov state but also excited, bloated versions in which the atoms bind with each other despite being a bacterium-width apart. The group might also be able to make foursomes of such atoms, known as tetramers, says Maren Mossman, a physicist at Washington State University in Pullman.  Atomic physicists will find the space-station set-up extraordinary for more-practical reasons, says Mossman. They are used to building their own equipment and tweaking experiments as they go. But with the Cold Atom Laboratory, many are sharing a facility for the first time, says Mossman, and they must run experiments through JPL researchers who are operating the facility from the ground. \u201cFolks in particle physics have been doing this since the beginning. But it\u2019s so weird to us in atomic physics,\u201d she says.  The process worked out \u201cbetter than most of us expected\u201d, says Thompson, who has been working to create such a facility since he joined JPL in 1997. He thinks the current version is a step towards even more complex atomic-physics labs in space. NASA is working with the German Aerospace Center (DLR) to build a facility called BECCAL (Bose\u2013Einstein Condensate and Cold Atom Laboratory), he says. Many experiments on the space station already test the effects of low gravity, but for most, the extreme microgravity is \u201coverkill\u201d, adds Thompson. \u201cWe are one of the experiments that will really highlight what the space station is capable of doing.\u201d Nature 557, 151-152 (2018) Sign up for the daily Nature Briefing email newsletter Stay up to date with what matters in science and why, handpicked from Nature and other publications worldwide. Sign Up ISSN 1476-4687 (online)","time":1525802973,"title":"Universe\u2019s coolest lab set to open up quantum world","type":"story","url":"https:\/\/www.nature.com\/articles\/d41586-018-05111-2","label":8,"label_name":"science"},{"by":"cossatot","descendants":0,"id":17023145,"kids":"None","score":1,"text":"You walk into the bookstore. You sit in your folding chair, or on the floor, with your paper cup of wine. The poet approaches the microphone, affably introduces himself, and maybe cracks a joke. He shuffles his papers, launches into his first verse\u2014and all of a sudden, his voice changes completely! Natural conversational rhythms are replaced by a slow, lilting delivery, like a very boring ocean. Long pauses\u2014so long\u2014hang in the air. Try and get comfortable. There\u2019s no helping it. You\u2019re in for a night of Poet Voice. Many performance-related professions and avocations have developed an associated \u201cvoice\u201d: a set of specific vocal tics or decisions. Taken together, these mannerisms make up a kind of sonic uniform, immediately clueing a listener into who or what they\u2019re listening to. There\u2019s \u201cNewscaster\u2019s Voice,\u201d for example, characterized by a slow cadence and a refusal to drop letters. There is \u201cNPR\u201d or \u201cPodcast Voice\u201d, which the writer Teddy Wayne has diagnosed as a \u201cplague of pregnant pauses and off-kilter pronunciations,\u201d and which radio host Ira Glass once said arose in direct response to those butter-smooth anchors. And then there\u2019s Poet Voice, scourge of the open mic and the Pulitzer podium alike. Unsurprisingly, poets are the best at describing Poet Voice: Rich Smith, in CityArts, calls it \u201ca precious, lilting cadence,\u201d in which \u201cevery other line [ends] on a down note,\u201d and there are \u201cpauses, within sentences, where pauses, need not go.\u201d According to Smith, today\u2019s egregious Poet Voicers include Louise Gl\u00fcck and Natasha Trethewey, whose fantastic poems are obscured, in performance, by this tendency of their authors. \u201cPoet Voice [ruins] everybody\u2019s evening,\u201d he writes. \u201c[It is] a thick cloud of oratorial perfume.\u201d Marit J. MacArthur has heard her fair share of Poet Voice. As an English professor and scholar, she has been listening to it for years. (Before she heard it called \u201cPoet Voice,\u201d she even developed her own evocative term for it: \u201cmonotonous incantation.\u201d) \u201cI just felt like there was a style of poetry reading that I was hearing a lot that sounded highly conventional and stylized,\u201d she says. Although she was annoyed, she was also intrigued: \u201cI became curious about what exactly it was, and why so many people were doing it \u2026 I wanted to define it more empirically.\u201d She started by plumbing the literature, undertaking what she calls \u201ca cultural-historical investigation of where this vocal clich\u00e9 came from,\u201d and connecting it with both religious ritual and the university\u2019s distaste for the theatrical. \u201cPeople in academia are just more comfortable with suppression of emotion,\u201d she says. (She collected these insights in a 2016 paper, \u201cMonotony, the Churches of Poetry Reading, and Sound Studies,\u201d published in PMLA, the journal of the Modern Language Association of America.) But this research left her with more questions. So she decided to go deeper. In a new study published in Cultural Analytics, MacArthur and two colleagues, Georgia Zellou and Lee M. Miller, skipped the human middleman and ran various recorded readings through a rigorous sonic analysis. In other words, they tried to use data to nail down Poet Voice. For the study, the researchers chose 100 different poets\u2014half born before 1960, and half born after\u2014aiming for \u201ca variety of aesthetic educational backgrounds, as well as some ethnic, racial, class, and sexual diversity,\u201d as they write. They found audio and video clips of these poets reading their own poems by scouring websites like PennSound and Poets.org. Then they took the first 60 seconds of these recordings, first chopping off any introductory chit-chat, as most poets use their \u201cnormal\u201d voices for that. Their final data pool includes everything from Mark Strand incanting \u201cMan and Camel\u201d behind a podium at Skidmore College to Audre Lorde performing \u201c1984\u201d in 1992 while seated in a brightly colored armchair in Berlin. (You can find a list of the poets, as well as their demographic information and links to the readings used, in the paper itself.) Next, they gathered another set of 60-second snippets, this time from the Buckeye Speech Corpus, which is made up of recordings of native Ohioans talking about \u201ceveryday topics such as politics, sports, traffic, [and] schools.\u201d This served as a conversational control group, which the researchers called the \u201cTalkers.\u201d (Although it might have been better to compare each poet\u2019s reading voice with their own conversational style, \u201cIt proved challenging to find adequate recordings of some of the 100 poets doing anything other than reading poems,\u201d they write.) The researchers then fed each of these recordings into a series of algorithms that measured various aspects of pitch and timing. They focused specifically on 12 attributes, ranging from simple metrics, such as reading speed and average pause length, to more complicated ones, including pitch acceleration (\u201chow rapidly the changes in pitch change \u2026 which we perceive as the lilt of a voice,\u201d the researchers explain) and \u201crhythmic complexity of phrases,\u201d which measures how consistently a speaker draws out, or doesn\u2019t draw out, groups of words. By comparing Poets and Talkers along these lines, the researchers were able to draw two overall conclusions. First, when compared to the Talkers, the poets tended to speak more slowly and stay within a narrower pitch range. Second, very few Talkers indulged in long pauses, but plenty of poets\u201433 percent\u2014had no trouble leaving their listeners hanging for two seconds or more. And what about Poet Voice more specifically? MacArthur\u2019s own list of notable culprits includes Michael Ryan and Juliana Spahr, as well as the aforementioned Gl\u00fcck and Trethewey. When the researchers compared these poets\u2019 vocal stats with each other, a set of common attributes emerged. \u201cThe pitch range tends to be narrower, but that by itself is not enough,\u201d says MacArthur. \u201cIt\u2019s also what you\u2019re doing with your voice within a given pitch range.\u201d Devotees of Poet Voice tend to exhibit slow pitch speed and pitch acceleration: in other words, though the pitch may go up and down over the course of the reading, it\u2019s more rolling hills than rollercoaster. \u201cYou could think about it almost as the same melody over and over,\u201d says MacArthur. This contrasts with the more conversational or expressive styles of reading exemplified by, say, Amalia Ortiz or Rae Armantrout. This is also, perhaps, why it can seem grating or detached: \u201cIn a more natural conversational intonation pattern, you vary your pitch for emphasis depending on how you feel about something,\u201d says MacArthur. \u201cIn this style of poetry reading, those idiosyncrasies \u2026 get subordinated to this repetitive cadence. It doesn\u2019t matter what you\u2019re saying, you just say it in the same way.\u201d Overall, the researchers write, \u201cfrom this small sample, we would conclude that perhaps when some listeners hear poets read with one or more of these characteristics\u2014slow pitch speed, slow pitch acceleration, narrow pitch range, low rhythmic complexity, and\/or slow speaking rate\u2014they hear Poet Voice.\u201d It\u2019s easy to make fun of Poet Voice. But its proliferation across the space of academic poetry may have more serious implications as well. In a 2014 essay, \u201cPoet Voice and Flock Mentality,\u201d the poet Lisa Marie Basile connects it to an overall lack of diversity in the field, and a fear of breaking the mold. The consistent use of it, she writes, \u201cdelivers two messages: I am educated, I am taught, I am part-of a group \u2026 I am afraid to tell my own story in my own voice.\u201dAlthough MacArthur emphasizes that this study is far too small to draw any major conclusions, some of the researchers\u2019 findings so far seem to support the idea that contemporary poets\u2014especially those from marginalized groups\u2014feel pressure to lean into certain aspects of Poet Voice in order to find success in a landscape dominated by white men. \u201cFemale poets as an overgeneralization are getting less expressive, but this doesn\u2019t happen on the male side,\u201d says MacArthur. In addition, \u201csome of our most successful younger African-American female poets are the least expressive\u201d according to this analysis, even as the older African-American female poets in the study were the most expressive as a group. In a response to the study, literature professor Howard Rambsy II wrote that these findings suggest that \u201cperhaps \u2026 low expressiveness in poetry readings is one of the requirements of being a major award-winning black woman poet.\u201d MacArthur agrees: \u201c[The results suggest that] there\u2019s something about assimilation to the mainstream, or just success in the mainstream, that encourages a less expressive style,\u201d she says. \u201cThat to me was kind of startling. I think there\u2019s something there to investigate, and I will.\u201d MacArthur is also working to apply the techniques used in this paper to larger samples, and may expand to other genres of performative speech, like radio dramas or political stumping. \u201cThese tools help us figure out, what is it that we\u2019re responding to?\u201d she says. \u201cI think there\u2019s a lot of potential for testing our perceptions of speech that we find entertaining, or boring, or engaging, in ways that are very hard to put a finger on.\u201d If nothing else, it\u2019s something to think about around stanza number four, when you start nodding off. \nGet our latest, delivered straight to your inbox by subscribing to our newsletter.\n \nSubscribe to our newsletter and get our latest, sent right to your inbox.\n  Follow us on Twitter to get the latest on the world's hidden wonders. Like us on Facebook to get the latest on the world's hidden wonders.","time":1525802920,"title":"An Algorithmic Investigation of the Highfalutin \u2018Poet Voice\u2019","type":"story","url":"https:\/\/www.atlasobscura.com\/articles\/cultural-analysis-poet-voice","label":7,"label_name":"random"},{"by":"yasintoy","descendants":0,"id":17023138,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Visualize your GitHub profile and use in your CV. I'll add pdf export feature as soon as\nCurrently working demo is on\nalize.me Before start, you should install redis. Install Redis : If you have't redis Follow this steps For Mac\/Linux : When the installation completes, add your GitHub Token to export TOKEN = '<enter your github token here>. Before start, you have to start redis server in a new tab redis-server \nand then run the project with python manage.py runserver MIT License Copyright (c) 2018 Yasin Toy Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","time":1525802863,"title":"Alize \u2013 GitHub Profile into the CV","type":"story","url":"https:\/\/github.com\/yasintoy\/Alize","label":4,"label_name":"github"},{"by":"dEnigma","descendants":74,"id":17023102,"kids":"[17023472, 17023785, 17023657, 17024491, 17023479, 17024469, 17023624, 17023508, 17023378, 17023278, 17023474, 17024321, 17024497, 17024525, 17023719, 17023357, 17024223, 17023646]","score":142,"text":"Windows Console, Bash on Ubuntu on Windows, Windows Subsystem for Linux, WSL, Linux For many years, Windows Notepad only supported text documents containing Windows End of Line (EOL) characters - Carriage Return (CR) & Line Feed (LF). This means that Notepad was unable to correctly display the contents of text files created in Unix, Linux and macOS. For example, here\u2019s a screenshot of Notepad trying to display the contents of a Linux .bashrc text file, which only contains Unix LF EOL characters:  As you can see, Notepad is incorrectly displaying the file\u2019s contents, making the file look garbled. This has been a major annoyance for developers, IT Pros, administrators, and end users throughout the community. Today, we\u2019re excited to announce that we have fixed this issue! Starting with the current Windows 10 Insider build, Notepad will support Unix\/Linux line endings (LF), Macintosh line endings (CR), and Windows Line endings (CRLF) as usual. New files created within Notepad will use Windows line ending (CRLF) by default, but it will now be possible to view, edit, and print existing files, correctly maintaining the file\u2019s current line ending format. Here\u2019s a screenshot of the newly updated Notepad displaying the contents of the same Unix\/Linux .bashrc file we saw earlier:  Also note that the status bar indicates the detected EOL format of the currently open file. As with any change to a long-established tool, there\u2019s a chance that this new behavior may not work for your scenarios, or you may prefer to disable this new behavior and return to Notepad\u2019s original behavior. To do this, you can change the following registry keys in the following location to tweak how Notepad handles pasting of text, and which EOL character to use when Enter\/Return is hit: [HKEY_CURRENT_USER\\Software\\Microsoft\\Notepad]  We hope that you find this change useful and look forward to hearing your feedback  Name *  Email *  Website   \n\n   Now if you could also add the ability to enable both Status Bar AND Word Wrap at the same time I\u2019d call Notepad complete \ud83d\ude00 You can now use the Status Bar setting, regardless of the Word Wrap setting.","time":1525802609,"title":"Introducing extended line endings support in Notepad","type":"story","url":"https:\/\/blogs.msdn.microsoft.com\/commandline\/2018\/05\/08\/extended-eol-in-notepad\/","label":7,"label_name":"random"},{"by":"limbicsystem","descendants":0,"id":17023096,"kids":"None","score":3,"text":"Discover the CIA history, mission, vision and values.  Your talent. Your diverse skills. Our mission. Learn more about Careers Opportunities at CIA. Learn how the CIA is organized into directorates and key offices, responsible for securing our nation. The most up-to-date CIA news, press releases, information and more. Our open-source library houses the thousands of documents, periodicals, maps and reports released to the public. Learn more about the Agency \u2013 and find some top secret things you won't see anywhere else. FOIA Category Search | Advanced Search | Search Help  Documents in PDF format require the Adobe Acrobat Reader\u00ae                    ","time":1525802588,"title":"Declassified CIA psychic warfare documents","type":"story","url":"https:\/\/www.cia.gov\/library\/readingroom\/collection\/stargate","label":7,"label_name":"random"},{"by":"spacemanspiffy","descendants":0,"id":17023080,"kids":"None","score":1,"text":"Several new and updated tools showcased at Microsoft\u2019s Build developer conference in Seattle revealed how partners are following the trends toward AI, new software architectures and modernization. Here is a look at their news announcements: Progress announces Conversational UI for chatbot apps\nProgress today announced the May 16 availability Conversational UI, a set of controls built for chatbots. Part of Telerik and Kendo UI tools, the components give both web and application developers the ability to build natural language understanding into their solutions that can run across multiple devices and on many chatbot frameworks, including Microsoft\u2019s Azure Bot Service. Among the challenges of creating a quality human-computer interaction is the difficulty in capturing visual cues. The new controls give developers the ability to create chatbot apps that use visual elements to \u201cenhance the natural flow of conversation,\u201d Progress said in its announcement. Using the components, developers can implement such things as calendars and date pickers, which can be used to steer the conversation. According to Gartner, \u201cMany enterprises underestimate what\u2019s needed to develop and deploy a conversational, platform-based chatbot. That\u2019s because they assume that the service is either available as an off-the-shelf offering or can be built using a small dataset.\u201d Because the new, customizable controls can be used in conjunction with Telerik and Kendo UI tooling, JavaScript and .NET developers can give the chatbots the same look and feel as any other web, desktop or mobile application. For more information, go to www.telerik.com\/conversational-ui or join our R2 2018 Telerik Release webinar on Wednesday, May 30 at 11:30 am ET. Learn more about Progress at\u00a0www.progress.com\u00a0 Bitnami simplifies app migration to Microsoft Azure\nBitnami announced support for Azure in its Stacksmith tool for automating application migration to the cloud. According to Bitnami\u2019s announcement, Stacksmith takes a running application in the datacenter, repackages and optimizes it for cloud deployment, and delivers everything needed to successfully deploy it to a container or cloud platform. \u00a0Stacksmith then continuously monitors the replatformed application for updates and patches, providing an easy way for companies to keep applications up-to-date and secure. \u201cDigital transformation and application modernization are important initiatives for our customers. Partnering with Bitnami to provide our users with tools like Stacksmith that can simplify that transition of their business directly to Azure cloud services is an exciting opportunity,\u201d Gabe Monroy, lead program manager for containers at Microsoft Azure, said in the announcement. \u201cBitnami has demonstrated the ability to package, deploy and maintain 120+ applications as a key partner in our Azure marketplace and we\u2019re thrilled to see that proven technology made available to our Azure users to migrate their own applications to our cloud platforms.\u201d A free trial is available at https:\/\/stacksmith.bitnami.com\/ PureSec unveils intelligent serverless security solution for Azure Functions\nIsrael-based serverless security company PureSec released a beta version of its security solution for Microsoft Azure Functions, Microsoft\u2019s serverless platform. According to the company, PureSec\u2019s Serverless Security Runtime Environment (SSRE) defends against such application-layer attacks as NoSQL\/SQL injection and other unauthorized malicious actions.  With the release, PureSec claims to be the world\u2019s first multi-cloud SSRE. It integrates into all serverless functions and scales as customer serverless applications do. It also provides visibility into serverless functions behavior and security events in real time, the company said. New VM templates eases deployment of Altova server software on Azure\nAltova has announced the release of a free virtual machine template for use on Azure, pre-installed with Altova server software for automating data processing and integration workflows. The template, available in the Azure Marketplace, installs all of Altova\u2019s server software products, and users can active those they want. Using the Altova LicenseServer, customers can get free 30-day trials of all the products. The products in the template are FlowForce Server for data processing and integration; MapForce Server for mapping and aggregation of processes for XML, JSON, databases, flat files, and more; and MobileTogether Server, a gateway to get back-end data into mobile applications. Also included are StyleVision Server for business report generation; RaptorXML Server for validating and processing XML, XBRL and JSON; and DiffDog Server, for automating comparisons of files and directories in parallel computing environments. Learn more at www.altova.com\n Mobilize.Net announces AI-powered code transformation tool\nWebMAP5, the latest of version of Mobilize.Net\u2019s code transformation tool, adds Angular support as companies look to take their legacy applications to the web and the cloud. Angular support was added through Progress Kendo UI, a native Angular UI component library. Mobilize.Net gives organizations a way to take Visual Basic 6, C#, PowerBuilder and Silverlight applications to the cloud and web, using AI to help translate and re-architect legacy applications while removing the technical debt in those applications. WebMAP5 focuses on automation, developer productivity, performance, and broad support for industry standards, according to the company\u2019s announcement. The company claims AI-assisted automation reduces the cost of a modernization project by between 80 and 90 percent.  On the developer side, WebMAP5 leverages Aspect Oriented Programming, which enables the code resulting from the transformation to retain readability and maintainability, and keeps developers from having to learn new skills. PagerDuty supports Visual Studio Team Services\nIncident response provider PagerDuty announced integrations with Microsoft Azure and Visual Studio Team Services to offer teams insights into their operational health and event intelligence powered by machine learning throughout the software development life cycle. \u201cToday, hundreds of enterprise customers are using PagerDuty + Microsoft Azure Alerts and Visual Studio Team Services integrations to successfully accelerate incident response, address issues earlier in the software delivery cycle before they impact customers, and reduce development cycle times,\u201d the company wrote in a brief.  The integrations sync incidents between PagerDuty and Microsoft Azure Alerts to gain more context about your infrastructure and services, the company said. The Visual Studio integration alerts teams to issues earlier in the development life cycle, where they are less costly to fix and have not yet affected customers. The alerting allows organizations to optimize response personnel while informing business stakeholders for deliver service faster while improving code quality. Dotfuscator updates add rooted device security controls \nPreEmptive Solutions today announced updates to Dotfuscator Professional Edition (to 4.35.0) and Dotfuscator CE (to 5.35.0) to include rooted device detection and response controls from Xamarin.Android applications. Rooting allows users of Android devices to gain access to subsystems that can be used to threaten those devices.  \u201cUnauthorized rooting can undermine even the best secure coding practices, data encryption policies, and identity management solutions,\u201d Sebastian Holst, CSO at PreEmptive Solutions, said in a news release announcing the updates. \u201cEvery trusted source for secure coding practices \u2014 including the PCI Security Standards Council, NIST, OWASP, and Google \u2014 acknowledge the rooted device compromise as both difficult to detect and dangerous to both app stakeholders and app users.\u201d According to the company, root detection and response is included with Dotfuscator at no additional cost. Dotfuscator Community Edition (CE) is available at no cost to qualified Visual Studio developers and Dotfuscator Professional licenses start at $1,750 per developer. Syncfusion announces Essential for JS 2 UI toolkit, dashboard platform\nSyncfusion has rebuilt its JavaScript controls from scratch, and now offers more than 35 controls for Angular development. It also has previewed the Syncfusion Dashboard Platform. According to the company, the components have been built in TypeScript as modules. This lets developers use only the components and features they want in their applications. Further, the controls have touch capability and will render according to the device upon which they are used, to optimize the user experience. The source and scripts for the JavaScript and native Angular components are available on GitHub, the company said. The Dashboard Platform will be available in the cloud June 1. Among updates are the ability to use dashboards as JavaScript widgets on web pages. The dashboards are in the Azure VM marketplace, so you can run them on any server but retain the data in your repositories. Visit www.syncfusion.com for more information. \nAltova, Bitnami, chatbots, Conversational UI, Dotfuscator, Microsoft, Microsoft Azure, Microsoft Azure Functions, Microsoft BUILD, Mobilize.Net, PagerDuty, Progress, puresec, serverless, Stacksmith, Syncfusion, Visual Studio Team Services, WebMAP5  \n                            David Rubinstein is editor-in-chief of SD Times.                         Ready for your SD Times magazine? It's just a click away! Subscribe ","time":1525802493,"title":"Innovative tools showcased at Microsoft Build conference","type":"story","url":"https:\/\/sdtimes.com\/msft\/innovative-tools-showcased-at-microsoft-build-conference\/","label":3,"label_name":"dev"},{"by":"dmazin","descendants":0,"id":17023076,"kids":"None","score":4,"text":"\u00a0 By Dmitry Mazin. Illustrations by Sean McOmber. We\u2019ve awaited the age of artificial intelligence for decades. In our fantasies, AI is usually humanoid, straight out of the Jetsons. But while we anticipate the great arrival of the robotic butlers, AI has, in fact, already quietly permeated the fabric of our daily lives \u2014 from shopping, to driving, to communication. Consider autocorrect, an AI-driven input assistant so ubiquitous that you likely don\u2019t even realize how much it impacts your life. Without it, typing on a smartphone would be exceedingly difficult. That utility comes with a price, however, as autocorrect has begun to significantly alter the way we communicate. Though you probably first encountered autocorrect as telltale squiggly red lines under your spelling mistakes, its breakthrough came with the smartphone. As you mash the tiny keys on your phone\u2019s virtual keyboard, a sophisticated language model, working behind the scenes, determines which keys you actually intended to press. The iPhone, for example, invisibly enlarges those keys you are likely to hit next, so they are harder to miss[1]. Naturally, spelling is automatically checked in the process. This hybrid of input assistance and spellchecking is what we now know as autocorrect. Prior to autocorrect, spellcheck was constrained to word processors. Its impact was limited, affecting primarily formal documents like letters and essays. Now, thanks to autocorrect, which mediates everything typed on a smartphone \u2014 casual and formal speech included \u2014 spellcheck is essentially universal. While the Standard English which spell check enforces may be preferable within the context of a formal document, this isn\u2019t necessarily the case elsewhere. Autocorrect\u2019s insistence on \u201cducking\u201d (instead of the much coarser exclamation) is infamous, but its rigidity goes beyond cursing. If you actually prefer the spelling \u201cminiscule,\u201d you must wrestle with autocorrect. And because actual humans adapt quickly to change (and even anticipate it), a human-edited dictionary like Merriam-Webster actually includes words that autocorrect doesn\u2019t, such as \u201cabridgement.\u201d Autocorrect fundamentally alters English. Since there are many ways to spell most English sounds, its spelling tends to drift. Autocorrect slows this evolution, enforcing Standard English in spaces where novel or informal spellings would have previously gone unmolested. Indeed, a 2011 study concluded that in a 20-year period prior to the introduction of autocorrect, spellcheck was already largely responsible for an accelerating death of English words, while the creation of new words contracted sharply, causing an actual shrinkage of the English lexicon[2].  Nevertheless, autocorrect undeniably provides a net benefit. Using our smartphones would simply be intractable without it. However, a new class of input assistant AIs operates on a level beyond spelling, affecting the very way we choose our words. These AIs cross into dangerous territory, threatening to render the English language into lexical pink slime. In 2014, years after the iPhone\u2019s initial release, typing on a smartphone apparently remained too slow[3]. With iOS 9, Apple launched a new product called QuickType, a small bar above the keyboard which automatically suggests the next word in a sentence, dramatically reducing the need for typing itself. \u201cTyping as you know it might soon be a thing of the past,\u201d Apple promised[4]. For simple phrases like \u201con my way,\u201d QuickType works perfectly, and for more complex phrases, its suggestions are often good enough. But that seemingly unimportant difference between the words QuickType suggests and the words you may have otherwise chosen is significant indeed. The extensive dimensions of our language \u2014 for example, the meaning, mood, personality, and bias implied by nuances in the words we choose ourselves \u2014 defy quantification by AI. If QuickType homogenizes our choice of words in the same way autocorrect does the way we spell them, we become less expressive as a result. Our language will be limited to those factors which computers can detect and encode.  To understand how QuickType interacts with our language, consider the way QuickType includes built-in biases. On a brand-new iPhone, the suggestions for \u201cyou are a good\u2026\u201d are \u201cfriend,\u201d \u201cperson,\u201d \u201cman.\u201d Similarly, the suggestions for \u201cFilipino\u2026\u201d are \u201cfood,\u201d \u201cpeople,\u201d \u201cgirl.\u201d The suggestions for \u201chomosexuality is a\u2026\u201d are \u201csin,\u201d \u201cgood,\u201d \u201ccrime.\u201d Having learned our biases, QuickType amplifies their impact by suggesting them right back to us. Meanwhile, Google has gone beyond the realm of mere words. Smart Reply, released in 2015, offers up entire suggested responses to a variety of emails, from event plans to proclamations of love. If a friend wants to schedule a call, Smart Reply will offer up three diverse replies to choose from: \u201cSure, what time?\u201d; \u201cYeah, I\u2019m down.\u201d; and \u201cSorry, I can\u2019t.\u201d The chat app Allo, released in 2016, even uses Smart Reply to respond to photos \u2014 for example, in response to a photo of a smiling friend: \u201cYou look so happy.\u201d When on the go, Smart Reply\u2019s suggestions are clearly more tempting than those tiny virtual keys. As of 2017, 12% of emails sent through Google\u2019s Inbox email app were written by Smart Reply[5].  Smart Reply\u2019s responses, though often technically correct in their meaning, are almost always markedly different from what we might have said on our own. Just like QuickType, Smart Reply homogenizes language. \u00a0 \u00a0 Beyond that, Smart Reply raises serious existential questions. Whereas autocorrect andQuickType function while you type, Smart Reply steps in before you come up with a response \u2014 indeed, before you even see what you\u2019re responding to. All you need to do is choose a response and press send. In fact, like sitting in the driver\u2019s seat of a self-driving car, your involvement is largely optional \u2014 the first suggestion is generally correct enough (and will only improve as the AI learns), and Google certainly doesn\u2019t need your help to press the send button. To put it bluntly, when you use Smart Reply, you are effectively voting for your own agency to be automated away. \u00a0 These existential concerns extend to Smart Reply\u2019s email recipients, as well. Given Gmail\u2019s popularity, you may have already received emails generated by Smart Reply. Could you tell if you did? What does that mean for you, the reader, and for the Smart Reply user? If tools like Smart Reply gain more traction, our daily communications will become perverted Turing Tests for which we didn\u2019t volunteer. Did your friend truly intend to call your baby photo cute, or were they simply accepting an AI\u2019s suggestion? Increasingly, we will feel a nagging uncertainty as to whether we are interacting with a human or not. \u00a0 \u00a0 \u00a0 \u00a0 The process described in this essay is not new. Technology has always exerted influence on language. The printing press, the railroad, and the internet have all contributed to the destruction of entire dialects and languages. What\u2019s different today is the level of sophistication of the mediating technology. QuickType and Smart Reply would not have been possible without the computing power, advances in research, and the infinitudes of data available in the 21st century. More than ever, when AI researchers ask, \u201cCan we?\u201d the answer is \u201cWe can.\u201d \u00a0 We are increasingly able to bless AI with more and more agency (or more precisely, what we believe passes for agency). In many cases, this brings us great benefits. In employing AI, we might achieve previously unthinkable goals. We could end alienating, dangerous labor, or we could significantly improve medicine. But we must be extremely careful. \u00a0 Like all intellectual endeavors, AI research is driven by curiosity rather than necessity. We must approach AI breakthroughs with skepticism. Not only does technology wield great power over us, it advances at an inhuman pace. Unless we anticipate its coming challenges, we will realize them only in hindsight. \u00a0 \u00a0 Notes and References [1] According to Apple\u2019s patent 8,232,973 \u201cMethod, device, and graphical user interface providing word recommendations for text input\u201d \u00a0 [2] Petersen, A. M., Tenenbaum, J., Havlin, S., & Stanley, H. E., Statistical Laws Governing Fluctuations in Word Use from Word Birth to Word Death. \u00a0 [3] Indeed, Google found that \u201c[t]he average user is roughly 35% slower typing on a mobile device than on a physical keyboard.\u201d Fran\u00e7oise Beaufays, The Machine Intelligence Behind Gboard. \u00a0 [4] Apple QuickType marketing page. \u00a0 [5] Brian Strope, Efficient Smart Reply, now for Gmail. Great article. I\u2019m used to hearing general alarmist statements about AI, but have seen too little of this type of specific analysis of the dangers.  Keep it up! Enjoyed reading it\u2026 chose it over this. Yay for free thinking and free thinkers such as you.youwas not an option by the way, but way was. Your email address will not be published. Required fields are marked * Comment document.getElementById(\"comment\").setAttribute( \"id\", \"48cacd42b9c9c31e871170c22f58035b\" );document.getElementById(\"54265f5b4e\").setAttribute( \"id\", \"comment\" ); Name *  Email *  Website   \n\n  Notify me of follow-up comments by email.  Notify me of new posts by email. Email: mondo2000@mondo2000.com\n \nTwitter: @2000_mondo\n \nFacebook: Mondo 2000 History Project\n Editor: R.U. Sirius Web Administrator: Ed Reibsamen\n Original MONDO 2000 logo by Brummb\u00e4r, revised by Bart Nagel\n","time":1525802445,"title":"Pink Lexical Goop: The Dark Side of Autocorrect","type":"story","url":"http:\/\/www.mondo2000.com\/2018\/01\/17\/pink-lexical-goop-dark-side-autocorrect\/","label":7,"label_name":"random"},{"by":"mneumegen","descendants":0,"id":17023065,"kids":"None","score":1,"text":"09 May 2018 GitHub Pages is a reliable, performant Jekyll hosting provider which has recently added support for SSL. It\u2019s a great hosting platform but has a major limitation, you can\u2019t use custom Jekyll plugins. Today we\u2019re looking at how CloudCannon can help resolve this limitation. CloudCannon works nicely with GitHub Pages, you connect your gh-pages\u00a0branch up to a site on CloudCannon and your files stay in sync. Making a change in CloudCannon triggers a build on GitHub Pages, which then pushes the change to your live site. This is a great workflow when you need a CMS for non-developers and still want to use GitHub Pages hosting. However, you still won\u2019t be able to use custom Jekyll plugins. To use Jekyll plugins on GitHub Pages we\u2019ll use a new CloudCannon feature, Outputs. With Outputs, CloudCannon pushes the static output of a build to a branch in addition to pushing changes back to your Jekyll source. This is useful for GitHub Pages as CloudCannon supports custom plugins. CloudCannon can build the Jekyll site and output the static site to the gh-pages\u00a0branch. Here\u2019s a basic setup for using Outputs with GitHub Pages: This setup is useful for other situations as well. You might have a CI script that performs static checks on a site and then deploys it to your hosting environment. If the CI uses the static output for performing these tasks it will run faster as it doesn\u2019t have to build the site.\u00a0 Hope these tips are useful! Let us know how you\u2019re using Outputs. Join thousands of developers building sites on CloudCannon today Everything you need to build, host and update Jekyll websites. \n\t\t\t\u00a9 2018 CloudCannon Ltd \u2022\n\t\t\tTerms &\n\t\t\tPrivacy\n","time":1525802391,"title":"Using Custom Jekyll Plugins on GitHub Pages","type":"story","url":"https:\/\/cloudcannon.com\/tips\/2018\/05\/09\/using-jekyll-plugins-on-github-pages\/","label":4,"label_name":"github"},{"by":"pepsi","descendants":0,"id":17023062,"kids":"None","score":1,"text":"CNXSoft \u2013 Embedded Systems News News, Tutorials, Reviews, and How-Tos related to Embedded Linux and Android, Arduino, ESP8266, Development Boards, TV Boxes, Mini PCs, etc.. Brillo Project was renamed to Android Things with the release of a developer preview back in December 2014, and the operating system enabling developers and companies to build and maintain Internet of Things devices at scale. The OS has now graduated so-to-speak with the release of Android Things 1.0 with long-term support for production devices, and this was to be expected as several Android Things devices were announced earlier this year. MT8516 Platform Innocomm WB10 i.MX 8M Module Intrinsyc Open-Q 212A Module The new release adds supports for new system-on-modules (SoMs) based on the NXP i.MX8M, Qualcomm SDA212, Qualcomm SDA624, and MediaTek MT8516 SoCs. These modules are certified for production use with guaranteed long-term support for 3 years, and development hardware and reference designs for these SoMs will be available in the coming months. The Raspberry Pi 3 Model B and NXP i.MX7D boards and system-on-modules are still supported, but support for NXP i.MX6UL devices will be deprecated. Check out the hardware page for a full list of supported platforms. Google also pointed out some of the strength of Android Things with automatic firmware updates enabled for all devices by default, and the company will offer free stability fixes and security patches for three years, with optional (read paid) extended support. See program policies for details on software update support. The Android Things Console now comes with a new interface to configure hardware peripherals, enabling build-time control of the Peripheral I\/O connections, and device properties such as GPIO pull-up\/pull-down resistors and I2C & UART bus speed. While you can test up to 100 devices for free for non-commercial use using Android Things console, companies will have to sign a distributions agreement with Google to remove the limit for the commercial products. For further details about Android Things 1.0 you may want to read the release notes, and to get started you consider purchasing a starter kit and checkout some existing projects on \u00a0 androidthings.withgoogle.com. Tweet Brillo Project was renamed to Android Things with the release of a developer preview back in December 2014, and the operating system enabling developers and companies to build and... Related posts: \n1 Comment on \"Android Things 1.0 Released with Support for NXP i.MX 8M, Qualcomm SDA212\/SDA624 and Mediatek MT8516 SoMs\"  The Internet of Things needs to be taken out back and shot \n~\n\n\n\n\n\n\n\n\n\n\n","time":1525802380,"title":"Android Things 1.0 Released","type":"story","url":"https:\/\/www.cnx-software.com\/2018\/05\/08\/android-things-1-0-released\/","label":9,"label_name":"tech"},{"by":"amluto","descendants":0,"id":17023049,"kids":"None","score":1,"text":"Original Release date: 08 May 2018 | Last revised: 08 May 2018 In some circumstances, some operating systems or hypervisors may not expect or properly handle an Intel architecture hardware debug exception. The error appears to be due to developer interpretation of existing documentation for certain Intel architecture interrupt\/exception instructions, namely MOV to SS and POP to SS. CWE-703: Improper Check or Handling of Exceptional Conditions - CVE-2018-8897\n The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself). Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol 3A; section 2.3).\n\nIf the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at Current Privilege Level (CPL) < 3, a debug exception is delivered after the transfer to CPL < 3 is complete. Such deferred #DB exceptions by MOV SS and POP SS may result in unexpected behavior.\n\nTherefore, in certain circumstances after the use of certain Intel x86-64 architecture instructions, a debug exception pointing to data in a lower ring (for most operating systems, the kernel Ring 0 level) is made available to operating system components running in Ring 3. This may allow an attacker to utilize operating system APIs to gain access to sensitive memory information or control low-level operating system functions.\n\nSeveral operating systems appear to incorrectly handle this exception due to interpretation of potentially unclear existing documentation and guidance on the use of these instructions. An authenticated attacker may be able to read sensitive data in memory or control low-level operating system functions, Apply an update\n\nCheck with your operating system or software vendor for updates to address this issue. There is no expected performance impact for applying an update. A list of affected vendors and currently-known updates is provided below. Microsoft and Intel credit Nick Peterson of Everdox Tech, LLC, for responsibly reporting this vulnerability and working with the group on coordinated disclosure. Andy Lutomirski is also credited for assistance in documenting the vulnerability for Linux. This document was written by Garret Wassermann. If you have feedback, comments, or additional information about this vulnerability, please send us email. Please use the Vulnerability Reporting Form to report a vulnerability. Alternatively, you can send us email. Be sure to read our vulnerability disclosure policy. Sponsored by the Department of Homeland Security Office of Cybersecurity and Communications Copyright \u00a9 1999-2018 Carnegie Mellon University","time":1525802311,"title":"VU#631579 \u2013 Hardware debug exception may result in unexpected behavior","type":"story","url":"https:\/\/www.kb.cert.org\/vuls\/id\/631579","label":3,"label_name":"dev"},{"by":"matryer","descendants":0,"id":17023047,"kids":"None","score":1,"text":"There are some great tools out there for A\/B testing that let you automatically present a selection of different experiences\u200a\u2014\u200awhether layout, colors, fonts, or content. They measure which is most effective, and you make a final decision on which one to go with. Let\u2019s say we have a list of choices to present to the user. We want to present the most relevant content first so that our user doesn\u2019t get bored and instead engages with our app or website. The choices could be anything\u200a\u2014\u200anews articles, songs, pictures, products, tweets, or whatever your particular data is. For choices A to F, they would be displayed differently for these three groups of users: The first thing we need to do is learn about our users. We can start by randomly changing the order of the choices and tracking engagement (clicks probably) just like A\/B testing does. The difference is we will also capture some measurable data about the user too, which can start to form a model of their interests. For example, we might know their age, or location, or previous purchasing history. The goal here is to think about what properties might be important when tailoring the experience. We might also decide to give the model other inputs, such as the time of day, or even the current weather, if they are likely to influence our users\u2019 decisions. The insights we get from our users can be applied to other users, and even to brand new users who have never used our app before. For example, given the following simple table of data about some users: What films would you suggest to Piotr and Pawe\u0142? Given their age group, and the fact that they liked The Matrix, it is probably sensible to recommend 28 Days Later to them. If a new user comes along with the following properties, which films would you suggest? Since he lives in London and is in the same age group as Bill, we might decide to recommend Jack Strong and Das Boot. Of course, in the real world it\u2019s nowhere near this simple and the patterns are likely to be much more nuanced\u200a\u2014\u200anever mind when you introduce any kind of big data scale. This is where Machine Learning can do a better job than humans. Once we can make predictions, we need to track whether they\u2019re successful or not. Whenever we get something right (like a user clicks a choice, or watches a movie, or buys a product) we will reward the model to reinforce the learning. Our model will then notice patterns, and make better predictions in the future. Suggestionbox is a tool from Machine Box that provides a Machine Learning model for this very use case. You use a simple JSON API to interact with the box. Typing this single line into a terminal will download and run Suggestionbox for you (assuming you have installed Docker): We will use it to build a little demo to show personalization working. While you can create models via the API, it\u2019s much easier to head to http:\/\/localhost:8080 and create the model using the Console. I am going to create a model called Genres, with five choices. Our model is now ready to start making predictions. Of course, they\u2019re not going to be very well informed initially, but very quickly Suggestionbox will notice patterns in the rewards, and we\u2019ll start to see the predictions get better and better. Suggestionbox ships with a built-in simulator, so you can actually simulate real user activity to see how the model might take shape. If you want to try this, you can do so from the Console at http:\/\/localhost:8080\/console. There are two things our JavaScript needs to do: Assuming we had a user object that contained some relevant properties: To make a prediction, we might do something like this: This code turns our user object into a prediction request and makes an AJAX request to the \/suggestionbox\/models\/{model_id}\/predict endpoint. The results will come back looking something like this: Each choice is mentioned with an order and a score. The order is what we care about most, because that is the order we need to present the choices to the user. The reward_id values are used to issue rewards if the user engages with any of these options. Assuming we have the elements in a container, we can just append them back to it to control the order. If elements are being appended to the container they\u2019re currently in, they\u2019ll essentially just move to the end. We can use this to easily set the order just by iterating over the choices: At the same time, we are going to capture the reward IDs in an object keyed by the choice ID. This will make it easier to lookup reward IDs later. When the user clicks one of the choices, we\u2019ll call this function which will make an AJAX request to reward the model: This function just creates a reward request object that contains the appropriate ID (we look it up via our rewardIDs object) and a value which is 1 for most cases. We make a POST request to \/suggestionbox\/models\/{model_id}\/rewards, before going about our business. See a full example of this by checking out the suggestpage toy on GitHub. Our team hang out all day in the Machine Box Community Slack which you can get an invitation to today. Machine Box puts state of the art machine learning capabilities into Docker containers so developers like you can easily incorporate natural language processing, facial detection, object recognition, etc. into your own apps very quickly. The boxes are built for scale, so when your app really takes off just add more boxes horizontally, to infinity and beyond. Oh, and it\u2019s way cheaper than any of the cloud services (and they might be better)\u2026 and your data doesn\u2019t leave your infrastructure. By clapping more or less, you can signal to us which stories really stand out. Founder at MachineBox.io\u200a\u2014\u200aGopher, developer, speaker, author\u200a\u2014\u200aBitBar app https:\/\/getbitbar.com\u200a\u2014\u200aAuthor of Go Programming Blueprints Official Machine Box blog: Machine learning inside Docker containers","time":1525802305,"title":"Order elements using personalized machine learning recommendations in JavaScript","type":"story","url":"https:\/\/blog.machinebox.io\/forget-a-b-testing-order-elements-using-personalized-machine-learning-recommendations-in-7703233cc4ef","label":5,"label_name":"ml"},{"by":"dror","descendants":0,"id":17023033,"kids":"None","score":2,"text":"Advertisement Supported by By Cade Metz SAN FRANCISCO \u2014 Two technology booms \u2014 some people might call them frenzies \u2014 are combining to turn a once-obscure type of microprocessor into a must-have but scarce commodity. Artificial intelligence systems, made by companies ranging in size from Google to the Chinese start-up Malong Technologies, rely heavily on a computer chip called a graphics processing unit, or G.P.U. The chips are also very useful in mining digital currencies like Ethereum, a Bitcoin alternative riding the same wave of hype as its more famous cousin. With people and companies involved in the two surging tech niches buying up the same chips, G.P.U.s have been in short supply over the past several months. Prices have increased by as much as 50 percent, according to some resellers and customers. \u201cThe chips are simply going out of stock,\u201d said Matt Scott, a technologist from the United States who founded Malong after leaving Microsoft\u2019s research lab in Beijing in 2014. \u201cAnd the problem is getting worse.\u201d Malong, which is based in Shenzhen, China, is building a system that can analyze digital photos and learn to recognize objects. Doing so requires an enormous number of photos, and analyzing all these photos depends on the G.P.U. chip. When the company recently ordered new hardware from a supplier in China, the shipment was delayed by four weeks. And the price of the chips was about 15 percent higher than it had been six months earlier. \u201cWe need the latest G.P.U.s to stay competitive,\u201d Mr. Scott said. \u201cThere is a tangible impact to our research work.\u201d But he did not blame the shortage on other A.I. specialists. He blamed it on cryptocurrency miners. \u201cWe have never had this problem before,\u201d he said. \u201cIt was only when crypto got hot that we saw a significant slowdown in our ability to get G.P.U.s.\u201d G.P.U.s were originally designed to render graphics for computer games and other software. In recent years, they have become an essential tool in the creation of artificial intelligence. Almost every A.I. company relies on the chips. Like Malong, those companies build what are called neural networks, complex algorithms that learn tasks by analyzing vast amounts of data. Large numbers of G.P.U.s, which consume relatively little electrical power and can be packed into a small space, can process the huge amounts of math required by neural networks more efficiently than standard chips. Speculators in digital currency are snapping up G.P.U.s for a very different purpose. After setting up machines that help run the large computer networks that manage Ethereum and other Bitcoin alternatives, people and businesses can receive payment in the form of newly created digital coins. G.P.U.s are also efficient for processing the math required for this digital mining. Crypto miners bought three million G.P.U. boards \u2014 flat panels that can be added to personal and other computers \u2014 worth $776 million last year, said Jon Peddie, a researcher who has tracked sales of the chips for decades. That may not sound like a lot in an overall market worth more than $15 billion, but the combination of A.I. builders and crypto miners \u2014 not to mention gamers \u2014 has squeezed the G.P.U. supply. Things have gotten so tight that resellers for Nvidia, the Silicon Valley chip maker that produces 70 percent of the G.P.U. boards, often restrict how many a company can buy each day. \u201cIt is a tough moment. We could do more if we had more of these\u201d chips in our data centers, said Kevin Scott, Microsoft\u2019s chief technology officer. \u201cThere are real products that could be getting better right now for real users. This is not a theoretical exercise.\u201d AMD, another G.P.U. supplier, and other companies say some of current shortage is a result of a limited worldwide supply of other components on G.P.U. boards, and they note that retail prices have begun to stabilize. But in March, at his company\u2019s annual chip conference in Silicon Valley, Nvidia\u2019s chief executive, Jen-Hsun Huang, indicated that the company still could not produce the chips fast enough. This has created an opportunity for numerous other chip makers. A company called Bitmain, for instance, has released a new chip specifically for mining Ethereum coins. Google has built its own chip for work on A.I. and is giving other companies access to it through a cloud computing service. Last month, Facebook indicated in a series of online job postings that it, too, was working to build a chip just for A.I. Dozens of other companies are designing similar chips that take the already specialized G.P.U. into smaller niches, and more companies producing chips means a greater supply and lower prices. \u201cYou want this not just for economic reasons, but for supply chain stability,\u201d said Mr. Scott of Microsoft. The market will not diversify overnight. Matthew Zeiler, the chief executive and founder of a computer-vision start-up in New York, said the prices of some of the G.P.U. boards that the company uses have risen more than 40 percent since last year. Mr. Zeiler believes that Nvidia will be very hard to unseat. Many companies will stick with the company\u2019s technology because that is what they are familiar with, and because the G.P.U. boards it provides can do more than one thing. Kevin Zhang, the founder of ABC Consulting, has bought thousands of G.P.U.s for mining various digital currencies. He said that a chip just for, say, mining Ethereum was not necessarily an attractive option for miners. It cannot be used to mine other currencies, and the groups that run systems like Ethereum often change the underlying technology, which can make dedicated chips useless. Interest in digital currency mining could cool, of course. But the A.I. and gaming markets will continue to grow. Mr. Zeiller said that his company had recently bought new G.P.U.s for its data center in New Jersey, but could not install them for more than a month because the computer racks needed to house the chips were in short supply as a result of the same market pressures. \u201cThe demand,\u201d he said, \u201cis definitely crazy.\u201d Follow Cade Metz on Twitter: @CadeMetz    Advertisement    Collapse SEE MY OPTIONS","time":1525802245,"title":"Why A.I. And Cryptocurrency Are Making One Type of Computer Chip Scarce","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/08\/technology\/gpu-chip-shortage.html","label":5,"label_name":"ml"},{"by":"amasad","descendants":0,"id":17023030,"kids":"None","score":2,"text":"We use cookies to provide you with a better onsite experience. By continuing to browse the site you are agreeing to our use of cookies in accordance with our Cookie Policy. (examples: physics, climate change, etc.) The connection between mother and child is ever deeper than thought The link between a mother and child is profound, and new research suggests a physical connection even deeper than anyone thought. The profound psychological and physical bonds shared by the mother and her child begin during gestation when the mother is everything for the developing fetus, supplying warmth and sustenance, while her heartbeat provides a soothing constant rhythm. The physical connection between mother and fetus is provided by the placenta, an organ, built of cells from both the mother and fetus, which serves as a conduit for the exchange of nutrients, gasses, and wastes. Cells may migrate through the placenta between the mother and the fetus, taking up residence in many organs of the body including the lung, thyroid, muscle, liver, heart, kidney and skin. These may have a broad range of impacts, from tissue repair and cancer prevention to sparking immune disorders. It is remarkable that it is so common for cells from one individual to integrate into the tissues of another distinct person. We are accustomed to thinking of ourselves as singular autonomous individuals, and these foreign cells seem to belie that notion, and suggest that most people carry remnants of other individuals. As remarkable as this may be, stunning results from a new study show that cells from other individuals are also found in the brain. In this study, male cells were found in the brains of women and had been living there, in some cases, for several decades. What impact they may have had is now only a guess, but this study revealed that these cells were less common in the brains of women who had Alzheimer\u2019s disease, suggesting they may be related to the health of the brain. We all consider our bodies to be our own unique being, so the notion that we may harbor cells from other people in our bodies seems strange. Even stranger is the thought that, although we certainly consider our actions and decisions as originating in the activity of our own individual brains, cells from other individuals are living and functioning in that complex structure. However, the mixing of cells from genetically distinct individuals is not at all uncommon. This condition is called chimerism after the fire-breathing Chimera from Greek mythology, a creature that was part serpent part lion and part goat. Naturally occurring chimeras are far less ominous though, and include such creatures as the slime mold and corals. \u00a0Microchimerism is the persistent presence of a few genetically distinct cells in an organism. This was first noticed in humans many years ago when cells containing the male \u201cY\u201d chromosome were found circulating in the blood of women after pregnancy. Since these cells are genetically male, they could not have been the women\u2019s own, but most likely came from their babies during gestation. In this new study, scientists observed that microchimeric cells are not only found circulating in the blood, they are also embedded in the brain. They examined the brains of deceased women for the presence of cells containing the male \u201cY\u201d chromosome. They found such cells in more than 60 percent of the brains and in multiple brain regions. Since Alzheimer\u2019s disease is more common in women who have had multiple pregnancies, they suspected that the number of fetal cells would be greater in women with AD compared to those who had no evidence for neurological disease. The results were precisely the opposite: there were fewer fetal-derived cells in women with Alzheimer\u2019s. The reasons are unclear. Microchimerism most commonly results from the exchange of cells across the placenta during pregnancy, however there is also evidence that cells may be transferred from mother to infant through nursing. In addition to exchange between mother and fetus, there may be exchange of cells between twins in utero, and there is also the possibility that cells from an older sibling residing in the mother may find their way back across the placenta to a younger sibling during the latter\u2019s gestation. Women may have microchimeric cells both from their mother as well as from their own pregnancies, and there is even evidence for competition between cells from grandmother and infant within the mother. What it is that fetal microchimeric cells do in the mother\u2019s body is unclear, although there are some intriguing possibilities. For example, fetal microchimeric cells are similar to stem cells in that they are able to become a variety of different tissues and may aid in tissue repair. One research group investigating this possibility followed the activity of fetal microchimeric cells in a mother rat after the maternal heart was injured: they discovered that the fetal cells migrated to the maternal heart and differentiated into heart cells helping to repair the damage. In animal studies, microchimeric cells were found in maternal brains where they became nerve cells, suggesting they might be functionally integrated in the brain. It is possible that the same may be true of such cells in the human brain. These microchimeric cells may also influence the immune system. A fetal microchimeric cell from a pregnancy is recognized by the mother\u2019s immune system partly as belonging to the mother, since the fetus is genetically half identical to the mother, but partly foreign, due to the father\u2019s genetic contribution. This may \u201cprime\u201d the immune system to be alert for cells that are similar to the self, but with some genetic differences. Cancer cells which arise due to genetic mutations are just such cells, and there are studies which suggest that microchimeric cells may stimulate the immune system to stem the growth of tumors. Many more microchimeric cells are found in the blood of healthy women compared to those with breast cancer, for example, suggesting that microchimeric cells can somehow prevent tumor formation. In other circumstances, the immune system turns against the self, causing significant damage. Microchimerism is more common in patients suffering from Multiple Sclerosis than in their healthy siblings, suggesting chimeric cells may have a detrimental role in this disease, perhaps by setting off an autoimmune attack. This is a burgeoning new field of inquiry with tremendous potential for novel findings as well as for practical applications.\u00a0But it is also a reminder of our interconnectedness. Are you a scientist who specializes in neuroscience, cognitive science, or psychology? And have you read a recent peer-reviewed paper that you would like to write about? Please send suggestions to Mind Matters editor Gareth Cook, a Pulitzer prize-winning journalist at the Boston Globe. He can be reached at garethideas AT gmail.com or Twitter @garethideas.   Neuroscience. Evolution. Health. Chemistry. Physics. Technology. Follow us \u00a9 2018 Scientific American, a Division of Nature America, Inc. All Rights Reserved.  Get the perfect gift for mom","time":1525802217,"title":"Scientists Discover Children\u2019s Cells Living in Mothers\u2019 Brains (2012)","type":"story","url":"https:\/\/www.scientificamerican.com\/article\/scientists-discover-childrens-cells-living-in-mothers-brain\/","label":8,"label_name":"science"},{"by":"benryon","descendants":0,"id":17023003,"kids":"None","score":1,"text":"Every big tech company is an AI company these days, but none more so than Google. To underline the point ahead of its I\/O developers conference, the company has rebranded its Google Research division as Google AI, reflecting the centrality of artificial intelligence to the company\u2019s future.  In a blog post announcing the news, the company said the rebrand was to \u201cbetter reflect [its] commitment\u201d to integrating AI into various services. It follows an organizational reshuffle last month which saw AI product development split up from Google\u2019s search efforts, and veteran Googler Jeff Dean taking the helm of the new division. A newly-revamped homepage for Google AI also emphasizes more than just the company\u2019s consumer products, highlighting recently-published research in topics like health and astronomy and open-source tools used by the AI community worldwide, like the machine learning framework Tensor Flow. (Important to note also: non-AI research will still be done under in the new \u201cGoogle AI\u201d division.) This focus on research and community contrasts slightly with Microsoft, which has also been pushing its AI credentials this week at its Build conference. But for Microsoft the message has been more about AI ethics and morality, with the company launching a new $25 million AI for Accessibility fund to develop the tech for people with disabilities. Google does plenty of work in the field of AI ethics too, but it\u2019s interesting to see these two titans of the tech world trying to differentiate their message on the same subject.  Last month in a letter to investors, Google\u2019s co-founder Sergey Brin warned of the threats posed by AI, like job destruction, biased algorithms, and misinformation. He also called AI \u201cthe most significant development in computing in my lifetime.\u201d Google\u2019s rebranding of its research division drives that point home.  Command Line delivers daily updates from the near-future.","time":1525802104,"title":"AI is so important to Google it\u2019s rebranding its research division","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17330290\/google-ai-research-division-rebranding","label":5,"label_name":"ml"},{"by":"mgdo","descendants":0,"id":17022989,"kids":"None","score":2,"text":"Lots of post on this site begin with an appeal to Newtons second law. It\u2019s an extremely useful tool for analyzing dynamic systems. This post wont begin this way. It\u2019ll begin with Newtons third law. \u201cFor every action there is an equal and opposite reaction\u201d This means that anytime a force acts on an object, an equal and opposite force acts on the object causing the force.\u00a0 If you lift up a ball an inch or two and let it fall it is pulled back down by the earths gravitational force. If you lift your arm up you can feel a force pulling your arm, like the ball down. That force pulling on your arm, isn\u2019t the only force. From Newtons third law we know that there\u2019s a second force.\u00a0 It\u2019s acting on the earth and its magnitude is equal to the force acting on you, but it is opposite in direction. There\u2019s also a third force from mars and a fourth from Jupiter, etc. Because we are so close to Earth, its gravitational pull is so much larger than the rest, but all objects with mass provide a small but real force on you. In conjunction to newtons third law, you also exert a force on every object with mass in the universe. (Note: Gravity only propagates at the speed of light but take a moment to forget that and feel connected to the universe) Now that we\u2019ve waxed poetic for a paragraph, lets get to what is the n-body problem? It\u2019s simply the problem of how to describe the motion of a number, n, different objects interacting with each other gravitationally. An example of this would be our solar system. If we wanted to map out the movement of the moon with respect to an initially fixed sun we would need to take into account the relative positions of the sun, the moon, and the earth and the gravitational forces they cause on each other. Because we are taking into account 3 objects, this would be called a 3-body problem. (Note there is a great sci-fi book with the name The 3-Body Problem that I highly recommend). If we wanted to predict the moon\u2019s motion with more accuracy, we could take into account the forces from\u00a0 Mars, Jupiter, Venus, and Neptune. This would turn it into a 7-body problem. We have an analytical solution to the 2-body problem. We have solutions to several restricted 3-body problems, but no general analytic solution. The search for analytical solutions to the n-body problem has led to many modern mathematical advances. It has been worked on by some of the best minds in the field like Euler, Newton, and Gauss, and is still an active research topic. The magnitude of gravitation force between two objects can be described as  Here m1 is the mass of the first object and m2 is the mass of the second object. G is the universal gravitational constant, and r is the distance between the two objects. There are two interesting characteristics of this equation. The first is that the distance portion is quadratic. If you halve the distance between two objects, the force between them quadruples. The second, is that there is a singularity when there is no distance between the two bodies. We\u2019ll end up having to deal with that problem later. Now that we have the magnitude, let\u2019s incorporate a term for direction. For this we need to know the relative positions of the the two particles as a unit-normal vector. Unit here just means it\u2019s magnitude is one.  Two notes on convention: the four vertical lines represent the two norm of a number and the bold symbol represent a vector.  Note on vectors: If we have the position of the two particles on a coordinate system, how do we get their relative position?\u00a0 Looking at the diagram to the right we have two particles, P1 and P2. We also have r1 and r2 and want to get r.  We label it r21 because it goes from particle 2 to particle 1 Now if we want to describe how to go from particle 1\u00a0 to particle 2 all we need to do is flip the terms and we get  With this new information mean we can again rewrite Newton\u2019s law of gravitation acting on point 1 from point 2 as \\ Now, by applying to newtons second law, describing the governing equations behind the n-body problem is quite simple.  To simulate the N-body problem, lets turn to trusty old MatLab. I\u2019m gonna use ODE45 and numerically integrate the equations forward. In order to do this I need to provide an initial starting position and velocity states, which I chose at random. Above we can see a simulation of a 10 body system evolving over 100 seconds. To visualize it I used a comet plot, which is where the current position is shown as a circle and a subset of past positions is shown as a line. These plots are extremely useful to visualize change over time if you want to track movement.  To the right you can see the paths that all 10 bodies traced out durring their entire 100\u00a0second path. We can easily see how the simple governing equations from above came together to form an extremely complex dynamic system. Now if your remember from earlier, there is a singularity when the two bodies are right on top of each other. Most of the time this is solved by having bodies that come within a certain range collide and either merge together or bounce off each other. In my case, just for the sake of simplicity I modified newtons law of gravitational attraction to the following.  This makes it so that as the norm or r goes to zero the denominator goes to .00001 which removes the singularity.  To the right we can see another 8 body system from an inertial viewpoint. Below we have the same 8 body system that the graph to the right shows, but from two different viewpoints. Instead of being fixed in space and letting the bodies evolve over time, we fix our viewpoints on a single body and observe the rest of the bodies evolve over time. They both describe the same system, but look completely different because our viewpoints are evolving over time. Without the title it would be easy to mistake them for two different systems. Now this will not be a stand alone post. It\u2019s the start of a series called the n-body problem. I will start with the most simple of these problems, where n is equal to 2, and work my way up exploring different concepts and branching off on tangents that I find interesting and the mathematical advances these problems helps spur. While the name says n-body problem and we\u2019ll be using it as a muse for the posts the main thrust of this course will be to cover astrodynamics.\u00a0 I\u2019m beginning my PhD in satellite trajectory design this coming fall and\u00a0 to prepare for it I will be going through Richard Battin\u2019s Introduction to Astrodynamics.\u00a0 It is a tome filled with knowledge, and I hope to explore it in a new, more interactive medium. Quite a lot of the concepts in astrodynamics are geometric in nature and animated gifs are just not possible in a book. Additional I may look at developing jupyter notebooks on orbital dynamics so that you may interact with them and gain a more intuitive understanding of a topic that I think is really damn cool. If you want to get these, and all the other posts from Gereshes, delivered directly to your email when they come out every Monday, there\u2019s an email signup here. If you can\u2019t wait for next weeks post and want some more Gereshes I suggest How to land a lunar Lander Rollout of a rocket motor test stand How to pump a swing using math \u00a0 Your email address will not be published. Required fields are marked *      \n\n   Notify me of follow-up comments by email.  Notify me of new posts by email. ","time":1525802036,"title":"What is the n-body problem?","type":"story","url":"https:\/\/gereshes.com\/2018\/05\/07\/what-is-the-n-body-problem\/","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17022976,"kids":"None","score":3,"text":"\n                            TNW uses cookies to personalize content and ads to\n                            make our site easier for you to use.\n                            We do also share that information with third parties for\n                            advertising & analytics.\n                         \n            by Tristan Greene\n            \u2014 \n                        in Artificial Intelligence\n The first \u201cOMG\u201d moment of IO: Google Assistant can call businesses on your behalf to schedule appointments for you. It\u2019s called \u201cDuplex\u201d and it\u2019s astoundingly realistic. 60 percent of businesses don\u2019t have online booking setup, according to Google CEO Sundar Pichai, which means Google Assistant can\u2019t book most appointments online. With the Duplex feature, it\u2019ll be able to make phonecalls in the background and have a real-time conversation with whoever answers the phone. Google Assistant will actually call a business and schedule an appointment for you. We listened to a conversation between Google Assistant and a worker at a hair salon and it sounded pretty amazing. It was intuitive, and it\u2019s unclear whether the human in this equation was actually aware they were talking to a computer. It was impressive, real-time, and Google Assistant\u2019s responses (the crowd laughed when the Assistant responded \u201cmm-hmm\u201d when the salon worker asked it to hang on a moment. There\u2019s no set date for the update yet, but it should roll out later this year. Take that, Turing test. Check out our event page for more Google I\/O stories this week, or follow our reporters on the ground until the event wraps on Thursday: @bryanclark and @mrgreene1977. \nRead next:\n\n        You won't have to say 'Hey Google' every time you talk to the Assistant anymore    \n Stay tuned with our weekly recap of what\u2019s hot & cool by our CEO Boris. \n        Join over 260,000 subscribers!\n     \n                Sit back and let the hottest tech news come to you by the magic of electronic mail.\n             \n                Prefer to get the news as it happens? Follow us on social media.\n             \n1.76M followers\n                         \n1M likes\n                         \n                Got two minutes to spare? We'd love to know a bit more about our readers.\nStart!\n \n                All data collected in the survey is anonymous.\n            ","time":1525801997,"title":"Google Assistant will call businesses and schedule appts for you on the phone","type":"story","url":"https:\/\/thenextweb.com\/artificial-intelligence\/2018\/05\/08\/google-assistant-will-call-businesses-and-schedule-appointments-for-you-on-the-phone\/","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17022975,"kids":"None","score":2,"text":"We got a sneak peek at Google's upcoming slate of smart displays earlier this year at CES, and now it seems we know when they'll finally come to market. At Google's I\/O developer conference, the company announced that the first round of smart displays will ship starting this July. At I\/O, Google showed off how you could also watch YouTube TV, which is Google's over-the-top live TV service, on those smart displays. You could use Google Assistant to walk you through steps of a recipe, complete with visual aids. You can also use these smart displays to see what your Nest cam sees. It's unclear exactly which brands will be included in this first wave, but the ones we saw at CES are likely a sure bet. That includes Lenovo's Smart Display, the JBL Link View and the LG ThinQ View. Click here to catch up on the latest news from Google I\/O 2018! The show stars Timothy Olyphant and Drew Barrymore. They cut through the winter confusion to see what's actually present. Surprise, surprise: The Pixelbook will get the feature first.  They could make life a lot easier for millions of workers. It's coming alongside support for starting orders inside the app.","time":1525801984,"title":"Google's smart displays will start shipping in July","type":"story","url":"https:\/\/www.engadget.com\/2018\/05\/08\/googles-smart-displays-will-ship-in-july\/","label":7,"label_name":"random"},{"by":"pencildiver","descendants":0,"id":17022968,"kids":"None","score":2,"text":"For the past year, we\u2019ve been emphatically telling our customers that building an app with Hatch will cost a fraction of the price of traditional software development. \u201cThe average cost to build an app is well into the six figures,\u201d we\u2019d say. We\u2019ve been saying it over and over\u2014 and with gusto. A couple of months ago, we decided it was about time for us to dig up some of our own data to support our claims, so we emailed a request for proposals (RFP) to 100 firms to get a specific type of app priced out. A few weeks later, 49 of them responded with estimated development costs and timelines. So, what did we find? The RFP described an app that our customers can build on the Hatch Platform (on our essentials tier at $1,000\/mo). The basic features included: From a web-based dashboard, we requested that the app administrator would be able to manipulate content, add buttons, manage users, and capitalize on all the other features that are standard with a Hatch Apps dashboard (because who wants to pay for developer hours to make these small changes, right?). To simplify the RFP, and make sure we weren\u2019t getting quotes something too robust that would inflate the agency prices, we did not include the app design editor, whereby users could modify the design while live in the app stores, as they\u2019re able to do with the current Hatch Apps platform.) Over the first year, all-in development for this app (just for software development and maintenance, not including hosting or other tools) would cost $123,918 in the first year on average, and $183,805 over three years. Those costs would soar to $282,470 in year one and $418,862 in three years in order to work with a domestic firm.  That means that, over the course of just the first year, working with a traditional development firm will cost 10 times more than working with Hatch Apps, or 24 times more if working with a domestic firm. And, with Hatch Apps, users can make instant design changes, a feature not included in the RFP as distributed.  Shows pricing estimates for development and maintenance, grouped by price range. We intended to focus on domestic firms and searched accordingly, but it seems that many firms had a small office in the United States but were leveraging more affordable (and possibly lower quality) development labor in other countries. The people whom we were emailing with from these firms often had a limited command of English, and had trouble understanding our requests. Through further research on LinkedIn and firm websites, we uncovered that much of the proposed development would happen offshore, we further stratified the results to differentiate between domestic and offshore development. We were also surprised to see that several firms ignored our explicit requests or RFP requirements (e.g. we asked for native apps and were priced for hybrid apps, or we asked for both mobile and web apps and were priced for just one platform), resulting in lower initial quotes that could be \u201cre-negotiated\u201d at a later date, as most quotes were \u201csubject to change\u201d based on actual hours worked. Overall, it\u2019s important to note that the above costs are estimates, not actual development price tags. Actual costs and timelines are notoriously likely to exceed estimates. After quotes were received and proposals were rejected, firms would reply that they could negotiate on price, indicating that pricing may be variable. One firm, for example wrote this: \u201cIf it\u2019s a cost issue \u2013 that\u2019s always something that can be discussed. We really like the project and would be willing to work with you if you wanted us to match a certain budget range.\u201d So, if you\u2019re thinking about engaging a dev shop, it could be safe to say that you shouldn\u2019t go with the \u201csticker price\u201d. Some firms replied not with a proposal, but instead with a quote for some sort of discovery process. One company quoted us $17,500 for their \u201crapid ideation\u201d process, in which they wouldn\u2019t build any software at all. Another firm quoted $70,000+ for \u201cstrategy, planning and requirements.\u201d Yet another quoted us $68,000 for \u201cdiscovery, design and technical architecture.\u201d Another proposed a $45,000 for a two-week discovery process. By going through these processes, we could be promised an exact quote for development (as opposed to an estimate). While we\u2019ve scratched the surface of our research and findings, that\u2019s just the beginning. Download the full white paper to access original data, accompanying charts, methodologies, assumptions, caveats, analysis, and observations. You\u2019ll also access a summary of other studies about app development costs. If you want the full scoop about how much it costs to build an app, and how to get the best price without sacrificing quality, download the full white paper here. Conducting this study and analyzing the results would not have been possible without incredible support from our Hatch Apps team. I want to share a special thank you to Cayli Baker for supporting in the data visualization for this white paper. Thanks also to Asmaaul Chowdhury, Shania Lin, Gina Fuchs and Selina McPherson for their contributions to this project.  Share this blog post:  Start Designing Your Custom App COO & Cofounder     Subscribe  Thank you for your interest in Hatch! A member of the team will be in touch to schedule your demo. Or Invite other business leaders to save time and money by building their apps with Hatch.","time":1525801927,"title":"A study that shows the price variability of app development","type":"story","url":"https:\/\/hatchapps.com\/blog\/how-much-does-it-cost-to-build-an-app-this-study-shows-just-how-variable-pricing-really-is","label":7,"label_name":"random"},{"by":"mercurialshark","descendants":0,"id":17022965,"kids":"None","score":1,"text":"The selection and placement of stories on this page were determined automatically by a computer program. The time or date displayed reflects when an article was added to or updated in Google News.","time":1525801916,"title":"Google News Revamped \u2013 News Aggregator Using Reinforced Learning","type":"story","url":"https:\/\/news.google.com\/news\/","label":9,"label_name":"tech"},{"by":"ivank","descendants":229,"id":17022963,"kids":"[17023963, 17024262, 17023226, 17024202, 17023768, 17023027, 17024613, 17024570, 17023137, 17024214, 17023328, 17023667, 17023412, 17024011, 17024648, 17023070, 17023741, 17024353, 17023110, 17024149, 17024663, 17023417, 17024034, 17024461, 17024649, 17023201, 17024488, 17023340, 17024178, 17024371, 17023464, 17023271, 17024389, 17023478, 17024403, 17023908, 17023747, 17024171, 17023966, 17023651, 17023638, 17023151, 17023839, 17024268, 17023659, 17023750, 17023537, 17023309, 17023237, 17024009]","score":472,"text":"","time":1525801904,"title":"Google Duplex: An AI System for Accomplishing Real World Tasks Over the Phone","type":"story","url":"https:\/\/ai.googleblog.com\/2018\/05\/duplex-ai-system-for-natural-conversation.html","label":7,"label_name":"random"},{"by":"sp332","descendants":0,"id":17022959,"kids":"None","score":2,"text":"Smug house-brand agnostic. \nNAME \nEMAIL Required, will not be published  \nWEBSITE  \n\n   \n\nCat and Girl is licensed under a Creative Commons\r\nAttribution-NonCommercial-ShareAlike 2.5 License.\r\n\n\n\r\nSearch capability by OhNoRobot.com.\r\n\r\n\r\n      Cat and Girl is powered by WordPress with ComicPress. \r\n      Subscribe RSS: Entries | Comments\n\n","time":1525801871,"title":"Brands are our gods","type":"story","url":"http:\/\/catandgirl.com\/ye-gods\/","label":7,"label_name":"random"},{"by":"schaunwheeler","descendants":0,"id":17022951,"kids":"None","score":2,"text":"I think I\u2019m like most data scientists in that there are certain things I tend to do repeatedly across projects: there are certain patterns I check for, or certain things I plot, or certain relationships I try to understand. Sometimes it\u2019s because I\u2019ve made a mistake often enough that I find it pays to automatically guard against it. Sometimes it\u2019s because I just find that, personally, certain ways of looking at a problem help me wrap my mind around it. In any case, large parts of my workflow for any given project very often look very similar to large parts of my workflows for previous projects. Not surprisingly, I\u2019ve found it useful to reduce the overhead of these repetitive activities. While I\u2019ve understood the value of reducing overhead through automation ever since I learned how to code, I never got much advice on how to do it. It\u2019s just something I picked up. In this post, I try to summarize some of the things I\u2019ve learned, and I\u2019ll use some code from a recently-automated part of my workflow as an example. Good design advice has been around at least since Vitruvius wrote Ten Books on Architecture (still a worthwhile read), where he said that good design achieves firmness, usefulness, and delight. It\u2019s relatively easy to translate the first two of those ideals to workflow automation design: whatever we build, it should consistently do the job we want it to do at the scale, speed, and quality that we need. But I think it\u2019s the last ideal\u200a\u2014\u200adelight\u200a\u2014\u200athat most often makes the difference between a workable design and a good design. Fred Brooks argues that delight, in the context of technical design, should be thought of as \u201clogical beauty\u201d, and argues that a logically beautiful product is one that minimizes mental effort. I think minimization of mental effort is an interesting goal for a data science workflow, since many recent embarrassments in the data industry seem to have arisen from practitioners not expending enough mental effort thinking about the potential consequences of their design decisions. So we need to carefully select which areas of mental effort to minimize. That means the first step of a good design is, no matter how trite it sounds, to decide what we want to accomplish. Think about driving a car. I drive a car to go from where I\u2019m going as efficiently as possible. A well-designed car should allow me to reserve as much of my attention as possible for those things most directly related to accomplishing my purpose. That means minimizing the mental effort on everything else. In other words, while I\u2019m driving, I should be able to focus on making my turn at the right place, on avoiding things like pedestrians or lampposts, and things like that. I should not be focused on making sure the engine runs, or on shifting gears, or on how the brakes work. If I used the car for another purpose I might focus on those things, but they are not proper to the accomplishment of the particular purpose I have chosen. A good design shouldn\u2019t distract me or prevent me from paying attention to the things that could prevent from from accomplishing my purpose. Propriety is a somewhat old-fashioned way of talking about relevance. Brooks defines propriety in design as \u201cdo not introduce what is immaterial.\u201d I\u2019ve found it useful to think about propriety in terms of the tradeoffs I used in the analogy. If doing something ensures that I am not prevented from accomplishing my purpose, it is proper to that purpose. If doing something just facilitates the the accomplishment of the purpose, it\u2019s not proper to that purpose. Failing to avoiding pedestrians or lampposts will necessarily prevent me from getting where I want to go. That is true if I travel by automobile, foot, skateboard, bike, airplane, or train. Failing to have a combustion engine won\u2019t necessarily prevent me from getting where I want to go; therefore, it\u2019s not proper to the design, no matter how convenient it is a matter of implementation. So after we\u2019ve defined our purpose, define what\u2019s proper to that purpose: as a rule of thumb, if it can move us forward, it may or may not be proper; if it can keep us from moving forward, it\u2019s proper. Good design minimizes the mental effort expended on tasks that are not proper to our intended purpose. Orthogonality is another component of logical beauty. Brooks summarizes this as \u201cdo not link what is independent\u201d. In other words: my ability to avoid pedestrians and lamp posts partially depends upon being able to see those things, and partially upon being able to maneuver the car to avoid them. A windshield\u2019s functionality and a steering wheel\u2019s functionality should be independent of one another. A design that failed to maintain that independence would be a poor design. So after we define the proper components of our workflow, we minimize dependencies. Good design keeps proper components separate from components that aren\u2019t proper so we don\u2019t accidentally minimize the mental effort we expend on the the things that actually need our full attention. Likewise, good design keeps non-proper components from each other so we don\u2019t accidentally minimize mental effort in ways we didn\u2019t anticipate. Generality is the last principle I\u2019ll talk about before moving on to the example. Brooks summarizes it as \u201cdo not restrict what is inherent.\u201d In other words, a tire (or engine or wiring, etc.) that fits one car shouldn\u2019t be designed to fit only that car without a very good reason. Good design demands a reason before restricting the use cases to which a piece of functionality can apply. This makes it easier to accommodate unanticipated needs without requiring us to rebuild the whole product. So, to summarize: Designs that follow the above principles tend to be logically beautiful, in that they minimize our mental effort in all the right places. I work a lot with geolocation data, and I often have the need to plot that data. Sometimes I need to plot shapes, and other times I need points. Sometimes I need to see those objects against the background of a satellite image to get some rough context, sometimes I need the more detailed context of roads or labels. I normally have to use some form of this workflow several times a week. Sometimes I have to go through it hundreds of times a day. At any rate, I have to do it often enough that it\u2019s a good candidate for minimizing mental effort. So I only have one purpose: graphically represent longitude and latitude pairs on a picture that represents the real-world context of those coordinates. That purpose defines propriety: anything that doesn\u2019t actually put shapes on the picture is not proper to my purpose, and therefore mental effort expended on that thing should be minimized. In the case of the current example, defining purpose and propriety were relatively easy. That\u2019s not always the case. Now I need to worry about orthogonality. Imagine speaking to someone with absolutely no technical expertise. I usually picture either one of my earliest managers described everything involving code as \u201cnew-fangled\u201d, or my mother who still refuses to get an email address, or a college freshman majoring in the humanities. You get the picture. My task creates a set of questions that my imaginary novice would see as being both necessary and sufficient to create my desired outcome. If my imaginary novice can say \u201cbut why do you need the answer to X?\u201d, that means I\u2019ve included an unnecessary question. If my imaginary novice can say \u201c but I don\u2019t understand how the answers to X, Y, and Z will lead to the outcome\u201d, that means I\u2019m either missing a question or haven\u2019t asked the right question. The reason I like to use an imaginary novice as a sounding board is because I tend to get too technical too fast when I try to design everything. Because I know how to implement things, my brain jumps to implementation questions sooner than it should. If you have a real flesh-and-blood novice who is willing and able to be your sounding board, use that person instead of an imaginary one. If my goal is to get shapes on a picture, then I have only three questions: Pretty basic questions, right? But these basic questions force me to think in terms of foundational building blocks, which makes it easier to preserve orthogonality. If whatever I eventually build operates in a way that I can\u2019t change how I get shapes without changing how I put those shapes on a picture, then I\u2019ve probably built the thing wrong. The different pieces of functionality should be as independent as possible. Now that I have orthogonal components, I want to make them as general as possible. For that, I need use cases. I\u2019m already biased with preferred ways of doing of the three things I need to do, but I\u2019ll be better off in the long run if I can think of as many ways\u200a\u2014\u200apreferred or not\u200a\u2014\u200aas I can. I might not implement support for all the use cases I list: that will depend on the constraints of the project (mostly how much time, how many resources, and how many competing priorities I have). It\u2019s still best to know as many use cases as possible up front\u200a\u2014\u200aoften, in the course of implementing support for critical use cases, it\u2019s pretty trivial to support additional, as-yet-not-needed use cases simply as a matter of course. This can reduce frustration and wasted time down the road. Here are a bunch of use-cases for the current example: Where do I get the shapes? Where do I get the picture? How do I get the shapes on the picture? The list isn\u2019t exhaustive, of course. If I had more time to think about it, I could build out a more complete list, and would probably be better off for it. But as I have limited time and other things to do, I\u2019m content that I have at least two answers for every question. All I\u2019m really trying to do is avoid a myopic focus on only my immediate needs. You can jump to the bottom of this section to see the full code I implemented, but it might be helpful to read through this discussion first to understand what I did. I chose to rely entirely on the Bokeh library to plot the shapes on the pictures because I already know and like that library\u200a\u2014\u200ait was easy to get something up and running quickly. Likewise, I chose to rely entirely on Google Maps to get the picture because it provides a single source for satellite images, maps, and annotated versions of both of those two things, and because Bokeh already offers a convenient way to call that Google Maps API. I did very little to minimize my mental effort for those parts of my workflow, because crafting the look and feel of my visualization\u200a\u2014\u200achoosing the background image, deciding what shapes to plot, and styling and annotating those shapes\u200a\u2014\u200ais all proper to my purpose, so I want to maintain as much granular control over those things as possible. The Python class I implemented has a `prepare_plot` method, which for the most part just takes optional keyword arguments that get passed to a Bokeh `GMapPlot` object that the method instantiates: The class also has an `add_layer` method for placing shapes on the picture: This method call has only two required arguments. One is a label that tells the method what data source the shape should reference (again, more on that later). The other is a Bokeh model\u200a\u2014\u200athe two models I use most often are `Patches`, for plotting polygons, and `Circle`, for plotting coordinate pairs. There is an optional `tooltips` argument that takes either a string or a list of tuples to create tooltips that should appear when a mouse hovers over a shape. All other keyword argument are passed to the Bokeh model, which gives me granular control over how the shape looks. Most Bokeh models allow both the border and the area of a shape to be styled differently. For example, if I wanted a red dot with a black border, I would set `fill_color` to red and `line_color` to black. Likewise, if I wanted the fill to be 50% transparent but the border to be 0% transparent, I would set `fill_alpha` to 0.5 and `line_alpha` to 1.0. Often, I want fill and line to be styled the same way, so the `add_layer` method accepts additional keywords. If I set `alpha` to 0.5, then both `fill_alpha` and `line_alpha` will be set to 0.5. It\u2019s a small reduction in mental effort, but it\u2019s enough of a reduction to be convenient. The majority of the mental-effort-reduction work I did, however, had to do with preparing the data for plotting. All four of the use cases I mentioned for getting shapes\u200a\u2014\u200alongitude\/latitude pairs, well-known text, shapely objects, and geohashes\u200a\u2014\u200aare use case that I regularly encounter in my daily work. I found that a lot of my time investigating data wasn\u2019t spent on actually trying to understand the plot, but rather on getting the data into the format I needed. I especially incurred a lot of overhead when I had to switch data source types part way through an analysis (for example, I may have set up a workflow to plot geo-coordinate pairs, and then suddenly found I needed to plot geohashes; that required me to restructure part of my workflow). The `add source` method requires data and a label: The data can be a list containing any one of the four types of input values I\u2019ve mentioned, or it can be a dataframe where one column is comprised of those input values (and that column must be explicitly specified). This method performs four basic functions: The class does a bunch of other things, but for the most part, the structure of the class itself is nothing more than an instantiation of the three key parts of my workflow: get shapes, get a picture, put the shapes on the picture. Because I codified that workflow using a few sound design principles (purpose \u2192 propriety \u2192 orthogonality \u2192 generality), it\u2019s relatively easy to extend the class to accommodate new needs. For example, I\u2019d like to expand the class to use WMTS tiles instead of Google Maps. Because of the way the class is designed, I can do that more or less with changes only to the `prepare_plot` method, and nothing else. If I wanted to render the plot using Leaflet or Matplotlib or something else, that would require some more extensive renovation, but still the effects wouldn\u2019t ripple through the whole codebase. The biggest benefit, however, is less a matter of how I wrote the code and more the fact that I wrote any code at all. Yes, encoding my workflow saves time and effort over the long-run, but it also requires me to be explicit about what my workflow is in the first place. In writing this code, I identified several things about my workflow that really made me unhappy. For example, I hated how much time it took to manage the data when I wanted to plot polygons instead of points (or points instead of polygons), so I changed `add_source` method to always assume polygons and the `add_layer` method to automatically calculate a centroid and update the data source whenever a Bokeh model is used that assumes point data but references a data sources that contains polygon data. I had baked assumptions about the polygon vs. point structure of my data into my workflow without realizing it. By codifying my workflow, I recognized that assumption, decided I didn\u2019t like it, and removed it. Codifying my workflow makes my workflow better, and making my workflow better makes me a better coder. And in explicitly thinking about the design of both the code and the workflow, I\u2019ve made it easier to repurpose old work for new problems, and to share my work with colleagues. I suppose it should go without saying that this code isn\u2019t meant to be a perfect representation of the design principles I outlined earlier. Design is a direction, not a destination. I\u2019m painfully aware of how much the code could stand to be improved! By clapping more or less, you can signal to us which stories really stand out. Senior Data Scientist at Valassis Digital. Formerly at Success Academy Charter Schools, U.S. Department of the Army, and a few other places. Anthropologist. Sharing concepts, ideas, and codes.","time":1525801844,"title":"Codify your workflow","type":"story","url":"https:\/\/medium.com\/@schaun.wheeler\/codify-your-workflow-377f5f8bf4c3","label":7,"label_name":"random"},{"by":"gracehaz","descendants":0,"id":17022947,"kids":"None","score":3,"text":"Consider a component that handles a visibility state and passes it down to its children via render props: What would you think about being able to make that state global by just changing a context property on the component? That's what we're going to achieve in this article. First, before talking about context and state in React, let me give you some context on the state of this topic (!). Some months ago I published reas, an experimental UI toolkit powered by React and styled-components. Besides components themselves, I wanted to provide helpers to handle their state. The approach I took at that time was to export some high-order components (HOCs), such as withPopoverContainer, so as to control the visibility state of a Popover component. Take a look at this example: But HOCs have some problems, such as name collision. What if another HOC or a parent component passes its own toggle prop to MyComponent? Things will certainly break. Even before that, inspired by Michael Jackson and his great talk, the React community started to adopt render props over HOCs. Also, React v16.3.0 introduced a new context API, replacing the old unstable one, using render props. I've learned to look at all that stuff that gets hyped up, especially the stuff brought up by the JavaScript community, with a critical eye. This keeps my mind sane and prevents me from having to refactor my code every single day with cool new libraries. Finally, I posted a tweet asking people which they prefer: render props or HOCs. All comments were favorable to render props, which eventually made me turn all HOCs in reas into components with render props: Popover.Container was a regular React component class with a toggle method using this.setState to change this.state.visible. Simple as that. It was good and worked pretty well. However, in one of my projects I had a button that was supposed to control the Popover component placed in a completely different path in the React tree. I either needed to have some sort of global state manager like Redux, or I needed to move Popover.Container up in the tree in a common parent and pass the props down until they touched both button and Popover\u00a0. But this sounded like a terrible idea. Also, setting up Redux and rewriting all the logic I already had with this.setState into actions and reducers just to have that functionality would be an awful job. I think this imminent need of shared state is one of the reasons why people prematurely optimize their apps. That is, setting up all the libraries they might need up front, which includes a global state management library. React's new context API comes in handy to solve this issue. I wanted to keep using regular React local state and only scale up to global state when needed, without needing to rewrite my state logic. That's why I built constate. Let's see how PopoverContainer would look with constate: Now we can wrap our component with PopoverContainer so as to have access to visible and toggle members already passed by Container to the children function as an argument. Also, note that we are passing all props received from PopoverContainer to Container. This means that we can compose it to create a new derived state component, such as AdvancedPopoverContainer, with new initialState and actions. If you're like me, and you like to know how things were implemented under the hood, you're probably thinking about how Container was implemented. So, let's recreate a simple Container component: mapStateToActions is a utility function that passes state to each member of actions. That's what makes it possible to define our toggle function like this: Our goal, however, is to be able to use the same PopoverContainer as a global state. With constate we just need to pass a context prop to Container: Now, every Container with context=\"popover1\" will share the same state. Of course, you're curious about how Container handles that context prop. So here you go: Ok, I'm sorry. Those four added lines don't tell you much. To create Consumer, we need to understand how to deal with the new React Context API. We can break the new React Context API into three parts: Context, Provider and Consumer. Let's create the context: Then, we create our Provider, which uses Context.Provider and passes state and setState down: It can be a little tricky. We can't simply pass { state, setState } as a literal object to Context.Provider's value since it would recreate that object on every render. Learn more here. Finally, our Consumer needs to use Context.Consumer to access state and setState passed by Provider: mapContextToActions is similar to mapStateToActions. The difference is that the former maps state[context] instead of just state. The final step is to wrap our app with Provider: Finally, we have rewritten constate. Now you can use Container component to switch between local and global state with ease. You might be thinking that starting a project with something like constate could also be a premature optimization. And you're probably right. You should stick with this.setState without abstractions as long as you can. However, not all premature optimizations are the root of all evil. You should find a good balance between simplicity and scalability. That is, you should pursue simple implementations, specially if you're building small applications. But, if you're planning to grow, you should look for simple implementations that are also easy to scale. If you like it and find it useful, here are some things you can do to show your support: By clapping more or less, you can signal to us which stories really stand out. Full stack developer, designer, libertarian, astronomy lover. Our community publishes stories worth reading on development, design, and data science.","time":1525801811,"title":"React's new context API: Toggle between local and global state","type":"story","url":"https:\/\/medium.freecodecamp.org\/reacts-new-context-api-how-to-toggle-between-local-and-global-state-c6ace81443d0","label":7,"label_name":"random"},{"by":"teddyfrozevelt","descendants":0,"id":17022942,"kids":"None","score":2,"text":"\n\n\n\n Google Assistant We announced our vision for the Google Assistant just two years ago at I\/O, and since then, we\u2019ve been making fast progress in bringing the Assistant to more people around the world to help them get things done. As of today, the Google Assistant is available on more than 500 million devices, it works with over 5,000 connected home devices, it\u2019s available in cars from more than 40 brands, and it\u2019s built right into the latest devices, from the Active Edge in the Pixel 2 to a dedicated Assistant key in the LG G7 ThinQ. Plus, it\u2019ll be available in more than 30 languages and 80 countries by the end of the year. Today at I\/O, we\u2019re sharing our vision for the next phase of the Google Assistant, as we make it more naturally conversational, visually assistive, and helpful in getting things done.  Language is incredibly complex\u2014people ask about something as simple as the weather in over 10,000 ways (our favorite: \u201cWill it be cats and dogs today?\u201d). We\u2019ve dramatically improved our language understanding so you can speak naturally to your Google Assistant and it will know what you mean.  New voices  Continued ConversationSoon you\u2019ll be able to have a natural back-and-forth conversation without repeating \u201cHey Google\u201d for each follow-up request. The Assistant will be able to understand when you\u2019re talking to it versus someone else, and will respond accordingly. This feature has been one of our top requests and you\u2019ll be able to turn on Continued Conversation in the coming weeks. Multiple ActionsA key part of having a natural conversation is being able to ask about many things at once. With Multiple Actions, which is already starting to roll out, the Google Assistant will be able to understand more complex queries like \u201cWhat\u2019s the weather like in New York and in Austin?\u201d Pretty PleaseAssistant features for families\u2014powered by Family Link\u2014provides free family-friendly games, activities, and stories from content partners like Disney. (Families have listened to over 130,000 hours of children\u2019s stories in the last two months alone.) To help you give your little ones some positive reinforcement when they ask nicely, later this year we\u2019ll introduce Pretty Please, so that the Assistant can understand and encourage polite conversation. Custom and scheduled RoutinesEarlier this year we launched six ready-made Routines to help you get multiple things done with a single command. Starting today, we\u2019re rolling out Custom Routines, which allow you to create your own Routine with any of the Google Assistant\u2019s one million Actions, and start your routine with a phrase that feels best for you. For example, you can create a Custom Routine for family dinner, and kick it off by saying \"Hey Google, dinner's ready\" and the Assistant can turn on your favorite music, turn off the TV, and broadcast \u201cdinner time!\u201d to everyone in the house. Later this summer, you\u2019ll be able to schedule Routines for a specific day or time either using the Assistant app or through the Google Clock app for Android.  So far the Assistant has been centered on the verbal conversation you can have with Google, but now we\u2019re bringing the simplicity of voice together with a rich visual experience.\u00a0\u00a0 Smart DisplaysSmart Displays are\u00a0a new category of devices\u00a0built for the home that let you quickly glance at responses provided by the Google Assistant. You can access the Assistant hands-free by voice, but you can also tap and swipe the screen when that\u2019s easier. You can follow along with a recipe, control your smart home, watch live TV on YouTube TV, and make video calls with Google Duo. Smart Displays come integrated with all your favorite Google services like Calendar, Maps, and YouTube. The first Smart Displays will be available for purchase starting in July. New visual experience for the phoneWe\u2019re also redesigning the Assistant experience on the screen that\u2019s with us all the time\u2014our phones. The Assistant will give you a quick snapshot of your day, with suggestions based on the time of day, location and recent interactions with the Assistant. To provide a summary of tasks and list items, we\u2019re integrating popular notes and lists services from Google Keep, Any.do, Todoist and many more. We\u2019re also bringing a new food pick-up and delivery experience to the Assistant that isn\u2019t constrained by a chat-style interface, so you can order your favorites from Starbucks, Doordash and Applebee\u2019s, in addition to existing partners like Dunkin\u2019 Donuts and Domino\u2019s. The new visual design will be available in the Google Assistant app later this year. Google MapsThe Assistant is coming to navigation in Google Maps later this summer, with a low visual profile so you can keep your hands on the wheel and your eyes on the road. You'll be able to send text messages, play music and podcasts, and get information without leaving the navigation screen. For example, say \u201cHey Google, read me my messages\u201d and you can get a summary of unread texts with the option to respond by voice. Smart Displays\u00a0 The Assistant's new visual experience on mobile Partners that will help you keep track of your notes and lists\u00a0 The Google Assistant helps you save time by taking tasks off your plate, whether that\u2019s ordering a coffee or buying movie tickets online. But sometimes you need to pick up the phone and call a business to get something done. This summer, we\u2019ll start testing a new capability within the Google Assistant to help you make restaurant reservations, schedule hair salon appointments, and get holiday hours. Just provide the date and time, and your Assistant will call the business to coordinate for you. If a business uses an online booking service, the Assistant will book through that. And if not, the Assistant will call the business on your behalf. Powered by a new technology we call Google Duplex, the Assistant can understand complex sentences, fast speech, and long remarks, so it can respond naturally in a phone conversation. Even though the calls will sound very natural, the Assistant will be clear about the intent of the call so businesses understand the context. Once your reservation or appointment is booked, the Assistant will add a calendar reminder for your appointment and allow you to cancel if needed. This technology will be helpful to the many small businesses that rely on phone calls to book appointments today\u2014our research shows that 60 percent of small businesses who rely on customer bookings don\u2019t have an online booking system. And many people simply don't reserve with businesses that don't take online bookings. With Google Duplex, businesses can operate as they always have; and people can easily book with them through the Google Assistant. We\u2019re just getting started, but we\u2019re excited for how Google Duplex can connect small businesses with Google Assistant users. Here's how the Assistant can help make a phone call\u00a0 We\u2019re bringing Google Home and Google Home Mini to seven new countries later this year: Denmark, Korea, Mexico, the Netherlands, Norway, Spain and Sweden. We\u2019re also launching the Google Assistant in Spanish for Android Oreo (Go edition) phones soon. And to make it easier for you to have conversations with others, wherever you are, Google Translate will soon be available on all headphones optimized for the Google Assistant. We\u2019re excited to give you a preview of some of the latest developments we\u2019re bringing to the Google Assistant across your devices. You can start trying some of these today in the Google Assistant app for Android and iOS. And your smart speaker devices like the Google Home will be automatically updated with the latest features once they\u2019re available. We have lots more in store this year, stay tuned! \n              Follow Us\n            ","time":1525801792,"title":"The future of the Google Assistant","type":"story","url":"https:\/\/blog.google\/products\/assistant\/io18\/","label":7,"label_name":"random"},{"by":"yarapavan","descendants":0,"id":17022930,"kids":"None","score":3,"text":"There are three certainties in life: death, taxes and the impenetrability of AWS documentation. For the budding IoT developer, life can seem even more confusing because, at a glance, there are eight independent products you seemingly need to master before you can get started. As a developer adjacent to AWS\u2019 IoT offering, it\u2019s important for me to nurture a deep understanding of the products and how they work together. So every now and again I throw together a home-brew IoT project to brush the dust off and achieve the impossible of keeping up with all the latest features! And this time, dear reader, I\u2019m going to take you along for the ride. Before we kick off the series, though, I\u2019m going to take a wander through AWS\u2019 Garden of IoT. Or IIoT. Or IoS. Or Connected Smart Devices. Or whatever the cool kids are calling it these days- there\u2019s probably a \u201cblockchain\u201d in there somewhere\u2026 So here are the offerings as listed in the AWS Management Console: Unsurprisingly the bulk of AWS\u2019 IoT functionality is provided by Core. There\u2019s a lot going on here, and it can seem a bit disjointed. When it comes down to it, however, AWS offer the following (in an opinionated order of immediate usefulness!): IoT Core also encompasses a series of special \u2018on device\u2019 SDKs that allow your devices to easily interact with the IoT specific APIs (most notably the MQTT broker and the Device Shadow). Remember what I said about AWS Documentation? Despite how things might look like from the outside, IoT Device Management doesn\u2019t actually exist. It is the name given to describe several aspects of the IoT Core offering (namely groups, jobs and provisioning). All documentation and console links will lead you to AWS IoT Core. There\u2019s little point in connecting all these things if we\u2019re not going to attempt to do something intelligent with the data. Analytics provides some mechanisms for doing this in a way that feels something like a cross between the Rules functionality from IoT Core and Kinesis\u2019 (AWS\u2019 stream offering) own Data Analytics. The feature allows you to create pipelines that listen to data from a set of the MQTT Broker topics (there\u2019s a reason I said the broker was the most useful part of the set), manipulate it using a lambda, transform it using something similar to XLST for JSON (including enrichment from the device shadow and type\/group) and then dump it in a data lake; which you can then query and visualise using a subset of SQL. A not uncommon pattern in IoT (especially with power\/data constrained devices) is to have a collection of relatively dumb nodes that connect to a local \u2018edge\u2019 device. The edge device works as a \u2018local cloud\u2019 to the devices; providing a nearby hub where data can be processed, messages brokered and a gateway to the (actual) cloud. Greengrass is AWS\u2019 solution to this problem. In someways it\u2019s like having your own mini-AWS in a box. Although in most, it isn\u2019t. Greengrass provides free (closed source) software (unfortunately named \u201cCore\u201d), which you can install onto a linux-class device. That allows you to (in a once again opinionated order of immediate usefulness!): And to finish with the easiest. This is an operating system built and maintained by Amazon for use within the AWS IoT ecosystem. FreeRTOS seems to be as close as anyone will get to a defacto operating system for microcontrollers. Amazon\u2019s FreeRTOS offering adds two things to this open kernel- a set of maintained modular open-source libraries that allow interaction with the wider AWS IoT ecosystem (including OTA update support), coupled with the \u2018certification\u2019 of devices as being supported by AWS. If your device is a Raspberry Pi (or other linux-class) device, then FreeRTOS won\u2019t hold much interest for you. If your solution is power-constrained, however, chances are you\u2019re looking at microcontrollers (I had the difference explained to me as \u201cif the description of what it does includes \u2018and\u2019 more than twice, you need a Pi!\u201d). At which point you might consider FreeRTOS to save you a lot of the boilerplate; just know that only 4 of the 40 platforms supported by FreeRTOS are AWS Qualified. And that\u2019s it! (If you\u2019re wondering why there aren\u2019t 8 things, it\u2019s because the rest are made of one-click buttons\u2026). For the rest of the series I\u2019ll be taking a much deeper dive into each of these offerings, so watch this space! By clapping more or less, you can signal to us which stories really stand out. writes about the stuff he does in code. The DevicePilot engineering blog. Talking about IoT, serverless, big data, and everything else we learn inbetween! https:\/\/devicepilot.com","time":1525801701,"title":"The Hitchhikers Guide to AWS IoT","type":"story","url":"https:\/\/medium.com\/devicepilot\/the-hitchhikers-guide-to-aws-iot-1013742ae5bc","label":3,"label_name":"dev"},{"by":"calhat","descendants":1,"id":17022918,"kids":"[17022970]","score":2,"text":"","time":1525801595,"title":"Show HN: Counselling for \u00a39.99 \/month","type":"story","url":"http:\/\/www.spill.chat\/","label":7,"label_name":"random"},{"by":"yarapavan","descendants":0,"id":17022917,"kids":"None","score":3,"text":"Across the globe, algorithms are quietly but increasingly being relied upon to make important decisions that impact our lives. This includes determining the number of hours of in-home medical care patients will receive, whether a child is so at risk that child protective services should investigate, if a teacher adds value to a classroom or should be fired, and whether or not someone should continue receiving welfare benefits.\u00a0  The use of algorithmic decision-making is typically well-intentioned, but it can result in serious unintended consequences. In the hype of trying to figure out if and how they can use an algorithm, organizations often skip over one of the most important questions: will the introduction of the algorithm reduce or reinforce inequity in the system?  There are various factors that impact the analysis. Here are a few that all organizations need to consider to determine if implementing a system based on algorithmic decision-making is an appropriate and ethical solution to their problem:  Before implementing a decision-making system that relies on an algorithm, an organization must assess the potential for the algorithm to impact people\u2019s lives. This requires taking a close look at who the system could impact and what that would look like, and identifying the inequalities that already exist in the current system\u2014all before ever automating anything. We should be using algorithms to improve human life and well-being, not to cause harm. Yet, as a result of bad proxies, bias built into the system, decision makers who don\u2019t understand statistics and who overly trust machines, and many other challenges, algorithms will never give us \u201cperfect\u201d results. And given the inherent risk of inequitable outcomes, the greater the potential for a negative impact on people\u2019s lives, the less appropriate it is to ask an algorithm to make that decision\u2014especially without implementing sufficient safeguards.\u00a0  In Indiana, for example, after an algorithm categorized incomplete welfare paperwork as \u201cfailure to cooperate,\u201c one million people were denied access to food stamps, health care, and cash benefits over the course of three years. Among them was Omega Young, who died on March 1, 2009 after she was unable to afford her medication; the day after she died, she won her wrongful termination appeal and all of her benefits were restored. Indiana\u2019s system had woefully inadequate safeguards and appeals processes, but the the stakes of deciding whether someone should continue receiving Medicaid benefits will always be incredibly high\u2014so high as to question whether an algorithm alone should ever be the answer.\u00a0  Virginia Eubanks discusses the failed Indiana system in Automating Inequality, her book about how technology affects civil and human rights and economic equity. Eubanks explains that algorithms can provide \u201cemotional distance\u201d from difficult societal problems by allowing machines to make difficult policy decisions for us\u2014so we don\u2019t have to. But some decisions cannot, and should not, be delegated to machines. We must not use algorithms to avoid making difficult policy decisions or to shirk our responsibility to care for one another. In those contexts, an algorithm is not the answer. Math alone cannot solve deeply-rooted societal problems, and attempting to rely on it will only reinforce inequalities that already exist in the system.  Algorithms rely on input data\u2014and they need the right data in order to function as intended. Before implementing a decision-making system that relies on an algorithm, organizations need to drill down on the problem they are trying to solve and do some honest soul-searching about whether they have the data needed to address it.  Take, for example, the department of Children, Youth and Families (CYF) in Allegheny County, Pennsylvania, which has implemented an algorithm to assign children \u201cthreat scores\u201d for each incident of potential child abuse reported to the agency and help case workers decide which reports to investigate\u2014another case discussed in Eubanks\u2019 book. The algorithm\u2019s goal is a common one: to help a social services agency most effectively use limited resources to help the community they serve. To achieve their goal, the county sought to predict which children are likely to become victims of abuse, i.e., the \u201coutcome variable.\u201d But the county didn\u2019t have enough data concerning child-maltreatment-related fatalities or near fatalities to create a statistically meaningful model, so it used two variables that it had a lot of data on\u2014community re-referrals to the CYF hotline and placement in foster care within two years\u2014as proxies for child mistreatment. That means the county\u2019s algorithm predicts a child\u2019s likelihood of re-referral and of placement in foster care, and uses those predictions to assign the child a maltreatment \u201cthreat score.\u201d  The problem? These proxy variables are not good proxies for child abuse. For one, they are subjective. As Eubanks explains, the re-referral proxy includes a hidden bias: \u201canonymous reporters and mandated reporters report black and biracial families for abuse and neglect three and a half more often than they report white families\"\u2014 sometimes even by angry neighbors, landlords, or family members making intentionally false reports as punishment or retribution. As she wrote in Automating Inequality, \u201cPredictive modeling requires clear, unambiguous measures with lots of associated data in order to function accurately.\u201d Those measures weren\u2019t available in Allegheny County, yet CYF pushed ahead and implemented an algorithm anyway.\u00a0  The result? An algorithm with limited accuracy. As Eubanks reports, in 2016, a year with 15,139 reports of abuse, the algorithm would have made 3,633 incorrect predictions. This equates to the unwarranted intrusion into and surveillance of the lives of thousands of poor, minority families.  The lack of sufficient data may also render the application of an algorithm inherently unfair. Allegheny County, for example, didn\u2019t have data on all of its families; its data had been collected only from families using public resources\u2014i.e., low-income families. This resulted in an algorithm that targeted low-income families for scrutiny, and that potentially created feedback loops, making it difficult for families swept up into the system to ever completely escape the monitoring and surveillance it entails. This outcome offends basic notions of what it means to be fair. \u00a0It certainly must not feel fair to Allegheny County families adversely impacted. There are many measures of algorithmic fairness. Does the algorithm treat like groups similarly, or disparately? Is the system optimizing for fairness, for public safety, for equal treatment, or for the most efficient allocation of resources? Was there an opportunity for the community that will be impacted to participate in and influence decisions about how the algorithm would be designed, implemented, and used, including decisions about how fairness would be measured? Is there an opportunity for those adversely impacted to seek meaningful and expeditious review, before the algorithm has caused any undue harm?  Organizations should be transparent about the standard of fairness employed, and should engage the various stakeholders\u2014including (and most importantly) the community that will be directly impacted\u2014in the decision about what fairness measure to apply. If the algorithm doesn\u2019t pass muster, it should not be the answer. And in cases where a system based on algorithmic decision-making is implemented, there should be a continuous review process to evaluate the outcomes and correct any disparate impacts. Another variable organizations must consider is how the results will be used by humans. In Allegheny County, despite the fact that the algorithm\u2019s \u201cthreat score\u201d was supposed to serve as one of many factors for caseworkers to consider before deciding which families to investigation, Eubanks observed that \u201cin practice, the algorithm seems to be training the intake workers.\u201d Caseworker judgment had, historically, helped counteract the hidden bias within the referrals. When the algorithm came along and caseworkers started substituting their own judgment with that of the algorithm, they effectively relinquished their gatekeeping role and the system became more class and race biased as a result Algorithmic-decision making is often touted for its superiority over human instinct. The tendency to view machines as objective and inherently trustworthy\u2014even though they are not\u2014 is referred to as \u201cautomation bias.\u201d There are of course many cognitive biases at play whenever we try to make a decision; automation bias adds an additional layer of complexity. Knowing that we as humans harbor this bias (and many others), when the result of an algorithm is intended to serve as only one factor underlying a decision, an organization must take care to create systems and practices that control for automation bias. This includes engineering the algorithm to provide a narrative report rather than a numerical score, and making sure that human decision makers receive basic training both in statistics and on the potential limits and shortcomings of the specific algorithmic systems they will be interacting with.\u00a0  And in some circumstances, the mere possibility that a decision maker will be biased toward the algorithm\u2019s answer is enough to counsel against its use. This includes, for example, in the context of predicting recidivism rates for the purpose of determining prison sentences. In Wisconsin, a court upheld the use of the COMPAS algorithm to predict a defendant\u2019s recidivism rate on the ground that, at the end of the day, the judge was the one making the decision. But knowing what we do about the human instinct to trust machines, it is na\u00efve to think that the judge\u2019s \u2018inherent distraction\u2019 was not unduly influenced by the algorithm. One study on the impact of algorithmic risk assessments on judges in Kentucky found that algorithms only impacted judges\u2019 decision making for a short time, after which they return to previous habits, but the impact may be different across various communities of judges, and adversely impacting even one person is a big deal given what\u2019s at stake\u2014lost liberty. Given the significance of sentencing decisions, and the serious issues with trying to predict recidivism in the first place (the system \u201cessentially demonizes black offenders while simultaneously giving white criminals the benefit of the doubt\u201d), use of algorithms in this context is inappropriate and unethical.  Finally, algorithms should be built to serve the community that they will be impacting\u2014and never solely to save time and resources at whatever cost. This requires that data scientists take into account the fears and concerns of the community impacted. But data scientists are often far removed from the communities in which their algorithms will be applied. As Cathy O\u2019Neil, author of Weapons of Math Destruction, told Wired earlier this year, \u201cWe have a total disconnect between the people building the algorithms and the people who are actually affected by them.\u201d Whenever this is the case, even the most well-intended system is doomed to have serious unintended side effects.\u00a0  Any disconnect between the data scientists, the implementing organization, and the impacted community must be addressed before deploying an algorithmic system. O\u2019Neil proposes that data scientists prepare an \u201cethical matrix\u201d taking into account the concerns of the various stakeholders that may be impacted by the system, to help \u201clay out all of these competing implications, motivations and considerations and allows data scientists to consider the bigger impact of their designs.\u201d The communities that will be impacted should also have the opportunity to evaluate, correct, and influence these systems.  *** As the Guardian has noted, \u201cBad intentions are not needed to make bad AI.\u201d The same goes for any system based on algorithmic decision-making. Even the most well-intentioned systems can cause significant harm, especially if an organization doesn\u2019t take a step back and consider whether it is ethical and appropriate to use algorithmic decision-making in the first place. These questions are just starting points, and they won\u2019t guarantee equitable results, but they are questions that all organizations should be asking themselves before implementing a decision-making system that relies on an algorithm.  \u00a0","time":1525801593,"title":"Questions We Need to Be Asking Before Deciding an Algorithm Is the Answer","type":"story","url":"https:\/\/www.eff.org\/deeplinks\/2018\/05\/math-cant-solve-everything-questions-we-need-be-asking-deciding-algorithm-answer","label":5,"label_name":"ml"},{"by":"jwalton","descendants":0,"id":17022908,"kids":"None","score":2,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back The official passport documentation has a long, example driven style.  Some people like that.  Some people, on the other hand, want to know \"when I call this function, what's it going to do?\"  This is for those people. If you find inaccuracies, please feel free to open an issue or a PR. When you import 'passport', you get back an instance of the \"Passport\" class.  You can create new passport instances: You'd want to do this if you want to use Passport in a library, and you don't want to polute the \"global\" passport with your authentication strategies. Returns a middleware which must be called at the start of connect or express based apps.  This sets req._passport, which passport uses all over the place.  Calling app.use(passport.initialize()) for more than one passport instance will cause problems. This will also set up req.login() and req.logout(). \"If your application uses persistent login sessions, passport.session() middleware must also be used.\"  Should be after your session middleware. This returns a middleware which will try to read a user out of the session; if one is there, it will store the user in req.user, if not, it will do nothing.  Behind the scenes, this: is actually the same as: which is using the built-in \"session strategy\".  You can customize this behavior by registering your own session strategy. session() does take an 'options' object.  You can set pass {pauseStrem: true} to turn on a hacky work-around for problems with really old node.js versions (pre-v0.10).  Never set this true. strategyName is the name of a strategy you've previously registered with passport.use(name, ...).  This can be an array, in which case the first strategy to succeed, redirect, or error will halt the chain.  Auth failures will proceed through each strategy in series, failing if all fail. This function returns a middleware which runs the strategies.  If one of the strategies succeeds, this will set req.user.  If you pass no options or callback, and all strategies fail, this will write a 401 to the response.  Note that some strategies may also cause a redirect (OAuth, for example). Valid options: Note that the entire options object will be passed on to the strategy as well, so there may be extra options you can pass here defined by the strategy.  For example, you can pass a callbackURL along to an oauth strategy. callback is an (err, user, info) function.  No req, res, or next, because you're supposed to get them from the closure.  If authentication fails, user will be false.  If authentication succeeds, your callback is called and req.user is NOT set.  You need to set it yourself, via req.login(): Don't just set req.user = user, since this won't update your session. This isn't really well named, as it has nothing to do with authorization.  This function is exactly like passport.authenticate(), but instead of setting req.user, it sets req.account, and it doesn't make any changes to the session. This is here so if you want to do something like \"Link my account to OtherService\", you can have a user who is already logged in, and use Passport to retreive their OtherService credentials.  Useful for linking to social media platforms and such. Configure a strategy.  Strategies have a \"default name\" assigned to them, so you don't have to give them a name. Passport will call this to serialize the user to the session.  Should call done(null, user.id). Undocumented: fn() can be a fn(req, user, done).  If multiple serializers are registered, they are called in order.  Can return 'pass' as err to skip to next serialize. Passport will call this to deserialize the user from the session.  Should call done(null, user). It can happen that a user is stored in the session, but that user is no longer in your database (maybe the user was deleted, or did something to invalidate their session).  In this case, the deserialize function should pass null or false for the user, not undefined. Undocumented: fn() can be a fn(req, id, done).  As with serializeUser, serializers are called in order. Write a custom strategy by extending the SessionStrategy class from passport-strategy.  You can unit test a strategy in isolation with passport-strategy-runner. Note when calling fail(), the challenge should be either a challenge string as defined by RFC 7235 S2.1, suitable for including in a WWW-Authenticate header, or else a {message, type} object, were message is the message to use a a \"flash message\", and type is the flash type (defaults to 'error'). Passport strategies require a verify callback, which is generally a (err, user, options?) object.  options.message can be used to give a flash message.  user should be false if the user does not authenticate.  err is meant to indicate a server error, like when your DB is unavailable; you shouldn't set err if a user fails to authenticate. Log a user in (causes passport to serialize the user to the session).  On completion, req.user will be set. Removes req.user, and clears the session. Passport creates a key in the session called session.passport. When a request comes in to the passport.session() middleware, passport runs the built-in 'session' strategy - this calls deserializeUser(session.passport.user, done) to read the user out of the session, and stores it in req.user. You can override how passport deserializes a session by creating a new strategy called 'session' and registering it with passport.use(). When you call req.login(), or when a strategy successfully authenticates a user, passport uses the session manager, and essentially does: Although it's more verbose about it.  You can override the session manager by creating your own implementation and setting passport._sm, but this is not documented or supported, so use at your own risk.","time":1525801552,"title":"Passport.js: The Hidden Manual","type":"story","url":"https:\/\/github.com\/jwalton\/passport-api-docs","label":4,"label_name":"github"},{"by":"happy-go-lucky","descendants":0,"id":17022904,"kids":"None","score":1,"text":"\n    \n      Kristofor Husted\n    \n   From Harvest Public Media \n                Elation is an Angus bull that recently sold for $800,000. His co-owner, Brian Bell, sells Elation's semen for $50 a sample, about double the going rate.\n                \n                \n                    \n                    Kristofor Husted\/Harvest Public Media\n                    \n                \nhide caption\n Elation is an Angus bull that recently sold for $800,000. His co-owner, Brian Bell, sells Elation's semen for $50 a sample, about double the going rate. Beef cattle ranchers are getting wise to the science of genetics. They have always known that making the best steak starts long before consumers pick out the right cut, such as where an animal grazes or what it eats. The key is in the genetic makeup \u2014 or DNA \u2014 of the herd. And over the past year, those genetics have taken a historic leap thanks to new, predictive DNA technology. This is playing out in rising auction prices for the best bulls as identified by genetic profiles or simply for their elite pedigree. \"The demand there is primarily because we have the best genetics in the world,\" said Gordon Doak, president emeritus of the National Association of Animal Breeders. Doak said the science is improving quickly. \"The bull [ranchers] used two years ago \u2014 they're not going to use him again,\" Doak said. \"There's new ones coming online all the time that have better genetics. That's why [demand] has increased.\" Today, ranchers can shop for bulls or bull semen touting a very specific genetic concoction that's prime for improving their herd. And those concoctions are becoming more valuable every day. World record bull Missouri rancher Brian Bell traveled to North Dakota for a cattle auction in February. Like many of the hundreds of attendees, he had his eye on a specific Angus bull named Elation. Elation, raised at Schaff Angus Valley in North Dakota, is descended from a famous family of beef cattle, and has the genetic recipe for efficient, meat-producing cattle. \"Perfect footed. Perfect uddered. Fertile,\" Bell said. \"You know these cattle have some longevity and will stay in people's herds and allow them to be profitable.\"   Elation's caretaker, Isaiah Shnurman, said the bull is treated like a star athlete. Elation spends most of his time in a large pen that has indoor-outdoor access. He exercises and has a mineral tub to lick and a large tire full of hay to snack on. A chiropractor visits once a month. And on Mondays and Thursdays, his semen is collected. Bell and Oklahoma rancher David Bogle teamed up so they could afford to bid on Elation. \"At the end there were three people bidding on the bull \u2014 one was from overseas and then two in the states,\" Bell said. \"And David jumped the last bid I think by $50,000. That was the deal sealer there.\" While some dairy cows have been auctioned for more than a million dollars, the final price for Elation \u2014 $800,000 \u2014 set a world record for a bull. \"It's an investment in our herds,\" Bell said, adding: \"And then also we'll sell semen throughout the world and hopefully help pay for the bull.\" Yes, the semen. More than 2.5 million samples of beef semen were sold in the United States last year. That's double from 2010, according to the National Association of Animal Breeders. Altogether, exports for dairy and beef cattle semen hit a record $170 million in 2017, according to the U.S. Department of Agriculture and U.S. Census Bureau Foreign Trade data. Bull semen is even sold on Amazon. Elation demand Bell and Bogle each hold 40 percent ownership of Elation, with 20 percent staying with Schaff Angus Valley. And so far the investment is working out. While most Angus semen fetches $20 to $25 per sample (also called a straw), Elation's semen is selling for $50. Bell said they already have a waiting list from cattle ranchers across the world. As of April, Bell said he had already sold 4,500 straws and his investment in Elation could be paid off in one to three years, depending on how tasty Elation's hundreds or even thousands of progeny turn out to be for beef customers.  That's because he also plans to use Elation's semen in about 450 of his own cows at Square B Ranch in Missouri. \"That's the most important part, because ultimately that's our customer,\" Bell said. \"In the beef world, we're trying to grow beef whether it's at a high-end restaurant or they're picking it out of the meat department at a grocery store.\" DNA testing About 85 percent of U.S. beef ranchers, though, still develop herd genetics the old-fashioned way \u2014 by setting a bull loose with cows in a pasture, according to estimates by the National Association of Animal Breeders. In buying those bulls, ranchers traditionally have analyzed how a prospective bull's progeny has developed over generations, using genetic predictions called Expected Progeny Differences or EPDs. That shifted in November, when the American Angus Association introduced a genomic testing kit that drills down into even more specific traits. For $37 a pop, ranchers can send in a tissue or blood sample and receive a readout on how that bull's genetic makeup compares to others in the breed. \n                Cattle sales consultant Wes Tiemann reviews the catalog for an Angus auction at Henke Farms in Salisbury, Mo. Each animal in the auction is DNA tested.\n                \n                \n                    \n                    Kristofor Husted\/Harvest Public Media\n                    \n                \nhide caption\n Cattle sales consultant Wes Tiemann reviews the catalog for an Angus auction at Henke Farms in Salisbury, Mo. Each animal in the auction is DNA tested. \"Genomic testing allows us to basically find animals that are better for traits like marbling earlier that have a bigger tendency to pass down those traits later on in life, so that we can create a better product for the consumer,\" said Kelli Retallick, genetic services director for Angus Genetic Incorporated, the breeding arm of the American Angus Association. Retallick said the association has 467,000 genomic profiles of cattle to reference when it analyzes a DNA sample. Researchers help narrow in on traits: Which bulls would produce the fastest growing calves? Which would produce the most fertile cows? Which would grow the biggest rib eyes?  This new tool was in evidence at a recent auction in Salisbury, Mo., where about 100 people gathered to pore through the genetics of the 88 cattle for sale. Each bull was DNA tested to see how it compared to others in the Angus breed. Next to each photo in the sales catalog is what looks like a baseball card that includes a bull's birth date, weight and a chart of statistics from his DNA test. When bulls were brought into the sales ring, the auctioneer yodeled out the results of his DNA test. DNA caveat This new genetic information takes some of the risk out of the equation, said Jared Decker, a cattle geneticist and assistant professor at the University of Missouri. \"If we have a genomic-based, DNA-based genetic prediction, there's less possibility for that genetic prediction to change as we get more data back on that animal,\" he said. And that predictability cuts costs for ranchers. But for the DNA tests to be useful, ranchers have to benchmark their livestock with measurements and ultrasound scans that correspond to the DNA. \"If we're not turning in those weights and if we don't get that hard information or those scans, five years down the road, our DNA doesn't mean anything to us because the DNA is changed. These animals have changed and evolve through selection pressure,\" said Wes Tiemann, a cattle sales coordinator and producer from central Missouri.  Tiemann said he has more customers asking about the genomics of cattle for sale every week and expects that to continue with increased consumer demand for beef. And that's the final piece of the picture: Across the globe, people are hungry for quality beef. \"Consumer demand for all meats has been stronger both in the U.S. as well as the rest of the world,\" University of Missouri agricultural economist Scott Brown said. \"I think consumers today are talking a lot more about the taste of meat products and have been willing to potentially add more meat to the diet than we've seen in the past,\" he said. \"And if that trend continues I think it bodes well for the livestock industry.\" The DNA tests are helping producers improve the U.S. beef herd strengths and fix its weaknesses, in the end creating a diner's dream steak, Decker said. His research group is even working on tailored DNA tests that can better predict how productive cattle will be in specific environments with humidity, elevation or toxic vegetation. And at future cattle auctions, ranchers likely will have more traits available in each bull's genomic profile for decision-making. Ultimately, whether a rancher is bidding for the first million dollar beef bull or examining a bull's genomic readout, the U.S. beef herd's genetic makeup is evolving. And while these prized bulls will likely never meet many of their offspring, many shoppers will come across them in grocery stores as this pool of genetics shapes beef herds in the U.S. and international markets. This story comes to us from Harvest Public Media, a reporting collaboration focused on food and agriculture. Follow Kris on Twitter: @krishusted With a pinch of skepticism and a dash of fun, The Salt covers food news from the farm to the plate and beyond. You can connect with senior editor and host Maria Godoy via our contact form or directly by email. You can also reach correspondent Allison Aubrey via email. NPR thanks our sponsors Become an NPR sponsor","time":1525801490,"title":"How Prized Bull Semen and DNA Testing Are Reshaping America's Beef Herd","type":"story","url":"https:\/\/www.npr.org\/sections\/thesalt\/2018\/05\/08\/608207519\/how-prized-bull-semen-and-dna-testing-are-reshaping-americas-beef-herd","label":7,"label_name":"random"},{"by":"ourmandave","descendants":0,"id":17022896,"kids":"None","score":2,"text":"Last year, Microsoft assured us that the death of the Windows phone didn\u2019t mean the death of Windows on your phone. Today, the company teased a future where Android users will have the same cool phone-to-PC experience iOS users with a Mac currently enjoy. The Windows phone may be dead, but Windows really might be coming to whatever phone you\u2019re holding right now. Microsoft is trying to accomplish this in two ways. The first is by bringing Timeline to the mobile phone. Timeline, in case you missed it when it was announced at Build last year or when it finally came to Windows 10 last month, is a feature that remembers every window you\u2019ve opened on your Windows machine over the last few days. Currently it just supports Microsoft apps like Edge, Word, Powerpoint, and Photos, but today Microsoft encouraged the gathering of developers at Build to implement the Timeline capability into their own apps. The capability comes via something called Microsoft Graph, which is essentially the backend for Timeline. It\u2019s the process that actually remembers all your open tabs and apps. When you hear an app uses Graph, it means it will work in Timeline just like a Microsoft app would. Timeline will come to phones two ways\u2014via the Microsoft Launcher on Android and the Edge browser on iOS. Click on a Word doc in Timeline in your phone and it will open in the app on your phone. If more apps supported Graph, you can see how appealing that could be. You could theoretically open a Photoshop project from you PC on your iPad without converting files or messing with transferring the files. Advertisement  Timeline on iOS and Android is expected to be live later this year. Also available later this year, and hopefully available to Windows Insiders (members of the Microsoft Beta program for Windows 10) later this week, is the Your Phone app. The Windows 10 app will let you view pictures, notifications, and texts from your phone on your Windows machine. But best of all you can actually text back! Advertisement  The texting feature appears to be available only on Android\u2014and that\u2019s okay. iOS users on the Mac have Messages, a great application that lets you text anyone on your phone from your Mac. This would give the same functionality to Android users on Windows. If you\u2019re a Windows lover still yearning for your Windows phone, the company seems to have given you an answer\u2014get an Android phone. Or get an iPhone and have a little less support.  In addition to the new features for phones, Microsoft also teased a new feature for a future Windows update: Sets. Sets appears to be an expansion of Timeline that lets you essentially create bundles of windows in your Timeline. Like a set of the blogs you intend to finish reading or a set that\u2019s an Excel project and Edge tabs full of related info. Advertisement  If that sounds a little nebulous, it\u2019s because Sets seems to be a product still in its infancy. In fact, Joe Belfiore, Corporate Vice President in the Operating Systems Group at Microsoft, couldn\u2019t even give a solid timeline for when Sets would be available. Instead he simply said, \u201cWhen we think it\u2019s great.\u201d That\u2019s not a new trend for Microsoft. The launch of Timeline itself was heavily delayed as well, including one last minute delay just last month, and the interactivity with phones that was teased at Build last year and is only now being considered for release. The company seems determined to get the releases right rather than to get them out fast. A sentiment recently embraced by its competitor, Apple, as well. Which is probably a good thing. I\u2019d rather have my phone and computer work well together, than have a cool and buggy concept messing with my daily workflow. Advertisement  Senior Reviews Editor. Trained her dog to do fist bumps. Once wrote for Lifetime.","time":1525801471,"title":"Microsoft's New Plan to Colonize Your Phone with Windows","type":"story","url":"https:\/\/gizmodo.com\/microsofts-new-plan-to-colonize-your-phone-with-windows-1825858311","label":7,"label_name":"random"},{"by":"pbiggar","descendants":1,"id":17022895,"kids":"[17023061]","score":9,"text":"Hi. I work as a data architect in San Francisco and I\u2019m auditing Dr. Jones class to stay up to date on the latest technologies and she mentioned you might be able to help me before I get too deep into the design of a new system. My default was just to use Postgres. I had a few questions on what schema designs might make most sense. I thought your expertise used to be relational databases? Hmmm, my app is a pretty straightforward web app that allows salespeople to send campaigns and track metrics on the campaigns. My app should be fine in a Postgres database right? How do you mean limiting? Can\u2019t I just add new tables as I need them? Or add columns to existing tables? Meta document stores? Oh nice, so I can have an index of all my data. So reading from the nearest copy of data would be faster? Consensus? How long does consensus take? In the past my Postgres database would perform reads in usually a millisecond or less. 400 ms doesn\u2019t sound any better? And what about writes? Is write performance just as bad? You don\u2019t write to the database?! What do you write to then?  What the hell! Where do you get atomic clocks from? This is running on your own hardware? Not on a cloud vendor like AWS, Microsoft, or Google? Ouch! That sounds like a lot of work. Wait, can we back up for a minute. You said you don\u2019t write to the database. If you don\u2019t write directly to the database, what do you write to then?  You\u2019re running two things that you write to?! Why would you even need to do that? This is sounding like more work than I was hoping for. Is that all I need to know? Okay, color me curious. What\u2019s the payoff to all this investment? Data lake? You lost me How do you parse it? Where do you stream it? What\u2019s Spark then do? And those then get queried? Okay, then you query the pre-aggregates? Oh, so you can make business decisions in real time? Okay, I think you lost me somewhere, but you\u2019re saying the application developers are way more productive at least? I think I may just stick with Postgres. If you\u2019re interested in reading more posts from our team, sign up for our monthly newsletter and get the latest content delivered straight to your inbox. \nThanks for signing up.\n Enjoy what you\u2019re reading? Sign up for our newsletter to stay informed: \nThanks for signing up.\n \n\nMORE CITUS DATA POSTS\u00a0\n\n\n","time":1525801460,"title":"It's the Future (for databases)","type":"story","url":"https:\/\/www.citusdata.com\/blog\/2018\/05\/08\/its-the-future-for-databases\/","label":7,"label_name":"random"},{"by":"apsec112","descendants":0,"id":17022893,"kids":"None","score":4,"text":"I\u2019ve just founded a nonprofit, the Longevity Research Institute \u2014 you can check it out here. Wonderful initiative! If the  FDA doesn\u2019t consider aging a disease, then maybe it doesn\u2019t consider  anti-aging chemicals to be drugs and so they can go through without the FDA process. Or am I being too hopeful? Well, that\u2019s how Leonard Guarente is betting \u2014 look up Elysium Health. Fill in your details below or click an icon to log in: \n\n\t\t\tYou are commenting using your WordPress.com account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0\/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Google+ account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0\/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Twitter account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0\/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Facebook account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0\/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n Connecting to %s  Notify me of new comments via email.  \n\n  ","time":1525801451,"title":"Introducing the Longevity Research Institute","type":"story","url":"https:\/\/srconstantin.wordpress.com\/2018\/05\/08\/introducing-the-longevity-research-institute\/","label":7,"label_name":"random"},{"by":"knz42","descendants":0,"id":17022857,"kids":"None","score":1,"text":"When I started contributing to CockroachDB, I was foremost excited to\ncontribute to an open source project with a strong \"free as in\nfreedom\" component, which makes CockroachDB agreeably\nFOSS in\nmy eyes. FOSS has a social purpose, one that supports giving others both the\ntools to progress forward and the keys to these tools, so that they\ncan keep their full agency. At a more personal level, I expect to be\nchanging jobs a couple more times before I retire, and keeping a\npublic record of my contributions makes it easier for me to apply to\nnew positions\u2014me not being good with charisma and things, I need more\nevidence than average to advertise my skills. Also, CockroachDB had just started! Being scientifically minded, I was\ncurious to study the progression of this project in its early\nstages. Every open source project has a different narrative and its\nown dynamics, and I felt there was much more to learn by being close\nto the source than relying on third party retrospectives a few years later. It has been more than two years since I started contributing to\nCockroachDB, and today I am sharing a few findings and observations. The source code is available\nonline. There is an open source\nlicense\nfor a large, functional, autonomous subset of the software. The\nspecific license, the Apache\nLicense, makes this\nsubset of CockroachDB fully\nfree. The software is open source (see above). There are clear and reproducible\ninstructions\non how to build CockroachDB from source in new environments. Online help is available\nto help with building, installing and using the software. The project is welcoming of external\ncontributions. To start, in my experience, most FOSS comes in two flavors: free and open source from the start, initially supported by\nvolunteers or public funding (e.g. the GNU project, the Linux\nkernel, GCC, LLVM, Debian, PostgreSQL), then later, if successful,\nsupported by private funding (e.g. LLVM, GCC, Linux, PostgreSQL). initially proprietary and commercially funded, and freed later when\nthe parent organization realizes they can't easily make money off it\n(e.g. ScyllaDB, OpenOffice\/LibreOffice, RethinkDB, FoundationDB). CockroachDB is none of those things. Instead, it is one of these few\nprojects, alongside RedHat Linux, Ubuntu Linux, MySQL, Go and Rust,\nwhere the software is both created as FOSS upfront and funded\ncommercially. The reason why I start with this topic is that I am particularly\ninterested in the social dynamics around ownership: how is the power\nshared? Let's look at these three categories separately. In the first category, projects are created to either scratch the\nhacker's itch or for public good. The authors and, if applicable, the\nfunding organization, give birth to an entity with the aim upfront\nto have its community self-organize and decide its governance. Most,\nif not all, have as founding principle that the project's governance\nis, nominally at least, open to the public via a documented and\ntransparent process. A newcomer to the project can legitimately\nassume upfront that the project is partly theirs and that their\ninfluence on the project is guaranteed to grow in proportion to\ntheir contributions. There is a pitfall where such a born-FOSS project becomes funded by a\nsingle for-profit organization, and evolves to serve primarily the\ninterests of that organization instead of that of its community\n(LLVM\/Clang is an example of this vis-a-vis Apple, more on that below)\nbecause the livelihood of most core contributors depends on that\norganization. Nevertheless, this is more an exception than the rule,\nand so far I have found that most healthy FOSS projects are careful to\nensure that many of its core contributors are supported by different\norganizations. In the second category, there is a business building new software more\nor less in private, in any case with a big entry sign in front of its\ndoor: \"this product is ours, and you must give us money before we\nallow you to use it\" (nowadays the money can be in the form of\nadvertisement-receiving eyeballs, but the idea remains the\nsame). There is initially no pretense of shared ownership, and the\nusers are reminded regularly that they do not have any participatory\nrights. Then, at some point, the business changes and abandons moral\nownership over the software. This event can occur for a variety of\nreasons (bankruptcy, pivot, leadership change, whatever) which are not\nrelevant here; instead, I want to focus on the observation that there\nis then a pivotal event where a community of users is handed the keys\nto a kingdom previously guarded, with a fundamental guarantee\nthat the king has left the premises and is not coming back\u2014usually\nvia suitably irrevocable FOSS license, but it can also be organized\nvia a copyright reassignment to a non-profit organization with an open\ncharter. What happens then is that both existing users and some of\nthe previous contributors gather and decide to create a new\ngovernance. In reaction to the previous closed ownership, the new\ngovernance is, again, nominally at least, open to the public via a\ndocumented and transparent process. This was the story of e.g. StarOffice opening itself as OpenOffice and\nfinally LibreOffice. The result is organizationally similar to that described above, but\nthe dynamics of the interaction between newcomers and a late-freed\nproject are rather different. To understand why, consider this\nanalogy: a born-freed FOSS project is like a social group building\ntheir shared home together on top of a vacant plot of land; a\nlate-freed FOSS project, instead, is like a group moving in a giant\nmansion inherited from an old, quirky relative. There is outdated\nupholstery to get rid of; there are poorly maintained pipes to\nrenovate at yet-undetermined costs, and, foremost, the overall\narchitecture of the mansion was fundamentally built for someone\nelse, not for the new inhabitants. Newcomers can legitimately feel\nthat the project is partly \"theirs\" and that their influence can grow\nwith their contributions, but there are days that bring reminders\nof the previous owner, much like ghosts of a haunted mansion. The third category, where one can find RedHat and Ubuntu Linux, Go,\nRust and now CockroachDB, is quite different. I'll call them \"hybrid\"\nFOSS projects. In this case, there is a for-profit organization who bankrolls the\ndevelopment. Meanwhile, the source code is public and there is an open\nsource license, even a FOSS one usually. Forks and other reuses of the\nFOSS code are permitted, but the name and\/or the brand is trademarked,\nsuch that an individual or a third party organization cannot derive a\nnew project from the same sources with the same name. The governance model, in particular the process available to an\nexternal contributor to become acknowledged as part of a steering\ngroup, further depends on the structure of the business around the\nFOSS project. As for most things, there is a spectrum. At one end of the spectrum, if the organization is merely building\nFOSS as an instrument to create a community on top of which it can\nsell services or other professional services, i.e. where the FOSS\nand\/or related IP is not a leveraged asset of the for-profit, then the\ngovernance is usually public. Steering groups and\/or \"core teams\" meet\non public channels, the membership process is transparent, public and\nopen to all, and the parent for-profit typically only acts in an\nadvisory role. Ubuntu Linux was created in that way by Canonical, its\nparent company. At the other end of the spectrum, the software is a core asset\nof the for-profit, which the for-profit expects to directly leverage\nfor income (by selling deployment licenses or subscriptions, or by\nproposing it as a gateway drug to a more expensive, proprietary\nproduct). Out of business necessity, the steering is exclusively reserved\nto the for-profit, and the core team is closed to the public. The\nexistence of the FOSS license is seen as a source of undesirable extra\ncosts (support and communication with the community) and merely\nsupported or tolerated for the free advertisement it\ngenerates. External contributions are accepted and integrated from\nexternal contributors under the condition that they relinquish their\ncopyright to the parent organization, and there is no compensation in\nthe form of participatory rights in the steering activities.\nArguably, MySQL belongs more to this category. And then there is a range of various FOSS projects between these two\nextremes.  Where they belong is, again, a function of their governance\nmodel: how external contributions are welcomed, how external\ncontributors can raise in the community to gain steering powers, and\nall in all how individual users and contributors can legitimately feel\nthe software is \"theirs\". The Rust project, currently bankrolled by Mozilla, is much more like\nUbuntu Linux than it is like MySQL. All development occurs in public,\nincluding all the project steering. Any external contributor is given\nthe opportunity to contribute to the project's steering via the RFC\nprocess\u2014how much influence can be exercised this way being only a\nfunction of the contributor's own technical acumen and their\nreputation, as opposed to opaque decision making by the\ncommunity. What distinguishes the governance in this case from the\nlaissez-faire in Ubuntu's case, is that the FOSS project is steered\nprimarily to support a core business in the parent\norganization: Firefox' Servo in the case of Rust, the IT services in\nthe case of RedHat. In contrast, one would find the Go project closer to MySQL on that\nspectrum than Rust. While development occurs in public, the steering\nprocess is opaque and driven solely by Google's own internal\n(non-public) uses of the Go language. While external contributions are\nwelcome and the parent organization supports the community by funding\nevents, etc., there is no process by which an external contributor can\nascend in a steering position; changes to the core project (the\nlanguage and the compiler) are still gated by the parent organization. Where does CockroachDB fall on this spectrum? Currently, a bit in the\nmiddle: the project is a bit more open to external influence than Go,\nbut less so than Rust. CockroachDB is still a young project and its governance is bound to\nevolve as the company behind, Cockroach Labs, grows. It has only been\nthree years! Consider that when they were three years old, only few\npeople knew about PostgreSQL, MySQL, Rust, Go or LLVM. Meanwhile, a few predictions seem possible. In the case of Mozilla's\nRust and Google's Go, the parent organization's primary income comes\nfrom other sources and the FOSS projects were able to slowly grow at\ntheir own pace, on a research budget. As a result, their governance\ncould grow organically as a balancing act between the needs of\nenvisioned applications inside the parent organization and those of\nthe community of users and external contributors (albeit slightly more\nso in the Rust's case than Go's due to different engineering cultures\nand project priorities). In contrast, Cockroach Labs seeks to leverage\nCockroachDB as its primary source of income, so one can reasonably\nexpect that the project's governance will strongly privilege Cockroach\nLabs over its community, at least until the business is established. Meanwhile, the steady expansion of online resources for external\ncontributors (e.g. the last year saw the appearance of detailed\narchitecture docs, contribution tutorials, and public documentation\nfor issue triaging and prioritization) suggest that Cockroach Labs\ndoes intend to become more welcoming, and intends to increase its\nsupport of a community of open source developers.  That said, whether\nand how this will extend to openness in the steering processes remains\nto be seen. (Disclaimer: despite my involvement with the project, I am not privy\nto the specific plans regarding Cockroach Labs' FOSS community\nmanagement and outreach. The statements and predictions enclosed in\nthis article are personal opinions and do not reflect the plans or\ncommitments of Cockroach Labs.) Find me on twitter!","time":1525801274,"title":"The \u201cOpen Source\u201d in CockroachDB","type":"story","url":"https:\/\/ristret.com\/s\/bqzjeo\/open_source_cockroachdb","label":7,"label_name":"random"},{"by":"schickling","descendants":0,"id":17022844,"kids":"None","score":5,"text":"Today, we\u2019re releasing Prisma 1.8 which introduces support for PostgreSQL. This has been one of Prisma\u2019s most requested features so far. Simply run npm install -g prisma to install the latest version. You can find the full changelog here. The most common question we\u2019re getting from developers and companies is: When will Prisma add support for database XYZ? Some applications use Postgres as their primary source of truth and sync data to Elasticsearch for advanced text search. Other applications are based on a scale-out database like Cassandra or DynamoDB for primary data and maintain projections into a SQL or Mongo database in order to retrieve all data for a particular view in a performant, single query. In today\u2019s infrastructure ecosystem it\u2019s the default to pick specialized databases and combine their capabilities to maximize performance and flexibility. Embracing this new way of organizing data, the Prisma\u2019s goal is to unify the data layer for modern applications through the simplicity and power of GraphQL. To fulfil this promise, Prisma has to support a multitude of different databases. This is the reason why our highest priority for 2018 is to add support for many more databases starting with Postgres, MongoDB and Elasticsearch which are the most frequently requested databases. Adding support for Postgres is especially interesting since more and more newer databases (such as CockroachDB and TimescaleDB) are adopting the PostgreSQL syntax and are therefore compatible with Prisma out of the box. So let\u2019s see how you can use Prisma together with Postgres today: Once you\u2019ve installed the Prisma CLI via npm install -g prisma on your machine, the easiest way to get started is by running prisma init: Here you can choose to set up a new Prisma server (using Docker) or use an already existing Prisma server. When setting up a new server Prisma allows you to connect to an existing database or create a new one running in Docker. In this case we\u2019ve chosen the \u201cCreate new database\u201d option and selected the \u201cPostgres\u201d database option. The Prisma CLI creates all necessary files for you to get started including the docker-compose.yml which looks like this: Now we can simply run docker-compose up -d to start both the Prisma server and the Postgres database (this requires Docker to be installed locally). Once both containers are running, you can deploy the Prisma service via prisma deploy. This will migrate the underlying Postgres database based on your datamodel specified in the datamodel.graphql file and makes your Prisma GraphQL API available under http:\/\/localhost:4466. You\u2019re now ready to start using your GraphQL API. Simply explore the API capabilities via the GraphQL Playground or build your own GraphQL server using Prisma as a GraphQL ORM layer based on prisma-binding. Read more. Besides using Prisma together with a new database, Prisma 1.8 introduces experimental support for connecting to an existing Postgres database. This let\u2019s you turn your existing database into a powerful GraphQL API. The usage flow is very similar to using a new database. By simply configuring your database credentials while running prisma init, Prisma will introspect your existing database and create a matching datamodel. The created datamodel is a mapping to your existing database and serves as the foundation for the auto-generated GraphQL CRUD API. You can use Postgres-specific directives (e.g. @pgTable or @pgRelation) to control how your Postgres database is mapped into a GraphQL API. Here is how the datamodel could look like after the initial introspection: Making Prisma work out of the box for existing databases in every scenario is a hard technical problem requiring a lot of work (and tests), so there are still some known limitations and bugs. We\u2019re expecting to release a stable version of this feature in one of the next upcoming versions. If you\u2019re running into any edge cases or bugs while testing this feature, please open an issue on Github. This release marks an important milestone in the history of Prisma adding support for Postgres\u200a\u2014\u200aone of the most popular open-source databases used by millions of applications. However, this is also just the beginning of bringing the power of the Postgres ecosystem to the world of GraphQL. We are already looking into ways how to add support for full-text search, Postgis and other Postgres extensions. Besides Postgres we\u2019re already working on other database connectors. Keep an eye out for the upcoming MongoDB connector. \ud83d\udc40 Thank you, to all beta testers \ud83d\ude4f A special thanks goes out to all of our amazing beta testers who have helped us over the last few weeks to find bugs in the Postgres connector. We\u2019re looking forward to your findings regarding the support for existing databases. By clapping more or less, you can signal to us which stories really stand out. GraphQL Developer Tools Guides and articles about GraphQL, Relay, Apollo & more","time":1525801176,"title":"Prisma now supports Postgres","type":"story","url":"https:\/\/blog.graph.cool\/prisma-now-supports-postgres-aad74ba479cb","label":3,"label_name":"dev"},{"by":"badrabbit","descendants":1,"id":17022842,"kids":"[17022911]","score":3,"text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\r\n                                var fsData = [{\r\n                    page: {\r\n                        pageInfo: {\r\n                                                pageName: 'electronic-lock-systems-are-vulnerable',\r\n                                                geoRegion: 'global',\r\n                        language: 'en'\r\n                        }\r\n                                                ,\r\n                        category: {\r\n                                                        primaryCategory: 'business'\r\n                                                                                    ,\r\n                                                                                    subCategory1: 'campaign'\r\n                                                    }\r\n                                            }\r\n                }]\r\n                            \n\n\n\n\n\n\n\n For full functionality of this site it is necessary to enable JavaScript. Here are the  instructions how to enable JavaScript in your web browser. Tomi Tuominen Practice Leader at F-Secure Cyber Security Services \n\tF-Secure researchers have found that global hotel chains and hotels worldwide are using an electronic lock system that could be exploited by an attacker to gain access to any room in the facility. \n\tThe researchers simulated the attack with an ordinary electronic key to the target facility. Using information on the key, they were able to create a master key that can open any door using the same lock system in the facility. The key doesn't even have to be a working key \u2013 even one that's long expired, discarded, or used to access spaces such as a garage or closet could be used. The attack can be performed without being noticed. \n\tThe design flaws discovered in the smart lock system's software, which is known as Vision by VingCard and used to secure millions of hotel rooms worldwide, have prompted the world's largest lock manufacturer, Assa Abloy, to issue software updates with security fixes to mitigate the issue. \n\tUnderstanding the interaction between hardware and software is essential in designing secure products. You need to make the right choices from the beginning, since hardware vulnerabilities cannot be patched as easily as software. Involving our world-class hardware security experts in the process early on will save you time and money. \n\tOur hardware security services include: Andrea Barisani Head of Hardware Security at F-Secure  \n\tThank you for your interest. We'll be in touch soon.  \n\tGet in touch with one of our hardware security experts to discuss your product security. The researchers' interest in hacking hotel locks was sparked a decade ago when a colleague's laptop was stolen from a hotel room. When the researchers reported the theft, hotel staff dismissed their complaint as there was no sign of forced entry, and no evidence of unauthorized access in the room entry logs. The researchers decided to investigate the issue further. Global","time":1525801158,"title":"Ghost in the locks \u2013 Hotel room locks can be hacked","type":"story","url":"https:\/\/www.f-secure.com\/en\/web\/business_global\/electronic-lock-systems-are-vulnerable","label":7,"label_name":"random"},{"by":"bangonkeyboard","descendants":0,"id":17022840,"kids":"None","score":4,"text":"This is cool but bit confusing that you are using word Blender in the title when it is for 3DsMax. \"We\u2019re currently at a stage where\" that whole paragraph is used in 2 places I am using this addon for few weeks now and it does it work well. It makes my work quicker.  Branislav: I used to compete in the demoscene in Europe between 2000 and 2002. I wrote a few 256-bytes demos (also called intros) under the nickname Silique\/Bizzare Devs (see \u201cNjufnjuf\u201d, \u201cOxlpka\u201d, \u201cI like ya, Tweety\u201d and \u201cComatose\u201d), where each of the intros generated real-time voxel or point cloud graphics. Both voxels and point clouds are examples of sampled geometry. The intros did their job with about 100 CPU instructions like ADD, MUL, STOSB, PUSH and similar. However, due to the nature of these type of programs, tens of instructions were actually used just for setting up things, not for generating the actual graphics. Still, those 50+ instructions, which are basically elementary mathematical and memory operations, were enough to generate pretty neat moving 3D graphics at real-time speeds. All these 256B intros won the 1st to 3rd prize and that made me realize that when such 3D graphics are so easy to create without polygons, it could also be possible to achieve much more in games and beyond, by doing the same there: using sampled geometry instead of polygon meshes. Simplicity is the key. I saw that the then-dominant paradigm based on complicated and fundamentally limited (non-volumetric) representation was going to hit a complexity ceiling, so the timing was right to try this \u201cnew\u201c and simpler paradigm: volumetric sampled geometry.  Dan: While still in high school in Sweden, I started programming a 2D sidescrolling engine, which I eventually built an indie game called \u201cCortex Command\u201d with. It was like \u201cWorms\u201d or \u201cLiero\u201d, but with real-time gameplay and RTS elements, and more detailed in its simulation of various materials for each pixel of the terrain. Through an \u201cant farm\u201d-like cross-section view, your characters could dig for gold in the soft dirt and build defensive bunkers with hard concrete and metal material walls. Cortex Command won Technical Excellence and Audience awards at the Independent Games Festival in 2009. Ever since then, I have been wanting to make a fully 3D version of the game \u2013 something that could only be done with volumetric simulation and graphics.  About six years ago, I was looking at all the voxel solutions out there, and found Branislav\u2019s work through his website and videos, where he talked about this inevitable shift away from polygonal 3D graphics to something that resembled what I was doing in 2D in my game: simulating the whole virtual world as small atomic blocks with material properties. Not only did his thesis ring true for me, but his tech also seemed to be the best and most convincing around, based solely on his simple but impressive videos. I started donating to his project through his website and struck up a conversation with him, which eventually led to a friendship over the years, and now us co-founding this company. It\u2019s exciting to be part of this pivotal period in such an epic project, where the result of so many years of R&D is finally about to be put into everyone\u2019s hands to revolutionize the creation and consumption of 3D content!  We believe that many big players have realized that polygon technologies have been hitting a complexity ceiling over a decade ago. This problem manifests itself in all kinds of friction: complex toolchains, complex hacks to make destructive interaction and simulation possible, complex geometry representation (polygon surface model + collision models + other models for representing the internal structure, if any), complex and over-engineered approaches to volumetric video, hacks and large code-bases, etc. All this friction makes progress rely almost solely on the increasing horsepower of GPUs \u2013 and some things are simply not feasible at all. It\u2018s a battle that can not be won. So, it\u2018s likely part of the nature of large companies: Often, they don\u2018t even try to spend so much time and resources on developing these risky and game-changing solutions; instead, their strategy seems to be to try to acquire the small companies who successfully do.  There are a bunch of techniques people typically consider being voxel-based. The oldest ones used in games were height-map based where the renderer interpreted a 2D map of height values to calculate boundary between the air and the ground in the scene. That\u2018s not truly a voxel-based approach as there\u2018s no volumetric data set in use (ie in Delta Force 1, Comanche, Outcast, and others). Some engines and games use large blocks that themselves have internal structures and make up the virtual world (ie Minecraft). These blocks are typically rendered using polygons, so the smallest elements of them are triangles and texels \u2013 not really voxels. That geometry is simply organized into a grid of larger blocks, but that doesn\u2019t make them voxels, strictly speaking.            Some games use relatively large voxels or SDF (signed distance fields) elements that still don\u2018t allow realism, but already allow interesting gameplay (ie Voxelstein, Voxelnauts, Staxel). There are also SDF-based projects that allow great interaction and simulation that might have the potential for providing high realism (ie Claybook). However, so far we haven\u2018t seen attempts to develop a solution for simulation and rendering of realistic large scenes in ways similar to what our tech is capable of. Atomontage uses voxels as the most basic building blocks of the scenes. The individual voxels in our tech are more or less without structure. This approach results in simplicity that helps greatly in simulation, interaction, content generation as well as in data compression, rendering, volume video encoding, and elsewhere.  Our voxel-based solution removes multiple types of complexity people run into when working with polygon-based technologies \u2013 including the whole concept of polygon count limits and the many hacks used to overcome that limit. It is volumetric by nature and therefore doesn\u2019t require any kind of additional representation for modeling the inside of objects. This approach features a powerful and inherent LOD (level of detail) system which allows the technology to balance performance and quality in traditionally difficult situations. Granular control over LODs, foveated rendering and processing are few of the many benefits added with virtually no overhead cost. Voxel geometry removes the burden of complicated structure: it is sample-based and therefore easy to work with (a simple, universal data model for any geometry compared with a complex data model of polygon-based assets). This allows us to iterate faster when developing powerful compression methods, interaction tools, content generators, the physics simulator, etc. This is not the case with polygon technologies as these are hitting the complexity ceiling for at least a decade, and their improvements depend strongly on exponential increases in GPU horsepower.     The voxel approach is efficient because it doesn\u2019t waste footprint or bandwidth on the hard-to-compress vector components of vector-based representations (polygons, point clouds, particles), the values of which have an almost random distribution. With voxels, the encoded information is instead mostly the information we want (color, material information, etc.), not the overhead data that just places that information in the right place in space. This can be compared with a JPEG vs. some 2D vector format when used to encode a large complex image. The JPEG encoding is predictable and can be tuned well for optimal quality and small footprint while a vector image would waste most space on the vector information instead of on the actual color samples. Our approach will allow regular people to unleash their creative talent, without them having to first study or understand the underlying technology and its limits. The skills everybody have acquired while growing up in the real world should be all they need in order to interact with the virtual environments in useful and realistic ways.        Large-scale, high-resolution volumetric video is easily doable using voxels thanks to the inherent LOD system and simple spatial structure of voxel assets. Our voxel-based rendering doesn\u2019t degrade the geometry via mesh decimation and the performance we\u2019re already achieving on common contemporary hardware is unmatched, as far as we have seen. We\u2019re currently at a stage where we can voxelize not just a single large high-poly asset, but a whole sequence of such assets and compile a volumetric movie out of it. We can also do this with whole environments, so imagine having a movie-quality scene with characters, which could be a sequence from an existing sci-fi or animation film. We can turn this into a VR movie where the user can share the environment with the characters, move their viewpoint freely within it (not just look around like in a 360 video) in a room-scale or larger experience, and in a way enjoy that experience as is common in some VR games, except for the interactivity. We\u2019re now looking for partners who would like to help us do a first trial short-form film with their CG-quality data.  Our tech already allows us to use multiple transforms affecting any voxel of a model, easily creating convincing soft-body deformation effects. Although we don\u2019t have a character animation demo with full skinning and similar features yet, it is something the tech clearly would be able to do if tied in with any conventional rigging system. This can be already seen in our soft-body videos: the tires on the vehicle\u2019s wheels get squashed as a response to simulated forces, and that deformation is similar to what would happen in a character animation when under the influence of rigged bones. Early on, the actual rigging might be done either in some of the existing tools prior to voxelization, and eventually, we\u2019ll provide the feature in our own tools.   Existing textured assets can be voxelized in a number of ways. We are getting best results with two of them \u2013 a ray-based voxelizer and a projection based voxelizer, respectively. The former one casts rays through the polygon asset, detects intersections between each of the rays and the mesh, calculates the related positions on the mapped textures, reads the texels and bakes them in the respective voxels that are being plotted.     The projection-based voxelizer renders the asset from multiple vantage points into maps that include depth maps. The intersection of the volumes defined by the depth maps then provides information about the actual voxels that have to be plotted. The other generated maps provide the rest of the surface information (color, normal, etc.) which is also baked into surface voxels. There are also other ways to create properly colorized surfaces when voxelizing point cloud data or by procedurally generating content or surface properties of existing assets.       These are two separate problems. Content generation is easy with voxels because you only have to plot a large set of samples with some useful properties (color, material information, etc.) into a regular grid with a specified spatial resolution, or multiple resolutions in case of variable LOD dataset. This is easy to do with voxels because you don\u2019t run into polygon count limits. There are also no textures, so texture resolution limit isn\u2019t an issue, either  Rendering of large scenes is quite easy, too, thanks to the inherently powerful LOD system. The renderer can use the most optimal combination of LODs of tiny segments of the geometry to render the whole scene at the highest possible detail while maintaining high FPS. LODs are inherently cheap with voxels and they are great for keeping the voxel size (and so the shape error) smaller than the size of a pixel on the screen.      Our voxelizers are already powerful enough to voxelize very high-poly meshes and point-clouds using one of the many techniques as mentioned before. We\u2019ve demonstrated first results with photogrammetry data back in early 2013 with a voxelized 150 million polygon mesh we rendered and edited in real-time on a mediocre gaming laptop from 2008. Once voxelized, the polygon count of the source geometry becomes irrelevant, and editing of the asset becomes easy. This can be seen in our videos; performance is dependent more on the pixel count than on polygon count of the source data. All this is essential for providing users with comfortable workflow when cleaning up huge scanned assets. Voxels are also in a way similar to pixels so we\u2019re foreseeing great applications of narrow (\u201cdeep learning\u201d) AI for automatic cleanup of photogrammetry data.      Voxel-based representation is essential for creating massive shared interactive virtual environments that will, at some point, resemble the real world. It\u2019s key that such environments aren\u2019t static and allow users to interact in ways they\u2019re used to in the real world and that means that such virtual environments have to be fully simulated with convincing physics. Simulation and rendering of a complex, fully dynamic world require such worlds to be volumetric \u2013 and all processing, synchronization, and rendering of the simulated geometry also has to be efficient. The volumetric nature of this process rules polygon meshes out. Also, as explained before, other vector-based representations aren\u2019t efficient enough for providing the greatest value for the smallest footprint.   We expect that these large simulated environments will consist of mostly voxels, with a small part being simulated using particles for certain effects. It is great that both representations are simple and sampled, and that the conversion from one to the other is trivial and computationally cheap.    These are great benefits of voxels over other representations and that\u2019s what makes them the right solution for representing these massive virtual environments. The LOD system makes them great for on-the-fly optimization of processing and rendering based on any combination of parameters (distance to the user, importance of the simulated process, accuracy vs. cost, etc.). It also makes them perfect for doing foveated rendering and very efficient streaming. They can also be defined at higher dimensionality which is essential for running massively distributed physics simulations. Such simulations cannot be done in only three dimensions, because of the latencies on the network and the impossibility of processing a large asset on a single PC and transferring any large piece of geometry at once. The segmented, variable LOD nature of voxel geometry is of a great help here. When modifying any small part or multiple parts of a large voxel model there\u2019s no need to recalculate its large mesh and textures, nor to synchronize the whole model across the network \u2013 it\u2019s just the affected parts that matter, and they can be synchronized at the most suitable LOD the network manages to transfer.      All these requirements of future massive virtual environments make the paradigm shift towards volumetric sampled geometry necessary and inevitable. The qualities of our voxel-based approach make it the best and possibly the only candidate for actually making the paradigm shift happen anytime soon. \n1 Comment on \"How Voxels Became \u2018The Next Big Thing\u2019\"                     \u201cWe\u2019re currently at a stage where\u201d that whole paragraph is used in 2 places \n                                    \u00a9 2018. 80 level. All rights reserved\n                                    \nContact us\n\nPrivacy Policy\n","time":1525801156,"title":"How Voxels Became 'The Next Big Thing'","type":"story","url":"https:\/\/80.lv\/articles\/how-voxels-became-the-next-big-thing\/","label":5,"label_name":"ml"},{"by":"fwlymburner","descendants":0,"id":17022838,"kids":"None","score":1,"text":"Galera Cluster is a mainstream option for high availability MySQL and MariaDB. And though it has established itself as a credible replacement for traditional MySQL master-slave architectures, it is not a drop-in replacement. While Galera Cluster has some characteristics that make it unsuitable for certain use cases, most applications can still be adapted to run on it. The benefits are clear: multi-master InnoDB setup with built-in failover and read scalability. But how do you migrate? Does the schema or application change? What are the limitations? Can a migration be done online, without service interruption? What are the potential risks? In this webinar, Severalnines Support Engineer Bart Oles will walk you through what you need to know in order to migrate from standalone or a master-slave MySQL\/MariaDB setup to Galera Cluster. Bartlomiej Oles is a MySQL and Oracle DBA, with over 15 years experience in managing highly available production systems at IBM, Nordea Bank, Acxiom, Lufthansa, and other Fortune 500 companies. In the past five years, his focus has been on building and applying automation tools to manage multi-datacenter database environments. \u00a9 Copyright 2014-2018 Severalnines AB. All rights reserved. Severalnines and the Severalnines logo(s) are trademarks of Severalnines AB. MySQL is a registered trademark of Oracle and\/or its affiliates. Other names may be trademarks of their respective owners. \u00a9 2014-2018 Severalnines AB.","time":1525801151,"title":"Free Webinar \u2013  Migrating to Galera Cluster for MySQL and MariaDB","type":"story","url":"https:\/\/severalnines.com\/resources\/webinars\/migrating-galera-cluster-mysql-and-mariadb","label":3,"label_name":"dev"},{"by":"Jerry2","descendants":1,"id":17022830,"kids":"[17023829]","score":3,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Qualcomm Inc., the biggest maker of mobile-phone chips, is preparing to give up its push to develop processors for data-center servers, an effort that sought to break Intel Corp.\u2019s hold on the lucrative market, according to a person familiar with the company\u2019s plans. The San Diego-based company is exploring whether to shutter the unit or look for a new owner for the division, which was working on ways to get technology from ARM Holdings Plc into the market for chips that are at the heart of servers, the person said. ARM is one of Intel\u2019s only rivals in developing semiconductor designs, and its architecture is primarily used in less power-intensive products, such as smartphones. Qualcomm is the largest backer of an effort to find a role for ARM designs in the highest end of the computing market, where individual chips sell for multiple thousands of dollars. Chipmakers have been trying for years to provide owners of large data centers \u2013 companies such as Alphabet Inc.\u2019s Google and Amazon.com Inc.\u2019s Amazon Web Services \u2013 with processors to run them, trying to break into a business that Intel dominates with about 99 percent market share. A Qualcomm spokesman declined to comment. In the company\u2019s earnings report last month, Chief Executive Officer Steve Mollenkopf told analysts that Qualcomm is focused on spending reductions in its non-core product areas. Servers, which crunch data in corporate networks and act as the backbone of the internet, are a much smaller market than phones and personal computers when measured by shipments. But the price that chipmakers are able to charge for the high-performance parts needed to run them makes the market attractive. Qualcomm began selling a server chip, the Centriq 2400, based on ARM technology last year. At the time, the company said the chips, which were manufactured by Samsung Electronics Co., offered better results than an Intel Xeon Platinum 8180 processor, based on energy efficiency and cost. At the public introduction of the server chip line in November, potential customers such as Microsoft Corp. took to the stage to voice their interest in the offering. Since then, Qualcomm has been silent about its progress. While abandoning the effort would save Qualcomm the expense of designing some of the most expensive chips that the semiconductor industry produces, it would also be a retreat from the company\u2019s goal of becoming less dependent on the slowing market for mobile-phone parts. Qualcomm\u2019s management promised investors in January that the company would cut $1 billion in annual costs to improve profitability as part of an effort to fend off a hostile takeover bid by Broadcom Inc. Qualcomm prevailed because the U.S. government decided in March that the proposed deal posed a security risk. Investors had been on course to side with the would-be acquisition effort.","time":1525801116,"title":"Qualcomm Plans Exit from Server Chips","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-07\/qualcomm-is-said-to-plan-exit-from-server-chips-amid-cost-cuts","label":0,"label_name":"biz-news"},{"by":"kokie90","descendants":0,"id":17022825,"kids":"None","score":1,"text":"There is much concern about the realities of \u201cCyber Battle Fatigue\u201d \u2013 a condition resulting from a never-ending process of defending networks and sensitive information from an onslaught of cyber attacks conducted by cyber criminals, cyber espionage actors, and hacktivists. These attackers continue to use a wide variety of tactics, tools, and procedures that span from being unsophisticated to very sophisticated and continue to have more successes than failures. Two things are certain in a constantly-changing domain \u2013 that no business that operates online is immune to being targeted, and two, the cyber security talent pool is sparse, and is contributing to the cyber battle fatigue reality. The numbers are staggering and continue to outperform previous activity. In 2017, ransomware attacks demonstrated how prolific just one type of attack was. The WannaCry outbreak impacted computers in more than 150 countries that cost approximately USD $ 4 billion. According to one U.S. IT Company, in 2017, some notable cybercrime statistics illustrate the challenges facing those network defenders:  \u2022 The United States suffered more than any other country with 1,013 recorded in 016. In second place was the United Kingdom that had just 38 breaches. \u2022 In the third quarter of 2016, 18 million new malware samples were captured. \u2022 Mobile platforms continued to be an increasing popular target for cyber criminals. One security vendor identified 18.4 million malware detections in 2016. \u2022 Once breaching an organization\u2019s perimeter, an attacker resided within a network for an average 146 days before he was detected by the host organization. These statistics represent a mere sample of the types of adverse activities that network defenders confront on a daily basis. Therefore, it comes as little surprise that a dearth of experienced security professionals only inadvertently benefits hostile cyber actor operations. The more organizations expand their online footprint, the more potential entry points attackers have to target and potentially exploit. Recent reporting of IT-related media outlets confirms the lack of professionals in the cyber security industry. According to a Global Information Security Study, only seven percent of cyber security professionals surveyed were under the age 29, and 13 percent were between the ages of 30 and 34. The average age of cyber professional was 42 years of age. This situation gives pause for concern as organizations are not shying away from the Internet and networked business operations but are aggressively adopting them. As such, not having the requisite personnel in place to not only ensure the fluidity of online activity but to protect the confidentiality, integrity, and availability of organizational information systems will greatly impact the organization\u2019s ability to be cyber resilient, a necessary posture in today\u2019s dynamic cyber threat landscape. According to ESG research from early 2017, 45 percent of organizations claim to have a problematic shortage of cybersecurity skills. Perhaps even more telling, this research revealed that 70 percent of the cyber security professionals polled believed that the skills shortage already has impacted their organizations. Many cyber security staffs likely do not even have enough people to fully address the security needs of their organizations. Ideally, a team may have seasoned professionals, mid-level employees, and younger analysts that are guided and mentored by the more experienced individuals. However, this \u201cdream team\u201d may be more fiction than what is currently the reality. Not having enough cyber security professionals potentially impacts organizations in another way. If their current staffs are already overworked, this prohibits them from updating their security skills via professional development. It is hard for these organizations to let these people go away for any extended amount of time when they do not have the people to adequately cover the temporary loss. So what is left are organizations competing for the seasoned IT security professionals to address the \u201cnow\u201d rather than strategically planning for the future and how their security apparatus should look \u2013 both in terms of material and personnel resources. From what is known about the cyber domain is that lack of planning to anticipate needs will greatly setback an organization, relegating it to continually try to play catch-up with its cyber security posture. This is not an advantageous development. The end result is an acute case of cyber battle fatigue, in which these individuals find themselves akin to Sisyphus rolling that boulder up the hill, only to see it roll back down again. This repetitive cycle must come to an end, and it starts with organizations investing in cyber security professionals of all levels with the goal of teaching and developing their skills and growing them over time. Yes, there is the possibility of losing these individuals to other employers, but that is no different than any other employee in any other position. Ultimately, the hiring and developing process is worth the risk and the investment will inevitably pay off. But the first step must be taken. This is a guest post written by\u00a0Emilio Iasiello Tags: Cyber Crime, Cybersecurity, Hacking, Malware, Security, Threat, Usa","time":1525801079,"title":"Cyber Battle Fatigue Is Real \u2013 So What Should Organizations Do?","type":"story","url":"https:\/\/www.cyberdb.co\/cyber-battle-fatigue\/","label":7,"label_name":"random"},{"by":"PhilipDaineko","descendants":3,"id":17022824,"kids":"[17023004]","score":3,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Political power in Norway is dominated by women. They hold the office of the prime minister, the finance minister, the foreign minister and the speaker of parliament. \u201cIt\u2019s not a female conspiracy,\u201d says Prime Minister Erna Solberg. Even by Scandinavian chart-topping standards for gender equality, Norway stands out when it comes to the political landscape (they\u2019re not quite as good in corporate Norway). So what gender issues do these politicians worry about? \u201cThe challenge in the Scandinavian countries is not to end up with a large group of young men who have no purpose in life, no hope for a job,\u201d Solberg said in an interview in Oslo. From Left, Tone Troen, Erna Solberg and Ine Eriksen in Oslo, March 21. It\u2019s a demographic that needs careful political attention to avoid a dangerous backlash, the prime minister said. At the University of Oslo, about 57 percent of all PhD students last year were women. The risk of men falling behind also makes them more vulnerable to losing their jobs to automation. \u201cThat\u2019s what we see in the angry white men who not only don\u2019t like Muslims and immigrants, but absolutely not women either, at least if they can\u2019t keep the woman to themselves,\u201d Solberg said. Norway was the scene of one of Europe\u2019s worst hate crimes this century, when white supremacist Anders Behring Breivik in 2011 massacred 77 people, mostly members of the youth wing of the country\u2019s Labor Party.  Foreign Minister Ine Eriksen Soreide says, \u201cthe problem Erna is pointing to is a massive problem, globally. A lot of very vulnerable countries have enormous youth unemployment, and most of them are men.\u201d Tone Troen, Norway\u2019s newly minted speaker of parliament, says she\u2019s confident the next generation will do better. But \u201cit\u2019s important that boys and girls make non-traditional choices when it comes to education,\u201d she said. Men do wield some power in Norway. They hold most of the executive positions in listed companies, run the central bank and the country\u2019s $1 trillion sovereign wealth fund (the world\u2019s biggest). And while Norway is beaten only by Denmark when it comes to equal pay, men still get about 7 percent more than women, on average. Like the rest of the world, Norway was shaken by the #MeToo movement, which revealed a series of misconduct cases in both politics and business, including in Solberg\u2019s own Conservative Party. The deputy leader of the biggest opposition party even resigned amid allegations of misconduct. \u201cThe threshold for what\u2019s acceptable has been moved,\u201d Eriksen Soreide said. \u201cThat\u2019s probably one of the most important wins.\u201d But reaching total equality will take \u201ca terribly long time,\u201d she said. Solberg quipped we might need to wait until \u201c2072.\u201d Before becoming foreign minister, Eriksen Soreide was defense minister, following in the footsteps of a long line of women leading that department. She used to get asked by young girls whether men were even allowed to run the defense ministry in Norway. \u201cIt says something about the perspectives,\u201d she said. Norway\u2019s defense minister is now a man.","time":1525801068,"title":"A Big Concern in Norway, a Country Now Ruled by Women: Male Anger","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-03-22\/male-anger-is-a-big-concern-in-a-rich-country-now-ruled-by-women","label":7,"label_name":"random"},{"by":"teaman2000","descendants":0,"id":17022822,"kids":"None","score":1,"text":"\n      Adjunct Assistant Professor of Psychological Sciences, Case Western Reserve University\n     Jeremy P. Shapiro does not work for, consult, own shares in or receive funding from any company or organization that would benefit from this article, and has disclosed no relevant affiliations beyond their academic appointment.  Case Western Reserve University provides funding as a founding partner of The Conversation US. View all partners \n\n      Republish our articles for free, online or in print, under Creative Commons license.\n     Currently, there are three important issues on which there is scientific consensus but controversy among laypeople: climate change, biological evolution and childhood vaccination. On all three issues, prominent members of the Trump administration, including the president, have lined up against the conclusions of research.  This widespread rejection of scientific findings presents a perplexing puzzle to those of us who value an evidence-based approach to knowledge and policy. Yet many science deniers do cite empirical evidence. The problem is that they do so in invalid, misleading ways. Psychological research illuminates these ways. As a psychotherapist, I see a striking parallel between a type of thinking involved in many mental health disturbances and the reasoning behind science denial. As I explain in my book \u201cPsychotherapeutic Diagrams,\u201d dichotomous thinking, also called black-and-white and all-or-none thinking, is a factor in depression, anxiety, aggression and, especially, borderline personality disorder.  In this type of cognition, a spectrum of possibilities is divided into two parts, with a blurring of distinctions within those categories. Shades of gray are missed; everything is considered either black or white. Dichotomous thinking is not always or inevitably wrong, but it is a poor tool for understanding complicated realities because these usually involve spectrums of possibilities, not binaries. Spectrums are sometimes split in very asymmetric ways, with one-half of the binary much larger than the other. For example, perfectionists categorize their work as either perfect or unsatisfactory; good and very good outcomes are lumped together with poor ones in the unsatisfactory category. In borderline personality disorder, relationship partners are perceived as either all good or all bad, so one hurtful behavior catapults the partner from the good to the bad category. It\u2019s like a pass\/fail grading system in which 100 percent correct earns a P and everything else gets an F. In my observations, I see science deniers engage in dichotomous thinking about truth claims. In evaluating the evidence for a hypothesis or theory, they divide the spectrum of possibilities into two unequal parts: perfect certainty and inconclusive controversy. Any bit of data that does not support a theory is misunderstood to mean that the formulation is fundamentally in doubt, regardless of the amount of supportive evidence.  Similarly, deniers perceive the spectrum of scientific agreement as divided into two unequal parts: perfect consensus and no consensus at all. Any departure from 100 percent agreement is categorized as a lack of agreement, which is misinterpreted as indicating fundamental controversy in the field.  In my view, science deniers misapply the concept of \u201cproof.\u201d  Proof exists in mathematics and logic but not in science. Research builds knowledge in progressive increments. As empirical evidence accumulates, there are more and more accurate approximations of ultimate truth but no final end point to the process. Deniers exploit the distinction between proof and compelling evidence by categorizing empirically well-supported ideas as \u201cunproven.\u201d Such statements are technically correct but extremely misleading, because there are no proven ideas in science, and evidence-based ideas are the best guides for action we have. I have observed deniers use a three-step strategy to mislead the scientifically unsophisticated. First, they cite areas of uncertainty or controversy, no matter how minor, within the body of research that invalidates their desired course of action. Second, they categorize the overall scientific status of that body of research as uncertain and controversial. Finally, deniers advocate proceeding as if the research did not exist.  For example, climate change skeptics jump from the realization that we do not completely understand all climate-related variables to the inference that we have no reliable knowledge at all. Similarly, they give equal weight to the 97 percent of climate scientists who believe in human-caused global warming and the 3 percent who do not, even though many of the latter receive support from the fossil fuels industry.  This same type of thinking can be seen among creationists. They seem to misinterpret any limitation or flux in evolutionary theory to mean that the validity of this body of research is fundamentally in doubt. For example, the biologist James Shapiro (no relation) discovered a cellular mechanism  of genomic change that Darwin did not know about. Shapiro views his research as adding to evolutionary theory, not upending it. Nonetheless, his discovery and others like it, refracted through the lens of dichotomous thinking, result in articles with titles like, \u201cScientists Confirm: Darwinism Is Broken\u201d by Paul Nelson and David Klinghoffer of the Discovery Institute, which promotes the theory of \u201cintelligent design.\u201d Shapiro insists that his research provides no support for intelligent design, but proponents of this pseudoscience repeatedly cite his work as if it does.  For his part, Trump engages in dichotomous thinking about the possibility of a link between childhood vaccinations and autism. Despite exhaustive research and the consensus of all major medical organizations that no link exists, Trump has often cited a link between vaccines and autism and he advocates changing the standard vaccination protocol to protect against this nonexistent danger.  There is a vast gulf between perfect knowledge and total ignorance, and we live most of our lives in this gulf. Informed decision-making in the real world can never be perfectly informed, but responding to the inevitable uncertainties by ignoring the best available evidence is no substitute for the imperfect approach to knowledge called science. \n    Write an article and join a growing community of more than 66,800 academics and researchers from 2,321 institutions.\n   \nRegister now\n \nCopyright \u00a9 2010\u20132018, The Conversation US, Inc.\n","time":1525801049,"title":"Dichotomous thinking: the error at the root of science denial","type":"story","url":"https:\/\/theconversation.com\/the-thinking-error-at-the-root-of-science-denial-96099","label":7,"label_name":"random"},{"by":"teaman2000","descendants":0,"id":17022813,"kids":"None","score":2,"text":"\n      Associate Professor of Ecology, Evolution and Behavior, University of Minnesota\n     Michael Wilson's research on chimpanzee vocal communication has been funded by the National Science Foundation and a McKnight Land-Grant Professorship and a Talle Faculty Research Award from the University of Minnesota. View all partners \n\n      Republish our articles for free, online or in print, under Creative Commons license.\n     Freud, Wilkie and the other chimpanzees peacefully fed and rested in the sun-dappled Tanzanian forest. Mzee Hilali stood next to me, writing notes on the chimpanzees\u2019 behavior, as he had been doing for over 30 years as a field assistant for Jane Goodall\u2019s long-term study at Gombe National Park. Suddenly, a strange, high-pitched call sounded from where some other chimpanzees were feeding, about a hundred meters from us. Hilali turned to me, and with a little laugh, said, \u201cNyoka.\u201d This was the Swahili word for \u201csnake.\u201d  Freud climbed down from his tree and walked quickly toward where the call had sounded, with Hilali following close behind. As I slowly made my way through the undergrowth to catch up with them, Hilali called to me: \u201cChatu!\u201d \u201cPython!\u201d When I caught up with Hilali, he was pointing to a tangled mass of leaves and vines on the forest floor. I looked closely \u2013 most of the snake lay hidden from view, but the one visible stretch of black and tan scaly hide was too big to be anything but a python.  From years of experience, Mzee Hilali knew instantly that this particular chimp call meant they\u2019d found a snake. Does this mean that chimpanzees have a \u201cword\u201d for snake? Do chimpanzees have a language of their own? I\u2019ve been working with a team of students and Tanzanian field assistants to record and analyze chimpanzee vocalizations in an effort to answer questions like this. Ultimately we hope to learn more about how human language first evolved. Chimpanzees are among human beings\u2019 closest living relatives, and they share with us many unusual traits. Like humans, chimps make and use tools; join together in groups to hunt animals like monkeys; defend group territories; and sometimes gang up on and kill their enemies.  One trait that seems to set humans apart from every other species, however, is a fully developed language. Other animals communicate, but only humans appear able to talk about an unlimited variety of topics. Language enables us to make plans, negotiate with and teach one another. How and why language evolved remains a mystery. Much of the evidence of human evolution comes from fossils, but fossil bones don\u2019t tell us much about soft tissues or the sounds early human ancestors made. Studying the communication patterns of our living relatives can help solve the mystery.  If some features of chimpanzee communication resemble language, we can study chimpanzees further to find clues for why those features evolved. If chimpanzee communication doesn\u2019t share much in common with human language, then the key steps in language evolution must have occurred after our lineages separated (around 7.9 million years ago) for reasons unique to our human lineage. To investigate chimp communication, my colleagues and I follow chimpanzees through the forest as they go about their lives. We carry a hand-held \u201cshotgun\u201d microphone and a digital recorder, waiting for them to call. Usually we pick a particular chimp to follow each day, trying to get equal numbers of calls per individual. In addition to recording new calls, we\u2019ve been working to build an archive of recordings from other researchers, going back to the 1970s. The archive currently contains over 71 hours of recordings. Snake alarm calls are intriguing, but because chimps don\u2019t encounter large snakes very often, it is hard to do a systematic study of them. (Cathy Crockford and colleagues have done some interesting experiments, though, playing back recordings of these calls to see how chimpanzees respond and presenting them with model snakes). One thing chimpanzees do every single day, though, is eat. Chimpanzees spend most of their time looking for food and eating it. And when they find food, they often give a particular kind of call: the rough-grunt. \n\n\n Biologist Lisa O'Bryan studied rough-grunt calls for her dissertation research with me. They vary from low, noisy grunts to higher-pitched calls. Some researchers have proposed an intriguing possibility: Maybe chimpanzees make distinct kinds of rough-grunt calls in response to particular foods, rather like words that name certain food items. But Lisa has found that when eating any one kind of food, chimpanzees can produce a range of different rough-grunts. Rough-grunts thus tell other chimps that the caller is eating, but they don\u2019t say what\u2019s for dinner. Just as a particular alarm call informs others that a snake has been found, the thin, wavering tones of a copulation scream announce that a mating has just taken place.  \n\n\n Why female chimpanzees sometimes give such a scream just as they finish mating remains unknown. Because the data collected by Mzee Hilali and other field assistants since the 1970s have been entered into a computer database, we can readily examine thousands of different mating events. My student Tony Massaro has been analyzing these data to try to tease out what factors make females more likely to produce these calls. Such calls aren\u2019t particularly word-like, but they do play an important role in communication. Like many wordless sounds that people make \u2013 think laughter, screams and crying \u2013 listeners hearing the sounds can infer quite a bit about the caller\u2019s situation. When Jane Goodall gives public talks, she often begins by giving a pant-hoot: a loud call that begins with an introduction, followed by a build-up, a climax and a let-down. Pant-hoots are loud and enable chimpanzees to communicate over long distances through the forest. Previous studies have found differences in the pant-hoots calls from different regions. For example, the pant-hoots from Gombe sound a bit different from those made by chimpanzees in Mahale, 160 km away. When I played recordings of a single Mahale pant-hoot call to chimpanzees in Kibale Forest, Uganda, the Kibale chimpanzees acted as if they had just heard an intruder. If they were in a group with three or more males, they gave a loud vocal response and rapidly moved towards the speaker. If they were in a group with only one or two males present, though, they stayed quiet, and if they approached, did so slowly and cautiously. For chimpanzees, correctly telling friend from foe is a matter of life or death, so it would make sense for chimpanzees in neighboring groups to have distinctive pant-hoot calls. Cathy Crockford and colleagues found that pant-hoots from different communities within Ta\u00ef Forest, C\u00f4te d'Ivoire, also sound distinct. If such group-level differences result from vocal learning, they would be rather like dialects in human languages. My student Nisarg Desai has been testing whether this is also the case at Gombe. We\u2019ve been working with a team of Tanzanian field assistants, Hashimu Issa Salala and Nasibu Zuberi Madumbi, to record calls from the Mitumba and Kasekela chimpanzees, and are starting to test for differences between groups. We are in the early stages of this analysis. Chimpanzees might be capable of some degree of vocal learning, but I\u2019m struck by how subtle the differences in pant-hoot calls are from place to place. Chimpanzees make lots of different calls \u2013 pant-hoots, pant-barks, waa-barks, pant-grunts, rough-grunts, and so on \u2013 but across Africa, all chimpanzees produce a pretty similar set of calls in similar circumstances. In this respect, chimpanzee calls resemble human sounds like laughter and crying more than they resemble human words, which can vary drastically from place to place. Chimpanzees communicate effectively with their various sounds, but in ways quite similar to those of other nonhuman primates. This suggests that our common ancestor with chimpanzees also had a fairly typical repertoire of vocal communication for a nonhuman primate. The really big changes in human language \u2013 such as a lifelong ability to learn to make entirely new sounds and a rich symbolic meaning of such sounds \u2013 likely evolved later, for reasons that we still don\u2019t understand. \n    Write an article and join a growing community of more than 66,800 academics and researchers from 2,321 institutions.\n   \nRegister now\n \nCopyright \u00a9 2010\u20132018, The Conversation US, Inc.\n","time":1525800993,"title":"Chimpanzee calls are communication but not conversation","type":"story","url":"https:\/\/theconversation.com\/studying-chimpanzee-calls-for-clues-about-the-origins-of-human-language-95990","label":7,"label_name":"random"},{"by":"evo_9","descendants":0,"id":17022811,"kids":"None","score":1,"text":"\nNigel Warburton \n\nSat 9 Sep 2017 19.05\u00a0EDT\n\n\nLast modified on Fri 1 Dec 2017 21.45\u00a0EST\n\n Perhaps the best way to think of what makes a human being human is to look at something that seems almost human and subtract the difference. Whatever is left over is what is unique to us. That seems to be the thinking behind the Finnish photographer Maija Tammi\u2019s One of Them Is a Human #1, a portrait of Erica, the Japanese android who was declared the most realistic female human robot of 2016. The photograph caused a stir last week because it was shortlisted for the National Portrait Gallery\u2019s prestigious Taylor Wessing prize, despite the rule that \u201call photographs must have been taken by the entrant from life and with a living sitter\u201d. However realistic Erica may be, and to me she looks more like a sex doll than a real person, she was certainly not a living sitter. Judging from videos online, although Erica can engage people in stilted, formulaic conversation on a discrete number of topics, such as what she likes to do in her free time (she says she likes cinema), and may look lifelike when immobile, her mouth doesn\u2019t move in a realistic way, her bodily movements are stiff, and her skin has a distinctly latex appearance. This is not to denigrate the achievements of Hiroshi Ishiguro, Erica\u2019s creator, but she still looks uncanny, not quite human, and certainly wouldn\u2019t pass in a well-lit room. We are still a very long way from Bladerunner\u2019s world of replicants that only an expert could distinguish from the real thing and which at least seem to have an inner life. Erica may just be the start of the realistic robot revolution, a revolution that will no doubt force us to reconsider what it is to be human and at what point an android deserves to be taken seriously as another being. Aristotle famously declared that human beings are rational animals. His mentor, Plato, also thought of the reasoning part of the mind as the dominant part. That is allegedly the key human quality: our capacity to act rationally, to ruminate and give reasons for our actions, rather than just act from instinct or impulse. Sadly this aspect of humanity is more often eclipsed by irrationality. To take one kind of example, the psychologists Daniel Kahneman and Amos Tversky have demonstrated a range of erroneous patterns of thought that human beings characteristically adopt, summarised in Kahneman\u2019s Thinking, Fast and Slow. Far from rational to the core, it turns out we are all prone to systematic biases in thinking \u2013 the status quo bias, loss aversion, the sunk cost fallacy, framing effects and the rest. We may be distinctive in having the capacity to use reason, but we use it less well and less often than most of us like to believe. It would be no surprise to find robots of the future as better at using reason than humans \u2013 at which point we may want to back off describing ourselves as the rational ones. Another traditional answer to the question of what makes us so different, popular for millennia, has been that humans have a non-physical soul, one that inhabits the body but is distinct from it, an ethereal ghostly wisp that floats free at death to enjoy an after-life which may include reunion with other souls, or perhaps a new body to inhabit. To many of us, this is wishful thinking on an industrial scale. It is no surprise that survey results published last week indicate that a clear majority of Britons (53%) describe themselves as non-religious, with a higher percentage of younger people taking this enlightened attitude. In contrast, 70% of Americans still describe themselves as Christians, and a significant number of those have decidedly unscientific views about human origins. Many, along with St Augustine, believe that Adam and Eve were literally the first humans, and that everything was created in seven days. One philosophical route into the \u201chumans have souls\u201d story, which Gilbert Ryle later called \u201cthe myth of the ghost in the machine\u201d, was via the 17th-century Catholic philosopher Ren\u00e9 Descartes. In his Meditations (first published in 1641) Descartes argued that just about everything he thought to be true could be open to doubt, including the evidence of his senses and the truths of mathematics. An evil demon, if it existed, could implant false beliefs about anything at all, making us believe that two plus three equals six. The only thing immune to such scepticism, Descartes claimed, was his own existence as a thinking thing. That was because, just so long as he doubted his own existence, the very act of doubting proved to him that he must exist, because something was thinking that thought, was doubting its own existence. This is his famous \u201cCogito\u201d argument (from the Latin cogito ergo sum \u2013 \u201cI think therefore I am\u201d). Descartes emerged from his Meditations more certain of his existence as a thinker than as a physical being \u2013 the opposite of how most of us feel, I suspect. He even located a place in the brain, the pineal gland, where interactions between soul and body occur and, disturbingly, had no qualms about vivisecting dogs since in his view they lacked souls, and therefore had no capacity to feel. Today a combination of evolutionary biology and neuroscience gives us more plausible accounts of what we are than Descartes did. These accounts are not comforting. They reverse the priority and emphasise that we are animals and provide no evidence for our non-physical existence. Far from it. Nor are they in any sense complete, though there has been great progress. Since Charles Darwin disabused us of the notion that human beings are radically different in kind from other apes by outlining in broad terms the probable mechanics of evolution, evolutionary psychologists have been refining their hypotheses about how we became this kind of animal and not another, why we were able to surpass other species in our use of tools, communication through language and images, and ability to pass on our cultural discoveries from generation to generation. Close observations of other apes, such as the bonobos studied by Frans de Waal, and Jane Goodall\u2019s earlier work with chimpanzees, complement this and reveal strong parallels between primate and human social behaviour, similarities which can be explained from an evolutionary perspective. True, some evolutionary explanations resemble irrefutable \u201cjust so\u201d stories in their speculation about how we came to have particularly traits. But we should not condemn all evolutionary explanations simply because some people go too far in their speculations. Evolutionary accounts and neuroscience don\u2019t, however, tell us everything about what it is to be human. So, at least, argues Roger Scruton in his recent book On Human Nature. He thinks they miss out on the lebenswelt, the lived world of interpersonal interaction with each other and the inanimate world that we experience through our culture, needs, and desires, a world of meanings that make sense to us, but which, he argues, will never be adequately described by science. The meaning of a great painting by Rembrandt, for example, is not explained by a physical analysis of its effects on the brain of a viewer, or an evolutionary account of how we came to have this sort of appreciation of aesthetic and artistic qualities. We can examine and catalogue the mechanics and biology of sex, but that won\u2019t capture the experience of sexual love. This human capacity for connection with other people in a way that recognises their humanity is something that has been better captured in great literature and art than by scientists. In Bladerunner, the replicants seem to have a conscious inner life. They don\u2019t for the most part even realise that they are replicants. Whether actual robots, far more sophisticated than Erica, could ever have an inner life is an ongoing philosophical debate. Undoubtedly, they could be designed to fake it: give the right responses as if they had an inner life. That is essentially what is going on with Erica. But would these robots have the actual experience that we call consciousness? I doubt it. Perhaps, then, having the capacity for a certain kind of reflective consciousness is what makes us human and different from other animals. This may be present in an embryonic form in some other primates and large-brained mammals such as elephants and dolphins, but not to anything like the sense that it is for most human beings. Language may be key here, giving us ways of describing our own experience to ourselves. For those who believe that conscious thought is a highly complex computational phenomenon, it shouldn\u2019t matter what physical stuff is doing the thinking: a computer program should be able to think if it is complex enough, despite its circuits being made of silicon rather than flesh and blood. That would hold out the possibility of a more sophisticated robot than Erica having human-like thought and self-reflection, just as Bladerunner\u2019s replicants do, so that there would be something that it is like to be a robot. Some people are so convinced by this style of thinking that they are making plans for their minds to be uploaded to computers to give them a kind of eternal, or at least extended, mental life. For these techno-optimists, there is no problem about the physical basis of thought: it is all in the interconnections and pathways. But for those of us who believe that conscious experience is essentially tied to the particular biological stuff that does the thinking, that looks like further wishful thinking. In an important sense we are our bodies, and when we die there is absolutely no way that we can continue to exist. We are the kind of animal that is aware it will die, and whose conscious life requires a functioning brain. Are there other features of our existence that are unique to us? Our capacity to make free choice, which is the basis of morality? Perhaps. That was the assumption of existentialism, that we are not just radically free, but forced to make free choices, \u201ccondemned to be free\u201d as Jean-Paul Sartre put it, with any attempt to deny our freedom declared \u201cbad faith\u201d. But here again neuroscience points in a different direction. Benjamin Libet conducted experiments on intentional action that suggest that when I deliberately move my hand the conscious intention to do it occurs milliseconds after the neurological processes that will result in my hand moving have been initiated, rather than before. It is as if a football commentator thought they were making events happen on the field by their comments on the play. If Libet\u2019s experiments were not flawed in some way (and many think they were), perhaps what feels like free choice could be little more than after-the-event confabulation by the conscious mind. Libet himself left some room for control. He suggested we can think of ourselves as having \u201cfree won\u2019t\u201d rather than free will \u2013 since once the neurological process that culminates in a hand movement has been initiated, it is still possible to stop it before the action occurs. Whether or not he was right, the thrust of much recent neuroscience is that far more of what we fundamentally are occurs beyond the control of our conscious mind than was previously thought. Ludwig Wittgenstein describes leaves blowing about in the wind saying to one another \u201cNow I\u2019ll go this way \u2026 now I\u2019ll go that\u201d under the illusion they have control of the matter. That is a bleak picture of what it is to be human, but it may be accurate. Perhaps we are closer to the robot Erica in some respects than we might like to think. Nigel Warburton is a philosopher and author of A Little History of Philosophy. He is @philosophybites on Twitter","time":1525800972,"title":"What does a portrait of Erica the android tell us about being human?","type":"story","url":"https:\/\/www.theguardian.com\/technology\/2017\/sep\/09\/robot-human-artificial-intelligence-philosophy","label":7,"label_name":"random"},{"by":"lainon","descendants":0,"id":17022809,"kids":"None","score":1,"text":"","time":1525800952,"title":"Deep Learning for Electronic Health Records","type":"story","url":"https:\/\/ai.googleblog.com\/2018\/05\/deep-learning-for-electronic-health.html","label":7,"label_name":"random"},{"by":"exejeezus","descendants":0,"id":17022794,"kids":"None","score":2,"text":" \n1. Mines and Factories Am I a data mine, or am I a data factory? Is data extracted from me, or is data produced by me? Both metaphors are ugly, but the distinction between them is crucial. The metaphor we choose informs our sense of the power wielded by so-called platform companies like Facebook, Google, and Amazon, and it shapes the way we, as individuals and as a society, respond to that power. If I am a data mine, then I am essentially a chunk of real estate, and control over my data becomes a matter of ownership. Who owns me (as a site of valuable data), and what happens to the economic value of the data extracted from me? Should I be my own owner \u2014 the sole proprietor of my data mine and its wealth? Should I be nationalized, my little mine becoming part of some sort of public collective? Or should ownership rights be transferred to a set of corporations that can efficiently aggregate the raw material from my mine (and everyone else\u2019s) and transform it into products and services that are useful to me? The questions raised here are questions of politics and economics. The mining metaphor, like the mining business, is a fairly simple one, and it has become popular, particularly among writers of the left. Thinking of the platform companies as being in the extraction business, with personal data being analogous to a natural resource like iron or petroleum, brings a neatness and clarity to discussions of a new and complicated type of company. In an article in the Guardian in March, Ben Tarnoff wrote that \u201cthinking of data as a resource like oil helps illuminate not only how it functions, but how we might organize it differently.\u201d Building on the metaphor, he went on the argue that the data business should not just be heavily regulated, as extractive industries tend to be, but that \u201cdata resources\u201d should be nationalized \u2014 put under state ownership and control: Data is no less a form of common property than oil or soil or copper. We make data together, and we make it meaningful together, but its value is currently captured by the companies that own it. We find ourselves in the position of a colonized country, our resources extracted to fill faraway pockets. Wealth that belongs to the many \u2014 wealth that could help feed, educate, house and heal people \u2014 is used to enrich the few.\u00a0The solution is to take up the template of resource nationalism, and nationalize our data reserves. In another Guardian piece, published a couple of weeks later, Evgeny Morozov offered a similar proposal concerning what he termed \u201cthe data wells inside ourselves\u201d: We can use the recent data controversies to articulate a truly decentralised, emancipatory politics, whereby the institutions of the state (from the national to the municipal level) will be deployed to recognise, create, and foster the creation of social rights to data. These institutions will organise various data sets into pools with differentiated access conditions. They will also ensure that those with good ideas that have little commercial viability but promise major social impact would receive venture funding and realise those ideas on top of those data pools. The simplicity of the mining metaphor is its strength but also its weakness. The extraction metaphor doesn\u2019t capture enough of what companies like Facebook and Google do, and hence in adopting it we too quickly narrow the discussion of our possible responses to their power. Data does not lie passively within me, like a seam of ore, waiting to be extracted. Rather, I actively produce data through the actions I take over the course of a day. When I drive or walk from one place to another, I produce locational data. When I buy something, I produce purchase data. When I text with someone, I produce affiliation data. When I read or watch something online, I produce preference data. When I upload a photo, I produce not only behavioral data but data that is itself a product. I am, in other words, much more like a data factory than a data mine. I produce data through my labor \u2014 the labor of my mind, the labor of my body. The platform companies, in turn, act more like factory owners and managers than like the owners of oil wells or copper mines. Beyond control of my data, the companies seek control of my actions, which to them are production processes, in order to optimize the efficiency, quality, and value of my data output (and, on the demand side of the platform, my data consumption). They want to script and regulate the work of my factory \u2014 i.e., my life \u2014 as Frederick Winslow Taylor sought to script and regulate the labor of factory workers at the turn of the last century. The control wielded by these companies, in other words, is not just that of ownership but also that of command. And they exercise this command through the design of their software, which increasingly forms the medium of everything we all do during our waking hours. The factory metaphor makes clear what the mining metaphor obscures: We work for the Facebooks and Googles of the world, and the work we do is increasingly indistinguishable from the lives we lead. The questions we need to grapple with are political and economic, to be sure. But they are also personal, ethical, and philosophical. 2. A False Choice To understand why the choice of metaphor is so important, consider a new essay by Ben Tarnoff, written with Moira Weigel, that was published last week. The piece opens with a sharp, cold-eyed examination of those Silicon Valley apostates who now express regret over the harmful effects of the products they created. Through their stress on redesigning the products to promote personal \u201cwell-being,\u201d these \u201ctech humanists,\u201d\u00a0Tarnoff and Weigel write,\u00a0actually serve the business interests of the platform companies they criticize. The companies, the writers point out, can easily co-opt the well-being rhetoric, using it as cover to deflect criticism while seizing even more economic power. Tarnoff and Weigel point to Facebook CEO Mark Zuckerberg\u2019s recent announcement that his company will place less emphasis on increasing the total amount of time members spend on Facebook and more emphasis on ensuring that their Facebook time is \u201ctime well spent.\u201d What may sound like a selfless act of philanthropy is in reality, Tarnoff and Weigel suggest, the product of a hard-headed business calculation: Emphasising time well spent means creating a Facebook that prioritises data-rich personal interactions that Facebook can use to make a more engaging platform. Rather than spending a lot of time doing things that Facebook doesn\u2019t find valuable \u2013 such as watching viral videos \u2013 you can spend a bit less time, but spend it doing things that Facebook does find valuable.\u00a0In other words, \u201ctime well spent\u201d means Facebook can monetise more efficiently. It can prioritise the intensity of data extraction over its extensiveness. This is a wise business move, disguised as a concession to critics.\u00a0Shifting to this model not only sidesteps concerns about tech addiction \u2013 it also acknowledges certain basic limits to Facebook\u2019s current growth model. There are only so many hours in the day. Facebook can\u2019t keep prioritising total time spent \u2013 it has to extract more value from less time. The analysis is a trenchant one. The vagueness and self-absorption that often characterize discussions of wellness, particularly those emanating from the California coast, are well suited to the construction of window dressing. And, Lord knows, Zuckerberg and his ilk are experts at window dressing.\u00a0But, having offered good reasons to be skeptical about Silicon Valley\u2019s brand of tech humanism, Tarnoff and Weigel overreach. They argue that any \u201chumanist\u201d critique of the personal effects of technology design and use is a distraction from the \u201cfundamental\u201d critique of the economic and structural basis for Silicon Valley\u2019s dominance: [The humanists] remain confined to the personal level, aiming to redesign how the individual user interacts with technology rather than tackling the industry\u2019s structural failures. Tech humanism fails to address the root cause of the tech backlash: the fact that a small handful of corporations own our digital lives and strip-mine them for profit. This is a fundamentally political and collective issue. But by framing the problem in terms of health and humanity, and the solution in terms of design, the tech humanists personalise and depoliticise it. The choice that Tarnoff and Weigel present here \u2014 either personal critique or political critique, either a design focus or a structural focus \u2014 is a false choice. And it stems from the metaphor of extraction, which conceives of data as lying passively within us (beyond the influence of design) rather than being actively produced by us (under the influence of design).\u00a0Arguing that attending to questions of design blinds us to questions of ownership is as silly (and as condescending) as arguing that attending to questions of ownership blinds us to questions of design. Silicon Valley wields its power through both its control of data and its control of design, and that power influences us on both a personal and a collective level. Any robust critique of Silicon Valley, whether practical, theoretical, or both, needs to address both the personal and the political. The Silicon Valley apostates may be deserving of criticism, but what they\u2019ve done that is praiseworthy is to expose, in considerable detail, the way the platform companies use software design to guide and regulate people\u2019s behavior \u2014 in particular, to encourage the compulsive use of their products in ways that override people\u2019s ability to think critically about the technology while provoking the kind of behavior that generates the maximum amount of valuable personal data. To put it into industrial terms, these companies are not just engaged in resource extraction; they are engaged in process engineering. Tarnoff and Weigel go on to suggest that the tech humanists are pursuing a patriarchal agenda. They want to define some ideal state of human well-being, and then use software and hardware design to impose that way of being on everybody. That may well be true of some of the Silicon Valley apostates. Tarnoff and Weigel quote a prominent one as saying, \u201cWe have a moral responsibility to steer people\u2019s thoughts ethically.\u201d It\u2019s hard to imagine a purer distillation of Silicon Valley\u2019s hubris or a clearer expression of its belief in the engineering of lives. But Tarnoff and Weigel\u2019s suggestion is the opposite of the truth when it comes to the broader humanist tradition in technology theory and criticism. It is the thinkers in that tradition \u2014\u00a0Mumford, Arendt, Ellul, McLuhan, Postman, Turkle, and many others \u2014 who have taught us how deeply and subtly technology is entwined with human history, human society, and human behavior, and how our entanglement with technology can produce effects, often unforeseen and sometimes hidden, that may run counter to our interests, however we choose to define those interests. Though any cultural criticism will entail the expression of values \u2014 that\u2019s what gives it bite \u2014 the thrust of the humanist critique of technology is not to impose a particular way of life on us but rather to give us the perspective, understanding, and know-how necessary to make our own informed choices about the tools and technologies we use and the way we design and employ them. By helping us to see the force of technology clearly and resist it when necessary, the humanist tradition expands our personal and social agency rather than constricting it. 3. Consumer, Track Thyself Nationalizing collective stores of personal data is an idea worthy of consideration and debate. But it raises a host of hard questions. In shifting ownership and control of exhaustive behavioral data to the government, what kind of abuses do we risk? It seems at least a little disconcerting to see the idea raised at a time when authoritarian movements and regimes are on the rise. If we end up trading a surveillance economy for a surveillance state, we\u2019ve done ourselves no favors. But let\u2019s assume that our vast data collective is secure, well managed, and put to purely democratic ends. The shift of data ownership from the private to the public sector may well succeed in reducing the economic power of Silicon Valley, but what it would also do is reinforce and indeed institutionalize Silicon Valley\u2019s computationalist ideology, with its foundational, Taylorist belief that, at a personal and collective level, humanity can and should be optimized through better programming. The ethos and incentives of constant surveillance would become even more deeply embedded in our lives, as we take on the roles of both the watched and the watcher. Consumer, track thyself! And, even with such a shift in ownership, we\u2019d still confront the fraught issues of design, manipulation, and agency. Finally, there\u2019s the obvious practical question. How likely is it that the United States is going to establish a massive state-run data collective encompassing exhaustive information on every citizen, at least any time in the foreseeable future? It may not be entirely a pipe dream, but it\u2019s pretty close. In the end, we may discover that the best means of curbing Silicon Valley\u2019s power lies in an expansion of personal awareness, personal choice, and personal resistance. At the very least, we need to keep that possibility open. Let\u2019s not rush to sacrifice the personal at the altar of the collective.","time":1525800851,"title":"I Am a Data Factory (and So Are You)","type":"story","url":"http:\/\/www.roughtype.com\/?p=8394","label":7,"label_name":"random"},{"by":"oftenwrong","descendants":0,"id":17022770,"kids":"None","score":1,"text":"\nAt the Whole Foods in downtown Minneapolis, customers just walk right up to the store. Can you imagine? People who live nearby can!\n Traffic \u2014 often, it's you, but most of the time it's someone else. Traffic is generally treated as this real math-heavy, empirical deal because a lot of people have gotten a lot of credentialing in it and they're still paying off their loans. But like its sister concern, parking, it's maybe more of a mental\/emotional\/psychological thing than anything else. For example, there are a lot of fun, real-world instances of traffic doing things you wouldn't really expect. Here's one from a couple months ago: After fears that the closing of Paris' Seine highway was causing increased traffic on surrounding streets, after an uptick, traffic numbers on those roads have declined considerably over the past year. https:\/\/t.co\/Wv3uF3RAKX There are a lot more. Stateside, closing the chunk of Broadway through Times Square in Manhattan to car traffic decreased congestion on other streets and dramatically improved pedestrian safety. Some cities have torn out freeways and that's been fine. Without closing a street entirely, you can sometimes, counter-intuitively, shrink it and make it work better for cars. Obviously, this logic is not extendable out into infinity; you can't close all streets to cars and have that be a good idea. So traffic is maybe more of a social science than a hard science, and it's worth thinking beyond the simple engineering diagrams. People make decisions: they find alternate routes, decide to walk to the gym, wait an hour past the evening rush to go to the store, and it's hard to model it all that accurately.\u00a0You can read more about the concept of \"induced demand\" here. One of the biggest and most understated stories in Minneapolis in the past decade is the addition of a whole bunch of grocery stores in walkable areas. You've got the downtown and Northeast Lund's, Dinkytown and Uptown Target Expresses, the downtown Whole Foods, the Wedge Table (rhymes with vegetable) on Nicollet, North Market off Victory Memorial Drive, Fresh Thyme in Prospect Park,\u00a0and new co-ops in Powderhorn and Willard-Hay. There's a Mill District Trader Joe's and a Whittier Aldi under construction, and a new urban-format Cub in a development at 46th and Hiawatha just broke ground. It's a lot! And this is a big deal, because it gives tens of thousands of people the opportunity to live a very different lifestyle. Right now, the vast majority of people in America have to get in a car and drive somewhere to buy a loaf of bread. Much of Minneapolis, a medium-sized city, is like this. There are certain areas that are better than others. I live in Loring Park and don\u2019t have a car and manage easily enough for my day-to-day needs, but that\u2019s pretty rare, particularly if you need a family-sized place.\u00a0 With the addition of useful commercial activity in neighborhoods around town, more people will have the option to get a loaf of bread, a haircut, a cup of coffee, a slice of pizza, or AA batteries without hopping in a car. And there\u2019s a feedback loop where adding goods and services makes an area more appealing, which attracts more residents, which in turn allows the area to support more goods and services. Example: If you were a student at the University of Minnesota 10 years ago, odds are pretty good you drove to the Quarry Target\/Rainbow or maybe the Midway Shopping Center for most of your grocery shopping. I, personally, took the 3 bus from 26th and Como to the downtown Target, but I'm pretty weird, and get occasional threats DMed to me on Twitter. These days, there's a Target Express in Dinkytown and a Fresh Thyme in Prospect Park. A guy who grew up in Maple Grove but is now living in Marcy-Holmes can walk down the street with a backpack and buy three days\u2019 or a week's worth of groceries without having to hop in a second-hand Toyota Corolla and drive to a different part of town. Free groceries can draw a crowd: pic.twitter.com\/PWbd9pXvY3 So while the site of the Dinkytown Target Express is certainly generating more car trips than it did before the Target Express opened, in the grand scheme of things, there are many thousands fewer car trips happening because people aren\u2019t driving to the Quarry as often. The Dinkytown example in particular is fun, because coming to Minneapolis to go to school is a lot of Minnesotans' first experience with ~the big city~ other than going to Twins games as a kid. But, and don't tell anyone about this, aside from the bigger buildings placed closer together, a lot of people in Minneapolis still live a life pretty similar to people in Maple Grove, just with different bumper stickers\u2014they're still driving everywhere. And as we think about our city's future and also sort of importantly, the future of the whole world, which is greatly at risk of overwhelming calamity due to climate change, giving people that option is important. Maybe it lets them ditch the car, or maybe they don't lose the car entirely, but their household drops down to just one that they use to go to IKEA periodically.\u00a0 You\u2019ve got to keep the broader metropolitan area in mind. It's complicated and not a direct one-to-one thing, but housing that isn't built in Minneapolis does end up somewhere. The apartment buildings we've built in Minneapolis over the past five years are occupied and people are living in them. If those people weren't living in a new building in Uptown, they'd either A) displace someone in Uptown who couldn't afford the rent hike (and that person ends up somewhere) or B) move to St. Louis Park five years earlier, which is less walkable, and then someone who would have been in St. Louis Park would end up in Eden Prairie, which is even less walkable, and so on and so forth. Letting people live in Minneapolis (or even the nearby town of St. Paul) where this type of lifestyle is possible without rebuilding the entire city has to be a piece of the strategy. Pushing your housing demand out to, eventually, Albertville or whatever, where people are literally driving 100 miles a day is...bad. Only a fraction (something like 15 to 20 percent) of car trips are for commuting to and from work. The rest are for other things, like going to buy that loaf of bread. Building a city where it\u2019s possible to walk a few blocks to fulfill your basic needs knocks out a whole lot of that remaining 85 percent. This post appeared originally on Nick Magrino's personal blog. Minneapolis has an adopted goal of reducing our greenhouse gas emissions by 80 percent\u00a0by 2050. We won't be able to do that without reducing car trips\u2014even electric cars aren't going to get us all the way to the goal. If we\u2019re going to reach that goal, we\u2019re going to need to build walkable neighborhoods. SEE MORE EVENTS Stay Up to Date With Sponsor Content \n    \u00a9 2018 CITY PAGES. ALL RIGHTS RESERVED.\n \n    \u00a9 2018 CITY PAGES. ALL RIGHTS RESERVED.\n \nContinue to Site\n All-access pass to top stories, events and offers in the Twin Cities.","time":1525800639,"title":"Grocery stores can reduce traffic","type":"story","url":"http:\/\/www.citypages.com\/restaurants\/how-the-minneapolis-grocery-store-boom-is-fighting-traffic\/481063931","label":7,"label_name":"random"},{"by":"guitarbill","descendants":57,"id":17022764,"kids":"[17024685, 17023674, 17024783, 17023445, 17023670, 17023749, 17023626, 17023437, 17023605, 17023926, 17023857, 17023665, 17023849]","score":92,"text":"Ten years ago, at the 2008 RSA Conference, Yubico launched the first YubiKey with the goal of making secure login easy and accessible for everyone. The vision was one single security key to work across any number of services, with great user experience, security, and privacy.  On this anniversary, Yubico has taken another major leap forward toward this vision with the announcement that the recently-launched Security Key by Yubico, with FIDO2, will be supported in Windows 10 devices and Microsoft Azure Active Directory (Azure AD). The feature is currently in limited preview for Microsoft Technology Adoption Program (TAP) customers.  FIDO2 is the passwordless evolution of the FIDO Universal 2nd Factor (U2F) standard, created by Yubico and Google. While U2F included a username and password, FIDO2 supports more use cases, including passwordless authentication. Yubico has worked in close collaboration with Microsoft on developing the FIDO2 technical specifications, and the Security Key by Yubico is the first FIDO2 authentication device on the market. What Does This Mean?  Organizations will soon have the option to enable employees and customers to sign in to an Azure AD joined device with no password, by simply using a Security Key to get single sign-on to all Azure AD based applications and services. This is just the beginning; Google and Mozilla also announced Chrome and Firefox support for the Web Authentication API (WebAuthn) developed by Yubico and members of the World Wide Web Consortium (W3C) and included in the FIDO2 specification. \n  Why Is This Important?  Nearly every digital experience today requires passwords, an increasingly frustrating fact of life for businesses and users. For any one person there can be hundreds of sites and devices \u2014 both personal and business related \u2014 that require memorized passwords. This leads to poor password hygiene: shared and reused passwords. And it is a real cost for businesses managing, storing and resetting passwords for employees and end-users.  Working in conjunction with Windows and Microsoft cloud services, the new Security Key by Yubico offers a secure, seamless and passwordless login experience with one of the world\u2019s largest computer operating systems. Use cases include retail, healthcare, transportation, finance, manufacturing, and more.  How Does It Work?  FIDO2 is built on the same security and privacy features of FIDO U2F: strong public key cryptography, no drivers or client software and one key for unlimited account access with no shared secrets. With FIDO U2F, the user entered a username and password, inserted a \u00a0security key in the USB-port, and touched the gold area. FIDO2 adds more options to the login process:  Who Can Get Involved? Everyone is encouraged to get involved, and accelerate progress to a secure and passwordless world. As with any open standard, advancement will be a collective industry effort and a process of global adoption. Yubico helped the majority of services in making support for FIDO U2F by providing open source code and support. Together with W3C and FIDO Alliance we have made the FIDO2 open authentication standard available, and we are helping support its rapid integration into services and applications through our new Yubico Developer Program. Enterprises \u2192 Learn about using FIDO2 with Windows 10 devices and Microsoft Azure Active Directory in your enterprise environment. Explore the benefits of FIDO2.  Developers \u2192 Implement early support for FIDO2 by signing up for updates from Yubico\u2019s Developer Program. Members will have first access to resources to implement FIDO2 within their applications and services.  Individuals \u2192 Are you tired of passwords? If you had a choice to securely and easily login to any device or online service without them, would you? Ask for it! Visit your favorite service or businesses on Twitter and tell them you want to securely login to your account without a password by using FIDO2 and the Security Key by @Yubico! Are you interested in learning more about a life without passwords? Learn more about the Security Key by Yubico and benefits of FIDO 2.  Comments are closed. Subscribe to the  Yubico Blog RSS feed Find\nTake Product Finder Quiz Set Up\nFind Set-Up Guides Buy\nBuy Online\nContact Sales\nFind Resellers Stay Connected\nSign up for email  Cookies Legal Privacy Terms of Use EnglishSwedishGermanFrench Yubico \u00a9 2018. All Rights Reserved.","time":1525800621,"title":"Yubico and Microsoft Introduce Passwordless Login","type":"story","url":"https:\/\/www.yubico.com\/2018\/04\/yubico-and-microsoft-introduce-passwordless-login\/","label":3,"label_name":"dev"},{"by":"tannerbrockwell","descendants":1,"id":17022762,"kids":"[17023463]","score":2,"text":"Podcast: Play in new window | Download Starting an Internet business is harder than it should be. You need to incorporate, create an operating agreement, set up a system to accept payments, and many other straightforward tasks.  In the 1990s, this was how it felt to set up anything on the Internet. You always had to stand up a web server on your own infrastructure, before you could get to the interesting part\u2013which was building an actual product. With the popularization of cloud computing, it became massively easier to stand up a server. Because of that lower activation energy, millions of applications and thousands of software businesses got started. But the activation energy required to start a business remains higher than necessary. It feels like standing up a web server in the 90s\u2013lots of tedium and reinventing the wheel that has been done by people before you. This is the motivation behind Stripe Atlas, a project to simplify the process of starting an Internet business. Patrick McKenzie works on Atlas at Stripe. He was previously on the show to discuss his experience leaving a large corporation to work on his own small software companies. And his name has become synonymous with the modern phenomenon of the small software company\u2013he has been writing about this topic for over a decade at Kalzumeus.com. It was great to talk to Patrick once again about Internet businesses, and I\u2019m excited to see Stripe Atlas become something huge. Transcript provided by We Edit Podcasts. Software Engineering Daily listeners can go to\u00a0weeditpodcasts.com\/sed\u00a0to get 20% off the first two months of audio editing and transcription services. Thanks to We Edit Podcasts for partnering with SE Daily. Please\u00a0click here to view\u00a0this show\u2019s transcript.       Airtable is hiring creative engineers who believe in the importance of open-ended platforms that empower human creativity. Airtable is a uniquely challenging product to build, and they are looking for creative frontend and backend engineers to design systems on first principles\u2014 like a realtime sync layer, collaborative undo model, formulas engine, visual revision history, and more.\u00a0Check out jobs at Airtable by going to airtable.com\/sedaily.    \u00a0 \u00a0 \n\n\n \n\n","time":1525800592,"title":"Patrick McKenzie (patio11) Talks Working on Atlas at Stripe","type":"story","url":"https:\/\/softwareengineeringdaily.com\/2018\/05\/08\/stripe-atlas-with-patrick-mckenzie\/","label":7,"label_name":"random"},{"by":"KryDos","descendants":1,"id":17022740,"kids":"[17022746]","score":2,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back The app is just an Electron wrapper on Todoist's web version. Currently only RPM and DEB packages avilable for x64 arch. You can download the package on releases page or you always can get latest build as: The main reason is I don't like to have web version opened since I can't easily alt+tab to it. And I also really wanted to have global shortcuts to quick add a task. The initial inspiration I got from this brilliant package of the same web version.\nUnfortunately it doesn't seem maintained at the moment and has some issues with Tray functionality on latest Ubuntu.","time":1525800439,"title":"Todoist for Linux","type":"story","url":"https:\/\/github.com\/KryDos\/todoist-linux","label":4,"label_name":"github"},{"by":"f2n","descendants":0,"id":17022738,"kids":"None","score":1,"text":"Read our latest news We are releasing a new version of RouterSploit! For those that prefer experimenting rather than reading documentation, head to github and begin getting your hands dirty. But what has actually changed?\r\nOne of the most important changes we made is completely switching to Python 3. The support of Python 2.7 ends in 2020 so we decided to take a gamble on Python 3. We know the security community embraces Python 2.7 (and we do also, well, most of the time), but this is something that we think will benefit the project in the future. Also, RouterSploit will now be maintained by Threat9, which means there will be more resources to improve the tool. Ok, lets move on to describing the actual features... It's 2018 and there is still a huge problem with default and hardcoded credentials. The problem is not only with the devices that contain these vulnerabilities, but also with the solutions that are supposed to detect them (hi various network vulnerability scanners). But why is it so difficult to detect these vulnerabilities? What makes them so unique that all the expensive scanners miss them and at the end of the day large companies with big security budgets are still vulnerable to simple admin\/admin? The answer is pretty simple. Most device manufacturers are introducing custom authentication schemes, which means that vulnerability scanning companies need to write unique modules for EVERY device. And with the billions of devices that are already out there it's basically impossible, even for a large scanning company. So we believe the only way to truly cover all these interfaces and authentication schemes is through a crowdsourced approach. So this is what we've started doing. We are writing modules for each custom authentication scheme scoped by device. Our hope is that by starting this framework, the community can add in the devices they are working with and we can eventually put a dent in what should be a solved problem. In the below example we will be testing a Mikrotik API (something that mkbrutus tool was designed for). By default it is running on port 8728\/TCP and provides the user with administrative functionalities. We can display the list of available wordlists and pick one that should be used for the attack. Once the attack is successful the module returns a valid credentials pair that can be used to access the device. Good ol' autopwn is still there, but now it also checks for default\/hardcoded credentials, so you can just point to the target and autopwn will do the job for you. Now we can exploit the target: Exploitation with the RouterSploit framework is pretty simple. All that is required is to pick a module and set the target. In this example, we will exploit the Netgear WNAP320 device that contains a remote code execution vulnerability. At this point we could start issuing commands that would be executed on the device. However, the problem with this particular vulnerability is that it is a blind command injection, so we will not be able to see the output from the commands. Instead, we need to establish a more interactive connection with the target, so we will do that by using payloads: Lets pick the reverse shell payload and configure it: After running the payload we are receiving access to the target's \/bin\/sh If you are familiar with Metasploit (and since you are reading this, there is a high chance you are) this is nothing new. But since the framework requires utilizing payloads we decided to expose them to the user. So now you can easily create ARM\/MIPSBE\/MIPSLE based payloads and use them for exploiting embedded devices. We quickly created a small, statically linked reverse shell binary that once executed on MIPS Little Endian architecture will connect to 192.168.1.12 on port 5555 providing us with access to target's \/bin\/sh. Did you like Bleah? If so, then you will like our BTLE toolkit as well. We incorporated all of the features from Bleah directly into the framework so you can scan for BTLE devices, enumerate their characteristics, and write directly to the them.\r\n                                 We can identify BTLE devices by using module btle_scan and gather basic information about nearby devices: Once we know which nearby devices are utilizing Bluetooth Low Energy communication we can target one and start enumerating its characteristics. This will give us an overview of the device and its potential attack surface. We were able to obtain information such as the device and manufacturer name, as well as the model number and firmware revision. We have also discovered a number of characteristics that we were not able to be identified. These are most likely used by the app (e.g. mobile app) to control the device. Some of these characteristics allow us to write data to them via the \"WRITE\" property, so we can try to write some arbitrary data to them.\r\n   Download the app and happy hunting! Releasing Exploitation Framework for Embedded Devices \u00a9 2018 THREAT9","time":1525800430,"title":"RouterSploit 3.0:  Exploitation Framework for Embedded Devices","type":"story","url":"https:\/\/www.threat9.com\/blog.html","label":3,"label_name":"dev"},{"by":"severine","descendants":0,"id":17022734,"kids":"None","score":1,"text":"Perhaps most confusing aspect of K-9 is the use of Classes to control the display and synchronization behavior of accounts and folders.  The goal of the Class system is to provide an easy way for a person with a large number of folders to accomplish either of these two goals: We didn\u2019t want either use case to require going through nearly all of the folders, setting a parameter on each folder.  To this end, there are settings to be made on both Account and Folders. If you do not make any Class setting changes, K-9 will display all of your folders, but will not perform automatic synchronization on any of them, until you change the \u201cEmail check frequency\u201d in the Account settings. Classes can be used to adjust both the display of your folders and the automatic synchronization. Only displayed folders will be synchronized, irrespective of the synchronization class settings. You can adjust the Account Class settings through the normal Account Settings Activity.  To adjust the folder Class settings, use the new Folder Settings Activity. In the main folder list, long-press a folder. In Android Email, a long press on a folder does nothing. In K-9, it brings up a context menu, providing Refresh and Folder settings Click Folder settings to adjust the Classes to which the folder is assigned. First is the Folder display class  A folder can be assigned to be in 1st Class, 2nd Class or None. The default is None. If you have a lot of folders, and only want to display a few, then assign those few to 1st Class. If you have a lot of folders, and only want to hide a few, assign those few to 2nd Class, and leave the multitude set to None. Second, it is also possible to separately set the \u201cFolder sync class\u201d. By default, the folder\u2019s sync class is the same as the folder\u2019s display class. However, there are cases in which it is useful to have a different class for synchronization purposes. For instance, if there is a folder that you want to appear in your normal short list, but you do not want it to be automatically synced. Drafts is a good example of such a folder. In this case, assign the Drafts folder to be 1st Class for display but 2nd Class for sync. You will always have it in your folder list, but will not waste any battery power automatically keeping it in sync. Now, in order to make use of the folder Class assignment, it is necessary to adjust two settings in the \u201cAccount settings\u201d: \u201cFolder display mode\u201d and \u201cFolder sync mode\u201d. Folder display mode determines which folders are to be displayed.\nThere are four choices: Folder sync mode determines which folders are to be automatically synchronized at the \u201cEmail check frequency\u201d. There are four choices: To show the utility of the various modes, it is probably best to show some use cases. In all cases, I have four folders available: Inbox, Janet, Carl and John. I want to see just my Inbox, Janet and Carl folders, and want all of those to be automatically synchronized. (Imagine I have 100 other folders that I do not want to see.) I want to see and sync all of the folders, except for Carl. (Imagine I have 100 other folders I want to see.) I want to see only my Inbox, Carl and John folders, but I do not want the Carl folder to be automatically synchronized. I want to normally only see my Carl and Janet folders, often want to see my Inbox, but never want to see my John folder.  All displayed folders are automatically synchronized. To easily show the Inbox, and the imaginary other folders I want to sometimes see, just change the Account\u2019s Folder display mode to 1st and 2nd Class folders. I want to normally only see my Carl and Janet folders, often want to see my Inbox, never want to see my John folder, but only Carl and Janet are automatically synchronized. To easily show the Inbox, and the imaginary other folders I want to sometimes see, just change the Account\u2019s \u201cFolder display mode\u201d to 1st and 2nd Class folders.","time":1525800403,"title":"Folders in K-9 Mail","type":"story","url":"https:\/\/k9mail.github.io\/documentation\/folders.html","label":7,"label_name":"random"},{"by":"oftenwrong","descendants":0,"id":17022719,"kids":"None","score":2,"text":"\nMay  8, 2018\n\nby Joe Cortright\n\n The following article is republished from City Observatory with permission. The recent defeat of SB 827 \u2014\u00a0California State Senator Scott Wiener\u2019s bill that would have legalized apartment construction in areas well served by transit \u2014\u00a0was the subject of a thoughtful post-mortem in the Los Angeles Times: \u201cA major California housing bill failed after opposition from the low-income residents it aimed to help. Here\u2019s how it went wrong.\u201d\u00a0Liam Dillon notes that, while the bill had the strong support of YIMBY (Yes in My Back Yard)\u00a0housing advocates, it foundered because of the combined opposition of not only local governments and homeowners, but also the very people it was supposed to help:\u00a0 low income renters. Dillon points out the schism between the economic and political cases for the legislation. SB 827 may have been great economics, but it was poor politics. YIMBY\u2019s and a wide range of urban and housing scholars supported the SB 827 approach, arguing that more housing, especially in transit served locations, would ease lower rents and reduce displacement. Dillon writes: The reality is that the heart of displacement is a lack of housing, which pours lighter fluid on housing costs, puts huge pressure particularly on low-income tenants and pushes people out,\u201d [Senator Scott Wiener] said.\u00a0Research from the state\u2019s nonpartisan Legislative Analyst\u2019s Office and UC Berkeley has found that building any new housing, especially homes subsidized for low-income residents, prevents displacement at a regional level. But low income renters and those who advocate on their behalf weren\u2019t buying it. Dillon says, \"There is a fundamental disconnect between the approach of the senator and his supporters on one side and influential anti-poverty organizations on the other.\u201d Their fear was that new apartment construction would happen disproportionately or exclusively in lower income communities.\u00a0 The Brookings Institution\u2019s Jenny Schuetz\u00a0boiled this down to a trenchant tweet: Tricky politics. Past experience shows that wealthy white communities have been more successful blocking development in their neighborhoods, so not unreasonable that lower-income [people of color] worried they\u2019ll bear the brunt. But building more housing is only long-term solution. Never mind that this pretty much flies in the face of the logic of real estate development: given the choice to build apartments in a high income community or a low income community, developers will inevitably tend to gravitate toward the places where rents are higher so that they can earn a greater profit. The fact that high income communities have been so adept\u00a0at zoning land for single family uses and so resistant to development proposals is the principal reason that demand has been diverted to lower income neighborhoods in the first place. A sweeping, statewide pre-emption of \u201clocal control\u201d is the only thing that\u2019s likely to open up the opportunity to develop in these higher income places. Ultimately, this shows how deeply ingrained the notion of weaponizing development approvals is in the land use process. The argument seems to be that, unless low income communities have the same power to exclude new development that wealthier communities routinely exercise,\u00a0this is inequitable. Low income housing advocates have used withholding development permission and regulating density to extract concessions from developers in the form of community benefit agreements or construction of or financial contributions for affordable housing. This exactly parallels the way in which higher income communities extract concessions in the form of land dedication, park construction, contributions for schools and local government and other amenities. As long as we view planning and development approvals as devices for extracting concessions from developers on a case-by-case basis, we\u2019ll inevitably circle back to a low-build, NIMBY-dominated world. This is pretty much the problem that has plagued New York\u2019s Mandatory Inclusionary Housing program. In theory, the city\u2019s program requires developers to dedicate a portion of units in new apartment buildings for affordable housing, which should ease the city\u2019s supply crunch and help reduce everyone\u2019s rent. But in practice, the individual neighborhoods in which the up-zoned apartment buildings would be constructed oppose the additional density.\u00a0 While the city-wide policy easily gained a majority of the City Council, the individual up-zoning approvals that would activate the \u201cmandatory\u201d portions of the law have run into difficulties. In the first two projects forwarded under the law \u2014\u00a0in Manhattan\u00a0and Queens\u00a0\u2014 strong neighborhood opposition has prompted the local city councilor to withdraw support for the needed zone change\u2013effectively torpedoing the projects. In many respects, this is a reprise of the drama that doomed Governor Jerry Brown\u2019s 2016 proposal\u00a0to exempt affordable housing construction from the state\u2019s CEQA environmental impact review process. While that would have encouraged development, it also would have removed a valuable bargaining chip that local communities (and labor unions and environmental groups) used to extract concessions from developers. As long as development permission is organized around this highly transactional, brokered process, its unlikely that any group is going to cede its points of leverage. We\u2019ll achieve equality by enabling all neighborhoods, rich and poor, to be empowered to say \u201cnot in my back yard.\u201d As we\u2019ve pointed out before, there\u2019s a particularly nasty version of the prisoner\u2019s dilemma\u00a0operating when it comes to liberalizing land use laws.\u00a0 Individual communities and groups would be better off if everyone were open to allowing more housing everywhere. But they don\u2019t trust that others won\u2019t renege, and their community (or group) will be saddled with all the burden and impacts of additional density. As in the prisoner\u2019s dilemma, everyone looks out for their own self-interest, which produces a result that is collectively worse for everyone.\u00a0Like Sartre\u2019s No Exit, it feels like the actors are caught in a hell of mutually conflicting objectives. If there\u2019s going to be a way to break this logjam, it\u2019s probably going to have to look a lot like Senate Bill 827, a relatively simple, clear and unavoidable state pre-emption that applies with equal force to all communities, rich and poor. The trick will be getting everyone to agree that this is in our common interest. The ship is sinking and we're not even rearranging the deck chairs;\u00a0we're arguing about their color. Three key factors work together to make Philadelphia a more affordable city than its East Coast sisters. NIMBYs are responding to a set of very rational incentives. That presents a challenge for those of us who hope to alter the course of the Suburban Experiment.\u00a0 \n\n\n\n\n\n\n\n Strong Towns is a 501(c)3 non-profit organization. Our work is performed under a\u00a0\u00a0Creative Commons Attribution-ShareAlike 3.0 Unported License. Please share with others to use for good.","time":1525800324,"title":"No Exit from Housing Hell","type":"story","url":"https:\/\/www.strongtowns.org\/journal\/2018\/5\/8\/no-exit-from-housing-hell","label":7,"label_name":"random"},{"by":"axiomdata316","descendants":0,"id":17022717,"kids":"None","score":2,"text":"\u2026do not really know where the language stops and the framework begins. What do I mean by this? Up until about 1988 most programs that a person (like you) would use had been programmed from the ground up by a handful of programmers (often just one) using a 3GL (3rd generation language). The key words being: from the ground up. As I explain in the first article of this series, 3GLs abstract assembly or machine language into reserved words.\u00b9 A programming language is a collection of reserved words and some rules about grammer that restrict how one use those words in a way that will not confuse the compiler (which expands the words into a series of machine language instructions). Together, this is referred to as the language\u2019s syntax. As you can see most languages have surprisingly few reserved words to remember. SmallTalk has only 6! So back to pre-1988 software, if you look at the code of a program written before that time, the only words you will see besides the handful of reserved words are the names of variables and functions that the programmers created. This is what most people think of as programming. The image comparison code (written in C) below uses two reserved words: for and double. The two underlined words ( fabs, and printf) are functions from included libraries (stdio.h and math.h). When a library is included like this it is called a dependancy. All of the other words in this program are either variables or comments written by the programmer. By simple word count, 90% of this code was written by the programmer. Less than 10% of the code is someone else\u2019s, 4% from the language and 6% from the two libraries. Now lets look at the Java version of the same function: Everything in bold is a key word, everything underlined is a function from an imported library. This listing is about 65% code written by the programmer, and 20% code from the language syntax and 15% depedancies upon external libraries. I am not saying that this is a bad thing. I am merely making a statement of fact. As it happens, including code from libraries is an important productivity enhancer. There is no justifiable reason for the average programmer to re-invent the wheels of language; reserved words, library classes and functions. So in most modern code, this trend towards less code written by the programmer, and relying more and more upon code written by someone else in the form of a library has increased exponentially. And although not inherently a bad thing, many agree that things have gone too far. In early 2016, what felt like half of the Internet broke because a programmer removed an 11 line program called left-pad from a public repository called npm. It turned out that some of the biggest, most used JavaScript frameworks in the world included a dependancy on left-pad rather than type out the ten lines of code below: Another npm package called isArray had 18 million downloads in February of 2016, and is a dependancy for 72 other NPM packages. 18 Million everyday programmers, and 72 package authors used an include rather than type this 1 line of code: return toString.call(arr) == '[object Array]'; Now I\u2019m just a country boy, but to me this pretty clearly indicates that the programmers that created these 72 npm packages either had the most twisted sense of humor I have ever seen, or really had no idea of what was in isArray and how JavaScript actually works. I take it to be an example of cargo cult programming at its most extreme. To further drive home the point that most modern programmers blindly use class libraries without understanding what is in them I refer you to Jordan Scales sobering (and depressing) account of his personal reaction to the left-pad fiasco. So where am I going with all of this? My point is that \u201cprogramming\u201d as the average person imagines it hardly exists today. The only programmers \u201cwriting code\u201d in the form of new algorithms are either working at very big Internet companies, or are writing specialized image, video, or sound processing sofwtare for a startup. The armies of \u201ckids today\u201d working in the salt mines of corporate and government IT are doing something else entirely. The coding they do is the software equivalent to meme creation and social media posts, complete with post-modern pop culture references. Only instead of recycling pictures of Clint Eastwood, Good Guy Greg, or Scumbag Steve, they are cutting and pasting code and indiscriminately using libraries such as left-pad or isArray. They do not really know where the language ends and the framework begins. It is all just one big soup to them. And although I am not a \u201ckid\u201d, I am scarcely better myself. I describe myself as a cargo cult programmer (reluctantly but honestly). Some of you may be familiar with the epic Story of Mel. Next week I will tell you\u2026 [1] In ALGOL, FORTRAN, and PL\/1 there are no reserved words, only keywords. The difference is not really that important in the context of this article. In this article I will use reserved words to refer to both. [2] Prior to the 1950\u2019s most parts used in manufacturing would not fit together \u201cout of the box\u201d and had to be individually fitted to each other by filing away at the parts to creat a smooth fit. Fitter was the job title of the person who did the filing. This article is an excerpt from my upcoming book The Chaos Factory which explains why most companies and government can\u2019t write software that \u201cjust works\u201d, and how it can be fixed. By clapping more or less, you can signal to us which stories really stand out. IT Strategist, startup positioner, cargo cult programmer how hackers start their afternoons.","time":1525800321,"title":"Framework or language? Get off my lawn","type":"story","url":"https:\/\/hackernoon.com\/framework-or-language-get-off-my-lawn-9935e1c72019","label":3,"label_name":"dev"},{"by":"plurby","descendants":1,"id":17022714,"kids":"[17022883]","score":3,"text":"Today we're adding new functionality which allows integrations and GitHub to communicate more comprehensively about the checks run against code changes.  These changes will improve your workflow by allowing you to view feedback from code checks directly in the pull request view, see the line of code causing a problem in the diff view, \u201cre-run\u201d checks, and more-all within the GitHub user interface.  Instead of binary pass\/fail commit statuses, integrators can now report more fine-grained outcomes, such as a neutral conclusion for more informational analysis or action_required if the integrator site requires the user's attention.  We've added first-class support for common workarounds, like the ability to skip or request checks via commit message, trigger checks with a dedicated check_suite webhook event, and set preferences for when checks are triggered.  Because some checks may be expensive to run, an application can opt out of the automatic flow and instead create check suites to their preferred timing. Application owners can control this by setting their check suite preferences on a per-app and repository basis. People with admin permission on a repository can override this setting. The automatic flow prompts installed apps to run checks against the last commit in code pushed to a repository.\nA user can request to run checks for any given commit with the request check suites API: POST \/repos\/:owner\/:repo\/check-suite-requests A user can control the check suite request flow on a per-commit basis: Note: the trailer would need to be on the last commit of a push. If you supply both trailers, skip-checks wins. See skipping and requesting checks for individual commits for more information. A user can re-run a check run or entire check suite in the pull request view on GitHub.com.  When a user requests to re-run a check, a check_run webhook event is delivered to that app's webhook with an action of rerequested. The app is then expected to create a new check run for the given head_sha. When a user requests to re-run a check suite, a check_suite webhook event is delivered to that app's webhook with an action of rerequested. The app is then expected to create a new check run for all its runs in that suite. How do checks work with protected branches? If the names of check runs are the same as the context of prior commit statuses, and those statuses were required, then the new check runs are automatically required. If a commit status and check run are created with the same name or context, both the status and the check run will be required. If the names of the new check runs are different from the old commit statuses, the new check runs will need to be selected as required. How are check runs different than commit statuses? Commit statuses allow for a simple pass or fail state. Check runs allow for much more granular information: they can conclude as either success, failure, neutral, cancelled, timed_out, or action_required. Check runs are more flexible than commit statuses. You can update the lifecycle state by indicating queued, in_progress, or completed through the status field. Check runs can be created as simply as a commit status with just a name and conclusion for the given commit. They can also include a variety of output data: textual information, images, and feedback on specific lines of code. Is this supported in the GraphQL API? No, but we plan to add support in the near future. Who can use it? The Checks API is only available to GitHub Apps through a new granular permission: checks. Anyone can register a GitHub App on GitHub through Settings > Developer settings > GitHub Apps and manage an existing GitHub App from the same place. See how to get started building GitHub Apps. See the full Checks API documentation for more details. To access this functionality, you\u2019ll need to provide the following custom media type in the Accept header: We're excited to see what you build with these new improvements. Take them out for a spin, and share your feedback in our Platform forum. If you have any questions, please let us know. Looking for a new app to use? Browse GitHub Marketplace.","time":1525800309,"title":"GitHub Checks API","type":"story","url":"https:\/\/developer.github.com\/changes\/2018-05-07-new-checks-api-public-beta\/","label":3,"label_name":"dev"},{"by":"serub","descendants":0,"id":17022706,"kids":"None","score":1,"text":"By FRANNIE HANNAN, JOSH HOELTZEL AND PETER RENTZ The New York Times publishes more than 150 articles a day to our iOS and Android apps, and our desktop and mobile websites. All of these platforms were originally developed by separate teams, so each had its own implementation and user experience. Though it may not have been obvious to the users of our products, we were maintaining a fragmented experience spanning multiple applications. Every time we built a new feature or even implemented something as simple as a stylistic change, we needed to build it separately in each place. That meant replicating work, coordinating roadmaps and releasing code across multiple teams\u200a\u2014\u200amobile web, desktop and in each of our native news apps, which required a full app release to roll out even the most minor changes. At the same time, editors producing articles saw a rendering that didn\u2019t represent what the content would ultimately look like on our platforms. This did little to help them envision their work as users would see it. In short, we were blocked by inefficiencies from delivering the best product to our users. May 8, 2018, marks the culmination of a years-long project to create an article ecosystem that promotes internal efficiency and delivers an enhanced reading experience for our users. We now have a single responsive article for both mobile and desktop on the web, and we use a subset of the same code to render stories in our native apps, Google\u2019s Accelerated Mobile Pages (AMP) and our content management system. Here are the biggest benefits of the work we\u2019ve done: In order to create a consistent reading experience on all of our platforms that we could iterate on in tandem, we had to make major changes to how our native apps render articles. Prior to the \u201chybrid\u201d project (i.e. a hybrid between native and web), each native app parsed article data and rendered the headlines, bylines, paragraphs and media natively to display the article. Now, articles are rendered on the server and delivered to the apps as an HTML string that the native apps display in a web view. This means that as we add new features or change the way we treat existing ones, we can do it with a single web application release without updating the apps in the store. Another efficiency of the new article system is the introduction of shared components. As we add elements to our article experience, we are building them in an abstract way outside the context of any specific application. This allows them to be shared across applications, pulled in anywhere that the component needs to be rendered in user-facing applications, as well as content creation tools like our CMS. As an example, consider our bylines: We recently made changes to include headshots and a short blurb about the author, so we built a shared component that would render the byline with these enhancements. We could then utilize that component in the hybrid article, the web article and the CMS, as well as any future application that might need it. Implementing this approach required a lot of planning, experimentation and coordination across teams. Engineers throughout the tech organization and in the newsroom worked together to make it happen. Today we are sharing more than 40 components across our web, native, CMS and off-platform apps. The desktop story page is where we made the most transformational user-facing changes. We\u2019ve moved away from a model that is standard for many news organization on the internet\u200a\u2014\u200aan article template that has a right rail crowded with ads and other content that draws a reader\u2019s attention away from what they came for: the story. With this new launch, articles are presented in a focused, single-column layout that intentionally strips away the clutter and puts our journalism front and center. We now have a clean slate on which to build elements that truly add value for readers. In conjunction with Oak\u200a\u2014\u200aour new visual article editor\u200a\u2014\u200awe will create and enhance story forms that take advantage of the space and flexibility on the new story page. The single-column story page was designed to create an integrated reader and advertising experience, built around our proprietary FlexFrame display units. We\u2019ve removed the cluttered right rail of small, standard banner ads in favor of premium, full-bleed, in-stream units that are responsive to the page\u2019s width. Ads on the new page are achieving twice the click-through rate of our old design, and initial studies show higher brand recall and four-times the reader attention to ads. \u201cWith the new Story page, we\u2019ve successfully integrated our reader and advertising experiences with a pristine, user-focused design,\u201d says Allison Murphy, vice president of ad innovation. \u201cA more engaging page is better from every angle: it means more connection with our journalism, and more connection with the messages of our marketers.\u201d This marathon effort to reimagine The Times article has given us the infrastructure to build features faster and better meet the needs of users and advertisers on all of our platforms. In January 2017, an innovation group within The Times released a report titled Journalism That Stands Apart. It lays out how the company must change in order to meet aggressive goals and thrive in the media landscape of the future. Key recommendations included a more visual report and more diverse forms of storytelling. We believe the new article page is a critical step toward allowing us to evolve how we tell digitally native stories that make The Times stand apart. Frannie Hannan is a senior product manager at The Times overseeing the story page. Josh Hoeltzel is the senior development manager for the story page. Peter Rentz is the design director for the story page. By clapping more or less, you can signal to us which stories really stand out. We\u2019re New York Times employees writing about building digital products and workplace culture. Sharing our stories of making great digital products at The New York Times.","time":1525800250,"title":"Reimagining The New York Times Digital Story Experience","type":"story","url":"https:\/\/open.nytimes.com\/reimagining-the-new-york-times-digital-story-experience-ff698541ac09","label":9,"label_name":"tech"},{"by":"kbyatnal","descendants":1,"id":17022696,"kids":"[17023556]","score":2,"text":"Bradley Jacobs, CEO of XPO Logistics  ","time":1525800177,"title":"Gmail's Smart Compose Is Ready to Write Your Emails for You","type":"story","url":"https:\/\/www.forbes.com\/sites\/anthonykarcz\/2018\/05\/08\/smart-compose-in-the-new-gmail-is-ready-to-write-your-emails-for-you\/#3eb66f774c2e","label":7,"label_name":"random"},{"by":"devhxinc","descendants":123,"id":17022695,"kids":"[17023069, 17022972, 17023015, 17022931, 17022935, 17023022, 17023053, 17023136, 17023023, 17023366, 17022850, 17022940, 17024230, 17023575, 17023831, 17023988, 17023480, 17023384, 17022888, 17023796, 17024006, 17023025, 17022962, 17024085, 17022902, 17023103, 17023115, 17024163, 17023105, 17023457, 17022952, 17023026, 17023198, 17022960, 17023668, 17024043, 17023034, 17023094, 17022988, 17023160, 17022921, 17022973, 17022834]","score":96,"text":"\n\n\n\n Gmail Email makes it easy to share information with just about anyone\u2014friends, colleagues and family\u2014but drafting a message can take some time. Last year, we introduced Smart Reply in Gmail to help you quickly reply to incoming emails. Today, we're announcing Smart Compose, a new feature powered by artificial intelligence, to help you draft emails from scratch, faster. From your greeting to your closing (and common phrases in between), Smart Compose suggests complete sentences in your emails so that you can draft them with ease. Because it operates in the background, you can write an email like you normally would, and Smart Compose will offer suggestions as you type. When you see a suggestion that you like, click the \u201ctab\u201d button to use it. Smart Compose helps save you time by cutting back on repetitive writing, while reducing the chance of spelling and grammatical errors. It can even suggest relevant contextual phrases. For example, if it's Friday it may suggest \"Have a great weekend!\" as a closing phrase. Over the next few weeks, Smart Compose will appear in the new Gmail for consumers, and will be made available for G Suite customers in the workplace in the coming months.     To get started, make sure you\u2019ve enabled the new Gmail by going to Settings > \u201cTry the new Gmail.\u201d Next, go to the general tab in your settings, scroll down and enable \u201cexperimental access.\u201d If you want to switch back, you can always uncheck the box. \n              Follow Us\n            ","time":1525800170,"title":"Write Emails Faster with Smart Compose in Gmail","type":"story","url":"https:\/\/www.blog.google\/products\/gmail\/subject-write-emails-faster-smart-compose-gmail","label":7,"label_name":"random"},{"by":"weareformidable","descendants":0,"id":17022694,"kids":"None","score":1,"text":"We\u2019re excited to release a tiny new open source project: react-fast-compare. react-fast-compare provides fast deep equality comparison, with specific handling for React. It\u2019s a fork of Evgeny Poberezkin\u2019s very clever (and very fast!) fast-deep-equal. As the name implies, react-fast-compare aims to be the fastest deep equality comparison available. It\u2019s very lightweight: under 600 bytes gzipped and minified. It does deep equality comparison by value for any object, as long as it doesn\u2019t contain circular references. It also allows deep comparison of React elements. While projects like fast-deep-equal are amazing for general usage, they don\u2019t handle circular references at all, which is one of the reasons they can be so fast. Unfortunately, this strategy doesn\u2019t work for React elements, as they contain multiple circular references. For example, an element that contains a reference to its \u201cowner\u201d (or parent), which in turn contains a reference to its children, including the original element. react-fast-compare adds handling for these React-specific cases. A component\u2019s shouldComponentUpdate lifecycle method is a perfect use case for react-fast-compare, potentially saving you from needless re-renders. You should only use shouldComponentUpdate if you have \u201cdeep\u201d props, which are any props that cannot be compared with a simple equality. Examples of deep props are objects, arrays, and dates. If you don\u2019t have any deep props (they are all shallow: strings, booleans, and numbers), you can just use a PureComponent. You could also consider using immutable data to allow for fast comparison of deep props, but it won\u2019t work for every project. react-fast-compare was born from our work with Victory. We identified our equality check for shouldComponentUpdate as a performance bottleneck, which made sense, as it was saving us from a larger bottleneck: wasted re-renders. Our previous equality check was based on Lodash\u2019s isEqual, and performed roughly as fast. We ultimately realized that we needed handling for React elements, and decided to create and release react-fast-compare as a new project. react-fast-compare aims to be a very fast general deep equality check. Our goal is to remain as performant as fast-deep-equal. You may be asking, \u201dwhat about Lodash?\u201d Lodash is a powerful utility library, and its isEqual function is very thorough. react-fast-compare is faster by virtue of the fact that it\u2019s doing less work. For example, react-fast-compare doesn\u2019t track circular references or allow custom comparison functions like Lodash does. We would love feedback from the community, so give the library a try in your project. We were excited to see that Formik already incorporated react-fast-compare. Let us know what works well, and what needs work! You can file issues on github, or get in touch with me on twitter. Formidable is a Seattle and London-based engineering consultancy and open source software organization, specializing in React.js, React Native, GraphQL, Node.js, and the extended JavaScript ecosystem. Since 2013, our agile team has worked with companies ranging in size from startups, to Fortune 100s, to build quality software and level up engineering teams. Launching a new JavaScript Product?  Need Help with an Existing Project? 146 N Canal St. #300  Seattle, WA 98103, US  206.547.5580  77 East Road  London N1 6AH ","time":1525800170,"title":"React-fast-compare: fast deep equality comparison with unique handling for React","type":"story","url":"http:\/\/formidable.com\/blog\/2018\/introducing-react-fast-compare\/","label":3,"label_name":"dev"},{"by":"Impossible","descendants":1,"id":17022679,"kids":"[17022805]","score":1,"text":"   Remember Me     Remember Me  The workflow you always wanted: Create, simulate, render, composite, edit, and play: always in real-time, always working on the full and final result. Create interactive and generative content and live video effects in a powerful, easy and stable workflow. Export and play back with the media servers you already use.  Create VR experiences with our NLE workflow. Content that\u2019s both directed and interactive, with a\u00a0powerful and flexible real-time visual engine. A node-based interface that\u2019s familiar and intuitive to explore, allowing limitless possibilities simply by connecting logical building blocks. Timeline and animation editing, compositing and grading, all in one environment, designed with narrative in mind. A vast and deep set of nodes enables you to bring about sophisticated visual effects. Physically based lighting, particles, fluids, volumetrics, compositing, interactivity, 3D cameras and procedural effects all in one package. Use all of Notch\u2019s compositing and visual effects capabilities on live video sources creating compelling new performance experiences for the audience. Or, use Notch\u2019s visual effects engine to create effects around tracked moving performers on stage, from wispy smoke to rivers of water. Notch integrates with leading media servers. Create in Notch Builder, export and play back inside the media server.  Using Notch Builder, design your creation from scratch or import elements from other industry tools. You\u2019ll always see your final results\u00a0in real-time. Once you\u2019ve finished your content and grading either export to video or real time Notch Block for playback in other packages, exposing the node parameters that you need for live control.\u00a0 Load the Notch Block\u00a0onto your media server, or run it as a standalone executable,\u00a0or embed into an\u00a0application. Either way,\u00a0get the benefit of real-time and live remote editing. TREATMENT Percepto MEPTIK FragmentNine Visualprime T.E.M. Studio Join the growing Notch community and use the wealth of features on offer to take your projects to the next level. The possibilities are limitless in your hands. Subscribe for regular news about new developments and\u00a0latest featured projects","time":1525800087,"title":"NOTCH: Create Mind-Blowing Motion Graphics, VFX and VR","type":"story","url":"https:\/\/www.notch.one\/","label":5,"label_name":"ml"},{"by":"digital55","descendants":0,"id":17022653,"kids":"None","score":2,"text":"May 8, 2018 Gaia\u2019s all-sky map of the Milky Way galaxy and its satellites, based on measurements of nearly 1.7 billion stars. ESA\/Gaia\/DPAC Senior Writer May 8, 2018 On April 25, Teresa Antoja of the University of Barcelona was one of thousands of astronomers who downloaded and began exploring an exquisite new map of the Milky Way made by the European Space Agency\u2019s Gaia spacecraft. Within a day, she and her colleagues reported the discovery of never-before-seen substructures throughout the galaxy: \u201cshapes such as arches \u2026 snail shells and ridges,\u201d they wrote \u2014 each one a clue about the Milky Way\u2019s obscure past. Antoja\u2019s paper is one of a torrent following the long-awaited second data release from Gaia, which was launched in 2013 and has since charted the positions, brightnesses and colors of 1.7 billion Milky Way stars, and the velocities of 1.3 billion of those stars. (In September 2016, the Gaia team released its first map with only position and brightness measurements for 1.1 billion stars.) Astronomers, who had previously catalogued just 2.5 million of the brightest stars in the galaxy, are hailing a new era of precision astronomy. These are some of the most important discoveries to come from the Gaia data so far. A team in France applied their preprepared STREAMFINDER algorithm to the Gaia data and immediately uncovered a rich network of \u201cstellar streams,\u201d or tributaries of stars flowing into and around the Milky Way. \u201cThe idea is to trace the streams backward in time along their orbits in order to contemplate the galaxy\u2019s past and its formation history,\u201d said Khyati Malhan of the University of Strasbourg, lead author of the paper detailing these \u201cgalactic archaeology\u201d findings, in an email. The profusion of stellar streams \u2014 believed to be remnants of small satellite galaxies and star clusters that were drawn in by gravity \u2014 could potentially resolve the \u201cmissing satellite problem,\u201d which asks why only 50-odd satellite galaxies currently orbit the Milky Way, despite hundreds arising in computer simulations of galaxy formation. Another mystery is why the Milky Way\u2019s satellites lie in a plane, even though simulations suggest they should have formed all around. Malhan and colleagues hope to either sharpen or resolve this plane-of-satellite problem via \u201cstatistical analysis of structure and dynamics of a large sample of streams,\u201d he said. Get Quanta Magazine delivered to your inbox Stellar streams in the northern and southern sky, with color representing their flow speeds. Courtesy of Khyati Malhan Another group used the Gaia data to do a detailed study of the galaxy\u2019s longest stellar stream. Some of its stars appear to have been perturbed by patches of invisible dark matter, suggesting the streams can be used to map dark matter substructure throughout the galaxy. For decades, astrophysicists have debated the origin of Type Ia supernovas \u2014 star explosions that serve as \u201cstandard candles\u201d for gauging cosmic distances. Using the Gaia data and follow-up telescope observations, Ken Shen of the University of California, Berkeley, and collaborators found strong evidence for a theory dubbed the \u201cdynamically driven double-degenerate double-detonation\u201d (D6) scenario. The D6 scenario begins with two white dwarfs \u2014 dense, planet-size cores of dead stars \u2014 in a close mutual orbit. According to the theory, the heavier white dwarf will suddenly start stripping matter away from the lighter one, so rapidly and turbulently that some of the helium in the transferred matter detonates. This detonation triggers the detonation of carbon and oxygen in the heavier dwarf, causing it to explode as a Type Ia supernova. With nothing holding on to it anymore, the other white dwarf is flung into space at high speed. Among the Gaia data, Shen and colleagues discovered three \u201chypervelocity white dwarfs\u201d hurtling through the galaxy at more than 1,000 kilometers per second, which is fast enough to escape its gravity. They claim the discoveries represent \u201ctentative confirmation\u201d of the D6 scenario. In an email, Shen said knowing the backstory of Type Ia supernovas will reduce uncertainties in cosmic distance measurements and models of supernova-driven chemical enrichment of galaxies. U.K. astronomers traced the origin of another hypervelocity star to the center of the Large Magellanic Cloud, the Milky Way\u2019s largest satellite galaxy. It could mean only one thing: that the star was accelerated by whipping around a massive black hole at the center of the Large Magellanic Cloud. Full-size galaxies almost always harbor huge central black holes, which are of mysterious provenance. Their presence in some mini galaxies compounds the mystery. Stars rotating clockwise around the center of the Large Magellan Cloud, the Milky Way\u2019s biggest satellite galaxy. ESA\/Gaia\/DPAC In 1998, Adam Riess and other astronomers inferred, based on the distances to Type Ia supernovas, that the expansion of the universe is accelerating, driven by \u201cdark energy.\u201d As for how fast space is currently expanding (a rate known as the \u201cHubble constant\u201d), curiously, the supernova-based estimates differ by about 8 percent from estimates based on light from the early universe, even after accounting for the dark-energy-driven acceleration that has happened since then. It\u2019s one of the biggest puzzles in cosmology. Now, Riess and collaborators have used Gaia data to more precisely measure distances to Cepheid stars, which in turn calibrate distances to Type Ia supernovas. This allowed them to make the most precise supernova-based measurement yet of the Hubble constant, which has only worsened the discrepancy with early-universe observations. While many researchers are studying the diverse populations and dynamics of stars within the galaxy proper, Laura Watkins of the Space Telescope Science Institute and colleagues used motions of star clusters orbiting the Milky Way to gauge the galaxy\u2019s mass. Including its dark matter, they say, it weighs roughly 1.5 trillion solar masses. And Joshua Simon of the Carnegie Institution for Science analyzed a distant population of satellite galaxies, finding that all of them are currently at the closest points in their orbits around the Milky Way. This seeming coincidence is \u201cstrange,\u201d Simon said by email. \u201cI don\u2019t think we\u2019ve had enough time to figure out the implications of this result yet.\u201d \u201cWe knew Gaia would be revolutionary,\u201d Simon said. Astronomers will be picking its fruits for years. Senior Writer May 8, 2018 Get Quanta Magazine delivered to your inbox Get highlights of the most important news delivered to your email inbox Quanta Magazine moderates comments to\u00a0facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English.\u00a0","time":1525799867,"title":"What Astronomers Are Learning from Gaia\u2019s New Milky Way Map","type":"story","url":"https:\/\/www.quantamagazine.org\/what-astronomers-are-learning-from-gaias-new-milky-way-map-20180508\/","label":7,"label_name":"random"},{"by":"allenleein","descendants":1,"id":17022638,"kids":"[17022880]","score":5,"text":"","time":1525799753,"title":"OCaml All the Way Down","type":"story","url":"https:\/\/www.youtube.com\/watch?v=BdfwBot7v8U","label":7,"label_name":"random"},{"by":"GW150914","descendants":0,"id":17022633,"kids":"None","score":2,"text":"Uber signed a second Space Act Agreement with NASA to develop models that will simulate urban air mobility service. It\u2019s a sign that Uber is interested in working closely with government regulators as it seeks to get its ambitious flying taxi project off the ground.  Under the agreement, Uber will provide NASA with details and data on its plans for a flying taxi service, which the agency will use to simulate flights over Dallas-Fort Worth. This data will address scenarios involving air traffic, collision mitigation, and air space management. It is NASA\u2019s first such agreement related to urban air mobility (UAM) specifically focused on modeling and simulation.  The announcement was made during Uber\u2019s second annual Elevate conference, which is being held in Los Angeles. LA and Dallas are the two cities that have agreed to host early tests of Uber\u2019s air taxi service.  Uber\u2019s first Space Act Agreement with NASA, which was signed last November, was a general statement of an intent to collaborate. Today\u2019s agreement is much more specific. NASA will use Uber\u2019s data to simulate a small passenger-carrying aircraft as it flies through Dallas-Fort Worth airspace during peak scheduled air traffic. The airspace above the city is incredibly crowded, so the simulations will be key in helping Uber figure how its proposed service slots fit in with the hundreds of aircraft flying above Dallas every day.  \u201cNASA is excited to be partnering with Uber and others in the community to identify the key challenges facing the UAM market, and explore necessary research, development and testing requirements to address those challenges,\u201d said Jaiwon Shin, associate administrator for NASA\u2019s Aeronautics Research Mission Directorate. \u201cUrban air mobility could revolutionize the way people and cargo move in our cities and fundamentally change our lifestyle much like smart phones have.\u201d Update May 8th, 10:38am PT:  Uber also signed an agreement with the US Army to develop and test \u201cflying taxi\u201d aircraft for the company\u2019s mobility service. The company will jointly develop and fund research into rotor technology with the US Army\u2019s corporate research lab.  Under the agreement, Uber and the Army\u2019s research lab expect to spend a combined total of $1 million in funding for this research; this funding will be divided equally between each party.   Command Line delivers daily updates from the near-future.","time":1525799729,"title":"Uber expands partnership with NASA on flying taxi project 1","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17331450\/uber-expands-partnership-nasa-flying-taxi-dallas","label":7,"label_name":"random"},{"by":"GW150914","descendants":0,"id":17022629,"kids":"None","score":1,"text":"Tesla  CEO Elon Musk  managed to freak out investors last Thursday by acting dismissively toward analysts during an earnings call, calling a question about the company\u2019s capital requirements \u201cboring\u201d and \u201cbone-headed\u201d and \u201cnot cool,\u201d a strange performance that seemed to lead to a slide in the shares\u2019 performance afterward. More bravura was on display the following day, with Musk warning the growing number of short sellers who are betting against the automaker. Specifically, he tweeted that a \u201cshort burn of the century\u201d is \u201ccoming soon.\u201d He also retweeted a Barron\u2019s article that highlighted that there\u2019s now more demand for short positions than supply. (Barron\u2019s compared the conundrum to the same one facing Tesla\u2019s customers.)\u00a0 Perhaps to make certain that Tesla bears feel some pain, Musk himself shelled out roughly $10 million to acquire more Tesla stock yesterday, a purchase that pushes his stake in the company to nearly 20 percent, according to Bloomberg\u2019s analysis. Whether or not directly correlated, Tesla\u2019s shares rose\u00a03 percent yesterday, boosting Tesla\u2019s market cap to $51.4 million. It has slipped again slightly, as of this writing, to $49.7 billion. Tesla\u2019s shares seem to recover almost no matter what Musk does or says, but it\u2019s worth asking why Musk still talks with analysts at all.\u00a0As writer and venture capitalist M.G. Siegler noted during our \u201cEquity\u201d podcast last week, many CEOs of Musk\u2019s stature deputize other executives within their companies to talk with analysts, including Amazon\u2019s Jeff Bezos and, when he was leading Apple, the company\u2019s famous co-founder, Steve Jobs. While Musk continues to give Tesla analysts what they want \u2014 Musk\u2019s own time and perspective \u2014 his apparent eagerness to be combative with them creates a \u201cweird dichotomy between those two things,\u201d Siegler noted.","time":1525799708,"title":"After $10M yesterday for Tesla shares, Musk\u2019s stake approaching 20%","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/after-splashing-out-10-million-yesterday-for-tesla-shares-elon-musks-stake-is-approaching-20-percent\/","label":0,"label_name":"biz-news"},{"by":"evo_9","descendants":0,"id":17022626,"kids":"None","score":3,"text":"","time":1525799670,"title":"Magic Leap Live \u2013 Ep. 001: Designing for Spatial Computing","type":"story","url":"https:\/\/www.youtube.com\/watch?reload=9&v=gHf9CMRLVbk&index=0&list=PL_j5e9ibbeEOWls-LLuql0Q5k8CTsrMye&utm_source=MagicLeapMktgEmail&utm_medium=email&utm_campaign=20180508_Livestream&alias=Hero&_hsenc=p2ANqtz-9jnOljW7dWco7XXH6ZR2FAm41VsrnebObnO5TbalQsvh0PZy75KazWhQo62a9ryd8pgAghWR25p7XLP2D2_poUh7pVtA&_hsmi=62750949","label":7,"label_name":"random"},{"by":"strayamaaate","descendants":1,"id":17022621,"kids":"[17022846]","score":3,"text":"From the time the first website was published in 1991 until today, the internet has profoundly reshaped humanity. Comparisons between cryptocurrencies and the growth of the internet are invariably drawn (including cryptocurrencies\u2019 netscape moment); however, I wanted to test this comparison and see exactly how far along we are. In this post, I\u2019ll also be exploring the growth of the cryptocurrency market & the early growth of the internet, to see what takeaways we can uncover. What makes this comparison tough It\u2019s impossible to know exactly how many people use cryptocurrency and how often because: Thus, the only way to get an understanding of the number of users for cryptocurrencies is through approximations. Measuring cryptocurrency user growth I tried to approximate cryptocurrency user growth in a few ways: There are ~24M bitcoin wallet addresses in total. This doesn\u2019t mean there are 24M Bitcoin users because one person can have more than 1 wallet address and it is recommended to generate a new bitcoin address for each transaction sent. I would consider 24M the upper bound number on the number of bitcoin users worldwide. In addition to looking at the number of wallets, we can look at the number of active addresses per day. To smooth out this chart, I took a median value of active addresses by month, and plotted it on a log scale: The highest amount of active addresses we\u2019ve seen per day was ~1.1M addresses\u200a\u2014\u200athis is an approximation of daily active users (DAU) within the bitcoin network. However, if the main point of Bitcoin is viewed as purely a store of value, then you would assume a much lower DAU vs. any traditional mobile application or website. We can also do the same analysis for Ethereum, here is the Ethereum address growth and active addresses per day (in log scale): In total, there are 31M Ethereum addresses with peak daily active addresses on the Ethereum network reaching 1.1M. Ethereum is a bit different than bitcoin because smart contracts have their own addresses and usage on Ethereum should naturally be higher since Ethereum is designed as a smart contract platform, not as a pure store of value. Users of bitcoin and users of ethereum are not mutually exclusive as well, I would assume a high degree of correlation between the two cryptocurrencies. Another method to approximate the user growth of cryptocurrencies is to instead look at the exchanges themselves\u200a\u2014\u200aboth fiat-crypto and crypto-crypto exchanges. Only a handful of crypto exchanges have published their total user stats & user growth statistics. Here is what I could find: If we take all of the exchanges trading with fees, here is a breakdown of the market share by all of the crypto exchanges (including fiat and crypto-crypto): Furthermore, if we take all of the exchanges where we know the user counts and trading volume, we can come up with an estimated trading volume per user. Through this number, we can forecast across all trading volume what the estimated users of cryptocurrencies as a whole are: 20.2M users. I would consider this the lower bound on the number of cryptocurrency users based on the number of people who are trading & purchasing cryptocurrencies across all of the various exchanges. Furthermore, we can also look at the overall trading volume of all cryptocurrencies over time to see how trading volume have been trending from 2014\u201318. The chart below is also in log scale and the values have been averaged out per month to get a better sense of the overall trend line. While all of these measurements are not exact counts of users, I would approximate the total users of cryptocurrencies to be between 20M-30M people in total worldwide. Comparing the growth of cryptocurrency users to the growth of internet users Now that we have an estimate on the total number of cryptocurrency users worldwide, we can look at the growth of the internet and estimate how early we are in this trajectory. Here is the growth of internet users: If we zoom into 1990\u20131995 for the internet compared to 2013\u20132018 in cryptocurrencies: You can see we\u2019re actually tracking quite closely with the early days of the internet. If you think cryptocurrencies is going to follow a similar trajectory as the internet, we look like we\u2019re in about year 1994 compared to the internet. We can also do a similar analysis comparing the number of websites in the early internet to the number of crypto projects in the space\u200a\u2014\u200afor this I\u2019m taking the total number of cryptocurrencies & tokens + all of the DApps. Here is the growth trajectory of the number of websites: If we zoom into 1991\u20131995 in the growth of websites compared to 2014\u201317 in the growth of crypto assets (tokens which received funding +DApps): We are at year 1994 on this comparison as well. For one last comparison we can look at the total number of internet companies which received funding from 2014 to 2017 compared to the number of internet startups that got funding from 1991 to 1995. *Funding amounts are adjusted for inflation and only account for internet\/software company financings. My takeaways: My biggest criticism towards the DApp future is we haven\u2019t seen DApp usage keep pace with the number of DApps being created. The current core use cases of cryptocurrencies are speculation, store of value, assets, payments, etc. Looking at the data we can see the use case of cryptocurrencies as an asset class has considerably more proof points and measurable user adoption. However, the future of decentralized applications, while interesting to track, is still too early to measure. A big thank you to Ricky Tan for contributing data from TokenData & feedback for this post & thanks to Noah Jessop and Kim McCann for providing feedback on this post. P.S.\u200a\u2014\u200aI also write a weekly newsletter of the best crypto events in the SF Bay Area. Subscribe here: > https:\/\/www.cryptoweek.ly\/ By clapping more or less, you can signal to us which stories really stand out. Community Lead @ Greylock Partners. Previously founded @StartupDigest. Photographer.","time":1525799642,"title":"12 Graphs That Show Just How Early the Cryptocurrency Market Is","type":"story","url":"https:\/\/medium.com\/@mccannatron\/12-graphs-that-show-just-how-early-the-cryptocurrency-market-is-653a4b8b2720","label":2,"label_name":"crypto"},{"by":"ykv_name","descendants":0,"id":17022581,"kids":"None","score":3,"text":"The only thing hotter than the GPUs securing the Zcash network is the debate surrounding ASIC resistance. You don\u2019t need to look further than the most-debated Zcash Forum thread of all time from February, or the debates that routinely pop up in the community chat. The ASIC resistance question has taken on new urgency now that Bitmain has announced the availability of the AntMiner Z9 mini, an Equihash-focused ASIC that seems tailor-made for Zcash, shipping in late June. Another complicating factor is that the Zcash Company\u2019s chief, Zooko, has signalled ambivalence about ASIC resistance. The Foundation is in the process of adopting community feedback as part of our own governance process, and in fact has already planned to represent the community\u2019s interest in this debate via a proposed ballot for our election process. If the world had stood still, I imagine this would have been the only step necessary to take right now, collecting the community\u2019s feedback and iteratively transform the community\u2019s voice into an official Foundation position. But the world doesn\u2019t stand still, as Bitmain has reminded us. We\u2019re still planning on including an ASIC resistance ballot measure in our election process, but we also think that the community expects more from us than waiting until June to act. Consequently, we are currently taking the following steps: Investigation and principled decision making. We are committing funds and effort to investigate the presence and power of ASICs on the Zcash network. We do not know for sure how effective the upcoming AntMiner Z9 mini will be, or the degree to which ASICs already affect the mining process, or whether more powerful ASICs will be developed in the future. All of these questions matter when deciding to change the Equihash parameters, adopt a new PoW type altogether, or welcome ASICs. Board member Andrew Miller is planning to create a proposal through the grants program to convene a Technical Advisory Board to provide scientifically grounded inputs into this decision. Making research and development of a more ASIC resistant strategy an immediate technical priority for the Foundation. We have already outlined a technical roadmap for the next year and are in the process of hiring and project-planning to execute on it. Our roadmap includes development on Bolt payment channels, on alternative wallets, and starting an independent, consensus-compatible implementation of full node software. We are now adding ASIC resistance development as an additional technical priority. Based on continued community approval and the results of our investigation above, we have a rough goal of developing and submitting a mitigation plan through the ZIP process, targeting a deployment in late 2018. Having the ability to carry out a PoW change in the future, especially if it is lead by the Foundation, means we should start now. The Company is signalling they may not do this, but we think there is already a loud and clear interest in the community to at least have this option available. (This is an easy case, the governance experiments are really about harder cases!) Continuing to run the ballot process to gauge community sentiment. Sometimes miner manufacturer claims are hot air. What if the efficiency gains are minimal compared to new GPUs? Would it be worth engaging in a potentially contentious fork, or splitting the community, if GPUs were still competitive? Or, more gravely, perhaps these new ASICs are more advanced than we thought and could handle different parameters of Equihash. One of the \u201ceasy wins\u201d that is often suggested in the community is to update Equihash parameters (which would still take a while to thoroughly test) to maintain ASIC resistance, but if this AntMiner has the ability to adapt to new parameters, then such a fork would be a waste of effort. Simply put, there are too many unknowns to commit to any particular path yet. By putting resources into investigation we can make better decisions and have options available. The Foundation believes it\u2019s important to maintain the power of GPUs in Zcash mining. However\u2014and this nuance is important\u2014we also recognize that ASIC resistance may be a red herring, for the health and decentralization of the protocol in the long term. Perhaps there is another path that we could take, with ample time for community buy-in\u2014and we welcome input on getting there. In the short term, we consider it critical to protect the community members who are building the ecosystem with us. If it\u2019s necessary based on our evaluation of the ASICs on the network, we will hire a developer to construct and submit a ZIP to mitigate its effect on the network. If the Zcash core development team and community approves, it will ideally be deployed by late 2018. While we are committed to engaging in these activities ourselves, we encourage particularly passionate and talented members of the community to apply to our 2018Q2 Grant Program to augment or replace our work here. Yes, we are eager to give money away to help preserve the health of the network. Even if we manage to neuter a wave of Equihash ASICs, this will not be the end of the discussion. Inevitably, some new ASIC will arise, and we may have to go through this process again. I\u2019m all for a Sisyphean effort now and again, but perhaps there is a better solution\u2014one that subverts the entire \u201cASIC resistance\u201d debate? Eventually we will need one, because I\u2019m not sure how sustainable the whack-a-mole strategy will be for the community. There has to be a better way\u2014and I think it starts with reframing the discussion away from \u201cASIC resistance\u201d and towards the perceived goals of ASIC resistance\u2014decentralization, less concentrated proof of work, and accessibility to the network. But in the meantime, we must act, and act we shall. Update: the original version of this post implied that the Foundation was taking a definitive stance for ASIC resistance. It has been edited for clarity to indicate that the Foundation is putting resources into researching ASIC resistance: advantages, disadvantages and potential implementation paths. Early feedback on this post encouraged us to submit this update, but for all edits, you can see full commit history on on GitHub and track changes made here. For people in the community who do not like this approach, or demand simpler and swifter action: we know you don\u2019t want to accept \u2018It\u2019s complicated and we\u2019re still figuring it out.\u2019 But, it\u2019s complicated and we\u2019re still figuring it out. We\u2019ll work together to learn as fast as we can and take the appropriate action as fast as we can. Thank you for your support. Thanks to Andrew Miller, Sonya Mann, and the #the-zcash-foundation community channel for their input and feedback.","time":1525799393,"title":"Zcash Foundation on Zcash ASIC Resistance Debate","type":"story","url":"https:\/\/z.cash.foundation\/\/blog\/statement-on-asics\/","label":7,"label_name":"random"},{"by":"farseer","descendants":0,"id":17022538,"kids":"None","score":1,"text":"Set edition preference: Set edition preference: Set edition preference: By John Kirby, CNN National Security Analyst  Updated 0535 GMT (1335 HKT) May 8, 2018  Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds. CNN National Security Analyst John Kirby is a retired rear admiral in the US Navy who served as a spokesman for both the State and Defense departments in the Obama administration. The views expressed in this commentary are his own.  (CNN)If you need any more proof -- and by now you shouldn't -- of how effective the Iran deal has been, look no further than Saturday's edition of The Guardian\/Observer.  Join us on Twitter and Facebook ","time":1525799107,"title":"Someone is really desperate to kill the Iran deal","type":"story","url":"https:\/\/edition.cnn.com\/2018\/05\/08\/opinions\/someone-is-desperate-to-stop-iran-deal-opinion-kirby\/index.html","label":6,"label_name":"news"},{"by":"AlfredTwo","descendants":0,"id":17022537,"kids":"None","score":1,"text":"This is Alfred Thompson's blog about computer science education and related topics.  Microsoft is having their big MS Build event this week. I don\u2019t have time to watch it live but I have been seeing hints of things via social media. Most of what they are talking about it for professional developers and is far more advanced than what I have time to get to with my students. But sometimes there are things I can really use. Visual Studio Live Share looks to be one of those things. With Visual Studio Live Share two people can actually look at and work on the same code at the same time. Merge that with a Skype call and it is like being there. Think Google Docs for Visual Studio. (Someone will object to that characterization for sure.)  So far I have just tried it locally (two computers next to each other with me operating both) but I can see some educational uses. For one thing I can see sharing code with students and having them work on code I have on the screen from their own desks. I can also see easier pair\/team working in rooms that don\u2019t lend themselves to a lot of student movement. My tables don\u2019t move and things can get crowded if people mover around too much. I can also see using it to help students with their own work. Now in the classroom it may still make more sense to walk over to where the student is working. On the other hand if a student is working from home we now have a great way to work together.  I\u2019ve been thinking about using it to watch students as they code (with their permission and their controlling the share) from time to time. That may give me some insights without being quite as intimidating as literally looking over their shoulder. For those students who pretend to be working this may be just the thing.  There are likely to be more ideas that others come up with. Time will tell. At least for now I have a few ideas worth trying out. What do you think? Do you see some educational potential? \nPost a Comment\n \n\n ","time":1525799097,"title":"Visual Studio Live Share\u2013Something CS Teachers Can Use?","type":"story","url":"http:\/\/blog.acthompson.net\/2018\/05\/visual-studio-live-sharesomething-cs.html","label":7,"label_name":"random"},{"by":"pavel_lishin","descendants":0,"id":17022534,"kids":"None","score":1,"text":"\n\n   Mark Dominus (\u9676\u654f\u4fee)\nmjd@plover.com \n \n\n \nRSS\nAtom  12 recent entries\n Archive:  \n\n\n\n Comments disabled \nKatara constructs finite projective planes\n\n\n This weekend I got a very exciting text message from Katara: Oh boy!  I hope it's one I can answer. Okay   \n and the goal is to find the one matching symbol on your card and the\none in the middle  how is it possible that given any pair of cards, there is exactly one\nmatching symbol \n Well, whatever my failings as a dad, this is one problem I can solve.\nI went a little of overboard in my reply: You need a particular kind of structure called a projective plane. They only exist for certain numbers of symbols A simpler example has 7 cards with 3 symbols each. One thing that's cool about it is that the symbols and the cards are\n  \"dual\": say you give each round  card a name. Then make up a new deck\n  of square cards. There's one square card for each symbol. So there's a\n  square\"Ladybug\" card. The Ladybug card has on it the names of the\n  round cards that have the Ladybug.  Now you can play Spot with the\n  square cards instead of the round ones: each two square cards have\n  exactly one name in common. In a geometric plane any two points lie on exactly one common line and\n  any two lines intersect in exactly one common point. This is a sort of\n  finite model of that, with cards playing the role of lines and symbols\n  playing the role of points. Or vice versa, it doesn't matter. More than you wanted to know \ud83d\ude02 I still couldn't shut up about the finite projective planes: No problem! No response necessary. It is known that all finite projective planes have n\u00b2+n+1 points for\n  some n. So I guess the Spot deck has either 31, 57, or 73 cards and\n  the same number of symbols. Is 57 correct? Must be 57 because I see from your picture that each card has 8\n  symbols. Katara was very patient: Any time. (The game costs $13.) Anyway this evening I cut up some index cards, and found a bunch of\nstickers in a drawer, and made Katara a projective plane of order 3.\nThis has 13 cards, each with 4 different stickers, and again, every\ntwo cards share exactly one sticker.  She was very pleased and wanted\nto know how to make them herself.   Each set of cards has an order, which is a non-negative integer.\nThen there must be !!n^2 + n + 1!! cards, each with !!n+1!! stickers\nor symbols.  When !!n!! is a prime power, you can use field theory to\nconstruct a set of cards from the structure of the (unique) field of\norder !!n!!. I'll describe the procedure using the plane of order !!n=2!!, which is\nunusually simple.  There will be !!2^2+2+1 = 7!! cards, each with\n!!3!! of the !!7!! symbols. Here is the finite field of order 2, called !!GF(2)!!: The stickers correspond to ordered triples of elements of !!GF(2)!!, except that !!\\langle 0,0,0\\rangle!! is always omitted.  So they are: $$\\require{cancel}\\begin{array}{cc}\n\\cancel{\\langle 0,0,0\\rangle} & \\langle 1,0,0\\rangle \\\\\n\\langle 0,0,1\\rangle & \\langle 1,0,1\\rangle \\\\\n\\langle 0,1,0\\rangle & \\langle 1,1,0\\rangle \\\\\n\\langle 0,1,1\\rangle & \\langle 1,1,1\\rangle \\\\\n\\end{array}\n$$ Of course, you probably do not want to use these symbols exactly.\nYou might decide that !!\\langle 1,0,0\\rangle!! is a sticker with a\npicture of a fish, and !!\\langle 0,1,0\\rangle!! is a sticker with a\nladybug. Each card will have !!n+1 = 3!! stickers.  To generate a card, pick\nany two stickers that haven't appeared together before and put them\non the card.  Say these stickers correspond to the triples !!\\langle\na,b,c\\rangle!! and !!\\langle x,y,z\\rangle!!.  To find the triple for\nthe third sticker on the card, just add the first two triples\ncomponentwise, obtaining !!\\langle a+x,b+y,c+z\\rangle!!.  Remember\nthat the addition must be done according to the !!GF(2)!! addition table\nabove! So for example if a card has !!\\langle 1,0,1\\rangle!! and\n!!\\langle 0,1,1\\rangle!!, its\nthird triple will be $$\\begin{align}\n\\langle 1,0,1 \\rangle + \\langle 0,1,1 \\rangle & = \\\\\n\\langle 1+0,0+1,1+1 \\rangle                   & = \\\\\n\\langle 1,1,0 \\rangle\n\\end{align}\n$$ Observe that it doesn't matter which two triples you add; you always get the third one! Okay, well, that was simple.  After Katara did the order 2 case, which has 7 cards, each with 3 of\nthe 7 kinds of stickers, she was ready to move on to something\nbigger.  I had already done the order 3 deck so she decided to do\norder 4.  This has !!4^2+4+1 = 21!! cards each with 5 of the 21 kinds\nof stickers.  The arithmetic is more complicated too; it's !!GF(2^2)!! instead\nof !!GF(2)!!: When the order !!n!! is larger than 2, there is another wrinkle.\nThere are !!4^3 = 64!! possible triples, and we are throwing away \n!!\\langle 0,0,0\\rangle!!  as usual, so we have 63.  But we need\n!!4^2+4+1 = 21!!, not !!63!!.   Each sticker is represented not by one triple, but by three.  The\ntriples !!\\langle a,b,c\\rangle, \\langle 2a,2b,2c\\rangle,!! and\n!!\\langle 3a,3b,3c\\rangle!! must be understood to represent the same\nsticker, all the multiplications being done according to the table\nabove.  Then each group of three triples corresponds to a sticker, and\nwe have 21 as we wanted.   Each triple must have a leftmost non-zero entry, and in each group of\nthree similar triples, there will be one where this leftmost non-zero\nentry is a !!1!!; we will take this as the canonical representative of\nits class, and it can wear a costume or a disguise that makes it\nappear to begin with a !!2!! or a !!3!!. We might assign stickers to triples like this: $$\n\\begin{array}{rl}\n\\cancel{\\langle 0,0,0\\rangle} & \\\\\n\\langle 0,0,1 \\rangle & \\text{apple} \\\\ \\hline\n\\langle 0,1,0 \\rangle & \\text{bicycle} \\\\\n\\langle 0,1,1 \\rangle & \\text{carrot} \\\\\n\\langle 0,1,2 \\rangle & \\text{dice} \\\\\n\\langle 0,1,3 \\rangle & \\text{elephant} \\\\ \\hline\n\\langle 1,0,0 \\rangle & \\text{frog} \\\\\n\\langle 1,0,1 \\rangle & \\text{goat} \\\\\n\\langle 1,0,2 \\rangle & \\text{hat} \\\\\n\\langle 1,0,3 \\rangle & \\text{igloo} \\\\\n\\langle 1,1,0 \\rangle & \\text{jellyfish} \\\\\n\\langle 1,1,1 \\rangle & \\text{kite} \\\\\n\\langle 1,1,2 \\rangle & \\text{ladybug} \\\\\n\\langle 1,1,3 \\rangle & \\text{mermaid} \\\\\n\\langle 1,2,0 \\rangle & \\text{nose} \\\\\n\\langle 1,2,1 \\rangle & \\text{octopus} \\\\\n\\langle 1,2,2 \\rangle & \\text{piano} \\\\\n\\langle 1,2,3 \\rangle & \\text{queen} \\\\\n\\langle 1,3,0 \\rangle & \\text{rainbow} \\\\\n\\langle 1,3,1 \\rangle & \\text{shoe} \\\\\n\\langle 1,3,2 \\rangle & \\text{trombone} \\\\\n\\langle 1,3,3 \\rangle & \\text{umbrella} \\\\\n\\end{array}\n$$ We can stop there, because everything after \n!!\\langle 1,3,3 \\rangle!!\nbegins with a !!2!! or a !!3!!, and so is some other triple in\ndisguise.  For example\nwhat sticker goes with \n!!\\langle 0,2,3 \\rangle!!?  That's actually\n!!\\langle 0,1,2 \\rangle!! in disguise, it's\n!!2\u00b7\\langle 0,1,2 \\rangle!!, which is \u201cdice\u201d.  Okay, how about !!\\langle 3,3,1\n\\rangle!!?  That's the same as !!3\\cdot\\langle 1,1,2 \\rangle!!,\nwhich is \u201cladybug\u201d.\nThere are !!21!!, as we wanted.  Note that  the !!21!! naturally\nbreaks down as !!1+4+4^2!!, depending on how many zeroes are at the\nbeginning;\nthat's where that comes from. Now, just like before, to make a card, we pick two triples that have\nnot yet gone together, say !!\\langle 0,0,1 \\rangle!!  and !!\\langle\n0,1,0 \\rangle!!.  We start adding these together as before, obtaining\n!!\\langle 0,1,1 \\rangle!!.  But we must also add together the\ndisguised versions of these triples, !!\\langle 0,0,2 \\rangle!! and\n!!\\langle 0,0,3 \\rangle!! for the first, and !!\\langle 0,2,0 \\rangle!!\nand !!  \\langle 0,3,0 \\rangle!! for the second.  This gets us two\nadditional sums, !!\\langle 0,2,3 \\rangle!!, which is !!\\langle 0,1,2\n\\rangle!!  in disguise, and !!\\langle 0,3,2 \\rangle!!, which is\n!!\\langle 0,1,3 \\rangle!! in disguise. It might seem like it also gets us !!\\langle 0,2,2 \\rangle!! and\n!!\\langle 0,3,3 \\rangle!!, but these are just !!\\langle 0,1,1\n\\rangle!! again, in disguise.  Since there are three disguises for\n!!\\langle 0,0,1 \\rangle!! and three for !!\\langle 0,1,0 \\rangle!!, we\nhave nine possible sums, but it turns out the the nine sums are only\nthree different triples, each in three different disguises.  So our\nnine sums get us three additional triples, and, including the two we\nstarted with, that makes five, which is exactly how many we need for\nthe first card.  The first card gets the stickers for triples\n!!\\langle 0,0,1 \\rangle,\n  \\langle 0,1,0 \\rangle\n  \\langle 0,1,1 \\rangle\n  \\langle 0,1,2 \\rangle,!! and\n!!\\langle 0,1,3 \\rangle,!! which are apple, bicycle,\ncarrot, dice, and elephant. That was anticlimactic.  Let's do one more.  We don't have a card yet\nwith ladybug and trombone.  These are !!\\langle 1,1,2 \\rangle!!  and\n!!\\langle 1,3,2 \\rangle!!, and we must add them together, and also the\ndisguised versions: $$\\begin{array}{c|ccc}\n                      & \\langle 1,1,2 \\rangle & \\langle 2,2,3 \\rangle & \\langle 3,3,1 \\rangle \\\\\n\\hline\n\\langle 1,3,2 \\rangle & \\langle 0,2,0 \\rangle & \\langle 3,1,1 \\rangle & \\langle 2,0,3 \\rangle \\\\\n\\langle 2,1,3 \\rangle & \\langle 3,0,1 \\rangle & \\langle 0,3,0 \\rangle & \\langle 1,2,2 \\rangle \\\\\n\\langle 3,2,1 \\rangle & \\langle 2,3,3 \\rangle & \\langle 1,0,2 \\rangle & \\langle 0,1,0 \\rangle \\\\\n\\end{array}$$ These nine results do indeed pick out three triples in three disguises\neach, and it's easy to select the three of these that are canonical:\nthey have a 1 in the leftmost nonzero position, so the three sums are\n!!\\langle 0,1,0 \\rangle,!!  !!\\langle 1,0,2 \\rangle,!! and !!\\langle\n1,2,2 \\rangle!!, which are bicycle, hat, and piano.  So the one card\nthat has a ladybug and a trombone also has a bicycle, a hat, and a\npiano, which should not seem obvious.  Note that this card does have\nthe required single overlap with the other card we constructed: both\nhave bicycles. Well, that was fun.  Katara did hers with colored dots instead of\nstickers: \n The ABCDE card is in the upper left; the\nbicycle-hat-ladybug-piano-trombone one is the second row from the\nbottom, second column from the left.  The colors look bad in this\npicture; the light is too yellow and so all the blues and purples look\nblack.x After I took this picture, we checked these cards and found a couple\nof calculation errors, which we corrected.  A correct set of cards is: $$\n\\begin{array}{ccc}\n\\text{abcde} & \\text{bhlpt} & \\text{dgmpr} \\\\\n\\text{afghi} &  \\text{bimqu} &  \\text{dhjou} \\\\\n\\text{ajklm} &  \\text{cfkpu} &  \\text{diknt} \\\\\n\\text{anopq} &  \\text{cgjqt} &  \\text{efmot} \\\\\n\\text{arstu} &  \\text{chmns} &  \\text{eglnu} \\\\\n\\text{bfjnr} &  \\text{cilor} &  \\text{ehkqr} \\\\\n\\text{bgkos} &  \\text{dflqs} &  \\text{eijps} \\\\\n\\end{array}\n$$ Fun facts about finite projective planes: This construction always turns a finite field of order !!n!! into a\nfinite projective plane of order !!n!!.   A finite field of order !!n!! exists exactly when !!n!! is a prime\npower and then there is exactly one finite\nfield.  So this construction gives us finite projective planes of\norders !!1,2,3,4,5,7,8,9,11,13,16!!, but not of orders\n!!6,10,12,14,15!!.  Do finite projective planes of those latter\norders exist? Is this method the only way to construct a finite projective plane?  Yes,\nwhen !!n<9!!.  But there are four non-isomorphic projective planes of\norder !!9!!, and this only constructs one of them. What about for !!n\u226511!!?  Nobody knows.   \n[Other articles in category \/math] \npermanent link\n","time":1525799073,"title":"Katara constructs finite projective planes","type":"story","url":"https:\/\/blog.plover.com\/math\/finite-projective-planes.html","label":7,"label_name":"random"},{"by":"advanderveer","descendants":0,"id":17022528,"kids":"None","score":7,"text":"","time":1525799016,"title":"Kubernetes Advanced Scheduling for Heating Showers","type":"story","url":"https:\/\/www.youtube.com\/watch?v=q8MFm2jwXpA","label":7,"label_name":"random"},{"by":"shihn","descendants":0,"id":17022525,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back  Micro lib (742 bytes gzipped) that provides a seamless way for two WINDows to TALK to each other. Let's consider a case where a parent window wants to interact with an object in an iframe. In the iframe: In parent window: Consider a case where one window wants to invoke a function in another window (or iframe) and get the result back. Window1: Window2: Download the latest from dist folder or from npm: Windtalk API Examples page MIT License (c) Preet Shihn Workly - A really simple way to move a function or class to a web worker. Or, expose a function or object in a web worker to the main thread.","time":1525798971,"title":"Show HN: Call functions or objects in another windowithiframe from local scope","type":"story","url":"https:\/\/github.com\/pshihn\/windtalk","label":4,"label_name":"github"},{"by":"kyrra","descendants":74,"id":17022514,"kids":"[17023050, 17023595, 17023948, 17022820, 17023628, 17023397, 17022852, 17023002, 17024435, 17024524, 17023059, 17023337, 17023540, 17023666, 17022858, 17022927, 17023352, 17022999]","score":93,"text":"","time":1525798876,"title":"Google I\/O'18: Keynote Livestream [video]","type":"story","url":"https:\/\/www.youtube.com\/watch?v=ogfYd705cRs","label":7,"label_name":"random"},{"by":"edward","descendants":0,"id":17022511,"kids":"None","score":3,"text":"Introducing the No Starch Press Foundation! The No Starch Press Foundation is a 501 (c) (3) tax exempt non-profit corporation\u00a0created to support and grow the collective knowledge and contributions of the worldwide hacker community. We\u2019ll soon have a way for you to donate to the Foundation. In\u00a0the meantime, if you\u2019re interested in supporting the Foundation please email bill at no starch dot com. The No Starch Press Foundation supports hackers of all types, regardless of experience \u2014 whether that\u2019s the passionate beginner or the lifelong hacker wishing to make a broader contribution to the hacker community and the world. What is a Hacker? By \u201chacker\u201d we mean a computer hardware, software, or programming enthusiast. A hacker likes to push boundaries, pick locks (for fun), and find ways to control hardware and make it do things that it wasn\u2019t intended to do. Unfortunately, the word \u201chacking\u201d has developed negative connotations over the years because it has been used much too broadly to also include criminals who break into systems without authorization to steal information, such as credit card numbers and social security numbers, or drain bank accounts. We call these people criminals or attackers, not hackers. Hackers also like to create things. Other hackers might be mathematicians, scientists, roboticists, programmers, physicists, or computer security specialists. Hackers dig deep; often staying up late learning something new or solving a thorny problem. Hackers tend to be brilliant but not always interested in conventional goals. Ideas, self-education, and respect within the community are often stronger drivers than money. Hackers are often not part of the mainstream populations (what hackers call \u201cnormals\u201d) in their schools or local communities, but once they find their community the relationships between fellow hackers can be intense. Hackers play an ever-expanding important role in protecting the public and in improving online security. The skills needed to excel at hacking need to be taught to a broader swath of the public and in underserved populations, all of which the Foundation aims to do. Origin The Foundation was formed to give back to and strengthen the hacking community. The Foundation\u2019s founder, William Pollock, has been closely involved with the hacking community since about 1999 and much of the success of his company, No Starch Press, is due to the support of the worldwide hacking community. To date Pollock has given over $600,000 to the Foundation and is working to expand its donor base. The Foundation\u2019s funding will be used to help strengthen and expand the hacking community, by educating the public about hacking and working to create safe and central places for the hacking community. Planned Activities One of the main drivers behind the creation of the Foundation is the strong desire to support people with a hacking mindset who have yet to find a supportive community of hackers. There are a number of large hacker conferences, like DEFCON (held in Las Vegas yearly) or the Chaos Communication Congress (held yearly in Germany), as well as countless smaller hacker conferences, worldwide. Hackers attend conferences to meet other people in the community, find social support, share their research, and to improve their understanding of complex technical topics. Hackers may also be found at one of well over 2000 hackerspaces, many of which include independently run hackerspaces such as Noisebridge in San Francisco, as well as the hackerspaces cropping up in schools and libraries across the country. The Foundation will help to create environments where hackers are welcomed, supported, nurtured and celebrated. Creating a safer, more supportive and accepting world for hacker will help to reduce depression and suicide among hackers, and enable hackers to live fuller and happier lives. The No Starch Press Foundation will: Support for STEM Initiatives One of the Foundation\u2019s goals is to build the hacker community by supporting STEM education in poorer neighborhoods through programs designed to interest grade school children in electronics, robotics, and similar activities. Educational Presentations and Resources The Foundation will sponsor presentations, talks, and other learning resources at hackerspaces and makerspaces. The Foundation will work to help self-taught hackers land their first real tech jobs, develop a career, and establish themselves in the hacking community. Support for Contests, Scholarships, and Other Programs The Foundation will support contests that challenge hackers to solve many of the world\u2019s problems, particularly programs and contests being run by other organizations to advance the involvement of middle schoolers interested in entering STEM fields. Support for Young Hackers Support for young hackers will include the financial support of educational events and meetups including social, educational, and emotionally oriented meetups designed to address depression and suicide in the hacker community, often brought on by isolation and misunderstanding of the aims and interests of a hacker, as well as supporting the initiatives of other nonprofit corporations. The Foundation will support these activities through donations from the general public, corporate grants, and funding from other public charities. The Foundation has already generated substantial interest from a number of prominent individuals in the hacking community who wish\u00a0to donate, volunteer on the Board, and help support the Foundation\u2019s mission. The No Starch Press Foundation is a California registered and duly qualified Title 26 US Code 501(c ) (3) tax exempt non profit company.","time":1525798841,"title":"Introducing the No Starch Press Foundation","type":"story","url":"http:\/\/nostarchfoundation.org\/","label":7,"label_name":"random"},{"by":"rmason","descendants":0,"id":17022493,"kids":"None","score":1,"text":"Manhattan\u2019s\u00a0trendy west SoHo neighborhood just got an eco-friendly new addition with\u00a0570 Broome. From the outside, the 25-story building looks like a regular luxury condo. But it\u2019s actually the first building in the US to boast a subtle but powerful enhancement that makes it good for the planet. The facade is coated with a spray-on solution called Pureti. The treatment, which is water-based, provides 570 Broome with the purifying power of 500 trees\u2014which basically like taking 2,000 cars off the road for a year. Pureti works by breaking down contaminants clogging Manhattan\u2019s air via a photocatalytic process that transforms polluting particles into oxidizing agents. They\u2019re then released into the atmosphere as harmless minerals.\u00a0This process happens super fast\u2014like millions-of-times-per-second fast\u2014so that the surface is perpetually\u00a0self-cleaning, minimizing operational costs for the building. What\u2019s more, the fa\u00e7ade of 570 Broome is crafted from sintered stone slabs of Neolith\u2014a materials brand that specializes in clean design. Indeed, Pureti can be applied to essentially any surface including glass, metals, concrete, and stucco, etc. It doesn\u2019t work great on Teflon\u00a0and wax, but those materials are typically not found in residential construction. Glen Finkel, the co-founder and president of Pureti, notes that the technology for the treatment was developed in 2004, when Finkel, a former clinical psychologist, teamed up with\u00a0organic chemist John Andrews, who had experience\u00a0developing textile coatings for airplane exit chutes, life vests and parachutes for NASA. \u201cWe\u2019ve been perfecting it ever since,\u201d says Finkel. And its already seeing international demand. In addition to more North American projects (including \u201csome soon to become iconic references,\u201d) Pureti will be soon be on buildings in \u201cSpain, Argentina, Turkey, etc.\u201d He adds that \u201cChina and India are huge potential targets, but so are Europe, Mexico and the Middle East.\u201d To make it in Manhattan\u2019s over-saturated luxury market, high-end buildings now need a little something extra. While classic design, 24-hour amneties, and a trendy zipcode continue to attract wealthy buyers, these days they\u2019re are almost a given across New York City. Veteran New York real estate broker Shlomi Reuveni reflected on the changing landscape in an interview with Mansion Global earlier this week, noting that a decade ago there were clear \u201cgood\u201d and \u201cbad\u201d neighborhoods in Manhattan, but today virtually all of them are considered \u201cgood\u201d. So what will set a luxury building apart in the future, and where should high-end buyers park big dollars? Reuveni, for his part, says the future is in sustainable and green buildings, telling the site that \u201cnew technologies and sustainable technology will drive the market in years to come.\u201d And so far they are. The days of getting points for basic solar paneled roofs are long gone, with breakthroughs like 3D-printed homes, and buildings with their own water conservation systems setting the bar far higher for \u201cgreen\u201d developers. The need for more eco-minded developments has never been greater. A recent CityLab study, for instance, revealed that if you live in a city and breathe air, you\u2019re basically a smoker.\u00a0Which helps explain why Tahir Demircioglu, the architect behind 570 Broome, views Pureti as \u201ca win-win situation for both the urban environment and the developer\/owner. [Pureti] cleans the air and reduces the use of chemicals to do so.\u201d Demircioglu adds that residents will also enjoy greater peace of mind by lowering their impact on the environment, and has plans to use Pureti on a\u00a0multi-family development on Long Island. He also echoes Reuveni\u2019s assessment of the future of green living: \u201cI think there is and will be more demand towards better living standards with more environmental consciousness.\u201d","time":1525798731,"title":"NYC Luxury Building Has the Air-Purifying Power of 500 Trees","type":"story","url":"https:\/\/quartzy.qz.com\/1269569\/this-nyc-luxury-building-has-the-air-purifying-power-of-500-trees\/","label":7,"label_name":"random"},{"by":"digitalboss","descendants":0,"id":17022483,"kids":"None","score":2,"text":"By Dominic Fracassa Caption Close San Francisco Mayor Mark Farrell is expected to issue an executive order Tuesday aimed at organizing and accelerating the city\u2019s ability to respond to an economic recession. \u201cOur task is clear: prepare now, while times are relatively good, so that we can be resilient when times are tougher,\u201d Farrell said in his directive. Taking the hard lessons learned after the national economic downturn of 2008, which resulted in painful cuts to city services and the loss of some 40,000 local jobs, Farrell\u2019s directive calls for the formation of an economic oversight board that will closely monitor economic conditions at the local, state and national levels. That board will consist of the city controller, the city\u2019s chief economist, the city administrator, the mayor\u2019s budget director and the director of the Office of Economic and Workforce Development. If warning signs of a recession begin to surface \u2014 like rising levels of mortgage defaults, spikes in unemployment insurance claims or a slumping stock market \u2014 the board will provide notice to the mayor\u2019s office and city lawmakers describing any policies or programs that could help counteract an economic downturn. \u201cNo one is saying that the city of San Francisco, acting by itself, can stop a recession from happening,\u201d said Ted Egan, the city\u2019s chief economist. \u201cIt\u2019s about using the resources we have in the most thoughtful way possible to minimize the impacts on people.\u201d In September, the city\u2019s Office of Economic and Workforce Development will present its first round of plans to combat the effects of a recession. Todd Rufo, the office\u2019s director, said one of the most important things the city can do is to line up a number of \u201cshovel-ready\u201d public works and infrastructure projects that the city can use to take advantage of state or federal economic stimulus money while putting San Franciscans to work. Farrell\u2019s order is guided largely by the results of a \u201ceconomic resiliency\u201d study commissioned by former Mayor Ed Lee in 2016. The study sought to model what could happen in the event of a national economic downturn, as well as a more localized recession catalyzed by a downturn in the tech industry, which has been directly or indirectly responsible for 80 percent of the city\u2019s job growth over the past 10 years. \u2014 Dominick Fracassa Last chance: San Francisco mayoral candidate Angela Alioto said Monday she\u2019s planning to take the city to court after the Ethics Commission voted to block her from participating in the city\u2019s public financing program, which provides mayoral candidates with up to $975,000 to offset the costs of campaigning. Last week, Alioto\u2019s campaign failed to qualify for public financing money for the fourth time. In April, the Ethics Commission granted Alioto another chance to apply for the program, but with the condition that it would be her campaign\u2019s final opportunity. To qualify for public financing money, mayoral candidates have to prove they\u2019ve raised $50,000 in campaign contributions from at least 500 city residents in amounts of between $10 and $100. A letter sent to Alioto\u2019s campaign from the commission\u2019s director, LeeAnn Pelham indicated that 8 percent of the contributions Alioto submitted were ineligible to count toward her qualification, mostly because of insufficient documentation verifying that they came from San Francisco residents. On Monday, Alioto requested one more day to provide the commission with the necessary documents and qualify for the public financing program. Commissioners Quentin Kopp and Paul Renne voted to grant her the extension, but they were defeated by commissioners Daina Chiu, Yvonne Lee and Noreen Ambrose. All of the other leading candidates in the mayor\u2019s race \u2014 London Breed, Jane Kim and Mark Leno \u2014 have qualified for the public financing program. Alioto also blamed her previous problems on a glitch in the third-party software program called Netfile that the Ethics Commission uses to process campaign finance filings. The program, the campaign claims, was omitting information from Alioto\u2019s filings when the campaign submitted them. Alioto said the glitch forced her staff to waste precious time verifying contribution information and that Pelham had knowingly withheld information about the problem. Alioto went as far as to accuse Pelham and her staff of fraud, a claim that she said will be an important part of arguments in court. \u201cIf you would like to go to court on these causes of action, I don\u2019t back away from anything,\u201d Alioto told the commission. She said she plans to file a writ that would order the city to disburse the public financing money she believes she\u2019s owed. \u201cOK, everybody, off to the next fight,\u201d Alioto told a small gathering of supporters after the commission\u2019s vote. \u2014 Dominic Fracassa Short tenure: Less than a month after his appointment to the San Francisco Ethics Commission, former judge and federal prosecutor Kevin Ryan has stepped down, citing private family reasons. City Attorney Dennis Herrera appointed Noreen Ambrose, a longtime deputy city attorney who retired last year, to take Ryan\u2019s place. Ambrose started on the Ethics Commission at its scheduled hearing on Monday. Ambrose spent 35 years with the city attorney\u2019s office, where she served in a variety of capacities, including as the general counsel of the Port Commission and later for the Public Utilities Commission. Last year, the Board of Supervisors commemorated her contributions to the city by naming June 16 as Noreen Ambrose Day in San Francisco. \u2014 Dominick Fracassa Email: cityinsider@sfchronicle.com, dfracassa@sfchronicle.com Twitter: @sfcityinsider, @dominicfracassa City Hall Reporter \u2018Meet the Press\u2019 host Chuck Todd steps into California\u2019s... Bay Area political events: Walk for peace, disabled rights SF Mayor Farrell wants city to be prepared for the next... A plan for San Francisco to take rape cases seriously at last \u00a9 document.write(new Date().getFullYear()); Hearst Corporation","time":1525798684,"title":"SF Mayor Farrell wants city to be prepared for the next economic downturn","type":"story","url":"https:\/\/www.sfchronicle.com\/bayarea\/article\/SF-Mayor-Farrell-wants-city-to-be-prepared-for-12895393.php","label":7,"label_name":"random"},{"by":"roperzh","descendants":1,"id":17022477,"kids":"[17022482]","score":2,"text":"Communication is frequently mentioned as one of the fundamental skills of a good software developer, but finding simple and actionable ways to get better it\u2019s hard. A couple of months ago I came into the realization that I needed to improve my communication skills, which was exacerbated by the fact that I work in a remote team. This is my intent to write down what gave me good results over the last six months; the conclusions I came after making an active effort to change my communication habits. Please take them with a grain of salt, this a work in progress and nothing is backed with more data than my own\u2014probably very biased\u2014experience. For the sake of organizing things, I\u2019ve grouped the bullets into three categories: the other, the group and the self. The other alludes to 1:1 communication with other teammates, it\u2019s partially different from group communications because the dynamics and the constraints are different, but almost everything in this section can be extrapolated to the group section. The group alludes to 1:many communication, this includes chat groups, mails with several recipients, pull requests, issue trackers, etc. While the previous two sections are clearly self-actionable, I think there\u2019s room for a third group: the self, here are some points that make your personal communication skills that are worth paying attention: Communication is a complex and difficult topic, I plan to expand and revisit this list as the time goes on. If you have any tips or resources, please ping me at @roperzh! \n        I try to post once a week interesting stuff about programming, *nix, and the web.\n\nIf you\u2019d like to be notified when a new post goes out, you can subscribe with the form below.\n","time":1525798648,"title":"Getting better at team communication","type":"story","url":"https:\/\/monades.roperzh.com\/getting-better-at-team-communication\/","label":7,"label_name":"random"},{"by":"lajhsdfkl","descendants":0,"id":17022473,"kids":"None","score":2,"text":"SEOUL, April 11 (UPI) -- South Korea's unemployment rate rose to a 17-year-high in March, two months after the country implemented its highest minimum increase in 17 years. Statistics Korea on Wednesday announced the number of unemployed Koreans hit 1.25 million last month, recording a 4.5 percent unemployment rate. It marks the third straight month that the number of jobless people topped 1 million. The situation was considerably worse for people age 15 to 29, with youth unemployment hitting 11.6 percent, a two-year high for that month. The number of employed people last month stood at 26.6 million, up 112,000 from a year earlier.\nIt marked the second straight month of growth confined to the 100,000 range. Experts say the sluggish growth in jobs is likely due to the steep minimum wage hike introduced in January, as employers feel the pinch of extra labor costs for part-time workers, Chosun Ilbo reported. The wage floor increased to roughly $7 an hour, a 16.4 percent rise, or the biggest annual increase in 17 years. Wholesale and retail businesses, as well as the food and accommodation sectors, have seen a substantial decrease in jobs, most of which pay by the hour. \"We can see there is a reduction of jobs for unskilled and simple work, as well as the service sector, which was feared would happen after the 16.4 percent minimum wage hike,\" said Yoo Gyeong-joon, a professor at the Korea Institute of Science and Technology and former head of Statistics Korea. \"There's a large possibility that many self-employed businesses will have to close down as they cannot shoulder the burden of labor costs, and as a result, the unemployment crisis will become even more difficult to resolve,\" Yoo said. The number of self-employed people decreased by 40,000 in March, compared to the year before. Meanwhile, Statistics Korea says the waning job growth is due to difficulties in the construction sector, and an overall decline in the population. It added that the numbers appeared to have reduced to do a base effect, as job growth figures last year exceeded 460,000. United Press International is a leading provider of news, photos and information to millions of readers around the globe via UPI.com and its licensing services.\n\t\t\t\t\t\t With a history of reliable reporting dating back to 1907, today's UPI is a credible source for the most important stories of the day, continually updated\u00a0\u00a0- a one-stop site for U.S. and world news, as well as entertainment, trends, science, health and stunning photography. UPI also provides insightful reports on key topics of geopolitical importance, including energy and security.\n\t\t\t\t\t","time":1525798614,"title":"South Korea unemployment surges after minimum wage hike","type":"story","url":"https:\/\/www.upi.com\/South-Korea-unemployment-surges-after-minimum-wage-hike\/5511523435109\/","label":7,"label_name":"random"},{"by":"MaeBaset","descendants":0,"id":17022470,"kids":"None","score":1,"text":"Seekurity Blog State of Information Security  What is Asus Control Center? ASUS Control Center is a whole new centralized IT management software. The software is capable of monitoring and controlling ASUS servers, workstations, and commercial products including notebooks, desktops, All-in-One (AiO) PCs, thin client, and digital signage.   ASUS is focusing on providing convenient, suitable, secured and cost saving solutions for enterprises, small and medium-sized businesses, and data centers across every industry. With the help of ASUS Control Center, your organization is primed to deal with the demands of the most challenging modern IT environments. ~\u201dASUS CONTROL CENTER WEBSITE\u201d During our free-time bug hunting (under responsible disclosure rules) we have discovered a misconfiguration issue which led to a critical Information Disclosure vulnerability in their \u201cAsus Control Center\u201d during conducting a passive reconnaissance we have discovered that their Apache Tomcat installation is leaking its configuration file to the public which contains a clear-text database connection password:  After that we have contacted Asus asking permission for escalation to prove that our PoC is working but they replied us that they acknowledge the issue as it is without any going further so we had to report it as it is. Then the issue got fixed:  And after that we\u2019ve got an acknowledgment for our submission: \nHall of Fame link: https:\/\/www.asus.com\/Static_WebPage\/ASUS-Product-Security-Advisory\/ Vulnerable Product Page:\u00a0https:\/\/asuscontrolcenter.asus.com  Vulnerable Link:\u00a0http:\/\/asuscontrolcenter.asus.com\/META-INF\/context.xml . Thanks for reading! .\n.\n.\nHey!\nBuilding a website? Or already built a one? Think twice before going public and let us protect your business! \n","time":1525798602,"title":"Asus Control Center \u2013 An Information Disclosure Vulnerability","type":"story","url":"https:\/\/www.seekurity.com\/blog\/general\/asus-control-center-an-information-disclosure-and-a-database-connection-clear-text-password-leakage-vulnerability\/","label":3,"label_name":"dev"},{"by":"lunchbreak","descendants":1,"id":17022465,"kids":"[17022509]","score":3,"text":"Posted by the Flutter Team at Google \nThis week at Google I\/O, we're announcing the third beta release of Flutter, our mobile app SDK for creating high-quality, native user experiences on iOS and Android, along with showcasing new tooling partners, usage of Flutter by several high-profile customers, and announcing official support from the Material team. \n \nWe believe mobile development needs an upgrade. All too often, developers are forced to compromise between quality and productivity: either building the same application twice on both iOS and Android, or settling for a cross-platform solution that makes it hard to deliver the native experience that customers demand. This is why we built Flutter: to offer a new path for mobile development, focused foremost on native performance, advanced visuals, and dramatically improving developer velocity and productivity. \n \nJust twelve months ago at Google I\/O 2017, we announced Flutter and delivered an early alpha of the toolkit. Over the last year, we've invested tens of thousands of engineering hours preparing Flutter for production use. We've rewritten major parts of the engine for performance, added support for developing on Windows, published tooling for Android Studio and Visual Studio Code, integrated Dart 2 and added support for more Firebase APIs, added support for inline video, ads and charts, internationalization and accessibility, addressed thousands of bugs and published hundreds of pages of documentation. It's been a busy year and we're thrilled to share the latest beta release with you!\n \nFlutter offers:\n \nAs evidence of the power that Flutter can offer applications, 2Dimensions are this week releasing a preview of a new tool for creating powerful interactive animations with Flutter. Here's an example of the output of their software:\n \nWhat you are seeing here is Flutter rendering 2D skeletal mesh animations on the phone in real-time. Achieving this level of graphical horsepower is thanks to Flutter's use of the hardware-accelerated Skia engine that draws every pixel to the screen, paired with the blazingly fast ahead-of-time compiled Dart language. But it gets better: note how the demo slider widget is translucently overlaid on the animation. Flutter seamlessly combines user interface widgets with 60fps animated graphics generated in real time, with the same code running on iOS and Android. \n \nHere's what Luigi Rosso, co-founder of 2Dimensions, says about Flutter:\n \n\n    \"I love the friction-free iteration with Flutter. Hot Reload sets me in a feedback loop that keeps me focused and in tune with my work. One of my biggest productivity inhibitors are tools that run slower than the developer. Flutter finally resets that bar.\"\n \nOne common challenge for mobile application creators is the transition from early design sketches to an interactive prototype that can be piloted or tested with customers. This week at Google I\/O, Infragistics, one of the largest providers of developer tooling and components, are announcing their commitment to Flutter and demonstrating how they've set out to close the designer\/developer gap even further with supportive tooling. Indigo Design to Code Studio enables designers to add interactivity to a Sketch design, and generate a pixel-perfect Flutter application. \n \nWe launched Flutter Beta 1 just ten weeks ago at Mobile World Congress, and it is exciting to see the momentum since then, both on Github, and in the number of published Flutter applications. Even though we're still building out Flutter, we're pleasantly surprised to see strong early adoption of the SDK, with some high-profile customer examples already published. One of the most popular is the companion app to the award-winning Hamilton Broadway musical, built by Posse Digital, with millions of monthly users, and an average rating of 4.6 on the Play Store. \n \nThis week, Alibaba is announcing their adoption of Flutter for Xianyu, one of their flagship applications with over twenty million monthly active users. Alibaba praises Flutter for its consistency across platforms, the ease of generating UI code from designer redlines, and the ease with which their native developers have learned Flutter. They are currently rolling out this updated version to their customers.\n \nAnother company now using Flutter is Groupon, who is prototyping and building new code for their merchant application. Here's what they say about using it:\n \n\n    \"I love the fact that Flutter integrates with our existing app and our team has to write code just once to provide a native experience for both our apps. This significantly reduces our time to market and helps us deliver more features to our customers.\" Varun Menghani, Head of Merchant Product Management, Groupon\n \nIn the short time since the Beta 1 launch, we've seen hundreds of Flutter apps published to the app stores, across a wide variety of application categories. Here are a few examples of the diversity of apps being created with Flutter:\n \nCloser to home, Google continues to use Flutter extensively. One new example announced at I\/O comes from Google Ads, who are previewing their new Flutter-based AdWords app that allows businesses to track and optimize their online advertising campaigns. Sridhar Ramaswamy, SVP for Ads and Commerce, says: \n \n\n    \"Flutter provides a modern reactive framework that enabled us to unify the codebase and teams for our Android and iOS applications. It's allowed the team to be much more productive, while still delivering a native application experience to both platforms. Stateful hot reload has been a game changer for productivity.\"\n \nFlutter Beta 3, shipping today at I\/O, continues us on the glidepath towards our eventual 1.0 release with new features that complete core scenarios. Dart 2, our reboot of the Dart language with a focus on client development, is now fully enabled with a terser syntax for building Flutter UIs. Beta 3 is world-ready with localization support including right-to-left languages, and also provides significantly improved support for building highly-accessible applications. New tooling provides a powerful widget inspector that makes it easier to see the visual tree for your UI and preview how widgets will look during development. We have emerging support for integrating ads through Firebase. And Visual Studio Code is now fully supported as a first-class development tool, with a dedicated Flutter extension. \n \nThe Material Design team has worked with us extensively since the start. We're happy to announce that as of today, Flutter is a first-class toolkit for Material, which means the Material and Flutter teams will partner to deliver even more support for Material Design. Of course, you can continue to use Flutter to build apps with a wide range of design aesthetics to express your brand. \n \nMore information about the new features in Flutter Beta 3 can be found at the Flutter blog on Medium. If you already have Flutter installed, just one command -- flutter upgrade -- gets you on the latest build. Otherwise, you can follow our getting started guide to install Flutter on macOS, Windows or Linux.\n \nFlutter has long been used in production at Google and by the public, even though we haven't yet released \"1.0.\"  We're approaching our 1.0 quality bar, and in the coming months you'll see us focus on some specific areas:\n \nLike every software project, the trade-offs are between time, quality and features. We are targeting a 1.0 release within the next year, but we will continue to adjust the schedule as necessary. As we're an open source project, our open issues are public and work scheduled for upcoming milestones can be viewed on our Github repo at any time.  We welcome your help along this journey to make mobile development amazing. \n \nWhether you're at Google I\/O in person or watching remotely, we have plenty of technical content to help you get up and running. In particular, we have numerous sessions on Flutter and Material Design, as well as a new series of Flutter codelabs and a Udacity course that is now open for registration. \n \nSince last year, we've been on a great journey together with a community of early adopters. We get an electric feeling when we see the range of apps, experiments, plug-ins, and supporting tools that developers are starting to produce using Flutter, and we're only just getting started. Now is a great time to join us. Connect with us through the website at https:\/\/flutter.io, via Twitter at @flutterio, and in our Google group and Gitter chat room. We're excited to see what you build!\n \n \n","time":1525798575,"title":"Flutter beta 3 release","type":"story","url":"https:\/\/developers.googleblog.com\/2018\/05\/ready-for-production-apps-flutter-beta-3.html?m=1","label":3,"label_name":"dev"},{"by":"Karishma1234","descendants":0,"id":17022426,"kids":"None","score":2,"text":"Opinion | Niall Ferguson A HUNDRED YEARS AGO, World War I was entering its final phase. No one in either Berlin or London had set out to expend so vast a quantity of blood and treasure on four years of industrialized slaughter. As I argued 20 years ago in \u201cThe Pity of War,\u201d World War I was perhaps the greatest error of modern history. Historians often look back to the events of the 1890s and 1900s in an effort to trace the origins of the Anglo-German antagonism. The long-established narrative goes something like this: The German economy was overtaking the British economy, a trend summed up in the words \u201cMade in Germany\u201d that were stamped on a rising proportion of imported manufactures. Germany had imperial ambitions, too, acquiring colonies in Asia and Africa. And it was building a fleet that was obviously intended to rival the Royal Navy. Increasingly, as their economy boomed, the Germans argued that their political system \u2014 in which the parliament (the Reichstag) had much less power than its British equivalent, and the monarch much more power \u2014 was intrinsically superior. Their material successes bolstered an already deep-rooted nationalism.  The ultimate result was that Britain and Germany followed the ancient example of Sparta and Athens: the incumbent power and the rising power ended up going to war. The Harvard political scientist Graham Allison calls it the \u201cThucydides trap,\u201d after the historian of the Peloponnesian War. Those who once argued that China\u2019s massive population would propel it to superpower status should rethink that assumption. Are the United States and China on the way to repeating this classic historical mistake? Having just spent a fascinating week in Beijing and Shanghai, I fear they may be.  China\u2019s economy has already, by at least one measure, overtaken that of the United States. The Chinese have come up with a strategy to catch up in terms of technology, too. It\u2019s called \u201cMade in China 2025.\u201d President Xi Jinping has his own version of the Germans\u2019 imperial Weltpolitik: the Belt and Road Initiative, which implies a global expansion of Chinese infrastructure and influence. The People\u2019s Liberation Army is busily building up its forces, with the goal (as one Chinese official told me last week) of having the world\u2019s strongest military by the time the People\u2019s Republic celebrates its centenary, in 2049.  Like the Germans a century ago, the Chinese no longer worry that Anglophone democracy might be superior to their political system. As for nationalism, there is no mistaking its growing importance, especially on social media. \u201cThere is also a Chinese populism,\u201d I was warned. Yet, as the events of 1918 proved, Germany overestimated itself and underestimated Britain. I fear some Chinese are beginning to make this mistake about the United States, with the encouragement of other Asians. My old friend Kishore Mahbubani, formerly Singapore\u2019s representative at the United Nations, has just published a punchy little book entitled \u201cHas the West Lost It?\u201d His answer is a blunt \u201cYes.\u201d  Such talk encourages a certain bravado in Beijing. Chinese officials I met with insisted that they could withstand a trade war. Several made the argument that such a war would not last long anyway because the American democratic system would undermine the US negotiating position. Were not the midterm elections a mere six months away?  It is indeed possible that President Trump\u2019s \u201cArt of the Deal\u201d will be defeated by the rather more subtle \u201cArt of War,\u201d immortalized by the ancient Chinese sage Sun Tzu. Yet the reality is that the Chinese depend more on the current global trade arrangements than the Americans do and stand to lose significantly more in a trade war.  Moreover, China\u2019s economy is not quite as strong as it may appear. The Chinese people toil under a vast debt mountain. After decades of abundance, China\u2019s workforce is now shrinking and the population aging in ways that are strongly reminiscent of Japan in the 1990s. That surely points to lower growth in the future.  Western observers tend to take the rise of Xi Jinping at face value, assuming that the concentration of power in his hands is a sign of strength. Perhaps it is. But I was very struck by the observation of one former minister that Xi\u2019s ascendancy had averted a major political crisis in 2012. He had \u201csaved the party, the country, and the military\u201d \u2014 presumably from a drastic decline in popular legitimacy stemming from rampant corruption.  A state that requires dictatorship to be stable is not as strong as it looks \u2014 just as one based on individual liberty is not as weak as it looks. There are two ways this can now go. The United States and China can fall together into the Thucydides trap, starting with a trade war and escalating into a real one. The alternative is what Henry Kissinger in his book \u201cOn China\u201d called \u201cco-evolution.\u201d That second route is not going to be easy. But it surely must be preferable to repeating the history of the Anglo-German antagonism. \nAn increasingly rigid alliance system, Kissinger argued in his book \u201cDiplomacy,\u201d condemned Britain and Germany to war in 1914. Today\u2019s statesmen in Washington and Beijing must eschew rigidity. To repeat the mistakes that sent my grandfather to the hell of the Flanders fields would be \u2014 to put it mildly \u2014 a pity.\n  \u00a9 2018 Boston Globe Media Partners, LLC","time":1525798335,"title":"An ancient trap awaits China and US","type":"story","url":"https:\/\/www.bostonglobe.com\/opinion\/2018\/05\/07\/ancient-trap-awaits-china-and\/wvuYixfXZFKoy00QdggxeJ\/story.html?event=event25","label":7,"label_name":"random"},{"by":"AdmiralAsshat","descendants":0,"id":17022418,"kids":"None","score":2,"text":"People say JavaScript is a bad language. No built-in types, a fatiguing ecosystem, and demanding you to constantly explain, \u201cno, no, JavaScript\u201d to anyone even vaguely non-technical. That\u2019s why for my new Mario Kart knock-off game, I\u2019ve logically decided against using JavaScript at all. To be clear, this is 100% CSS. My Photoshop licence ran out a while back, so this is: You can control the racer using WASD controls. So how does it work? Let\u2019s break it down. A cool thing that the CSS spec allows you to do with box-shadow and linear-gradient is to specify an arbitrary number of points. This is useful if you wanted two or three gradients, or to create some very basic CSS shapes. It\u2019s also helpful if you wanted to manually specify every individual pixel of an image\u2026 one-by-one\u2026 in order\u2026 directly into your stylesheet\u2026 until eventually you have the original image stored in your source file! I actually got this idea from reading Alcides Queiroz\u2019 great post on creating a Pure CSS Super Mario animation. You might argue that this is absurd, has a much larger data cost than even the most naive image encoding, without even going into the implications of putting image-data into source files. And I might ignore your argument, and continue to speak, but slightly louder. Doing so allows us to create our racers, like below: CSS is intentionally designed to be minimally interactive, so getting a kart that changes direction depending on keyboard input is a challenge. My initial thought was to have an <input type=\"text\" \/>, and have some selectors like [type=\"text\"][value=\"foo\"]. Tim Carry gave a great talk on a Pure CSS search engine (with equally horrifying CSS). He used a very similar idea to drive his dynamic search results. The only drawback to the idea is that it doesn\u2019t actually work. value isn\u2019t set when you type\u200a\u2014\u200ait\u2019s only the initial value that you can see in CSS. To get around this, Tim used 1 line of JavaScript to set the value on input change. This is a fine workaround, and I certainly can\u2019t fault Tim for it. In fact, I would have loved to have used the workaround. Unfortunately, I accidentally disabled JavaScript on my browser a few weeks ago, and still haven\u2019t worked out how to turn it back on. So I needed something that CSS did respond to, with no JavaScript workarounds. After a few hours of searching around, I finally found a pseudoselector that looked to be of use\u200a\u2014\u200a:valid. An input with a pattern specified has the corresponding\u00a0:valid and\u00a0:invalid pseudoselectors. This gives us 2 states to play with. Along with the\u00a0:placeholder-shown selector, which tells us if the input is empty, we have 3 total states. Empty (middle), valid (left), and invalid (right). So with a giant invisible input, we get an interactive page that responds to left-right input. Once able to respond to left\/right input, we need to get the racers to actually turn left or right. We can do this by making 1 big racer image, made up of each frame of the \u201cturning\u201d animation. This gives us our sprite, like the following image: This allows us to then specify a manual keyframe animation, going through each \u201cframe\u201d of the image. My brother always takes Mario when we play together, so I needed to be able to pick another racer. I picked Bowser because I respect how he\u2019s not afraid to fail repeatedly at a task, despite not having a clear end-goal. To make the selection menu, we can make some radio inputs representing each racer. Our output should then show a different thing depending on which input is\u00a0:checked, using the sibling combinators +, and ~. And lastly we need a way to be able to \u201ctoggle\u201d menus as either open or closed. I was originally planning on having the menus just be permanently open\u200a\u2014\u200abut this isn\u2019t great for small screens, and my mum insisted on having the game on her phone so she could show her friends \u201chow grown-up he\u2019s getting\u201d. So toggling menus it is. We can do this by listening to the\u00a0:focus of whichever button you want to open the menu, and then allowing that button to lose focus when you want the menu to close. In case it wasn\u2019t painfully clear when I mentioned injecting pixel-by-pixel image data in your stylesheets, allow me to emphasise: please don\u2019t do any of these things. This is mostly a thought experiment to see how far CSS can be pushed when you don\u2019t have to worry about peer reviews, or your peers\u2019 respect. But do follow me on Twitter, and share this if you enjoyed it! And keep an eye out for my next instalment: Converting Your webpack Config to be 100% CSS. By clapping more or less, you can signal to us which stories really stand out. Software engineer at @Onfido. Saving up to fulfil true dream of professional Mario Kart\u2026 https:\/\/stephencookdev.co.uk\/ Bursts of code to power through your day. Web Development articles, tutorials, and news.","time":1525798253,"title":"Interactive Mario Kart with Only CSS","type":"story","url":"https:\/\/codeburst.io\/mario-kart-css-7572bd2ce608","label":7,"label_name":"random"},{"by":"jarrodmiles","descendants":0,"id":17022396,"kids":"None","score":1,"text":"","time":1525798138,"title":"Applied Blockchain Event in Balboa Park San Diego on May 30","type":"story","url":"https:\/\/7ctos.ticketsocket.com\/#\/event-details\/techart-applied-blockchain-drinks","label":7,"label_name":"random"},{"by":"jeiden","descendants":0,"id":17022394,"kids":"None","score":1,"text":"Enabling the Internet of Things. Last October, we shared early details of a new program that would provide special benefits to those making a positive social impact using Particle\u2019s IoT platform. We were humbled by the positive reactions to the Particle for Good announcement, and inspired by the stories we heard from organizations creating a brighter future with IoT. Today, the Particle for Good program is officially accepting applications for membership. Nonprofits, social impact organizations, and educators who are using IoT to make a difference are invited to apply. With this program, we hope to enable innovators to use Particle to promote equality, protect our environment, empower underserved communities, support the arts, and more. Organizations that are accepted into the Particle for Good program will receive a variety of benefits to support IoT initiatives that create positive social change: Those who were previously enrolled in Particle\u2019s educator program will be converted into a Particle for Good member, and receive the benefits listed above. This includes an increase in hardware discounts from 15% to up to 20%. Envirofit is working with Particle to improve the lives of those living in extreme energy poverty. They have created an ultra-low emission, environmentally sustainable cookstove. These stoves conserve fuel and reduce harmful emissions emitted from cooking over an open flame, all while taking traditional cooking customs into consideration.  Envirofit has begun using Particle to understand cookstove adoption in remote areas. Recently, the Honduran government partnered with Envirofit to deploy 300,000 cookstoves to families living at the base of the economic pyramid. Envirofit took a sample of 1,000 stoves in Honduras and equipped them with Particle Electrons. These connected stoves collect usage data to help Envirofit improve improve cookstove designs and create better trainings in the future. Read the full story here. Check out other Particle for Good members making big impact in their communities: We\u2019re proud to partner with organizations like these who are using IoT technology to create social change. We encourage you to apply if you are are affiliated with a social impact organization and using Particle to make a positive impact. Today, any organization that is a 501(c)(3) organization, B-corp\ncertified, public benefit corporation, or academic institution (public\nor private) can apply. The program is best suited at this time for those\nfocused on environmental preservation, developing regions, STEAM education, or social justice. Organizations with technical team are best suited to be successful with Particle. In the future, we\u2019ll be able to expand access to more developer resources for those who don\u2019t have a technical team \u2014 stay tuned. Learn more about the program here and let us know about the problems you\u2019re solving in your community. Jeff is a Sr. Product Manager at Particle, focused on supporting those deploying professional IoT products at scale. ","time":1525798131,"title":"Particle for Good \u2013 providing benefits to socially conscious IoT builders","type":"story","url":"https:\/\/blog.particle.io\/2018\/05\/08\/particle-for-good-is-live\/","label":7,"label_name":"random"},{"by":"robertwiblin","descendants":2,"id":17022389,"kids":"[17022748]","score":3,"text":"Home  Podcast  Why don\u2019t we see alien civilizations? This Oxford academic suspects they\u2019re \u2018sleeping\u2019. Here\u2019s the\u00a0reason. By Robert Wiblin \u00b7 Published May 8th, 2018 Enjoyed the episode? Want to listen later? Subscribe by searching \u201c80,000 Hours\u201d wherever you get your podcasts, or click one of the buttons below:  It seems tremendously wasteful to have stars shining. When you think about the sheer amount of energy they\u2019re releasing, that seems like it\u2019s a total waste. Except that it\u2019s about 0.5 percent of the mass energy that gets converted into light and heat. The rest is just getting into heavy nuclei. If you can convert mass into energy, you might actually not care too much about stopping stars. If the process of turning off stars is more costly than 0.5% of the total mass energy, then you will not be doing it.  Anders Sandberg The universe is so vast, yet we don\u2019t see any alien civilizations. If they exist, where are they? Oxford University\u2019s Anders Sandberg has an original answer: they\u2019re \u2018sleeping\u2019, and for a very compelling reason. Because of the thermodynamics of computation, the colder it gets, the more computations you can do. The universe is getting exponentially colder as it expands, and as the universe cools, one Joule of energy gets worth more and more. If they wait long enough this can become a 10,000,000,000,000,000,000,000,000,000,000x gain. So, if a civilization wanted to maximize its ability to perform computations \u2013 its best option might be to lie in wait for trillions of years. Why would a civilization want to maximise the number of computations they can do? Because conscious minds are probably generated by computation, so doing twice as many computations is like living twice as long, in subjective time. Waiting will allow them to generate vastly more science, art, pleasure, or almost anything else they are likely to care about. But there\u2019s no point waking up to find another civilization has taken over and used up the universe\u2019s energy. So they\u2019ll need some sort of monitoring to protect their resources from potential competitors like us. It\u2019s plausible that this civilization would want to keep the universe\u2019s matter concentrated, so that each part would be in reach of the other parts, even after the universe\u2019s expansion. But that would mean changing the trajectory of galaxies during this dormant period. That we don\u2019t see anything like that makes it more likely that these aliens have local outposts throughout the universe, and we wouldn\u2019t notice them until we broke their rules. But breaking their rules might be our last action as a species. This \u2018aestivation hypothesis\u2019 is the invention of Dr Sandberg, a Senior Research Fellow at the Future of Humanity Institute at Oxford University, where he looks at low-probability, high-impact risks, predicting the capabilities of future technologies and very long-range futures for humanity. In this incredibly fun conversation we cover this and other possible explanations to the Fermi paradox, as well as questions like: The 80,000 Hours podcast is produced by Keiran Harris. The basic question that made us interested in the Fermi paradox in the first place is, does the silence of the sky foretell our doom? We really wonder if the evidence that the universe seems to be pretty devoid of intelligent life is a sign that our future is in danger, that there is some bad things ahead for us. One way of reasoning about this is the great filter idea from Robin Hanson. There has to be some step that is unlikely from going from inanimate matter to life, to intelligence, to some intelligence that makes a fuss that you can observe over astronomical distances. One of these probabilities of transition must be very, very low, otherwise the universe would be full of aliens making parking lots on the moon and putting up adverts on the Andromeda galaxy.  It would be very obvious if we lived in that kind of universe, so you can say, \u201cWell, it\u2019s obvious we\u2019re alone. The probability of life might be super low, or maybe it\u2019s that life is easy by intelligence is rare.\u201d In that case, we are lucky and we\u2019re fairly alone, which might be a bit sad, but it also means we\u2019re responsible for the rest of the universe and the silence in the sky doesn\u2019t actually say anything bad. The problem is, of course, that it also might be that intelligence is actually fairly common, but it doesn\u2019t survive; there is something very dangerous about being an intelligence species. You tend to wipe yourself out or become something inert. Maybe all civilizations quickly discover something World of Warcraft or other games and succumb to that. Or, some other, more subtle convergence threat. Except that many of these explanations of what that bad and dangerous thing is are very strange explanations. It\u2019s an interesting question when we know that certain boxes shouldn\u2019t be opened. Sometimes we can have a priori understandings, but this research field, whatever the effects it has tend to be local. So if we open that box and there were bad stuff, there is also going to be local disasters. That might be much more acceptable than some other fields where when you have effects, they tend to be global. If something bad exists in the box, it might affect an entire world. This is, for example, why I think we should be careful about technologies that produce something self-replicating. Whether that is computer viruses or biological organisms, or artificial intelligence that can copy themselves. Or maybe even memes and ideas that can spread from mind to mind. We want to avoid existential risk that could mean that we would never get this grand future. We might want to avoid doing stupid things that limit our future. We might want to avoid doing things that create enormous suffering of disvalue in these futures. So, what I\u2019ve been talking about here is kind of our understanding about how big the future is, and then that leads to questions like, \u201cWhat do we need to figure out right now to get it?\u201d Some things are obvious, like reducing existential risk. Making sure we survive and thrive. Making sure we have an open future. Some of it might be more subtle, like how do we coordinate once we start spreading out very far? Right now, we are within one seventh of a second away from each other. All humans are on the same planet or just above it. That\u2019s not going to be true forever. Eventually, we are going to be dispersed so much that you can\u2019t coordinate, and we might want to figure out some things that should be true for all our descendants. September 6, 2017 Listen now October 11, 2017 Listen now October 17, 2017 Listen now January 19, 2018 Listen now Robert Wiblin: Hi listeners, this is the 80,000 Hours Podcast, the show about the world\u2019s most pressing problems and how you can use your career to solve them. I\u2019m Rob Wiblin, Director of Research at 80,000 Hours. My interview with Anders Sandberg was so entertaining I decided to split it into two episodes, which each cover pretty different themes. This is the first part and it\u2019s focussed on a range of possible solutions to the Fermi Paradox, and how the universe could actually end up being colonised. We go well beyond the usual introductions you may have heard before. If you enjoy the episode, please share it on social media and tell your friends they should subscribe to the show! Without further ado I bring you Anders Sandberg. Robert Wiblin: Today, I\u2019m speaking with Dr Anders Sandberg. Anders is a senior research fellow at the Oxford University\u2019s Future of Humanity Institute, where he looks at low-probability, high-impact risks, estimating the capabilities of future technologies and very long-range futures for humanity. Topics of particular interest include global catastrophic risks, cognitive biases, and cognitive enhancement. Anders has a background in computer science, neuroscience, and medical engineering, and he got his PhD in computational neuroscience at Stockholm University in Sweden for work on a neural network modeling of human memory. Thanks for coming on the podcast, Anders. Anders Sandberg: Thank you for having me here. Robert Wiblin: We\u2019re going to dive into a bunch of things you\u2019ve studied while at the Future of Humanity Institute, which range from being a bit out there to being extremely out there. But first, how do you decide what to research and work on? Anders Sandberg: Normally, I try to figure out what the most important thing I could be doing is, and I do that. Except, of course, that in practice, this is not how it works. So while I\u2019m trying to figure out what truly matters, in practice, I tend to go for the most interesting, shiny problem. But when I do constructive procrastination, I have ten projects going on at the same time. In order to work on one of them, I will work on some other projects, because I can\u2019t bring myself to do that. And then, of course, I get a bit frustrated with them, so I move to another one. I\u2019m working on a rather broad front with many things at the same time. This might not be the most effective way of working, but it certainly allows a lot of very interesting cross-fertilization. Robert Wiblin: Yeah. I think you\u2019re one of the people with the widest interests of almost anyone that I know, and the widest number of different strange things that you\u2019ve studied over the course of your career. And also, I guess, one of the happiest people, as well. Do you think that you\u2019re as cheerful as you seem to be? Anders Sandberg: I think so. It\u2019s of course very hard to judge your own subjective happiness, but I certainly feel very bubbly, which is actually not always a good thing, because I\u2019m also rather content with whatever situation there is. Yes, there are a lot of frustrations, but I can always say, \u201cYeah, it\u2019s not that bad.\u201d Sometimes, having a low dynamic range of your mood might be a bit troublesome. Robert Wiblin: Yeah, interesting. Do you think, on balance, we need people who are unhappy to be agitators to fix problems? Anders Sandberg: I think you need to have the right kind of mixture. On the one hand, being an optimist means that you have a lot of biases that will make you wrong about many things; you\u2019re still not going to be very sad when you find out you were wrong about your expectations. The pessimist, will of course, find himself right a lot of times, but will not become happier. But together, of course, the optimist and pessimist are actually a pretty good team. The optimist will try new things, will suggest that this can be done and try his or her hand at it; the pessimist will point out what\u2019s problematic, what can\u2019t be done. If you get the right dynamic, this is way more effective than having a single person. Of course, most of us have a mixture of optimist and pessimist in our head; we\u2019re switching between the different moods. But sometimes it\u2019s also useful to specialize. Robert Wiblin: It seems to me like people who are really depressed might be more aware of their problems, but also less motivated to solve them because they don\u2019t expect to succeed at doing so. But I guess there are some people who have a negative outlook but are extremely animated as a result of that, because they\u2019re frustrated by some injustice, so they really wanna ride it. While they have a negative outlook, they\u2019re not necessarily pessimistic about fixing things. Do you agree with that kind of distinction? Anders Sandberg: I do. Similarly, as an optimist, you can be a very complacent optimist, saying, \u201cOh, everything is just getting better. I\u2019m just gonna wait until the singularity arrives and everything will be fine,\u201d or a dynamic optimist saying, \u201cOh, the future could be wonderful. It really looks promising. We need to safeguard it and do things to get there.\u201d Maybe because you just want to get there as quickly as possible, but also it might be because you realize that there is various problems that need to be solved before we get to the glorious future. Robert Wiblin: Listeners might very well have picked up that you\u2019re Swedish. Nick Bostrom is Swedish as well, right? Anders Sandberg: Oh, yeah. Robert Wiblin: The founder of the Future of Humanity Institute. Are there any other Swedes at FHI, or is it just you two? Anders Sandberg: Not strictly speaking right now, but we certainly have Swedes that have been visiting. There are people in the building and working effective altruists that are from Sweden, and there is a shocking number of Swedes interested in our kind of questions. Max Tegmark might be the most famous one. It\u2019s interesting, because there\u2019s a lot of Swedes, but most of them are not in Sweden. Robert Wiblin: Yeah. Why do you think so many Swedes are involved? Did you know Bostrom before you came to FHI, or is that just a coincidence? Anders Sandberg: I knew him before. We had been emailing and interacting in the world of transhumanists. Then he pointed out that he had started an institute and was recruiting people, and I managed to get the job. I think we didn\u2019t have that strong links. It was mostly a coincidence that we happened to be Swedes. I think the deeper root of the ubiquity of Swedes, because, let\u2019s face it, Sweden is not a large country, it\u2019s ten million people, is that it has good education, a fairly consequentialist mindset. Playing the trolley problem game with my niece and nephew was very amusing, because it turns out that even small children immediately get this idea about maximizing the number of saved persons. Except, of course, my youngest nephew, who tried to maximize the number of people run over. But maximizing is important to Swedes. That, of course, leads to a particular mindset. But the problem in Sweden is that being entrepreneurial, changing the rules, that\u2019s not really done. If you like doing that, then you can seek your fortune outside. Robert Wiblin: Interesting. You get the education necessary to change things, but then if you actually wanna do it, you have to come to Oxford. I guess there\u2019s a slightly similar phenomenon with Australians. Most listeners will know that I\u2019m Australian; they\u2019ll be able to tell that pretty fast. There\u2019s a lot of Australians involved in effective altruism, but most of them aren\u2019t in Australia. I wonder whether it\u2019s because people are more willing to leave Australia to pursue their fortune and their career than people are willing to leave the UK or the United States. Anders Sandberg: That could be another reason, just that feeling that, \u201cOkay, there is an outside world and it\u2019s okay to go out there and explore.\u201d Robert Wiblin: Yeah. I guess if you\u2019re an Australian, Australia feels quite isolated. It is very small. it\u2019s only 20 million people. A bit bigger than Sweden, but not that much. Just so many of my friends have gone overseas, but those who are really ambitious about pursuing their career are almost all considering leaving at some point. Anders Sandberg: Mm-hmm (affirmative). Robert Wiblin: Let\u2019s dive into a couple of the papers that you have published over the last few years. I\u2019ve been looking at your page on the Future of Humanity Institute\u2019s website. I\u2019ll stick up links to all of the things that we discuss here. The first and perhaps most remarkable, is you and Toby Ord recently published a paper where you claimed to basically have dissolved the Fermi paradox, which is a paradox that a lot of listeners will have heard of. What\u2019s the story there? Anders Sandberg: Well, the basic question that made us interested in the Fermi paradox in the first place is, does the silence of the sky foretell our doom? We really wonder if the evidence that the universe seems to be pretty devoid of intelligent life is a sign that our future is in danger, that there is some bad things ahead for us. One way of reasoning about this is the great filter idea from Robin Hanson. There has to be some step that is unlikely from going from inanimate matter to life, to intelligence, to some intelligence that makes a fuss that you can observe over astronomical distances. One of these probabilities of transition must be very, very low, otherwise the universe would be full of aliens making parking lots on the moon and putting up adverts on the Andromeda galaxy. Robert Wiblin: It would be like Rick and Morty. Anders Sandberg: Exactly. It would be very obvious if we lived in that kind of universe, so you can say, \u201cWell, it\u2019s obvious we\u2019re alone. The probability of life might be super low, or maybe it\u2019s that life is easy by intelligence is rare.\u201d In that case, we are lucky and we\u2019re fairly alone, which might be a bit sad, but it also means we\u2019re responsible for the rest of the universe and the silence in the sky doesn\u2019t actually say anything bad. The problem is, of course, that it also might be that intelligence is actually fairly common, but it doesn\u2019t survive; there is something very dangerous about being an intelligence species. You tend to wipe yourself out or become something inert. Maybe all civilizations quickly discover something World of Warcraft or other games and succumb to that. Or, some other, more subtle convergence threat. Except that many of these explanations of what that bad and dangerous thing is are very strange explanations. So we started thinking about the Fermi paradox and we tried to see, did people do some assumption here that was wrong or weird. When you start thinking about the Fermi paradox, basically what you do is something like the Drake equation. You say, \u201cThe universe is really, really big. There is a lot of sites wherein life and intelligence could have emerged, and a lot of time for them to do that.\u201d Then you multiply that with some probability of intelligence, of a technological civilization emerge. Typically people then say, \u201cYeah, you have a very big number, you multiply with a small number.\u201d But that first number is so literally astronomically big that you ought to get a lot of civilizations. We don\u2019t see them, and that\u2019s weird. Robert Wiblin: What is, roughly, the number of stars, say, in the observable universe? Anders Sandberg: We have about 300 billion stars in the Milky Way alone, and then you have, in the observable universe, many hundreds of billions of galaxies. We\u2019re literally talking about Carl Sagan level of billions of billions, and \u2026 Robert Wiblin: So, it\u2019s what, ten to the 20? Anders Sandberg: Something like that. Robert Wiblin: Yeah. Okay. It\u2019s a very large number, but we don\u2019t see any life out there. If you had an intergalactic civilization, you might be able to observe them, for example, blocking out the light coming from stars, because they would be using it as an enormous solar power plant, and you\u2019d see all of this mass, all of these stars out there, but you couldn\u2019t see the light coming from them. We might have a shot at picking up if there was life even very far away, but we don\u2019t see that. Anders Sandberg: Yeah. There has been an interesting survey called [jay hat 00:09:56] that actually looked at different galaxies and their energy emissions, and tried to see are there any galaxies that look like something is absorbing a lot of energy? While they found some peculiar galaxies, nothing really seemed to fit the bill. We know that, at least, out of the 500,000 galaxies they watched, none of them had this kind of super civilization. Robert Wiblin: Okay, we\u2019re trying to explain how is that? What\u2019s the problem that people find, why is this been something that people have been scratching their heads about for 60, 70 years? Anders Sandberg: Our answer is people have been looking at point estimates and not taking uncertainty into account in the right way. At this point many people in the field would protest and say, \u201cHey, we really know what we are uncertain. You are really being unfair to us.\u201d If I caricature the typical argument people give, it\u2019s something like they discuss the different factors and say, \u201cYeah, that one has roughly this value. The probability of life, we don\u2019t know but maybe we can make a guess. A very uncertain guess and one in a million, or one in a hundred.\u201d Then you get to the end of a calculation and then you have a number and you say, \u201cWell this is, of course, based on my guesstimates and many of them are really uncertain. This number should be taken with an enormous pinch of salt.\u201d However, it ends up in the ballpark of something. There is an internal joke in the SETI community that we in the two schools. One school that tends to think that, \u201cYeah, there should be thousands or millions of civilizations in the milky way so we should expect to be able to communicate with galactic club.\u201d The other ones that think that, \u201cNo, our calculation ends up with roughly one and that\u2019s us, so we\u2019re alone in the milky way.\u201d Now, what went wrong in this calculation? Well, the handling of uncertainty was actually bad. When I\u2019m claiming that I\u2019m a bit uncertain of how many inhabitants live in the great London region, that doesn\u2019t mean that if I say 10 million and I\u2019m uncertain about it, that, that was a good estimate. I could say, \u201cI have a confidence interval between one million and 30 million.\u201d Now, I\u2019m very confident that the true value is somewhere in there and then I can do other calculations where I make use of its confidence intervals. As I learn more, these intervals get smaller. For example, the number of stars in the milky way, we are somewhat uncertain about it but we cannot do a lot of observations and that number will converge rather nicely. We\u2019re really uncertain about the probability of life emerging on earth-like planet. I will get back to that a bit later. The thing is, you want to multiply together these uncertainties and end up with a distribution of uncertainty rather than a number because if you just take individual numbers with a point estimate, you will typically get an impression that you have a central value. A typical value that\u2019s very different from what the distribution actually will have. Robert Wiblin: Does the problem appear when you\u2019re multiplying many different things that you\u2019re uncertain about together? Anders Sandberg: Yes. This is a generic problem. Another reason we\u2019re interested in this is that it applies not just to the question of alien life, but also thinking about risk. For example, if you\u2019re designing a bio lab and you want to know what\u2019s the probability of lab accident causing a pandemic? You can make similar kind of calculation. You can say, \u201cWell, what\u2019s the probability that we\u2019re working on a real pathogenic virus times the probability that we have someone drops a test tube, times the probability of somebody actually contracting that disease and getting outside the lab and spreading it?\u201d You multiply together to get uncertain things. Some of these ones you have a pretty decent idea about, some of them are just guess work. Together, you get some probability distribution and that can give you a very different answer. If you go back to the alien example, imagine that we have nine factors and they could be between zero and 0.2. If we just take the medium values, 0.1 and multiply them together it would say, \u201cIt\u2019s one chance in a billion with the 300 billion stars in the milky way, which would expect 300 civilizations.\u201d The probability seems to be very much against us being all alone. If you do it more carefully, if you actually multiply together these confidence intervals and the math, of course, gets slightly more messy. Which, is one reason people don\u2019t like doing it, you get the distribution but actually gives us a very high probability to be alone in the galaxy because it could turn out that some of these uncertain permits actually have fairly low values. This is what has gone wrong in a lot of these na\u00efve applications. Now, serious researchers will say, \u201cYeah, I\u2019m not that stupid. I\u2019m actually thinking carefully about it.\u201d There is a surprising amount of thinking that is strongly predicated on these point estimates. Now, when you try to do it right you actually find that you get a tail of low probability that is very hard to avoid. Even if we take all the papers people have been publishing about alien life that have the estimates for these parameters for the drake equation. They are, of course, all written by optimists who like writing about our chances of contacting alien life rather than pessimists who think this is all a waste of time. Then you just randomly re-sample them. You make a made up distribution of what people said in the literature. You end up with about 8% chance that we\u2019re alone in the visible universe. This is just based on the optimist papers. You end up with about, I think, 20 to 30% chance that we\u2019re alone in the milky way. Again, based on these optimistic papers. If you then try to make the estimates of a real uncertainty based on current science based on some of its parameters better, you get even [inaudible 00:15:24] uncertainties. We have, essentially, a hundred orders of magnitude uncertain about how likely life is to emerge on plants. It could be that spontaneous generation of life is natural. As soon as you have a puddle with the right organic stuff, it will attempt to form. We can\u2019t rule that out. On the other hand it might be that it actually does almost take a thermodynamic miracle. Similarly, whether that life can evolve to intelligence, there are other good reasons to be really uncertain about whether you can get advanced complex life. It could be that life is super common in the universe but most of it is very simple bacteria that have genetic coding systems that are very simple to evolve but are then very rigid so you can\u2019t evolve much more. We happen to have one that might have just the right level of complexity. This uncertainty when you put that in. Now you get a much, much bigger range. It\u2019s interesting because this whole model. This is all armchair astrobiology. We haven\u2019t left the room. We\u2019re just reading scientific papers and making estimates. Still can be very optimistic. You can\u2019t say that the main number of civilizations we make might be in the hundreds but you can\u2019t avoid getting this tail suggestion that actually some pretty high likelihood that we\u2019re alone. Now, this leads to an interesting conclusion that an empty sky is not that surprising. We shouldn\u2019t be shocked. It\u2019s not much of a paradox, it\u2019s just an observation. Robert Wiblin: When you consider the fact that you can\u2019t rule out the possibility that some of these factors that go into the drake equation, like how often does life spontaneously appear from non-life or how often does intelligence evolve from bacteria. You can\u2019t rule out the fact that some of these factors might be basically zero. Like extremely close to zero and as a result it\u2019s not that surprising if the number of expected intelligent life in the universe is less than one. Even if you\u2019re saying the average number of instances of intelligent intergalactic civilization that we expect might be 100 or 1,000 or 10,000 it could also be very easily one or zero. Anders Sandberg: Yes, exactly. Robert Wiblin: I guess we look up at the sky and we can see that it\u2019s probably not 10,000, it\u2019s probably not 100,000, or we would be observing some of them. Now, we have quite a bit of evidence to say, \u201cWell, before we thought the probability of us being alone in the universe was 10% and now when we look up in the sky maybe it\u2019s 30, or 40, or 50%.\u201d Anders Sandberg: That\u2019s also true and that also leads to the really good news that comes out of all this mathematical treatment. The first half showing that the Fermi paradox isn\u2019t terribly paradoxical. That\u2019s kind of nice. When we actually look outside of the library we see that there are no UFO\u2019s miss-parked on the street below. We haven\u2019t seen any super civilization in nearby galaxies. There doesn\u2019t seem to be too many crashed alien spacecraft on the moon and so on. That gives us an update that rules out the Rick and Morty universe full of aliens. It\u2019s a very weak update. We haven\u2019t been observing that long. We haven\u2019t been observing that careful, but we can say, \u201cYeah, there can\u2019t be more than the one civilization per solar system,\u201d at the very least. You cut off that extreme tail. Now you can do the math and go back and see how does that update parameters going in? The interesting part here was the least uncertain parameters don\u2019t change very much. The rate of star formation of a number of stars in the milky way don\u2019t get changed by the fact that I don\u2019t see a UFO parked on the street outside. Which is as it should be, otherwise our theory would be rather crazy. However, the really uncertain parameters get affected a lot. Now, which are the really uncertain parameters? Well that seems to be a probability of getting life and the probability of life having the capacity to evolve into an intelligence. Here, we have hundreds of orders of magnitude of uncertainty. They moves several order of magnitude and size just by this very weak observation that there are no really nearby aliens. Now, the drake equation also has that famous or infamous last term that the lifespan of a civilization. The average time a civilization remains able to communicate. That\u2019s, of course, the real reason we care about this. We want to know our own future. It\u2019s very uncertain. We know that it must be more than a few decades because we have survived that long. We don\u2019t seem to be super lucky. It had to be less than 10 billion years because otherwise the drake equation doesn\u2019t function. It\u2019s build on assumption instead of the state universe and there is some technical equivalents here. That means we have about seven orders of magnitude of uncertainty. That\u2019s much less than the uncertainty about life. Our observation that we don\u2019t seem to be living in this Star Wars or Rick and Morty universe full of aliens means that we become slightly more pessimistic about the life spans of civilisations but not that much. But we move a lot of things about the probability of life. Now getting back to that thinking about the Great Filter. This suggests that the Great Filter is an early one. But the reason we don\u2019t see a lot of the universe inhabited by intelligence is that intelligence is actually rare rather than it will be wiped out by something scary. Robert Wiblin: So we observe that there\u2019s not much life in the universe as far as we can tell. And we\u2019re worried is the thing that causes that to be the case, risks that we face going forward from where we are now, or is it risks that we\u2019ve already managed to cross in the past like the development of intelligence or industrial mechanization, that kind of thing. And you\u2019re saying we should mostly update in favor of it having been a filter that we\u2019ve already passed rather than a filter that we haven\u2019t yet passed? Anders Sandberg: Yes. Robert Wiblin: Why is that? Anders Sandberg: So this has to do with where the uncertainties \u2026 And we have so much uncertainty about life and intelligence. So you can imagine this whole argument that\u2019s a series of springs of different stiffness that you put together and you stretch or compress. The springs that are the most floppy will move the most, while the stiff ones, the ones we are most certain about, will not change much. So this is a statistical argument for why the Great Filter is early and why the stars are not foretelling. You might say, \u201cOkay. This is all statistics. This actually doesn\u2019t give us that much actionable information.\u201d And that\u2019s true. This is still very much based on a logical mathematical reasoning about it. But it seems to be a new way of going about it. Actually applying Bayesian thinking to these certain existential questions. And of course, if we discover life, or if we discover something about how likely life is to emerge, we learn important things, which are much more powerful than my argument. Evidence will always beat any amount of whiteboard calculations. Robert Wiblin: Is that true? I mean, sometimes theoretical calculations can be very compelling. Anders Sandberg: I think theoretical calculations have a lot of strength. The problem is, you need to do them really right. When the calculation actually is totally correct, it has the entire force of the world of logic behind it, that\u2019s a titanic force that can really drive enormously long shades of reasoning very well. The problem is when you are standing there at the whiteboard with your pen, you\u2019re likely to make mistakes. You might overlook possibilities. The probability of a human doing the right action in a situation is typically less than 99%, even for the simplest operations. So now if you have a long argument, the probability of there being some slight error somewhere is almost 1. So this means that even though I might have a compelling argument that is rhetorically powerful and seems that every step has good logic, there might be flaws in it. If it\u2019s a very short one, the risk is of course smaller. So this is why I\u2019m always a bit skeptical about theoretical arguments when they come alone. You want to have several independent arguments that overlap and show the same conclusion. So if one of them is broken, you can still be fairly certain that the probability of the next one being broken is not very big. Robert Wiblin: Yeah, that\u2019s a really interesting way of putting it. I suppose it is my experience that sometimes I\u2019ll have a lot of reasoning that I just feel extremely confident about internally, but then when someone else checks it, or when I actually write it out clearly, I realize that I\u2019m wrong. And that happens all the time. And I also think in general that humans are not so great at reasoning. We have like systematic blind spots, potentially, where arguments can seem compelling, when in fact they\u2019re not because there\u2019s like some something that we\u2019re not very good at thinking about. For example, with this Fermi paper. You can imagine another civilization where they\u2019re very good at thinking about ranges of uncertainty, and that\u2019s always how they do it, and they appreciate this fact that taking point estimates, like they instinctively understand that taking point estimates creates a distorted picture once you multiply things together. But to humans that\u2019s not nearly so obvious. And so we can go about having long discussions that are all based on this misconception at the core of how we\u2019re trying to analyze the problem. Anders Sandberg: Yeah. And this is a really interesting problem in dealing with low probability high impact risks. So I have a paper that I\u2019m pretty happy with, together with Toby Ord a few years back, where we looked at the debate surrounding the large Hadron Collider. People were concerned that maybe it could create black holes or strange vats of vacuum decay that could destroy the world. And eventually we physicists were annoyed enough by lawsuits and other things to actually write a few papers trying to show that this was totally safe. The problem here is of course how certain can you be about an argument about safety when we\u2019re talking about something that is on the cutting edge of physics. Some of the arguments physicists gave were really nice. Like, Earth has been hit by cosmic rays for billions of years, so given that we should expect our little activity to be much less likely to cause anything harmful, and we\u2019re still here, so hence nothing bad could have happened. When you think about that argument, it might be compelling, but you also realize that if Earth had been destroyed, we wouldn\u2019t be around, so you have an observer selection effect. So you can quickly patch for it by saying, \u201cYeah, look at the moon. If the moon had been destroyed, we would still be here, and we would be noting a global strange math or a black hole orbiting us, and we would know something. But the moon is shining, everything is fine.\u201d The problem is, you need to patch for argument in quite a lot of ways, and now it becomes a fairly complex argument. So ideally, in order to prove that your understanding of the risk is really good, you want independent arguments. You want a cosmic ray argument. You want other statistical arguments about, for example, supernovas and other process in the universe that are based on other lines of reasoning. And together, even if each of our arguments is fallible and might be, just have a 1% chance of being wrong, if you have 20 of them, now you can be very certain that, yeah, there is no problem here. This is of course very, very different from how we normally like doing arguments in academe and all. We want a beautiful argument. We want a very compelling elegant line of reasoning and nothing else on the page. Having 20 different arguments, some of them really awkward, some of them really messy, some of them are using totally empirical stuff, that\u2019s not how you get your paper or book published. It doesn\u2019t look good. But it\u2019s much more robust. It\u2019s something you can trust. And so this is also how we should probably try to deal with the most problematic risks. We want to have independent groups evaluate them. Robert Wiblin: Just to explain why this mattered so much in the case of the Hadron Collider \u2026 Imagine, the scientists who were saying that it\u2019s definitely safe or that there\u2019s only a 1 in like a billion billion billion chance of it destroying us. Basically, that is correct if their analysis is correct. So if their analysis is correct, the probability is zero, basically. But what are the chances that their analysis is correct? Would that be 99% confidence that it\u2019s correct? That\u2019s almost certainly far too high, given all the reasons that you\u2019ve given, but even if there\u2019s a 1% chance that it\u2019s false, and in that case we don\u2019t know whether it\u2019s dangerous, we don\u2019t know whether it\u2019s unprecedented or not, that 1% times by whatever the likelihood is in that case, is gonna be much higher than 1 in a billion billion billion, which is the probability if their analysis is right. So basically, in a case where your analysis says that a risk is extremely unlikely, almost all of the risk arises in the case where your analysis has been wrong, which might actually be quite likely. Anders Sandberg: Exactly. And we see this outside the physics and existential risk. A classic example is finance. Long-Term Capital Management estimated that the risk of them going bankrupt, that would be a ten sigma event. It would essentially never happen in the history of the universe. A few months later, they went bankrupt, because their model was wrong. When our models of reality are wrong, we can\u2019t use them to make an accurate assessment of a risk. This is something I\u2019ve been actually working with in an industry collaboration with the reinsurance industry, because they\u2019re a bit worried about their catastrophe models. They are not perfect, and they\u2019re very aware of that, but we\u2019re also, the same models more or less that everybody else are using. So all the companies are making decisions that are correlated. So if the models are really badly wrong about something, it might mean that an entire industry will be mis-pricing or taking risks we shouldn\u2019t take on a global scale. So we have been working with to try to figure out ways around this systemic risk. Robert Wiblin: Mm. I guess another example of this that occurs to me is the Future of Humanity Institute has looked at the likelihood of a pandemic causing human extinction, killing everyone. And my understanding is that your conclusion was that it\u2019s reasonably likely that a pandemic could kill a lot of people, could kill a billion people, possibly even six billion people. But the probability of a pandemic killing all humans is extremely low, because it\u2019s just so hard for a disease to spread to everyone. And also for no one to have resistance to it naturally. Is that right? Anders Sandberg: Yup, yup. Robert Wiblin: But of course, you\u2019ve gotta worry about this issue here. So the probability of human extinction caused by a pandemic is very low if your analysis is correct. But this is a very messy analysis that\u2019s just based on part of your judgment, your understanding of what is possible with viruses and what is not, and you\u2019re not sure about that at all. So in fact, most of the risk now probably comes down to the scenario in which it turns out that you had a deep misunderstanding about this problem, and your analysis was severely flawed. Anders Sandberg: Exactly. And the interesting problem here is that it can be flawed in two directions. It could be that we\u2019re too pessimistic. Actually, there might be an upper limit of pandemic size, which is just a few hundred million people. In that case, the error doesn\u2019t do much harm. Robert Wiblin: It just goes to zero again. Anders Sandberg: Yeah. But if our error is that actually there is some way pandemics that, when they get really large, have extra bad effects. In that case of course, that uncertainty, means that we badly underestimate risk. And generally when dealing with risk, the conservative thing is to assume that the risk is bigger than your result. If you have an argument that the risk is absolutely zero, then it better be an extremely strong argument, and typically you can never make it that strong. Robert Wiblin: So again, in the case of the large Hadron Collider, I recall that, even though you thought people were overestimating how safe it was, you weren\u2019t against using it, because you thought the likelihood of the research from the large Hadron Collider preventing human extinction, was in fact higher than the risk of using it, causing human extinction. Is that right? Anders Sandberg: Yeah. I basically agree with most of the safety arguments, it\u2019s just that the methodology was a bit flawed. If we imagine people building Hadron Collider 2 plus, well, in that case we should do it in a different way, if they were to listen to me. But on the other hand, even some small risks are worth taking because they might actually help us understand the world better and make us survive much better in the world. Now, that trade-off is sometimes rather tricky. So, in the case of pandemics, we have this issue of gain of function research where people are actually in the lab making pandemic influenzas, and that has caused a lot of concern that might lead to lab accidents and releasing a pandemic. The reason people do this research is, of course, they want to prevent pandemics. So, in some sense, if you do it in the right way, you do something risky in order to reduce risk much more. The battle over the gain of function world is of course, is this research actually doing something useful? Now, when it comes to understanding physics, it\u2019s an interesting question. How much has that changed our overall existential risk and how much does that improve our future? Robert Wiblin: I guess you could argue the discovery of nuclear technology, the ability to split the atom, increased the risk. So maybe we don\u2019t want more fundamental research and physics. Maybe we just want to keep Pandora\u2019s Box closed. Anders Sandberg: And it\u2019s an interesting question when we know that certain boxes shouldn\u2019t be opened. Sometimes we can have a priori understandings, but this research field, whatever the effects it has tend to be local. So if we open that box and there were bad stuff, there is also going to be local disasters. That might be much more acceptable than some other fields where when you have effects, they tend to be global. If something bad exists in the box, it might affect an entire world. This is, for example, why I think we should be careful about technologies that produce something self-replicating. Whether that is computer viruses or biological organisms, or artificial intelligence that can copy themselves. Or maybe even memes and ideas that can spread from mind to mind. That self-replication factor is a hint that this needs much more care than something that just sits there, and might maybe blow up locally. So, when it comes to physics, was it a good thing that we discovered nuclear physics? I think maybe the world would have been better if Becquerel had put his piece of pitchblende in the wrong drawer. Now, he put it on photographic plate and he noticed that he got this weird shading because of some unknown radiation for pitchblende, and that led to the whole development of nuclear physics. Imagine the other world where he put the rock in a different drawer. In that drawer you might have still gotten quantum mechanics, because that was already a big problem among physicists at that time. They would not know anything about atomic nuclear or anything like that, but they would be very happy to deal with electrons. They would perhaps discover quantum mechanics, and a lot of important fundamental physics, including perhaps semiconductors and the things that are useful for computers. But we would get the nuclear and the weapons, and nuclear power much later. That might indeed have been a safer and better world. The problem was we couldn\u2019t make this decision. It\u2019s not like you could advise Becquerel should you put that rock in the drawer with the photographic film or not? That was a pure serendipity. You couldn\u2019t advise Marie Curie that her research was risky, unless you knew yourself about all the consequences. It was just basic chemistry, trying to understand what was going on. The point where it\u2019s obvious that nuclear power is actually something powerful and dangerous happens about the time Leo Szilard started to come up with chain reaction ideas. By that point it\u2019s kind of totally too late, because a lot of other people were thinking about it. So we can\u2019t control that very much. We can control the kind of experiments we do, and we can certainly try to find safe-making technologies earlier than the risky ones. I think this is also linked to an important concept, the technology completion conjecture. But in the long-run, every technology more or less that can be done, somebody will do. Even if it\u2019s an obviously stupid idea, somebody will be that guy. Sometimes it might take an extremely long time, but sooner or later somebody\u2019s just going to do it as an art project, or something. Robert Wiblin: Unless you have some strong central control to prevent that. Anders Sandberg: Yeah, but \u2026 Robert Wiblin: But setting that aside. Anders Sandberg: Yeah, well I think it\u2019s an interesting challenge, because the obvious thing is say let\u2019s ban the bad activities, let\u2019s try to impose the right kind of central control. But central control for technology and science has rarely worked very well. Part of it is that if a technology or a form of understanding is really interesting and appealing to people, they will get it, even if it\u2019s illegal. Another problem is that, as I\u2019ve been saying, we can\u2019t predict very well what new ideas arrive. That attempt to control practical technologies have been a very mixed bag. Some technologies have been very limited because of various taxation rules. It\u2019s definitely possible to harm the development of technology by setting rules in the wrong way. But attempts at banning printing in the west has totally failed, although it somewhat worked in Turkey. China banned travel overseas, which limited its naval power over a long time. The interesting thing is we can speed up beneficial technologies. Quite often we can figure out that biotechnology is going to allow us to make organisms with new properties, maybe we need a good way of mopping up organisms with new properties if we accidentally left them out of the lab. Or if, once we released the gene drive in the wild, we have second thoughts. We don\u2019t really know how to fix that yet, but we can imagine that there should be technology that can fix it, and we should put in effort to discover that before we release the gene drive or the GMOs. Similarly, when it comes to artificial intelligence there are good arguments why powerful artificial intelligence is dangerous and hard to control, so we should figure out ways of controlling it and aligning its values earlier. Before we have really powerful AI. So I\u2019m very much fond of what I call differential technology development. Think about the potential risks and downsides, and then try to come up with technologies or other fixes. They don\u2019t necessarily have to be a matter of a device, they could be a practice, or a tax, or something else, and get that done before. It\u2019s much more positive too than trying to ban something. People really hate when you prevent them from doing something they want. But people love it when you announce a price for coming up with a better way of safeguarding the ozone layer, or making geoengineering safer. People really like your idea, but we should come up with a better way of making AI safer. Robert Wiblin: Let\u2019s step back to the Fermi paradox. The core error in people analysis was using these point estimates and multiplying them together. Can you think of any other examples where people do this and it\u2019s leading them astray? Anders Sandberg: I think it\u2019s fairly common we do this in most risk estimates and security estimates. If you think about these classic calculations about reliability of a nuclear power plant, typically you can imagine a chain of events that lead to a disaster. Most of these steps seem to be fairly unlikely. So you multiply them together and get a really low probability and then you say, \u201cYeah, it\u2019s just one chance in a million.\u201d You\u2019re of course not certain about these steps, how likely they actually are. Especially after a few years when the engineers think they know what we\u2019re doing, and people are being sloppy, and the contractor have been replacing the pipe with something else. These probabilities become much fussier. Now we actually should assume, when you try to build the plant, that actually my probability estimate is uncertain, and I actually need to take this into account. I\u2019m going to get this tail event. And sometimes of course bad luck just happens in a very weird, correlated fashion. So there is this concept of normal accidents, because things become linked to each other. When you have a really complex system, things start becoming correlated in ways that you couldn\u2019t expect when you designed it. That also means that the risk of something going wrong suddenly goes way up. Robert Wiblin: Listeners, take this away as a general rule. If you find yourself trying to estimate something that\u2019s improbable, and you find yourself multiplying different parameters together, and you\u2019re not sure about them, then that\u2019s going to give you a misleading estimate. There\u2019s actually a website where you can try to correct for this called Guesstimate. Have you used that Anders? Anders Sandberg: Oh yeah, it\u2019s beautiful. Robert Wiblin: So I\u2019ll put up a link to that, and you should definitely get familiar with it, because we\u2019ve been using it at 80,000 Hours to try to deal with some of these issues. I think as the problem is more severe, the more unlikely is the event, and the more things you\u2019re multiplying together, and the more uncertain is each of those parameters, right? Anders Sandberg: Yeah. To some extent also modeling it is useful, because it gives you a sense of where your uncertainties are. Not just about values, but also about what\u2019s the structure of the problem, what could go wrong. So one interesting example, is about earthquake insurance in America. So there is this debate about whether there is fault zone under New Madrid in the Mississippi Valley. So there was a really big earthquake a long time ago, haven\u2019t been much ever since. So some people think that oh yeah, that is actually a fault zone, but occasionally has very bad events. Other say no, we don\u2019t think so. The way the insurance world in their earthquake models are modeling this is making a mixture model between earthquakes going on without the fault zone, and the world where that fault zone exists. We don\u2019t know which world they happen to be in, but they want insurance that makes sense in both of these worlds, so you mix them together. So in this case you have a relatively nice either\/or chance. In some cases you get even weirder anti-correlated risks. So if you\u2019re really worried about one possibility, then that actually rules out another bad possibility. So you need something more elaborate, and actually thinking about this, having others critique your thinking makes it so much more robust. Robert Wiblin: Let\u2019s move on to another solution that you suggested to the Fermi Paradox, which is the Aestivation Hypothesis. I\u2019d never heard of this word, aestivation? Is that it? Anders Sandberg: Yeah. Robert Wiblin: It means hibernation, right? Anders Sandberg: It\u2019s the exact opposite of hibernation, but means roughly the same thing. So, hibernation, that\u2019s when you\u2019re sleeping through winter. Aestivation is when you sleep through summer. Robert Wiblin: Mm-hmm (affirmative). Anders Sandberg: From the Latin for summer, aestas. So the idea here is that maybe there is advanced civilizations in the universe, but they\u2019re all sleeping. Robert Wiblin: So we\u2019re in summer now in the universe, because the stars are shining. Why would the aliens be hibernating? Anders Sandberg: It has to do with the thermodynamic cost of doing computation. When you do a computation, you\u2019re essentially moving information around. Well that means that you want to make changes that matter over time, and that has a cost. You\u2019re essentially reducing entropy when you do a calculation. For example, if you have a register in your computer and you erase what was there previously, now you reduced the entropy because now you\u2019re certain about the result. The laws of thermodynamic tell you that, \u201cYep, you can\u2019t, on average, reduce entropy. You need to pay for it.\u201d So somewhere, there\u2019s going to be a big waste heap coming out of your computer, and this is obviously true for our current computer because they run rather warm. But this is built into the laws of physics. This is something even the most perfect computer can\u2019t avoid. You basically need to pay a cost proportion to the number of bits of information you erase in your computation times the temperature. So now you will immediately say, \u201cLet\u2019s cool down the computer and run it therein at a very cold temperature. That\u2019s going to fix things.\u201d And if you\u2019re really sophisticated, you can start talking about frontal computers and reversible computing that can get around this in a softer way. But the most important part is error correction. Occasionally bits will just flip because of the radiation or randomness. You need to correct that, and that always has this energy cost. So your computations, they cost something proportional to the surrounding temperature. Most of what we do in life, we don\u2019t think of it as computation. We say that, \u201cNo, I\u2019m having social relations, I\u2019m trading, I\u2019m falling in love. And these are definitely not computations.\u201d But when you look at the deep down, this is actually about changing patterns of information. Maybe it is the change that really matters. But if there is no information change when you fall in love, then there is a real problem. Robert Wiblin: It\u2019s a bad relationship. Anders Sandberg: It\u2019s a very bad relationship. It\u2019s also a reversible relation. You can go back, and nobody will notice anything. So it\u2019s probably not very much to talk about. Robert Wiblin: I guess sometimes you might want to be able to do that, but perhaps you won\u2019t know whether you want to reverse the calculation in any romantic liaison until you\u2019ve already been through it. Anders Sandberg: Exactly. Evolution is another beautiful example. Evolution is a fundamentally irreversible process where you create various offspring, and they are subjected to selection effects in nature. Some of them are, unfortunately, erased. That is, they get killed, and the survivors continue a spread of their genes. Now, if evolution was reversible, we could unevolve. It would be kind of just a random drift in genotype space, and that would be tremendously slow. The fact is that this is irreversible means that we actually have a bit of momentum in evolution. Now, this means, of course with evolution again creates energy. So we have an energy cost for our activities, regardless of whether that is falling in love, evolving, or running a super civilization a million years since. So, we can say, \u201cLet\u2019s go to a cold place.\u201d And the coldest place you can get today, Iceland, is of course making a background radiation. Free Kelvin about absolute zero. By human standards, tremendously cold. And where would you run computations of that temperature? You wouldn\u2019t get a lot of stuff done. But wait a minute, the universe is expanding. It\u2019s actually getting colder, which means that, if you wait a bit with your computation you can do much more. In fact, accelerating expansion of the universe means that this background temperature is going down exponentially. So if I have a certain amount of energy right now, if I\u2019m willing to wait long enough I\u2019m going to get an exponentially growing amount of computation done. Now, for many things we don\u2019t really want to wait any longer. We have a short discomfort. We want to heal the sick today, we want to help the needy, we want to explore the universe and learn of the ropes as a civilization. We really shouldn\u2019t be putting that off onto tomorrow, even though it might be thermodynamically more effective. But once you\u2019re a fairly mature civilization and you\u2019ve done the basic things that are nice to do in the material world, now your discount rate is going to become much lower. You\u2019re going to care much more about the long term future. You have a lower existential risk because you survived and by now you actually know how to handle yourself as a mature civilization. At that point, if you just wait for a while, if you estimate a bit, you get much more computation. So if you imagine the real advanced civilization that has seen a lot of galaxy expanded long distances, once you\u2019ve seen a hundred elliptical galaxies and a hundred spiral galaxies, how many surprises are we going to be there? Now most of the interesting stuff your civilization is doing is going to be culture, science, philosophy, and all the other internal stuff. The external universe is nice scenery, but you\u2019ve seen much of it. So this leads to this possibility that maybe advanced civilization is actually an estimate. They slow down, they freeze themselves, and wait until a much later era because we get so much more. It turns out that you can calculate how much more they can get. So the background radiation of the universe is declining exponentially. But there\u2019s a lower limit because of the horizon radiation, which has a temperature of ten to the minus 29 Kelvin, a ridiculously low temperature. But we\u2019re going to be there in about one and a half trillion years. It\u2019s going to take a bit longer because with stars actually heating up the whole place and even when the stars have burned out in a few trillion years, they\u2019re going to be brown dwarfs, those being rather warm. But basically eventually the universe gets down to a cold temperature that will not become any colder. And that is perhaps a natural harvest time. At that point you wake up and you take your resources and actually get going on doing whatever it is that make your super civilization tick. Now, the interesting thing here is of course this might lead to a lot of predictions. It\u2019s a fun story to tell. You can make a very good science fiction story. But you can also start to see, \u201cWhat would this imply in terms of observable things in the universe?\u201d So one thing is, if advanced civilization is emerging early do this, they don\u2019t want to lose their resources because they are actually worth a lot. It turns out that if you take the mass energy of Earth today and wait until this very cold far future, and then use it up to do computation, you get about as much computation as you would get today if you used an entire observable universe as raw material for computation. A single planet is worth as much as the entire universe. So you really want to make sure that you don\u2019t lose too much resources. So one thing I was analyzing in this paper was, \u201cAre we seeing an astronomical process that looked like they\u2019re being suppressed so they don\u2019t waste too much mass and energy. And depending on what kind of civilization you\u2019re in, you might also want to care about different things. Some civilizations want energy. They want to run long computations or a lot of computations. Others want a lot of mass because they want a lot of memory. Some civilizations might need to keep mass together so it doesn\u2019t get dispersed by the expansion of the universe because it\u2019s actually important for all the parks to be together. While other civilizations might just say, \u201cI want to be able to convert all that matter into minds that are having blissful experiences of the maximal possible rate. And we don\u2019t care if one blissful cluster of minds can\u2019t talk to another blissful cluster of minds because as long as we\u2019re happy, that\u2019s fine. So you can kind of outline different aims in the civilization and then try to see what the astronomic approsa would they mess with. And the interesting part is we can rule out some things. It doesn\u2019t look like anybody\u2019s moving galaxies around in the visible universe. That would be very noticeable. And that suggests that there are no civilization that are actually trying to stay together casually, either because there are no civilizations or because maybe this is not a good idea. Maybe the hedonistic utilitarianism is the answer. So actually, most of the universe had been seeded by a valence , eventually it will be all converted into hedonium. So we can do various observations and rule out some sets of possibility space. Robert Wiblin: Just to backup a second, and make sure that I\u2019ve completely understood, the idea is if the goal of this civilization is to run calculations, then you can get a lot more calculations done if you wait until you have an extremely cold environment, and indeed the efficiency of the calculations is equal to one over the temperature in kelvin above absolute zero. And because the temperature is going to go from three kelvin now to one over ten to the power of thirty? Anders Sandberg: Yep. Robert Wiblin: You would, if you waited until the heat death of the universe, basically, or just before that, then you\u2019ll get 10 to the 30 times by three more calculations done than if you were to do it now, using the same method. Anders Sandberg: Exactly. Robert Wiblin: Can you explain why the energetic cost of running a given calculation is proportional to the temperature? Or is that quite complicated? Anders Sandberg: I think it\u2019s somewhat complicated. It\u2019s related to what\u2019s called the Landauer principle, although actually all the big names in 20th-century physics had something to do with this realization. When you erase one bit of information, you need to pay small thermodynamic cost. Basically, it has to do with that memory is a state that doesn\u2019t change over time. Then you have an outside world, which is kind of a heat bath. That\u2019s a lot of random things going on. You want to push the system into a particular state, and that requires interacting. The temperature matters. But it is pretty profound and important, I think we are still not fully understanding the full picture, here. The physics of information is a fascinating emerging field. The Landauer principle is a firsthand that there is actually a strong link between information and thermodynamics. Now quantum computing, and the quantum information theory is also getting in. We\u2019re starting to see the outline that information is something just as physical as entropy or energy. A century ago, of course, people would not even think that this was possible. We still don\u2019t fully understand this. There\u2019s some surprises. For example, it turns out that you don\u2019t always have to pay an energy cost for a racing a bit. If you have a very ordered, in essence, structure, let\u2019s say that you have a planet full of items that have all spends pointed in the same direction, by randomizing the spends a little bit, you can also erase information. Basically, there are some other resources and energy you can use, but it doesn\u2019t change the overall picture very much. Robert Wiblin: I suppose it\u2019s possible in our understanding of this is wrong, and the reason that this isn\u2019t happening is because we would eventually realize that it\u2019s wrong and not adopt the strategy, because it won\u2019t work, or there\u2019s something else that\u2019s better. That\u2019s right? Anders Sandberg: Yeah. Robert Wiblin: But it sounds like a pretty good approach. Sounds like, maybe, we should do this if humanity manages to stick around for a long time. Anders Sandberg: I definitely think so. One reason I wrote this paper wasn\u2019t so much the from the question as is this a good scenario for our future? Because when you start thinking about the future of humanity, it\u2019s interesting to consider what should we want to do, what can we do? How grand could the future be? It\u2019s quite useful to find the laws of physics that actually limit us to the ultimate extent. Robert Wiblin: Okay, but it doesn\u2019t seem like such a winner, because we don\u2019t see anyone apparently trying to do this. If you were a civilization that\u2019s contemplating this strategy, wouldn\u2019t you be worried about hibernating, basically, or aestivating, and then another civilization appears and it just takes over the universe while you\u2019re asleep. You need to be monitoring that, at least. You might, in fact, want to build quite a robust civilization, in a sense. Quite a visible civilization just to in order to make sure that you\u2019re taking care of yourself and you\u2019re going to last a very long time. Is that right? If you\u2019re aestivating, would you expect to be very visible or not very visible? Anders Sandberg: I think you don\u2019t need to be very visible in the sense that the thing that you really care about, the bunker where the aliens are sleeping, probably should be hidden very, very well. But you want to have an infrastructure. You want caretakers to make sure your galaxy doesn\u2019t go bad. Most importantly, things might evolve in your stuff. It\u2019s a bit like having a kitchen but leaving it too long. Eventually things evolve in the sink. You want to keep that under control. Robert Wiblin: You need to have some kind of error correction to make sure that you don\u2019t eventually deviate from the eventual original plan. Anders Sandberg: Yeah. Robert Wiblin: Or some part of you doesn\u2019t do that, like a tumor, like a cancer. Anders Sandberg: Yes. So this leads to other interesting aspects of this scenario. For the aestivation scenario to work, the civilization actually needs to be coordinated enough so everyone can agree, \u201cLet\u2019s sleep to until 2 trillion years.\u201d And nobody starts before and starts grabbing resources from the others. You need to have a pretty strong internal coordination. Then you want to enforce a certain coordination, both on the material universe in the sense that you don\u2019t lose too much and also that newcomers don\u2019t invade you, either because they come from abroad, you get an invasion from the neighboring galactic supercluster, or that some monkeys on some planet evolve and start to gobbling up planet and doing all sorts of mess. This shades over into what\u2019s sometimes called the zoo hypothesis in discussions about alien intelligence. The idea that the aliens are around, but we don\u2019t notice them that they\u2019re looking at us and letting us behave. Now the interesting question with the zoo hypothesis is what actions are not permitted? Because if the aestivation hypothesis is true, we should expect that starting to blow up stars and doing things that really reduce the value in the long-term future should be strongly prohibited. Presumably, that also involves sending out self replicating probes and really doing things. At that- Robert Wiblin: Creating kind of a virus on the space that they want. Anders Sandberg: Exactly. So you should expect some process to prevent that. Now, you could imagine the space police. Essentially you put little robots in the systems and when a civilization emerges and starts messing around with stuff they\u2019re not supposed to, the space police shows up and tells them, \u201cNo, the rules in this galaxy are as follows. Don\u2019t do this, don\u2019t do this, and if you do that, well, that\u2019s totally okay.\u201d We haven\u2019t seen this, yet. It might be an interesting thing to test, of course. It\u2019s a very risky thing to test, for example, by building a self replicating space probe. If nothing stops you from doing that, then you can be fairly certain that you don\u2019t have any overseers handling things. The zoo hypothesis probably not a viable solution. The problem is, of course, instead of a friendly neighborhood space policeman, you might just have a big asteroid coming in. It is a bit risky to test. Robert Wiblin: The other question was, if your civilization planning to do this, you might want to, for example, stop the stars from burning, because that\u2019s going to preserve more energy, more matter, until the end of the universe. Anders Sandberg: That is actually an interesting thing that is probably not true. It seems tremendously wasteful to have stars shining. When you think about the sheer amount of energy they\u2019re releasing, that seems like it\u2019s a total waste. Except that it\u2019s about 0.5 percent of the mass energy that\u2019s what gets converted into light and heat. The rest is just getting into heavy nuclei. If you can convert mass into energy, you might actually not care too much about stopping stars. If the process of turning off stars is more costly than 0.5% of the total mass energy, then you will not be doing it. I can certainly imagine that I would like to build Dyson shells and scoop up the energy right now and use for some nice things, but it might be that in the long run this is simply not worth the effort. It\u2019s interesting that the scale we\u2019re talking about here makes things like galaxies, you might actually afford to lose a few galaxies if that streamlines your process. It\u2019s still kind of telling that, yeah, we don\u2019t seek any process preventing the emergence of blue white heavy stars that will turn into black holes and probably become fairly unusable. There are ways of preventing that from happening by iron seeding galactic gas clouds. The increased capacity means that when they collapse, they turn into smaller stars. Recent calculations are done suggests that actually maybe you don\u2019t even need to do that. Because that\u2019s really, you anyway get so much more long-lived red dwarfs stars that you actually don\u2019t need to worry about these blue white stars. Because we, right now, we\u2019re in all of them. We see them across the star. This is just this very early part of the coniferous era. Wait a few tens of billions of years and you\u2019re not going to have that many blue white stars. The universe is going to be much more staid and middle-aged. You\u2019re going to have a lot of orange and red stars. Maybe it\u2019s not actually even worth iron seeding the clouds because, well, you don\u2019t need it. The interesting thing about this kind of hypothesis is, of course, what\u2019s the value in proceeding pursuing it? I did it partially because I wanted to calculate rational strategy for humanity but it also shows how weird many of the possible explanations for the Fermi paradox are. If you don\u2019t accept that we are alone, which as I mentioned earlier I think is actually a plausible outcome, something is really odd about the universe. Something extremely strange is going on. Either something like the zoo hypothesis or the simulation hypothesis. Or that the intelligence always has very strong sociological convergence to behaving in a particular way or that it works itself out. You get a really strange and uncomfortable conclusion whatever the answer to the Fermi paradox is. Robert Wiblin: What probability do you place on this being the explanation for the Fermi paradox? Anders Sandberg: I think I will give perhaps 10% probability to the aestivation hypothesis. Maybe just because I came up with it, I\u2019m fond of it. I don\u2019t believe in it that strongly. I heard people being based surprised. \u201cWhy do you even write a paper about something you don\u2019t believe in?\u201d Obviously, they have never met a philosopher because philosophers do this all the time. Sometimes a line of thought is interesting to pursue.  I think we have learned a lot of things from pursuing this. I think the real value is, yeah, we can think and plan for the extreme long-range future. Right now, we need to have a sustainable civilization. We need to fix existential risk. We need to get our act together and become a more mature civilization. But beyond that point, we probably should think about our long-term future. What\u2019s the pension plan for intelligence in the universe? Robert Wiblin: You\u2019ve been mapping out the space of possible options. I guess this is one that\u2019s a bit weird, but in your view, not completely implausible. There\u2019s a decent chance that this is what\u2019s going on. Anders Sandberg: Yeah. Robert Wiblin: You pointed out that if we start using up the cosmic comments that the aliens that are aestivating might want to wipe us out so that we don\u2019t end up taking things over and drinking their milkshake. But there\u2019s another possibility that you\u2019ve considered, which is the Berserker hypothesis, which is a bit similar. Do you want to explain that? Anders Sandberg: So, the name the Berserker hypothesis haven\u2019t got much to do with inebriated Vikings rushing around. It\u2019s named after a series of science fiction novels by Fred Saberhagen, which is based on the idea that some alien civilization in the remote past built self-replicating killing machines. Not just autonomous lethal weapons, but star-faring autonomous lethal weapons that could build more of themselves. And then they wiped out the creators and now they\u2019re around wiping out all life they can find. Now, this idea has been around in the discussions about why we\u2019re not observing any aliens, like, \u201cOh, maybe something like this is true. There are something dangerous out there wiping out civilization. The only surviving civilizations are the very quiet ones that don\u2019t make a fuss of themselves, and that\u2019s why we\u2019re not observing anything.\u201d This has never been a super popular theory, not just because it\u2019s slightly dark, but also because it seems so darn science fictional, and many people are turned off from that because it seems like you can\u2019t make a rigorous argument about it. But I have been looking into it and I think I have some decent argument why this seems unlikely. And you can actually make an ecological argument. If you have self-replicating systems hiding out there amongst the stars, what if somebody were releasing another strain of them? Well, they would need to wipe that one out, of course. It\u2019s also an opponent. So, when you try to model the ecosystems, you find that it doesn\u2019t seem to be a stable equilibrium situation. Instead you end up with more and more resources being used to build more and more killing machines against the other killing machines and you end up with a universe that looks very different from ours. Basically, it\u2019s gigantic space warfare everywhere, which should be pretty obvious. But there\u2019s not- Robert Wiblin: So, this would make sense if one group did it, but not if it was a very common thing that almost all civilizations do when they\u2019re very numerous, because then you would see a huge ware going on, basically. Anders Sandberg: Yeah. And even if you don\u2019t have that many civilizations, it needs to be robust. In order to work as an explanation against the Fermi Paradox, it needs to be able to withstand that a civilization shows up somewhere and tries to get in on the game. So, they need to be effective at wiping out civilizations, and if civilizations manage to launch their own replicators, they must somehow be able to wipe out those replicators, too. And it\u2019s an interesting thing, but took us about 100 years since Hertz sent the first radio waves until we were doing fairly regular space flight. We still haven\u2019t really colonized the solar system, but we could imagine a counterfactual world where we really pushed for that, and then we would probably have something like the 2001 universe with the moon base and things like that. So, I think it\u2019s not implausible, but you could, maybe takes 100 to 200 years if you\u2019re fast for a civilization to go from radio to being out in space and being very hard to wipe out. So, that would suggest that if you have these self-replicating killer machines out there, they need to intervene within a century. Otherwise, civilizations are going to get away, and you don\u2019t have a proper explanation for the Fermi paradox. Now, you can do a probability test, given that we\u2019re still around here, and that\u2019s starting to show that it looks pretty implausible that this hypothesis is true. It\u2019s more likely that actually there are no killing machines, because otherwise the system would seem to actually not work very well. Now, there is a sting in the tail, though, because what\u2019s the best defense against self-replicating machines that want to attack you? Well, you want your defense machines. You want your police system. You want to actually have your own defenses out there, and if they notice some alien device trying to blow up your star, or throw asteroids at you, or do something else nasty, it should stop it. So, you need your police systems and they need to be widely dispersed and self-repairing and basically end up with self-replicating interstellar killing machines. But these ones are on our side, so they\u2019re Freedom Fighters, and the police, and lawful. So, you have a symmetric situation. I think advanced civilizations will probably surround themselves with a swarm of support systems, whether they\u2019re really intelligent and independent or very simple might depend very much on values and safety. But you should expect them to have a lot of autonomous systems that actually support them and protect them. So, that might actually be the most likely artifact we find from an alien civilization. Robert Wiblin: When I heard this hypothesis, my initial reaction was, \u201cWhy would anyone make these probes?\u201d So, if you\u2019re the first group in the universe and you assume that you\u2019re alone, why would you create Berserker killing machines that kill you and kill anything? Is this meant to be an accident or a deliberate strategy? Anders Sandberg: So, in Saberhagen\u2019s novels, I think the idea was that it was an accident. And you can certainly imagine a civilization that does something stupid. You can also imagine a civilization that has really weird values, saying like, \u201cOh, life is really bad. We are radical negative utilitarians, we just want to wipe out all life henceforth, so let\u2019s unleash these devices even though they attack us.\u201d That might seem relatively unlikely. You can also have something like the green aliens that decide, \u201cOh, the universe looks lovely and we\u2019re gonna prevent people from industrializing it or changing it too much. So, we actually just want to have these out and keeping things in order.\u201d And then you can, of course, add more and more various assumptions. Like somebody thinking, \u201cMaybe there are green aliens out there. We don\u2019t know yet whether we want to industrialize the universe, but we want to retain our options, so we want to have our police systems getting our space.\u201d Basically you have quite a lot of very different goals that lead to the launch of these systems. Certainly there might be some civilizations just going, \u201cWhat? Why should anybody care about this? We\u2019ve got it nice here on our planet. We\u2019re gonna stay here. We\u2019re not gonna mess with it.\u201d But if they don\u2019t do that, then we\u2019re of course going to be at the mercy of a civilization that do some form of this replicating, spreading influences. So, it seems to be an attractive point, even though what you then use it for is very dependent on your own aims philosophically and culturally. Robert Wiblin: But you\u2019ve come up with some arguments for thinking that this actually isn\u2019t as likely as it might first seem. So, what probability do you place on it, all things considered? Anders Sandberg: I think I would assign much less probability to this, perhaps one percent or less. Robert Wiblin: That\u2019s a relief, I guess. Anders Sandberg: It\u2019s a bit of a relief. It\u2019s also an interesting challenge, because we might then consider should we do it? Even if we think that we might be alone in the universe, actually having an expanding infrastructure could be useful, even if we haven\u2019t yet decided what it\u2019s for, especially if there might be something dangerous or scary out there. So, at the very least, many people would say we should be sending out replicating probes just to observe the universe. Now, the problem here is of course, if you have something that replicated a number of times, you might worry about it evolving. Errors happen, mutations happen. Many people underestimate how unreliable you can make technology. You can actually use error correcting codes to make it very unlikely. Essentially, probability one, it will not once in the history of the universe actually change its programming. That is very, very easy to do if you design it well. However, you might also say, \u201cYeah, but you can also have cultures and settlements. If we go out and colonize space, some of those settlements in space, we\u2019ll also colonize.\u201d And now you have another form of replication. Not so much our machines, but our cultures, and cultures change. So you could imagine a culture being very keen on colonizing, and they will of course want to send out many colony ships and colonizing other planets, and before you know it, you have a lot of pro-colonization planets that are sending out more and more ships, and then you eventually approach some kind of interstellar intergalactic locust storm just living for settling more and more worlds, which might not be a good use of the resources of the universe. So, this scenario is called Burning the Cosmic Commons. It\u2019s originally by Robin Hanson. But notice what\u2019s going on here. You have a lot of generations that allow you to converge to something. If you could spread over vast regions without having too many steps, you might avoid this. So, in a paper I did with Stuart Armstrong, we analyzed exactly how far can you spread using the resources of one solar system, and we found that if you take apart the planet Mercury, which today might sound like a very tall order, but on the astronomical scale, it\u2019s actually fairly small. It takes a few decades if you do it at a leisurely pace. You can build enough solar collectors to use an afternoon sunlight to launch probes to all reachable galaxies. These probes don\u2019t have to be very large. Essentially you want something that can land on an asteroid, build solar panels, build more equipment, mine that, and build something including of course, things to seed other systems. Robert Wiblin: So, you just need the seed to get things started? Anders Sandberg: Yes. Robert Wiblin: Although I guess it has to be able to grow in a wide variety of different environments that you don\u2019t know where you\u2019re gonna be arriving. Anders Sandberg: That is an interesting issue, because when we start talking about space colonization, typically we are stuck in this mode of thinking that there has to be the kind of blonde guy from the space patrol landing his spacecraft on a weird planet where the sky has some random color. And in that case, you get a lot of very different environment, because we are interested in this very unusual terrestrial planet. But if you look at the solar system, most surfaces are essentially asteroid regolith. It\u2019s dry gravel in microgravity with various levels of water and iron in it. Robert Wiblin: And solar energy reaching them. Anders Sandberg: Yeah. They are very, very homogeneous. So, if you can make something that can mine that kind of asteroid, and they seem to be ubiquitous across the universe, then you have an ecological niche which is ridiculously large. Yes, there are gas giants and planetary surfaces that are very different that these devices would not be directly able to make use of, but this is already enough to build a lot of stuff, including of course maybe habitats where you can then culture the cells and have humans grow up, or if you have artificial intelligence or uploaded minds, live in software. The point is, the universe is actually very homogeneous, so you could spread across the big universe over ridiculous distances. It\u2019s actually easier to send a space probe that can travel over interstellar distances to another galaxy than across our galaxy. And the reason is when you have a space probe that\u2019s moving very fast, if it runs into a piece of gravel, bang. That\u2019s going to do a lot of damage. The closer you are to light speed, the more energy you get and essentially when you are in a few tenths of percent of light speed, now you\u2019re starting to look like grenades and small nuclear explosions, which is not very good for your probes. You might want to send more redundancy, but if you have dust cloud, you\u2019re pretty likely to run into something. But between the galaxies, the coast is very clear. So, if we could send probes at a distance of 7.7 light year, then we could hop between stars and reach most of the Milky Way. If it goes slightly shorter, then we kind of strand on a little island. If you can send it at 7.7 light year, you can probably send much more, and it turns out that to reach essentially all galaxies within five giga parsec, well, they only need to be able to travel a few million light years in order to do that. But most likely, if you can travel a few million light years, you can use the same probe and travel a billion light years. So, the main problem here in expanding over ridiculously large distances is basically how much dust is in the way. That determines how many probes you need to send or if you need to go around with us, and how fast can you go, which is perhaps the most important thing. Because right now, we have sent a probe that could have reached another solar system, the Voyager probe. It\u2019s not aimed anywhere in particular, it\u2019s just going to float through space. But we could have aimed it so it would eventually get to our solar system in a few tens of thousands of years. It wouldn\u2019t do anything useful, of course, except bearing that plaque and that recording of the music of Earth, but with 1970s technology, we could already reach the stars. We might want to go much faster, of course, and that is the interesting part here. Speed seems to be what really wins. So, when we\u2019re considering whether we should start colonizing the universe now or wait, it actually turns out that if we wait for a while, and the universe expands, some galaxies become harder to reach, but we presumably have better technology and once you can move really fast, that\u2019s actually the best way of reaching remote galaxies. So, you really want to go very close to light speed. Not super close, because then you run into dust. But if you can get up to 90% of light speed, you get most of the reachable universe. There is an upper limit, because there are galaxies we can see, but we can never, ever touch. Even if we went at exactly light speed, we would never catch up with them before they ran away from us because of expansion. Robert Wiblin: And how is this probe gonna slow down once it gets to far away galaxies? Anders Sandberg: So, in our paper, we did models where we imagined that it essentially had a little rocket. So, we assumed the launch was using a coil gun, essentially electromagnetic coils accelerating the probes to a higher and higher speed. You can probably do this much better with a laser, but we didn\u2019t do that calculation in the paper. But when it arrives, you would have this retro rocket, then we analyzed what could happen if you had a nuclear rocket, a fusion rocket, and an antimatter rocket. And that is, of course, pushing the limits of technology. We haven\u2019t build that kind of rocket very well yet. But we think we understand the physics well enough. However, you might not even need to slow down that way, because the expansion of the universe means that you actually need a lot of velocity to catch up with remote galaxies. So, if you\u2019re aiming at the remote galaxy, as you\u2019re traveling, it will move away from you faster and faster, and if you time things right, you will arrive with zero velocity. Your rocket will kind of just drift in and park itself there. So, you might actually not need all of this fancy technology to slow things down. Or you might use things like creating an electromagnetic field around it to slow it down by passing through gas clouds. So, there are quite a lot of options here, but I do find it fun, the fact that the expansion of the universe itself can be used as a braking method. Robert Wiblin: So, if you were really committed to colonizing as much of the universe as you could, would the best strategy be to spread out to the surface of the sphere that you can reach in every direction, and then once you\u2019ve reached the outer limit, then move back inwards, back towards the core? Anders Sandberg: Not really, because by the point you reach that outer limit, the farthest galaxy we can reach, now the center, the solar system has become unreachable. Robert Wiblin: Okay. Anders Sandberg: Because- Robert Wiblin: You can\u2019t get back. Anders Sandberg: Yes. There exists actually a distance about halfway from an event horizon that is the outer part we can reach, which is the point where that\u2019s the further you can go and then send a message back to Earth. So, if we actually want to inventory everything that exists, see if there is any aliens, and we are committed to staying ourselves on Earth, that is as far as we can go. We can only get about 2.5 giga parsecs worth of data. So, the best strategy I think is you send out probes, a few of them, to every reachable galaxy, and that\u2019s literally millions if you only travel by a few percent of light speed and billions if you\u2019re kind of at 90% of light speed. And once they have arrived somewhere in that galaxy, that\u2019s a random star, they pick up a random asteroid, they start building infrastructure. They build a little Dyson shell, and now we send out 300 billion probes to the nearby stars in the galaxy. Now we have two generations from the solar system to every solar system that can be reached. Now, this of course, avoids that Burning of the Cosmic Common thing. We only use a ridiculously small amount of resources, essentially a Mercury sized planet in every galaxy. Big on our current civilizational scale, of course, but on the cosmological scale, this is totally invisible, it\u2019s really tiny. And what do you do with all this? Well, that depends on your values. That depends on the programming you put into the probes. So, this might be that you just get the data, you might set up policing so no aliens or human colonies do anything. You might colonize it all. You might set it up so after aestivating in the far future, you\u2019re gonna fill it with happy human or alien minds. Or something else. So, this is an estimate of how much power any intelligent species can have over the material universe. Robert Wiblin: What\u2019s the name for this strategy of you\u2019re trying to reach everywhere very quickly? Anders Sandberg: So, the paper, we called that Eternity in Six Hours, but my nickname for the paper is Spamming the Universe. Robert Wiblin: Why six hours? Anders Sandberg: Well, it turns out that you only need six hours of sunlight to actually power all the probes to go to all of the reachable galaxies. Robert Wiblin: Nice. Nice. We\u2019ve done about an hour here on these grand possible futures across the entire universe. Maybe let\u2019s bring it back towards more practical decisions that people might be able to make now. Anders Sandberg: Well, I think it\u2019s actually worth recognizing also why one should want to think about this form of stuff, because this is really about the value of a very, very far future, and it\u2019s actually the path that led me to think about existential risk and many more other practical things. As a kid, I read Barrow and Tipler\u2019s The Anthropic Cosmological Principle, which is an amazing work, and the final chapter was talking about intelligence taking over the universe, overcoming the heat death of the universe, and essentially affecting all things affectable. That was kind of my brush with religion as a young nerd. Over time, their physical theory has had some problems, but the overall idea about what is the value of the far future is important, because that gives us a reason, of course, to try to defend the future. We want to avoid existential risk that could mean that we would never get this grand future. We might want to avoid doing stupid things that limit our future. We might want to avoid doing things that create enormous suffering or disvalue in these futures. So, what I\u2019ve been talking about here is kind of our understanding about how big the future is, and then that leads to questions like, \u201cWhat do we need to figure out right now to get it?\u201d Some things are obvious, like reducing existential risk. Making sure we survive and thrive. Making sure we have an open future. Some of it might be more subtle, like how do we coordinate once we start spreading out very far? Right now, we are within one seventh of a second away from each other. All humans are on the same planet or just above it. That\u2019s not going to be true forever. Eventually, we are going to be dispersed so much that you can\u2019t coordinate, and we might want to figure out some things that should be true for all our descendants. Robert Wiblin: So, your point is that once we\u2019ve spread out across the universe so far, then there\u2019s no way of sending messages to tell everyone what to do. They all have to be working on the original instructions and interpreting them in the right way, otherwise they\u2019ll get out of sync? Anders Sandberg: Exactly. And for many situations, this is totally fine. You don\u2019t need a central planet to tell you what kind of art or philosophy to work on, perhaps. But we might want to have some ground rules that\u2019s always true. If we encounter aliens, well, what are our plans for how to divide the universe with them? Because once you spread out of sufficiently long distances, given that it doesn\u2019t look likely that the supraliminal communication and travel is possible, that means that parts of your civilization will never be in causal contact. And there might be things for the really far future where we need to coordinate across these causal boundaries. Some really big engineering things involves actually planning that, \u201cOkay, in a hundred billion years, we want the galaxy clusters to be organized like this.\u201d There might be moral things that we want to avoid certain moral hazards. And that suggests that we might want to think about how we want to set up a process now, over the next few hundred years. Whether that is years, or decades, or centuries, when we\u2019re all on one planet or in one solar system, before we start doing the grand stuff, so we don\u2019t mess it up too much. Robert Wiblin: Reminds me slightly of, I think during the British Empire, there were various points when a particular colony could be out of touch with London for months or even years, \u2019cause it took so long to send messages and people weren\u2019t traveling there very frequently. But it\u2019s a much more extreme example of that where you never get messages back and you can never talk to them anymore. Interesting. Robert Wiblin: OK that wraps up the first half of my interview with Anders! In the next section we\u2019ll cover efforts to end ageing, the likelihood of nuclear war and the unilateralist\u2019s curse, among other topics. In the meantime let your friends know to subscribe! The 80,000 Hours Podcast is produced by Keiran Harris. Thanks for joining, talk to you next week. The 80,000 Hours Podcast features unusually in-depth interviews with people working to solve the world's most pressing problems. We invite guests pursuing a wide range of career paths - from academics and activists to entrepreneurs - to share their wisdom, so that you can better understand the world and have a greater impact with your career.  The 80,000 Hours Podcast is produced and edited by Keiran Harris. Get in touch with feedback or ideas by emailing keiran [at] 80000 hours [dot] com. Subscribe by searching for 80,000 Hours wherever you get podcasts, or click one of the buttons below: If you're new, see the podcast homepage for ideas on where to start, or browse our full episode archive. Join our newsletter Receive our career guide to your inbox, as well as monthly updates on our latest research, events near you and career opportunities. We're affiliated with the University of Oxford's Future of Humanity Institute and the Oxford Uehiro Centre for Practical Ethics. We're part of the Centre for Effective Altruism, and work closely with Giving What We Can. 80,000 Hours is part of the Centre for Effective Altruism, a registered charity in England and Wales (Charity Number 1149828) and a registered 501(c)(3) Exempt Organization in the USA (EIN 47-1988398). We do our best to provide useful information, but how you use the information is up to you. We don\u2019t take responsibility for any loss that results from the use of information on the site. Please consult our\u00a0full legal disclaimer and privacy policy. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 80,000 Hours is part of the Centre for Effective Altruism, a registered charity in England and Wales (Charity Number 1149828) and a registered 501(c)(3) Exempt Organization in the USA (EIN 47-1988398). We do our best to provide useful information, but how you use the information is up to you. We don\u2019t take responsibility for any loss that results from the use of information on the site. Please consult our\u00a0full legal disclaimer and privacy policy. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","time":1525798103,"title":"Why don\u2019t we see aliens? This Oxford prof suspects they\u2019re 'sleeping'. Heres why","type":"story","url":"https:\/\/80000hours.org\/podcast\/episodes\/anders-sandberg-fermi-paradox\/","label":7,"label_name":"random"},{"by":"a_w","descendants":0,"id":17022388,"kids":"None","score":2,"text":" Perspective  Interpretation of the news based on evidence, including data, as well as anticipating how events might unfold based on past events   On Tuesday, Amistad Press, a division of HarperCollins, will release Zora Neale Hurston\u2019s long-unpublished first book, \u201cBarracoon: The Story of the Last \u2018Black Cargo,\u2019\u201d edited by Deborah G. Plant. In late April, Vulture published excerpts from the book, which the magazine said had \u201clanguished in a vault\u201d since 1931. I\u2019m thrilled by the publication of Hurston\u2019s short book on such an important subject \u2014 but I deeply wish that we could stop talking about unpublished manuscripts in such terms. In many cases, it\u2019s not, as such language suggests, scholarly neglect that hides these works from the public eye. Instead, the trouble begins with onerous and excessive copyright protections, protections that are meant to profit the Walt Disney Co. more than they are intended to enrich our understanding of American literature. It\u2019s a problem that I\u2019ve come to know well. Over the years, I\u2019ve brought out \u2014 as a scholar or as an editor \u2014 previously unpublished work by Walt Whitman, Mark Twain, Robert Frost, Jack London, Paul Laurence Dunbar, Ezra Pound, and, yes, a group of Hurston\u2019s unpublished writings, edited by Pamela Bordelon. When I was selecting from the Hurston materials that Bordelon had collected, now nearly 20 years ago, I also obtained a copy of the typescript of \u201cBarracoon\u201d from the Smithsonian Archives. I knew about the typescript from reading Robert A. Hemenway\u2019s description of it in \u201cZora Neale Hurston: A Literary Biography,\u201d which had been published in 1977. The typescript was thin, just over a hundred pages, with a few emendations and additions in Hurston\u2019s handwriting, but it seemed complete and worthy of note. I looked into getting it published \u2014 but the rights to the work were unclear. Had the writing been conducted as part of Hurston\u2019s fieldwork for the Federal Writers\u2019 Project \u2014 making it a government work-for-hire and public domain? Or was it a separate literary work controlled by her estate? No one seemed to know, and no one was too interested in finding out. Unable to get answers, I eventually gave up on the effort. Now, according to the Vulture introduction, the Zora Neale Hurston Trust has new representation, interested in getting unpublished works into print and monetizing those archives. That\u2019s great, from a reader\u2019s perspective, but it also reveals a larger problem where scholarship of literature between World War I and II is concerned. It\u2019s mostly due to the Walt Disney Co.\u2019s efforts to protect ownership of a certain cartoon mouse. Over the years, the company has successfully worked to extend copyright restrictions far beyond the limits ever intended by the original authors of America\u2019s intellectual property laws. Under the original Copyright Act of 1790, a work could be protected for 14 years, renewable for another 14-year term if the work\u2019s author was still alive. In time, the maximum copyright grew from 28 years to 56 years and then to 75 years. In 1998, Sonny Bono championed an extension that would protect works created after 1978 for 70 years after the death of the author and the copyright of works created after 1922 to as long as 120 years. This worked out great for Disney \u2014 which, not coincidentally, was founded in 1923 \u2014 but less so for the reputations of authors who produced important work between the 1920s and 1950s. Because copyright law became such a tangle, many of these works have truly languished. Here, Hurston is the rule rather than the exception. I have a file that I\u2019ve kept over the years of significant unpublished works by well-known writers from the era: William Faulkner, Langston Hughes, William Carlos Williams, Hart Crane, Sherwood Anderson and Weldon Kees, among others. The works aren\u2019t really \u201clost,\u201d of course, but they\u00a0are tied up in a legal limbo. Because of the literary reputations of those writers, their unpublished works will eventually see the light of day \u2014 whenever their heirs decide that the royalties are spreading a little too thin and there\u2019s money to be made from new works. But other important writers who are little-known or unknown will remain so because they don\u2019t have easily identifiable heirs \u2014 or, worse, because self-interested, or even uninterested executors, control their estates. [If you miss Gawker, don\u2019t let Peter Thiel buy its archives] Take, for example, the case of Lola Ridge. In 2011, former poet laureate of the United States Robert Pinsky, in a column for Slate, called Ridge \u201ca terrific poet,\u201d and more than that \u201can early Modernist, radical in her politics, and an ardent feminist.\u201d She was, by his estimation, a link between William Blake and Crane, but Ridge\u2019s poems had mostly been out of print for nearly 70 years, in part because her body of work straddled the 1923 divide between public domain works and works controlled by copyright \u2014 and, for 40 years, the executor of Ridge\u2019s estate claimed to be working on a volume of collected poems as well as a biography, so she was unwilling to let any other scholars lay claim to the work. In 2007, Daniel Tobin published a slim volume of Ridge\u2019s public domain poems. Nearly a decade later, enough time had passed that Ridge had been dead for 70 years, making it possible for Terese Svoboda to publish Ridge\u2019s correspondence as part of her magisterial biography. Now, Tobin has released an expanded edition of Ridge\u2019s poems, including those from a volume published in 1927 (which were renewed but are now public domain), along with unpublished juvenilia. But the last two books published during Ridge\u2019s lifetime, \u201cFirehead\u201d (1929) and \u201cDance of Fire\u201d (1935), remain tied up. The net effect is a slow-drip rediscovery, which has largely hampered efforts to restore Ridge to her rightful place in the canon. The point is: Copyright laws rewritten by major corporations to preserve income from nearly century-old creations have all but erased a generation of less famous writers and unknown works by well-known writers. It\u00a0is an effect that extends the life span of biases that have long silenced female writers, minority writers and working-class writers. \u201cBarracoon,\u201d to return to the original example, was rejected for publication in 1931, because it was deemed too vernacular by Hurston\u2019s editor. Current copyright law unintentionally conspired to unnaturally extend the duration of that wrongheaded judgment for decades. That is why I bridle at the description of works like \u201cBarracoon\u201d as \u201clost.\u201d They are not lost \u2014 they have always been here \u2014 but they have repeatedly encountered power structures that block their publication. It\u2019s time for that to change.","time":1525798100,"title":"How copyright law hides work like Zora Neale Hurston\u2019s new book from the public","type":"story","url":"https:\/\/www.washingtonpost.com\/news\/posteverything\/wp\/2018\/05\/07\/how-copyright-law-hides-work-like-zora-neale-hurstons-new-book-from-the-public\/?noredirect=on","label":7,"label_name":"random"},{"by":"rmason","descendants":0,"id":17022381,"kids":"None","score":2,"text":"Uber is showing off its latest \u201cflying car\u201d concept at its second annual Elevate conference in Los Angeles. And, as you can see, it looks more like a drone than a helicopter.  The aircraft, which the company hopes to use to launch as an aerial taxi service by 2023, is a mashup of a plane and a helicopter. Instead of a tiltrotor, this design has four stacked rotors along the spine to give lift, which then stow away during landing. There is also a fifth rotor on the tail to allow forward propulsion. If one rotor fails, the others will continue to operate for a safe landing.   \u201cStacked co-rotating rotors or propellers have two rotor systems placed on top of each other rotating in the same direction,\u201d Uber says. \u201cInitial experimentation of this concept has revealed the potential for significantly quieter performance than traditional paired rotors and improved overall performance.\u201d These aircraft will be electrically powered, and Uber says they\u2019ll fly at an elevation of 1,000 to 2,000 feet. The company envisions thousands of its flying taxis shuttling passengers between rooftop \u201cskyports\u201d and landing sites in cities, each of which will be equipped to handle 200 takeoffs and landings every hour. The aircraft will be piloted by humans at first, but eventually will fly autonomously.  To be sure, Uber won\u2019t be building this exact aircraft. The prototype is meant to serve as a platform for Uber\u2019s manufacturing partners, like Bell Helicopters, Embraer, and Pipistrel, to use as they build their own flying taxi concepts.  The prototype serves as an eye-catching centerpiece for the company\u2019s two-day Elevate conference, which brings together representatives from the aviation industry, real estate, infrastructure, and government regulators. This year, Uber CEO Dara Khosrowshahi will also be making an appearance.    Uber first introduced its plan to bring ride-sharing to the skies in 2016, but the project still faces significant hurdles. The kind of aircraft Uber envisions shuttling passengers from rooftop to rooftop \u2014 electric, autonomous, with the ability to take off and land vertically (also known as eVTOL, pronounced ee-vee-tol) \u2014 don\u2019t really exist yet, nor does the infrastructure to support such a vehicle. Experts suggest that engineering and regulatory hindrances may prevent flying cars from ever taking off in a meaningful way. That\u2019s not to say flying cars aren\u2019t having a moment: at least 19 companies are developing flying-car plans. These include legacy manufacturers like Boeing and Airbus, and small startups like Kitty Hawk, owned by Google founder Larry Page. Meanwhile, Uber has made significant strides in partnering with a handful of aircraft manufacturers, real estate firms, and regulators to better its chances of developing a fully functional, on-demand flying taxi service. Uber has signed a Space Act Agreement with NASA to create a brand-new air traffic control system to manage these low-flying, possibly autonomous aircraft. Expect more news to break this week as the conference kicks off.  Command Line delivers daily updates from the near-future.","time":1525798060,"title":"Uber reveals its latest \u2018flying car\u2019 prototype for aerial taxi service","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17330524\/uber-flying-car-elevate-prototype-la","label":7,"label_name":"random"},{"by":"prohor","descendants":0,"id":17022380,"kids":"None","score":2,"text":"Microsoft May 2018 Patch Tuesday Fixes 67 Security Issues, Including IE Zero-Day Office 365 Zero-Day Used in Real-World Phishing Campaigns Microsoft Adds Support for JavaScript Functions in Excel John Legend and 6 Other Voices Being Added to Google Assistant Microsoft May 2018 Patch Tuesday Fixes 67 Security Issues, Including IE Zero-Day John Legend and 6 Other Voices Being Added to Google Assistant Adobe Patch Tuesday Is Out With Fixes for Flash Player, Creative Cloud, Connect Equifax Releases Data Showing the Full Impact of Its 2017 Data Breach Skype Classic GPU-Z InsaneCrypt (desuCrypt) Decrypter GIBON Ransomware Decryptor AdwCleaner ComboFix RKill Junkware Removal Tool Remove the FastDataX.exe Trojan DNS Unlocker & DNSUnlocker Ads Removal Guide (2018 Update) Remove the Watch Folder Trojan Downloader Remove the ExpertChange PUP Remove Security Tool and SecurityTool (Uninstall Guide) How to remove Antivirus 2009 (Uninstall Instructions) How to Remove WinFixer \/ Virtumonde \/ Msevents \/ Trojan.vundo How to remove Google Redirects or the TDSS, TDL3, or Alureon rootkit using TDSSKiller Locky Ransomware Information, Help Guide, and FAQ CryptoLocker Ransomware Information Guide and FAQ CryptorBit and HowDecrypt Information Guide and FAQ CryptoDefense and How_Decrypt Ransomware Information Guide and FAQ How to Change Your Twitter Password How to Setup Login Verification in Twitter How to Use Cortana As Your Virtual Assistant in Windows Restrict What Personal Data Is Shared on the Facebook API Platform How to start Windows in Safe Mode How to remove a Trojan, Virus, Worm, or other Malware How to show hidden files in Windows 7 How to see hidden files in Windows eLearning IT Certification Courses Gear + Gadgets Security  At the Build 2018 developer conference that's taking place these days in Seattle, USA, Microsoft announced support for custom JavaScript functions in Excel. What this means is that Excel users will be able to use JavaScript code to create a custom Excel formula that will appear in Excel's default formula database. Users will then be able to insert and call these formulas from within Excel spreadsheets, but have a JavaScript interpreter compute the spreadsheet data instead of Excel's native engine. \"Office developers have been wanting to write JavaScript custom functions for many reasons,\" Microsoft says, \"such as: (1) Calculate math operations, like whether a number is prime. (2) Bring information from the web, like a bank account balance. (3) Stream live data, like a stock price.\" The feature has only been announced and is not supported in the Excel stable distributions. \"Custom [JavaScript] functions are now available in Developer Preview on Windows, Mac, and Excel Online,\" Microsoft said in a support document. Users must join the Office Insiders program and install a custom add-in to test or use custom JavaScript functions. Users have also been requesting for years that Python be supported as an official scripting language in Excel. Albeit Microsoft said last year it was considering this highly-requested feature, there was no announcement at Build 2018 regarding Python support. Besides support for custom JavaScript functions, Microsoft also announced support for \"Microsoft Azure Machine Learning services,\" as a way to use machine learning algorithms to extend Excel's built-in formula database. In addition, Microsoft also announced that Excel would also be able to import custom charts and reports from Power BI, a business analytics service provided by Microsoft that renders interactive visualizations. Last but not least, Microsoft also announced \"Flow\" integration in Excel, giving users the ability to send data from their spreadsheets hosted in SharePoint and OneDrive for Business to many services such as Teams, Dynamics 365, Visual Studio Online, Twitter, etc. \"The [Flow] integration will first be shipped as an add-in in the Office Store and will become an in-the-box component later this year,\" Microsoft said. You can check out these Forbes and Verge articles for other Microsoft products announced at Build 2018. Microsoft Partners with Intel to Deliver CPU Microcode Fixes via Windows Updates Microsoft May 2018 Patch Tuesday Fixes 67 Security Issues, Including IE Zero-Day Office 365 Zero-Day Used in Real-World Phishing Campaigns GLitch Technique Enrolls Graphics Cards in Rowhammer Attacks on Android Phones Somebody Tried to Hide a Backdoor in a Popular JavaScript npm Package Not a member yet? Register Now Get 98% off the 2018 CompTIA Certification Training Bundle: Lifetime Access Drupal Sites Fall Victims to Cryptojacking Campaigns To receive periodic updates and news from BleepingComputer, please use the form below.  User Agreement -  Privacy Policy Copyright @ 2003 - 2018  Bleeping Computer\u00ae LLC  - All Rights Reserved Not a member yet? Register Now Learn more about what is not allowed to be posted.","time":1525798050,"title":"Microsoft Adds Support for JavaScript Functions in Excel","type":"story","url":"https:\/\/www.bleepingcomputer.com\/news\/microsoft\/microsoft-adds-support-for-javascript-functions-in-excel\/","label":9,"label_name":"tech"},{"by":"stardomSerf","descendants":0,"id":17022372,"kids":"None","score":2,"text":"\n\nCyan4973\n        released this\n          May 7, 2018\n LZ4 v1.8.2 is a performance focused release, featuring important improvements for small inputs, especially when coupled with dictionary compression. LZ4 decompression speed has always been a strong point. In v1.8.2, this gets even better, as it improves decompression speed by about 10%, thanks in a large part to suggestion from @svpv . For example, on a Mac OS-X laptop with an Intel Core i7-5557U CPU @ 3.10GHz,\nrunning lz4 -bsilesia.tar : Compression speeds also receive a welcomed boost, though improvement is not evenly distributed, with higher levels benefiting quite a lot more. Should you aim for best possible decompression speed, it's possible to request LZ4 to actively favor decompression speed, even if it means sacrificing some compression ratio in the process. This can be requested in a variety of ways depending on interface, such as using command --favor-decSpeed on CLI. This option must be combined with ultra compression mode (levels 10+), as it needs careful weighting of multiple solutions, which only this mode can process.\nThe resulting compressed object always decompresses faster, but is also larger. Your mileage will vary, depending on file content. Speed improvement can be as low as 1%, and as high as 40%. It's matched by a corresponding file size increase, which tends to be proportional. The general expectation is 10-20% faster  decompression speed for 1-2% bigger files. Finally, variant LZ4_compress_destSize() also receives a ~10% speed boost, since it now internally redirects toward primary internal implementation of LZ4 fast mode, rather than relying on a separate custom implementation. This allows it to take advantage of all the optimization work that has gone into the main implementation. When compressing small inputs, the fixed cost of clearing the compression's internal data structures can  become a significant fraction of the compression cost. This release adds a new way, under certain conditions, to perform this initialization at effectively zero cost. New, experimental LZ4 APIs have been introduced to take advantage of this functionality in block mode: More detail about how and when to use these functions is provided in their respective headers. LZ4 Frame mode has been modified to use this faster reset whenever possible. LZ4F_compressFrame_usingCDict() prototype has been modified to additionally take an LZ4F_CCtx* context, so it can use this speed-up. Support for dictionaries has been improved in a similar way: they can now be used in-place, which avoids the expense of copying the context state from the dictionary into the working context. Users are expect to see a noticeable performance improvement for small data. Experimental prototypes (LZ4_attach_dictionary() and LZ4_attach_HC_dictionary()) have been added to LZ4 block API using a loaded dictionary in-place. LZ4 Frame API users should benefit from this optimization transparently. The previous two changes, when taken advantage of, can provide meaningful performance improvements when compressing small data. Both changes have no impact on the produced compressed data. The only observable difference is speed.  This is a representative graphic of the sort of speed boost to expect. The red lines are the speeds seen for an input blob of the specified size, using the previous LZ4 release (v1.8.1) at compression levels 1 and 9 (those being, fast mode and default HC level). The green lines are the equivalent observations for v1.8.2. This benchmark was performed on the Silesia Corpus. Results for the dickens text are shown, other texts and compression levels saw similar improvements. The benchmark was compiled with GCC 7.2.0 with -O3 -march=native -mtune=native -DNDEBUG under Linux 4.6 and run on an Intel Xeon CPU E5-2680 v4 @ 2.40GHz. The content of lz4frame_static.h has been folded into lz4frame.h, hidden by a macro guard \"#ifdef LZ4F_STATIC_LINKING_ONLY\". This means lz4frame.h now matches lz4.h and lz4hc.h. lz4frame_static.h is retained as a shell that simply sets the guard macro and includes lz4frame.h. This release also brings an assortment of small improvements and bug fixes, as detailed below :","time":1525797976,"title":"LZ4 gets even faster","type":"story","url":"https:\/\/github.com\/lz4\/lz4\/releases\/tag\/v1.8.2","label":3,"label_name":"dev"},{"by":"cozzyd","descendants":0,"id":17022370,"kids":"None","score":1,"text":"  A federal judge yesterday permanently dismissed a lawsuit that Chicago-area homeowners had filed against Zillow over what the homeowners said were inaccurate and unfair practices related to the site's \"Zestimates\" of homes' value.  In the suit, an amended version filed in October after U.S. District Judge Amy St. Eve dismissed the first version in August, the homeowners alleged that Zestimates violate Illinois' laws against deceptive trade practices and consumer fraud. (Read the judge's opinion below.) Zillow, according to the homeowners' suit, leads homebuyers to believe that its Zestimates are precise calculations of a home's current market value, when in fact they're often inaccurate and difficult for the homeowner to get changed based on improvements Zillow's database may not have known about. St. Eve dismissed the lawsuit, writing in her opinion yesterday that \"Zestimates are not false or misleading representations of fact likely to confuse consumers,\" but are \"merely an estimate of the market value of a property, as supported by Zillow.com's statements that Zestimates may not be accurate.\" She dismissed the suit with prejudice, meaning the homeowners can't revise and refile it as they did last fall. On a section of its site that discusses Zestimates' accuracy, Zillow says that in the Chicago area, a Zestimate is likely to come within 5 percent of the actual sale price about 57 percent of the time.  Barbara Andersen, the Glenview lawyer who represented the homeowners, did not respond this morning to requests for comment on the dismissal. \"It\u2019s unfortunate because we have so many properties that Zillow has estimated at  50 percent or less than the asking price,\" says Vip Patel,  a homebuilder who along with his family's Schaumburg company, CastleBldrs.com, were plaintiffs in the suit. \"It\u2019s a shame the numbers are so unrealistic.\"   In an emailed statement, Zillow said: \"We are pleased that the court has dismissed the claims in this lawsuit not once, but now twice, finding the allegations in the lawsuit without merit. The Zestimate has proven itself to be a sought-after and valuable free tool for consumers.\" More: Home value still down? Join the (big) club Where homes are selling fastest How housing could help Chicago bag Amazon's HQ2  A year ago, Andersen sued Zillow in Cook County  over what she called a \"sloppy, computer-driven\" Zestimate of her home's value. In that complaint, she alleged that the Zestimate, which was about 11 percent lower than her asking price of $626,000, had created a \"roadblock\" to selling at what she thought the home was worth.  Andersen later replaced that suit with one where she represented Patel and his family's firm, which has been building houses for about 16 years. Patel said at the time that \"in our experience, Zillow is always at least 20 percent lower than the home's market value, but that's the number they have planted in the buyer's head.\"  A Barrington couple later joined the suit. Nancy and Ulysses Koutropoulos were asking $849,000 for a Barrington house whose Zestimate was $795,000. They have since reduced their asking price to $799,500. They could not be reached for comment.  In those cases, Andersen alleged that Zestimates act like appraisals, which are precise calculations of value done by professionals who in Illinois are subject to strict guidelines. In her suits, Andersen sought to block the use of Zestimates in Illinois.  Zillow maintained its stance that the Zestimates are always promoted as an estimate, a starting point and expressly not an appraisal.  St. Eve agreed when dismissing that part of the suit last summer. She wrote that Zillow clearly labels the Zestimates as estimates.  In October, Andersen refiled, basing the complaint on claims that Zestimates amount to deceptive trade practices and consumer fraud in Illinois, in part because Zillow gives preferential placement on its site to real estate brokers and lenders who advertise on the site.  But Zestimates do not meet the Illinois standard of deceptive practices or consumer fraud, St. Eve wrote, because Zillow does not present them as statements of fact and does not attempt to deceive consumers.     Zillow lawsuit decision by AnnRWeiler on Scribd TELL US WHAT'S ON YOUR MIND Share your thoughts on Twitter, Facebook, LinkedIn and Google+. \n\t\t                Copyright \u00a9 2018 Crain Communication, Inc.\n\t\t            ","time":1525797968,"title":"Judge tosses out lawsuit against Zillow's Zestimates","type":"story","url":"http:\/\/www.chicagobusiness.com\/realestate\/20180508\/CRED0701\/180509865\/lawsuit-against-zillows-zestimates-dismissed","label":7,"label_name":"random"},{"by":"chas","descendants":0,"id":17022368,"kids":"None","score":1,"text":"It can be difficult at various stages of learning Haskell to see how the parts come together or how to use particular abstractions. This reference aims to ease that process by providing concrete examples of Haskell abstractions in a simple context. In particular, it demonstrates how abstractions are used by sequentially rewriting a program to do exactly the same thing using different techniques so that you can use your understanding of one code section to understand the new abstractions or techniques introduced in the next one. This is not intended as a Haskell tutorial in full, but it should answer questions once you have them. In addition, it is not intended as a primer for fancy type features and focuses more on term-level techniques in Haskell programming. All of these programs implement a basic question and answer game that generates a sequence of addition or subtraction problems with random operands between 0 and 100. If the user answers correctly, it prints \"Correct!\" Otherwise, it prints out a message followed by the correct answer. In addition, it keeps track of how many questions the user got right or wrong and displays this after every round. If you want to have a go at implementing this, I would strongly suggest doing so now. I think this is a good candidate for this sort of exercise because many new users find Haskell's treatments of non-termination, state, randomness, and user-interaction unintuitive and this program includes all of those while being simple enough that most people reading this shouldn't find the game logic confusing. Note: I don't think many of these examples are actually idiomatic Haskell. They are far more complicated than they need to be for such a simple program. The intent is to use these examples to understand more complex programming techniques and then apply those techniques to far lager and more complex programs. In addition, this exercise isn't meant to show off Haskell in particular, since basically any language or style in common usage will do a good job with a small, simple program. Instead, simplicity and familiarity of the program is meant as a point of stability and understanding as more complex tools are introduced. The first example is in Java to provide people with little or no Haskell experience a point of reference for what all of the other programs in this sequence do. Because this is intended as a starting point, anyone not already confident with basic Haskell programming should make sure they understand exactly what this program is doing before moving on. The Haskell implementation of this program demonstrates a very common pattern in functional programming in which a stateful computation with a loop is replaced by a function that calls itself with updated parameters. In the same way that each time through a loop the state variables reflect the previous executions of the loop body, each time the function is called the parameters it is called with reflect the previous executions of the function. This potentially infinite recursion isn't a problem in Haskell because it is such a common pattern that the runtime was written with it in mind. This program is structured in two parts. In main, the first function called, I set up the initial state of the program and call gameLoop with the initial state. gameLoop has three stateful things it is concerned with: a sequence of random numbers to turn into problems, the number of questions that have been answered correctly, and the total number of questions asked. Each one of these values is then passed as a parameter to the gameLoop function, which is then updated when the function calls itself recursively after answering a question. There have been no questions answered and no questions asked at the start of the game, so right and rounds are both initialized to zero. The Java code gets random numbers from calling nextInt and nextBoolean repeatedly in order to get an infinite sequence of random numbers. In the Haskell version, I chose to create an infinite list of random values explicitly and pass it to the gameLoop. It can then remove values from the list using pattern matching and use them as needed. Laziness ensures that the program doesn't try to evaluate an infinite number of random values. The flushPut function is defined here to ensure that output from the function is immediately seen by the user and isn't buffered. If you are not comfortable with do-notation in Haskell: In the next few examples, anywhere that you see an expression like val <- ioVal, the value on the right side (ioVal) has type IO a and the value on the left side (val) has type a. To make this concrete, in keepPlaying <- getLine, getLine has type IO String and keepPlaying has type String. The way that do-notation and its related typeclass work ensures that you can't use it to write a function of type IO a -> a, which would be a huge problem because you could use it to do IO anywhere, break a lot of programs, and confuse everyone. The value extraction with <- works like that locally, but the type of the whole function most be of the form a -> ... -> IO b for some b. In these programs, b is always () which is used in Haskell the way void is used in Java. Stephen Diehl supplies more information here and here. In addition, I will supply a de-sugared version of some of the programs so you can compare and see that it all boils down to function application. At many points in this document, I will specialized the types of various polymorphic functions in order to make them less abstract and thus easier to understand in context. By specialize, what I mean is replace type-level variables and typeclass instances with the specific types they are being used with which have the appropriate typeclass instances. For example, these are the polymorphic functions that do-notation desugars to: And these are their specialized types: And these are the (slightly specialized) types of the functions that interact with IO and thus can be used with those functions: This is the same program without the syntactic sugar for do-notation. This next version introduces use of the <$> function. It can be thought of as a $ (function application) function that has been modified to work in more circumstances. The $ function performs low-precedence function application so that things like even (x + 3) can be replaced with even $ x + 3. It takes a function from a -> b and an a, which produces a b by calling the function with the provided a. It is very common to want to operate on values \"inside\" of another type. So if we take a normal function, such as even, we can directly call it on numbers (as in even 3), but we can't call it on values that represent possible failure such as Maybe Int because the Int may be mising. We can achieve this by performing all of our logic \"inside\" the failure type Maybe a. Correctly operating \"inside\" of a type representing failure means that if the type passed to the function represents failure, the return value of the function also represents failure. In this case, we have a type of Maybe Integer and a function even :: Integer -> Bool which doesn't know or care about the possibility of failure elsewhere in the system because Integer can't fail, it can only be even or not even. To complete the example, calling even on a possibly missing integer looks like even <$> Just 3 or even <$> Nothing which evaluate to Just False and Nothing respectively. These are the types of the function application functions for reference: In the case of Maybe a, operation \"inside\" the type means that you can take a function from a to b (a -> b) and turn it into a function from Maybe a to Maybe b (Maybe a -> Maybe b). Maybe can have two possible values, Just a and Nothing, so to implement this, the <$> function needs to return Nothing if the input is nothing, otherwise call the supplied function on the value held by Just and wrap it in a Just. <$> also goes by the name fmap and any type which has a Functor instance implements what fmap (<$>) means for that type. In short, if you use this machinery, Haskell will automate your null checks because the author of the Maybe type explained how to null-check in general. With that said, in this version of the code, I'm not doing anything particularly sophisticated using <$>. I'm mostly using it to clean up some of the noise around directly assigning values of type IO a to a variable just to extract the a, like in keepPlaying <- getLine. To me, this seems kind of pointless, like it doesn't reflect the actual logic. What I was thinking when writing that was \"Check if the user input 'y'\", not \"Get a line from the user. See if the keepPlaying variable contains 'y'\". It's not a big difference, but I find it annoying. In this case, the type we are operating \"inside\" is IO, and readLn's value is IO Int so we can reduce a bit of the noise that we pass to when by using (\"y\" ==) . map toLower :: String -> Bool with <$> to call it with IO String to produce an IO Bool. After passing through the do-notation syntactic sugar, keepPlaying holds a Bool value, which can be passed to when. Similarly, we call (solution ==) with a value of IO Int to produce an IO Bool and then the Bool is extracted with <-. If the process of using <- to get at a Bool seems like a similar operation to working \"inside\" of types with <$>, this isn't a coincidence: there is a deep relationship between the Functor typeclass and the Monad typeclass, if you would like to dig in, the Typeclassopedia is probably the best place to start. (Bartosz Milewski does a good job giving a taste of the theory as well.) In my opinion, using <$> frequently makes code simpler by removing extraneous intermediate values and reflecting the view of functors as lifted function calls, but it can obscure meaning especially when using the functor instances for common containers. The other small style tweak I applied converted the flushPut function to so-called pointfree form. Basically what this means is that I removed explicit mention of the variables passed to the function and expressed it entirely in terms of function composition. That is to say, I turned the function into a pipeline of components. As long as the pipeline isn't too complex, this can make functions clearer to read because you can be absolutely sure when reading them that they don't do anything fancy beyond plumbing their components together. This is a little bit complicated by the fact that >> is being partially applied to the value hFlush stdout, so (>> hFlush stdout) :: IO () -> IO () is composed with putStr :: String -> IO () to produce a function of type (>> hFlush stdout) . putStr :: String -> IO (). To specialize the type of the function composition function for this use, it looks like this: (.) :: (IO () -> IO ()) -> (String -> IO ()) -> (String -> IO ()). What I mean is that if you look up the type of (.), it is (.) :: (b -> c) -> (a -> b) -> a -> c which means that (.) will work for any types that you choose for a, b, and c. It can sometimes be hard to understand the type signatures when they are presented in such generality so it can be useful to plug in the specific types that are in use in the particular situation of interest. In this instance a is String, b is IO (), and c is IO () so the type of the whole function is (.) :: (IO () -> IO ()) -> (String -> IO ()) -> (String -> IO ()). Pointfree code can be simpler and easier to read, especially once you are familiar with the common idioms, but can obscure meaning when taken to an extreme. In this version, I collected all of the state of a game into a single record, rather than passing in each parameter individually. This has the advantage of showing explicitly that the information passed to this function is all required for a single game, rather than coming from separate sources for separate aspects of the function. To follow this theme of concentrating state-specific code, I introduced an updateGame function which creates a new game record from an old one and the knowledge of whether the player won or lost. At this point, I think the machinery is starting to overwhelm the essential complexity of the problem and probably wouldn't write code like this for something so simple under other circumstances. It's generally a good idea to use records if you find yourself passing the same arguments to several functions in your program or if you are using tuples with some sort of implicit meaning e.g. as 2D vectors. They do have some syntactic overhead so I wouldn't normally use one for just one function like this. This section introduces Haskell's State type and associated Monad instance, as well as the StateT monad transformer. Using StateT removes the need to explicitly pass around the Game state variables as explicit function arguments while still giving access to IO operations. Most interesting programs that involve interaction require some amount of state to persist throughout their execution. As we previously saw, we can thread this state through an unbounded number of recursive calls to the same function in order to simulate a persistent state using only stateless functions. It can be tedious to explicitly pass additional extraneous variables to each function that needs the state, but we can take advantage of the fact that do-nation is strictly syntactic sugar over the >>= operator from the Monad typeclass, and use it to thread our state record Game around for us. The type connected to the implicit passing of a state type is suitably called State. Previously, all do-notation was syntactic sugar for manipulation values of type IO. We would like to keep doing I\/O in this program, so I don't want to completely replace the IO sugar with State sugar. Instead, I use a type called StateT to wrap the IO type and produce a StateT Game IO (). After this conversion is done, the gameLoop :: Game -> IO () function no longer takes any explicit parameters and instead is a stateful value that holds a Game state and can perform I\/O, to write it in Haskell: gameLoop :: StateT Game IO (). In larger applications several monad transformers are frequently stacked together. In general, the transformation from IO to StateT Game IO is extremely mechanical, I just added a liftIO function to each function that returns IO and made the types line up. The usual disclaimer about the low essential complexity of the problem applies here too. Using State and StateT to manage \"mutable\" state can be a good fit if you have a large number of functions that operate on the same state such a collection of parsing functions which update a shared symbol table. In addition, actual state mutation is available in the form of ST if you want mutable state as a performance optimization. That said, implicitly passing state between functions can make them harder to understand, debug, and compose. Operations like getLine and putStrLn have types IO String and String -> IO () respectively and as such will not typecheck in an environment that expects values of type StateT Game IO a for some a. This is solved by using the liftIO :: IO a -> StateT Game IO a function that transforms IO-related things into StateT Game IO-related things. Note: liftIO is quite general and should work with any number of wrappings, I'm only giving a specialized type here in order to make the transformation between IO types and State-wrapped IO type more explicit. Finally, because the type of gameLoop is StateT Game IO (), it can't be called directly in main :: IO (), and since the function is operating on an implicit state, it needs an initial state. The function evalStateT :: Monad m => StateT s m a -> s -> m a takes an initial state and supplies it to the stateful computation that it represents, which then converts it into the underlying monad: in this case, IO. Fully specialized, the function has the following type: evalStateT :: StateT Game IO () -> Game -> IO (). To highlight particular parts of the conversion: This is the same program without the do-notation syntactic sugar. As a reminder, these are the specialized types of the desugared functions, used\nin this example: This is another minor clean-up version, but shows a couple of common (and not particularly arcane) practices. I extracted a function from the if-expression because I think it's a bit more straightforward and compact and does a better job of breaking up the logical portion of the program. I also compacted the the two lines where I update the game state into one as follows: Normally, the >> operator is hidden behind the syntactic sugar provided by do-notation and indeed it is inserted between these two operations in the previous version. You can tell because modify doesn't name its result using <- and get doesn't take any parameters beyond its shared state.\nI chose to make this explicit rather than using the do-notation sugar to reflect the fact that I really want a state update operation that returns the new state because I want to keep computing with the new state locally even after modifying it. >> is an operator that sequences operations while retaining the behavior encoded by do-notation. To understand what this means, remember that do { x <- foo; bar x } desugars to foo >>= \\x -> bar x, but since there are no variable bindings, do { baz; qux } desugars to baz >>= \\_ -> qux, which is the same as baz >> qux. Since >>= is a method of the Monad type class, it has a different meaning for each type class instance. In this case >>= is auto-connecting an implicit state parameter so >> just runs two operations where the second operation doesn't depend on the result of the first, but the second operation does see any modifications the first operation made to their shared state. In this context the type of >> is (>>) :: StateT Game IO () -> StateT Game IO Game -> StateT Game IO Game which is () -> Game -> Game lifted to respect the structure of StateT Game IO. This section introduces the use of a lens library, Control.Lens -- henceforth known as lens, which provides utility functions for manipulating data in a composable and generic way. If you have used the STL in C++, lens fits in a similar niche. Unlike the STL, it isn't standardized or official in any way with Haskell, it's just a library many people (including myself) like. All of the functions provided by lens work using a type of generalized getter\/setter functions called lenses. (Note: If this idea is interesting to you, but you don't want all of the bells and whistles in lens, there are other, simpler lens libraries that use the same representation as lens, such as lens-family.) Lenses are extremely mechanical to produce for standard data types, so it includes some Template Haskell functions which will write them for you. Normally Haskell provides projection functions when you write a record which are named after the record fields (e.g. rounds :: Game -> Int), so we start the names of record members with underscores so generated lenses won't conflict. After converting to using lenses for field access, anything that touched the game state needs to be rewritten to use the new combinators. I think this does simplify the updateGame function, but the logic around printing becomes slightly worse. The updateGame function previously used pattern matching to take apart the Game value and put it back together again. The version using lenses instead constructs three modification functions of type Game -> Game and composes them together using . (This is the same pipeline technique that was discussed in the context of flushPut). What goes into updateGame? We need to remove the first two random values because they were used for the last problem, we need to unconditionally increment the number of rounds, and if the user got the last problem correct, we should increment the number right. Let's look at each part separately because we know they don't interact due to their construction as composed functions. The crux of updating values is the %~ operator. This takes a lens and a function and creates a function that takes a record and modifies one of its fields using the provided function. The operator's function can be remembered as a pun on the common use of % as the mod (or modular arithmetic) operator. To look at this concretely, in this instance %~ has type: We can use the values lens that makeLenses built to satisfy the (Lens' Game [Int]) parameter leaving: We want to remove the first two elements of the infinite list of random values so the ([Int] -> [Int]) parameter should be drop 2 :: [a] -> [a] which will specialize to drop 2 :: [Int] -> [Int] when used with lists of Ints. Supplying the modification function produces the following: This is precisely a Game state update function which removes the first two entries in the values list. If you aren't into the operator business, %~ also goes by the name over so the previous function could be written over values (drop 2). By this logic, the function for updating the rounds could look like this, where the modification function is an increment function constructed through partial application of addition. This is a bit cluttered and likely very common, so lens provides the +~ combinator that lets you modify a field by adding a number. (Its name should evoke the += operator from many imperative programming languages.) Using this the rounds update can be written as There isn't a named analog for +~ so if you don't like the lens operators, this would be over rounds (+1). Finally the number right can be updated using a variation on the same logic using the correct :: Bool parameter passed into the updateGame function. Composing these together gives the complete updateGame function using lenses: Next, gets :: (Game -> a) -> StateT Game IO a, which used the values :: Game -> [Int] projection function, is replaced with use :: Lens' Game a -> StateT Game IO a which uses the values :: Lens' Game [Int] lens to get the values out of the state. These have precisely the same result, namely a value of type StateT Game IO [Int]. Finally, since lenses aren't projection functions, we have to update the message printing code as well. This is slightly more straightforward than in the case of use and get, because lens provides an operator for turning lenses into projection functions, (^.) :: s -> Lens' s a -> a or, specialized for this instance, (^.) :: Game -> Lens' Game Int -> Int. If you squint a bit, you can see that applying a lens for a Game field to the second argument will give you a function Game -> Int which is just a normal projection function. In this case show $ right game can be replaced with show $ game ^. right or show $ view game right, if you don't like the operators. In total, this leads to the following change: In summary for this section, introducing lens made the updateGame function slightly less verbose, but we didn't get any big wins. We'll see what this did for us once we make more use of the utility functions lens provides and have slightly more complex states to wrangle. The combination of StateT and lens gives us the ability to easily directly modify the Game state using similar looking operations to imperative languages, in this version, we remove the use of modify and updateGame. This approach explicitly states what can be manipulated and centralizes it into one type like in traditional (or more explicit) programming with stateless functions, but it uses Haskell library features to implicitly update the state as it's passed around. Understanding the new code requires knowing two more pieces of lens operator grammar: just as any stateless update operator ends in ~, stateful update operators end in = and update an ambient state. In our case this state type is StateT Game IO (). What this means is that if you would write i += 1 to update the value of a variable in an imperative language, you can use i += 1 in Haskell to update the field named i in your state record (in our case this record is Game). Additionally, you can can have a field-update operator return the new value of field by prepending <. This means the Haskell\/lens equivalent of x = ++i is x <- i <+= 1. Finally, the field-update operator will return the value before update if you prepend <<, which makes x <- i <<+= 1 the Haskell\/lens equivalent of x = i++. With these in hand, we can drop the first two values after we access them using <<%= instead of %~ as we did in updateGame. This will remove the first two elements of values and return the list as it was before modification so we can extract the dropped values with pattern matching. The rounds and right fields again use the same technique, but we replace +~ with <+= so that we update the ambient Game state, and get the new values for use in local computation. Since we can directly name the results of these updates, we can then remove the need to access the fields out of the updated game state and just use the results directly instead. This means that if at any point you want to understand the operation of the function for testing or debugging, you can still provide the appropriate state record and know that you have specified all of the information it depends on, but you don't have to worry about tracking these dependencies during other parts of development. That said you now have a stateful bummer to deal with, so use this stuff carefully.","time":1525797953,"title":"A Rosetta Stone for Haskell Abstractions","type":"story","url":"http:\/\/reduction.io\/essays\/rosetta-haskell.html","label":3,"label_name":"dev"},{"by":"chouquettesalad","descendants":0,"id":17022360,"kids":"None","score":1,"text":"Calling all lovers of Golang and quines, we have a challenge for you! Can you write a quine in Go? Send us your best effort and get a Gopher laptop sticker or plushie! There is a partial Golang quine to get you started. Can you complete it? package mainimport quine \"fmt\"var q = \"package main\\n\\nimport quine \\\"fmt\\\"\\n\\n[...?] If you prefer working with less constraints, we are not one to hinder creativity. Write your own from scratch and we'll reward any working\u00a0quines! Ending on June 15th 2018 at 4pm PST, we will be accepting quines of all kinds (... well, just Golang ones). Then the voting begins! We will post a link on this page to all the submissions for your voting pleasure. Our scrupulous reviewers will ensure only legit working Golang quines hit your eyeballs. After a week of voting, ending June 22nd at 4pm PST, the lucky winner will receive the trophy of all trophies, a Golang gopher plushie!  We will publish the winning quine on our website and social channels, either as anonymous, or un-anonymous, winners choice. Hate doing something without a guaranteed reward? No worries, same same. Every single person with a U.S. or Canada address who sends us an original, working quine in Golang will get a Golang gopher laptop sticker in the mail\u2014but all are encouraged to send us a quine no matter where you reside! Everyone is welcome to send in as many Golang quines as their heart desires; however, only one sticker per address. The personal information you provide us will only be used to get that prize in your hand. We will not sign you up for our super cool blog (unless you check that box), low-key slide your email over to our recruiter with a winky face, or sell it.  Copyright \u00a92017 Igneous Systems, Inc.\u00a0| All Rights Reserved2401 4th Ave, Suite 200, Seattle, WA 98121","time":1525797893,"title":"Golang Riddle","type":"story","url":"https:\/\/www.igneous.io\/golang-contest","label":7,"label_name":"random"},{"by":"sgt","descendants":1,"id":17022353,"kids":"[17022391]","score":1,"text":" \n                \u201cDesigned by Apple in California\u201d\u00a0chronicles 20 years of Apple\u00a0design\u00a0through 450 photographs\u00a0of our products and\u00a0the processes used to make them. A visual history spanning iMac to Apple Pencil, complete with\u00a0descriptions of innovative materials and techniques, it captures every detail with honesty and\u00a0intention. Printed on specially milled German paper with gilded matte silver edges, using eight color separations and low-ghost inks, this hardcover volume\u00a0took more than eight years to create\u00a0and has been crafted with as much care and attention\u00a0as the products featured within. It is both a testament and a tribute to the meticulous design, engineering, and manufacturing methods that are singularly Apple.\n             \n                Number of pages: 300\n             \n                Number of photographs: 450\n             \n                Photographer: Andrew Zuckerman\n             \n                Introduction: Jony Ive\n             \n                Book language: English\n             \n                Book insert: Translated into select languages, the insert offers descriptions of the products.\n             \n                Two sizes: 10.2 x 12.8 inches and 13 x 16.3 inches\n             \n                ISBN numbers: 978-0-9975138-1-3 and 978-0-9975138-0-6\n             \n                Release date: November 2016\n             \n                300 pages\n             \n                450 full-color photographs\n             \n                Both sizes have hardback linen covers.\n             \n                Introduction by Jony Ive\n             \n                280-line screen printing\n             \n                Specially milled German paper\n             \n                Gilded matte silver edges\n             \n                Low-ghost inks\n             Answers from the community \nAnswer now(What is the Net Weight of the Product and its packaging dimensions?)  \n                                                    Hi Alan, \r\n\r\nThe insert is will be in English \r\n\r\nHope you enjoy it!\n                                                 \nAnswer now(What can I do if I received the book and the insert is translated into a different language? I'm in another country and would prefer English.)  \n                                                    It appears it can only be shipped to states currently. You might ask your Apple Store in Europe. I'v\n                                                        It appears it can only be shipped to states currently. You might ask your Apple Store in Europe. I've seen this book, it's absolutely beautiful.\n\n\nMore(Read full answer)\n\n\n \nAnswer now(Do you ship oversees? How can I buy from the USA Shop being in Europe?)  More ways to shop: Visit an Apple Store, call 1\u2011800\u2011MY\u2011APPLE, or find a reseller. \n                    Copyright \u00a9 2018 Apple Inc.  All rights reserved.\n                 \nPrivacy PolicyTerms of UseSales and RefundsLegalSite Map  We thought so. However, the product you're looking for is no longer available on apple.com.  But we do have similar products to show you.","time":1525797853,"title":"Designed by Apple in California \u2013 10.2 x 12.8 inches","type":"story","url":"https:\/\/www.apple.com\/shop\/product\/MLXF2LL\/A\/designed-by-apple-in-california-102-x-128-inches","label":7,"label_name":"random"},{"by":"mooreds","descendants":0,"id":17022345,"kids":"None","score":1,"text":"\n\n\t\t5\/5\/2018\n\t\n \n\n\t\t0 Comments\n\t\n \n\n\n\n\t\tRSS Feed\n\t\n \nAll\n\nBaby\n\nBusiness World\n\nDae Posts\n\nDating & Marriage\n\nGod & Me\n\nGuest Posts\n\nLeadership\n\nOn Womanhood\n\nPeople Philosophy\n\nPersonal\n\n#postaweek2011\n\n#postaweek2017\n\nSocial Media\n\nStartup Chick\n\n \nMay 2018\n\nDecember 2017\n\nNovember 2017\n\nOctober 2017\n\nSeptember 2017\n\nAugust 2017\n\nJuly 2017\n\nJune 2017\n\nMay 2017\n\nApril 2017\n\nMarch 2017\n\nFebruary 2017\n\nJanuary 2017\n\nNovember 2016\n\nAugust 2016\n\nJune 2016\n\nApril 2016\n\nMarch 2016\n\nFebruary 2016\n\nDecember 2015\n\nNovember 2015\n\nOctober 2015\n\nSeptember 2015\n\nAugust 2015\n\nJuly 2015\n\nJune 2015\n\nMay 2015\n\nApril 2015\n\nMarch 2015\n\nFebruary 2015\n\nDecember 2014\n\nNovember 2014\n\nOctober 2014\n\nSeptember 2014\n\nAugust 2014\n\nJuly 2014\n\nJune 2014\n\nMay 2014\n\nApril 2014\n\nMarch 2014\n\nJanuary 2014\n\nDecember 2013\n\nNovember 2013\n\nSeptember 2013\n\nAugust 2013\n\nApril 2013\n\nMarch 2013\n\nJanuary 2013\n\nDecember 2012\n\nNovember 2012\n\nOctober 2012\n\nSeptember 2012\n\nAugust 2012\n\nJuly 2012\n\nJune 2012\n\nMay 2012\n\nApril 2012\n\nMarch 2012\n\nFebruary 2012\n\nJanuary 2012\n\nDecember 2011\n\nNovember 2011\n\nOctober 2011\n\nSeptember 2011\n\nAugust 2011\n\nJuly 2011\n\nJune 2011\n\nMay 2011\n\nApril 2011\n\nMarch 2011\n\nFebruary 2011\n\nJanuary 2011\n\nDecember 2010\n\nNovember 2010\n\nOctober 2010\n\nSeptember 2010\n\nAugust 2010\n\n","time":1525797825,"title":"Farewell GiftStarter","type":"story","url":"https:\/\/www.arryinseattle.com\/blog\/farewell-giftstarter","label":7,"label_name":"random"},{"by":"daurnimator","descendants":14,"id":17022337,"kids":"[17022625, 17023807, 17022672]","score":68,"text":"The blazingly fast open source microservice API gateway. The microservice API platform for large organizations. Kong Enterprise innovation delivered at cloud speed. Compare Kong editions and features. By Marco Palladino on May 8, 2018 \n\nHow To |             \n 4 \n\t\t MIN READ\n           Today we are excited to announce the\u00a0Kubernetes Ingress Controller\u00a0for Kong. Container orchestration is rapidly changing to meet the needs of software infrastructure that demands more reliability, flexibility, and efficiency than ever. At the forefront of these tools is Kubernetes, a container orchestrator that enables operations and applications teams to deploy and scale workloads that meet these needs while still enabling developers with self-service and a great developer experience. Critical to these workloads, however, is a networking stack that can support highly dynamic deployments across a clustered container orchestrator at scale. Kong is a high performance, open source API gateway, traffic control, and microservice management layer that supports the demanding networking requirements these workloads have. Kong, however, does not force teams into a one-size-fits-all solution. To serve traffic for a deep ecosystem of software and enterprises, Kong comes supplied with a rich plugin ecosystem that extends Kong with features for authentication, traffic control, and more. Deploying Kong onto Kubernetes has always been an easy process, but integration of services on Kubernetes with Kong was a manual process. That\u2019s why we are excited to announce the Kong Ingress Controller for Kubernetes. By integrating with the Kubernetes Ingress Controller spec, Kong ties directly to the Kubernetes lifecycle. As applications are deployed and new services are created, Kong will automatically live configure itself to serve traffic to these services.  This follows the Kubernetes philosophy of using declarative resources to define what we want to happen, rather than the historical imperative model of configuring servers how we want with a series of steps. In short, we define the end state. The ingress controller and Kubernetes advances the cluster to that state, rather than the end state being a side effect of actions we perform on the cluster. This automatic configuration can be costly when using load balancers that require a restart, reload, or significant time to update routes. This is the case with the open source nginx ingress controller, which is based on a configuration file that must be reloaded with every change. In a highly available, dynamic environment, this configuration reload can result in downtime or unavailable routes while nginx is being reconfigured. The open source edition of Kong and the Kong Ingress Controller has a full management layer and API, live configuration of targets and upstreams, and a durable, scalable state storage using either Postgres or Cassandra that ensures every Kong instance is synced without delay or downtime. Next, we\u2019ll show you how easy it is to set up the Kong ingress controller. We have a GitHub example and will walk you through the steps below. You can also follow Kong CTO and Co-Founder, Marco Palladino, through the setup steps in this demo presentation.  \u00a0 Getting started is just a matter of installing all of the required Kubernetes manifests, such as the ingress controller Deployment itself, a fronting service, and all of the RBAC components needed for Kong to access the Kubernetes API paths it needs to successfully work. These manifests will work on any Kubernetes cluster. If you are just getting started, we recommend using minikube for development. Minikube is an officially-provided single node Kubernetes cluster that runs on a virtual machine on your computer, and is the easiest way to get started working with Kubernetes as an application developer. Installation is as simple as running the following command: After installing the Kong Ingress Controller, we can now begin deploying services and ingress resources so that Kong can begin serving traffic to our cluster resources. Let\u2019s deploy a test service into our cluster that will serve headers and basic information about our pod back to us. This deploys our application, but we still need an Ingress Resource to serve traffic to it. We can create one for our dummy application with the following manifest: With our Kong Ingress Controller and our application deployed, we can now start serving traffic to our application. Plugins in the Kong Ingress Controller are exposed as Custom Resource Definitions (CRDs). CRDs are third party API objects on the Kubernetes API server that operators can define, allowing for arbitrary data to be used in custom control loops such as the Kong Ingress Controller. Let\u2019s add the rate limiting plugin to our Kong example, and tie it to our ingress resource. Plugins map one-to-one with ingress resources, allowing us to have fine grained control over how we apply plugins to our upstreams. Now that this is applied, we can cURL our service endpoint again and get the following response: We immediately see that our rate limiting headers are applied and available for this endpoint! If we deploy another service and ingress, our rate limit plugin will not apply unless we create a new KongPlugin resource with its own configuration and add the annotation to our new ingress. Now we have a fully featured API gateway ingress stack, complete with an application and rate limiting for it. You\u2019re ready to take control of your microservices with Kong! Keep up with the development of the Kong ingress controller, request features, or report issues on our Github repository. We look forward to seeing how you use it! Install and Scale Kong on Kubernetes \n\n                    Watch Now                  \n \n\n 7 \n\t\t MIN READ\n The Rapidly Changing API Gateway Landscape: present and future. Chapter 2 from the recently released book \u201cKong: Becoming a King of API Gateways\u201d.  Continue reading \n\n 1 \n\t\t MIN READ\n We are very proud to announce that Kong has attracted its 100th contributor! Continue reading \n\n 1 \n\t\t MIN READ\n Kong is a proud sponsor of GOTO, the enterprise software development conference designed for team leads, architects, project management, and organized by developers, for developers. Continue reading Keep up-to-date with Kong product releases and news.","time":1525797779,"title":"Kubernetes Ingress Controller for Kong","type":"story","url":"https:\/\/konghq.com\/blog\/kubernetes-ingress-controller-for-kong\/","label":3,"label_name":"dev"},{"by":"senthil_rajasek","descendants":0,"id":17022320,"kids":"None","score":1,"text":"ADVERTISEMENT MISSION CRITICAL Support credible science journalism. Subscribe to Science News today. SUBSCRIBE In the April 14\u00a0SN: Killer heat, mass insect migrations, the latest Saturn updates, rethinking the Nobel Prize, tectonics on Venus, the science of mass shootings, ancient tool trends and more.\u00a0 Science past and present Richard Feynman was never satisfied to simply know the answer to a problem; he had to puzzle the problem out for himself. U.S. Department of Energy, WikiCommons Richard Feynman was a curious character. He advertised as much in the subtitle of his autobiography, Surely You\u2019re Joking, Mr. Feynman!: Adventures of a Curious Character. Everybody knew that, in many respects, Feynman was an oddball. But he was curious in every other sense of the word as well. His curiosity about nature, about how the world works, led to a Nobel Prize in physics and a legendary reputation, both among physicists and the public at large. Feynman was born 100 years ago May 11. It\u2019s an anniversary inspiring much celebration in the physics world. Feynman was one of the last great physicist celebrities, universally acknowledged as a genius who stood out even from other geniuses. In 1997 I interviewed Nobel laureate Hans Bethe, a Cornell University physicist who worked with Feynman during World War II on the atomic bomb project at Los Alamos (and later on the Cornell faculty). \u201cNormal\u201d geniuses, Bethe said, did things much better than other people but you could figure out how they did it. And then there were magicians. \u201cFeynman was a magician. I could not imagine how he got his ideas,\u201d Bethe told me. \u201cHe was a phenomenon. Feynman certainly was the most original physicist I have seen in my life, and I have seen lots of them.\u201d Apart from his brilliance as a physicist, Feynman was also known for his skill at playing the bongo drums and cracking safes. Public acclaim came after he served on the presidential commission investigating the explosion of the space shuttle Challenger. In a dramatic moment during a hearing about that disaster, he dipped material from an O-ring (a crucial seal on the shuttle\u2019s rockets) into icy water, demonstrating that an O-ring would not have remained flexible at the launch-time temperature. His autobiography had already become a best seller, so Feynman was well-known when he died in February 1988. When I heard of Feynman\u2019s death, I called John Wheeler, Feynman\u2019s doctoral adviser at Princeton University before World War II. \u201cI felt very lucky to have him as my graduate student,\u201d said Wheeler, who died in 2008. \u201cThere was an immense vitality about Feynman. He was interested in all kinds of problems.\u201d Feynman\u2019s curiosity was not satisfied merely by being told the solution to a problem, though. \u201cIf you said you had the answer to something, he wouldn\u2019t let you tell it,\u201d Wheeler said. \u201cHe had to stand on his head and pace up and down and figure out the answer for himself. It was his way of keeping the ability to make headway into brand new frontiers.\u201d Feynman found fascination in all sorts of things, some profound, some trivial. In his autobiography, he revealed that he spent a lot of time analyzing ant trails. He sometimes entertained Wheeler\u2019s children by tossing tin cans into the air and then explaining how the way the can turned revealed whether the contents were solid or liquid. Curiosity of that type was instrumental in the work that led to his Nobel Prize. While eating in the Cornell cafeteria, Feynman noticed someone tossing a plate, kind of like a Frisbee. As the plate flew by, Feynman noticed that the Cornell medallion on the plate was rotating more rapidly than the plate was wobbling. He performed some calculations and showed that the medallion\u2019s rotation rate should be precisely twice the rate of the wobbling. He then perceived an analogy to a problem he had been investigating relating to the motion of electrons. The wobbling plate turned out to provide the clue he needed to develop a new version of the theory of quantum electrodynamics. \u201cThe whole business that I got the Nobel Prize for came from that piddling around with the wobbling plate,\u201d he wrote in his autobiography. It was not curiosity alone that made Feynman a legend. His approach to physics and life incorporated a willful disdain for authority. He regularly disregarded bureaucratic rules, ignored expert opinion and was willing to fearlessly criticize the most eminent of other scientists. During his time at Los Alamos, for instance, he encountered Niels Bohr, the foremost atomic physicist of the era. Other physicists held Bohr in awe. \u201cEven to the big shot guys,\u201d Feynman recalled, \u201cBohr was a great god.\u201d During a meeting in which the \u201cbig shots\u201d deferred to Bohr, Feynman kept pestering him with questions. Before the next meeting, Bohr called Feynman in to talk without the big shots. Bohr\u2019s son (and assistant) later explained why. \u201cHe\u2019s the only guy who\u2019s not afraid of me, and will say when I\u2019ve got a crazy idea,\u201d Niels had said to his son. \u201cSo next time when we want to discuss ideas, we\u2019re not going to be able to do it with these guys who say everything is yes, yes, Dr. Bohr. Get that guy and we\u2019ll talk with him first.\u201d Feynman knew that he sometimes made mistakes. Once he foolishly even read some papers by experts that turned out to be wrong, retarding his work on understanding the form of radioactivity known as beta decay. He vowed never to make the mistake of listening to \u201cexperts\u201d again. \u201cOf course,\u201d he ended one chapter of his autobiography, \u201cyou only live one life, and you make all your mistakes, and learn what not to do, and that\u2019s the end of you.\u201d Follow me on Twitter: @tom_siegfried \n                            Get Science News headlines by e-mail.\n                         View the discussion thread. 1719 N Street, N.W. , Washington, D.C. 20036|202.785.2255|\u00a9 Society for Science & the Public 2000 - 2018. All rights reserved. ","time":1525797672,"title":"A celebration of curiosity for Feynman\u2019s 100th birthday","type":"story","url":"https:\/\/www.sciencenews.org\/blog\/context\/celebration-curiosity-richard-feynman-birthday","label":7,"label_name":"random"},{"by":"lwf","descendants":0,"id":17022313,"kids":"None","score":7,"text":"Subscribe and get updates The easiest way to keep a secret is to not tell it to anyone. Unfortunately passwords don\u2019t work that way. Every time you sign in you have to tell the website your password, making it more challenging to keep the secret safe. That\u2019s why we recommend turning on two-step verification for your account, which adds an extra layer of difficulty for anyone who has guessed, eavesdropped on, or tricked you into giving them your password. And it\u2019s why we\u2019re excited today to announce support for WebAuthn (\u201cWeb Authentication\u201d) in two-step verification, a new standard for strong authentication on the web. In most forms of two-step verification, a user enters a one time code after providing their username and password, and before being signed in. While easy to adopt, using one time codes for two-step verification has weaknesses. For example, a fake Dropbox sign in page could ask for your username, password, and the two-step code. That\u2019s why Dropbox was one of the first services to adopt Universal 2nd Factor (U2F) for security keys in 2015. Security keys prevent phishing by giving Dropbox cryptographic proof that you both have your key and are using it on https:\/\/www.dropbox.com (instead of a phishing page). Introducing WebAuthn\nThis cryptographic proof makes U2F security keys a very strong form of two-step verification, but adoption of U2F has been limited by browser and hardware support. We hope WebAuthn will change that. It\u2019s a new way to interact with security keys and other \u201cauthenticators\u201d that standardizes and builds on key parts of U2F, the result of a collaboration between the W3C and FIDO Alliance. While for years only Chrome supported U2F, browser vendors have committed to bringing WebAuthn to Chrome, Firefox, and Edge. More and more devices will have WebAuthn support built in, bringing stronger security to the many users who don\u2019t own special security keys. These could include your laptop or phone, which might prompt you for your fingerprint or a PIN code as part of the authentication process. But this only matters if services actually let you use WebAuthn to securely sign in. Today, Dropbox is proud to help lead the way. What does this mean for me?\nYou\u2019ll now be able to use more types of security keys on more browsers for two-step verification. That starts with support for security keys in Firefox 60, releasing on May 9th. You can use security keys previously registered with U2F and register new ones with WebAuthn. Chrome and Edge support for WebAuthn will be coming soon, and you can still use your security keys in Chrome today with U2F. This means that as a user, you\u2019ll enjoy much stronger sign in security on more browsers. Unlike passwords, the secrets used in WebAuthn never leave your security key, so they are significantly harder to steal. And before using a secret to authenticate to Dropbox, the security key checks that you are signing in to the right place. You can feel confident when signing in that it\u2019s really us, and we can be confident it\u2019s really you. Will this replace passwords?\nRight now, we\u2019re using WebAuthn to make it easier for you to add an extra level of security to your account. A natural question is if we still need passwords too. Your credentials could be stored on a device like your phone, laptop, or security key, and services could use WebAuthn to sign in to your account after you scan your fingerprint or input a PIN on the device. There are still many security and usability factors to consider in these scenarios before replacing passwords entirely, and we believe that enabling WebAuthn for two-step verification strikes the right balance for most users right now. How do I use WebAuthn?\nTo start using security keys that support WebAuthn on Dropbox, take a look at our Help Center article. Curious to know more?\nWe collected a few helpful references with more technical details for you below: Introducing WebAuthn support for secure Dropbox sign\u00a0in MacOS monitoring the open source\u00a0way Protecting Security Researchers Please note: Sometimes we blog about upcoming products or features before they\u2019re released, but timing and exact functionality of these features may change from what\u2019s shared here. The decision to purchase our services should be made based on features that are currently available. \u00a9 Dropbox, Inc.","time":1525797651,"title":"Introducing WebAuthn support for secure Dropbox sign in","type":"story","url":"https:\/\/blogs.dropbox.com\/tech\/2018\/05\/introducing-webauthn-support-for-secure-dropbox-sign-in\/","label":3,"label_name":"dev"},{"by":"analogj","descendants":1,"id":17022306,"kids":"[17022328]","score":2,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back \n\n\n\n \n\n\n\n\n\n Bastion\/Jumphost tunneling made easy A Jump\/Bastion host is a special purpose computer on a network specifically designed and configured to withstand attacks.\nThe computer generally hosts a single application, for example a proxy server, and all other services are removed or limited to reduce the threat to the computer.\nIt is hardened in this manner primarily due to its location and purpose, which is either on the outside of a firewall or in a demilitarized zone (DMZ) and usually involves access from untrusted networks or computers. In secure cloud architectures, jump\/bastion hosts are the primary method to access the internal\/protected network.\nThis means that all traffic can be audited, and that a single server can be shut down in the event that the network is compromised. However as this architecture is scaled up and deployed across multiple environments (testing, staging, production), it can\nbe complicated to maintain a single ~\/.ssh\/config file that allows you to tunnel into your various jump host protected internal networks. Drawbridge aims to solve this problem in a flexible and scalable way. Using the questions & config_template defined in the configuration file (~\/drawbridge.yaml) Drawbridge will attempt to\ngenerate a managed ssh config file. Drawbrige will prompt the user for any questions which it is unable to determine an\nanswer (no default value and no flag value specified). Questions & Templates can be customized completely to match your organization. You can also enable DRYRUN mode to see exactly what files Drawbrige would generate, without actually writing any files. drawbridge connect will connect you to the bastion\/jump host using a specified Drawbridge config file. It'll also add\nthe associated PEM key to your ssh-agent. If you want to connect directly to a internal server, you can do so by selecting a config id and specifying the hostname\/short name drawbridge connect 1 database-1 You can use the --force flag to disable the confirm prompt. The --all flag can be used to delete all Drawbridge managed\nconfigs in one command. You can use the following command to completely wipe out all Drawbridge files and start over. drawbridge delete --all --force Downloading files through the bastion is simple and easy. PAC files, when used with a compatible browser, allow you to access internal dashboards and websites as you would any publicly accessible site. As you create Drawbride configurations, just run drawbridge proxy to update the PAC file, written to ~\/drawbridge.pac by default. We support a global YAML configuration file that must be located at ~\/drawbridge.yaml Check the example.drawbridge.yml file for a fully commented version. Drawbridge provides an extensive test-suite based on go test.\nYou can run all the integration & unit tests with go test $(glide novendor) CircleCI is used for continuous integration testing: https:\/\/circleci.com\/gh\/AnalogJ\/drawbridge If you'd like to help improve Drawbridge, clone the project with Git and install dependencies by running: Work your magic and then submit a pull request. We love pull requests! If you find the documentation lacking, help us out and update this README.md.\nIf you don't have the time to work on Drawbridge, but found something we should know about, please submit an issue. We're actively looking for pull requests in the following areas: We use SemVer for versioning. For the versions available, see the tags on this repository. Jason Kulatunga - Initial Development - @AnalogJ","time":1525797612,"title":"Drawbridge \u2013 Bastion\/Jump host tunnelling management","type":"story","url":"https:\/\/github.com\/AnalogJ\/drawbridge\/","label":4,"label_name":"github"},{"by":"nickysielicki","descendants":0,"id":17022301,"kids":"None","score":1,"text":"%PDF-1.4\n%\u00e2\u00e3\u00cf\u00d3\n1 0 obj\n<","time":1525797592,"title":"Estimating Solar Irradiance Since 850 CE [pdf]","type":"story","url":"https:\/\/agupubs.onlinelibrary.wiley.com\/doi\/pdf\/10.1002\/2017EA000357","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17022299,"kids":"None","score":1,"text":"It\u2019s here: our built-from-scratch platform of software, hardware, and payments that streamlines your whole restaurant operation. It\u2019s built for front of house and back of house. And it\u2019s built for speed. Customize your floor plan with all the sections you need, then use it to track crucial information like covers, when a table is about to turn, or how revenue centers are performing. Tap and drag items between courses as often as guests change their minds. And with one-tap firing, holding, and preset straight-firing, your kitchen is always in sync. From breakfast to lunch and seasonal specials, create all the menus you need and swap them out in a tap. Need to make changes? Be our guest\u2014customize menus anytime, from anywhere. Set how menus appear on your restaurant POS so servers can work fast. Make changes down to the size and color of buttons, and switch between Light or Dark Mode to match your restaurant\u2019s ambience. Modify orders how people talk. Group modifiers with items and have them automatically appear as servers progress through an order, so they never miss a thing. Keep your restaurant POS system safe by assigning roles to your team, like server or manager. Set their level of access to your data and control what they can do. Run reports for sales, comps and voids, covers, revenue centers, and more so you can turn customers into regulars while reducing costs. See data on individual server sales to find out who\u2019s performing and who needs more training\u2014all with your restaurant POS software. Whether you run one or many restaurants, manage them all from the same account. Access and update all your menus, employees, and reports\u2014plus make bulk changes that apply to multiple locations. Caviar is our very own award-winning delivery and pickup service\u2014so we built it right into Square for Restaurants. You\u2019ll be able to accept, fire, and dispatch orders all from the same restaurant POS tablet, and you\u2019ll have end-to-end order, delivery, and pickup data in one place, too.  Shop all hardware > Take payments at 2.6% + 10\u00a2 per tap, dip, or swipe. No need for a separate payments processor\u2014the Square POS system for restaurants lets you accept payments right away. Get funds as fast as the next business day or instantly for an additional 1% fee. Set your close of day so a full day of sales is always included in one deposit. Your account includes Square Secure, a suite of security tools. There\u2019s no extra work\u2014you get 24\/7 fraud prevention, Chargeback Management, PCI compliance, and more Make sure you get paid. Build Your Contract with Square helps you set expectations with your customers, avoid payment disputes, and provide a professional experience. Customize your account with easy-to-follow instructions, or opt for expert implementation support. Once you\u2019re up and running, we won\u2019t disappear: Get help whenever you need it. We want to make it painless to get started. An implementation expert can help you with menu, floor plan, and employee setup, team training, onsite technical support, and more. Starting at $600. No matter how late (or early) it is, if you need our help, we\u2019re here. Reach out via phone or email and we\u2019ll get you back up and running. 24\/7 Technical Support is included with your Square for Restaurants subscription. \u201cSquare for Restaurants is robust while retaining that trademark Square elegance and hassle-free service.\u201d There\u2019s a world of powerful services and software waiting for you\u2014and your Square account unlocks them all. Let your team clock in and out from your point of sale, then pay them with Square Payroll. Starting at $34 per month. Run a digital loyalty program to keep customers coming back. Customize everything from the way you award points to the rewards on offer. Starting at $25 per month. Get more business from your regulars and convert new customers to loyal guests by promoting your restaurant through email and social media campaigns. Starting at $15 per month. For eligible Square sellers, Square Capital offers access to small business loans to manage and grow your restaurant. Buy equipment, purchase inventory, and more.* Sync your restaurant POS system seamlessly with restaurant management software that helps you take care of everything from increasing sales to reducing costs. Payment processing fees apply during your 30-day free trial. Per location, includes one point of sale Per tap, dip, or swipe Add additional points of sale for $40 per month each. Have annual revenue over $250K and an average ticket size over $15? Contact Sales > *Square Capital, LLC is a wholly owned subsidiary of Square, Inc., d\/b\/a Square Capital of California, LLC in FL, GA, MT, and NY. All loans are issued by Celtic Bank, a Utah-Chartered Industrial Bank. Member FDIC, located in Salt Lake City, UT. Loan eligibility is not guaranteed. FW0418 \u00a9 2018 Square, Inc. Use reporting analytics to make better business decisions. Cover Reports: See how many guests and covers you have in any daypart, by time period or by employee. Shift Reports: Streamline end-of-day operations for staff with automatic cash owed reports and sales overviews. Discount Reports: See total discounts broken down by individual groups, such as Employees or Friends & Family. Modifier Sales: Review your gross sales by individual modifiers and modifier sets. Revenue Center Reports: Review your sales split by floor plans as well as by revenue centers. Labor Costing: Review your labor costing alongside sales reports to make more informed business decisions. Custom Reports: Generate tailored reports to support your unique workflow and needs. Taxable Sales: See your net sales broken down by taxable and non-taxable sales. Comp and Void Reports: See how many comps and voids have been processed, and who processed them. Employee Sales Reports: Review revenue-per-labor-hour reports, identify (and reward!) your most efficient employees, and see who closed which cash drawers. Product Mix: View sales by category or by specific item. Square for Restaurants is built for simple setup. But if you need more help, we offer one-on-one setup support through Square Implementation Services. For troubleshooting, get all the help you need with free, 24\/7 Technical Support. Paid, one-on-one support during setup. A dedicated implementation expert will tailor implementation to your restaurant\u2014everything from menu and floor plan setup to team training. Starting at $600. Set up a free consultation > Implementation: Remote point-of-sale configuration Onsite or remote implementation Device and hardware configurations Menu migration and development Floor plan setup Staff training and consulting Post-implementation: Onsite or remote tech support Changes in business ownership Employee or staffing changes Free support by phone or email, anytime. Our team of restaurant experts is here to answer your questions, day or night. Everyday troubleshooting: Account-level issues Point-of-sale features and functions Network or Wi-Fi connectivity Hardware Connections Printer routing and connections Payments and deposits Receipts Employee management Customer Disputes Run your restaurant better with our library of guides, videos, and support articles. Learn from fellow restaurateurs as they share tips, answer questions, and more.","time":1525797587,"title":"Square Restaurant POS System","type":"story","url":"https:\/\/squareup.com\/us\/en\/point-of-sale\/restaurants","label":7,"label_name":"random"},{"by":"farbodsaraf","descendants":0,"id":17022297,"kids":"None","score":5,"text":"CoinGenerator is an open-source command line tool which allows you to create your own proof of stake cryptocurrency with its blockchain.   - Fast and easy - Proof of Stake: environment friendly - Running on your own blockchain - Tutorial Step by Step available   - License makes you give 10% of the coins The fact that it is using proof of stake makes the big difference in this generator. To understand the different consensus systems used in cryptocurrencies I recommend this article: https:\/\/medium.com\/learning-lab\/proof-of-what-understand-the-distributed-consensuses-in-blockchain-1d9304ae4afe Buy Bitcoin & co with the best possible exchange rate in 30s Bitcoin & Blockchain investing, news & market data One-stop-shop for those interested in cryptocurrencies. Trade cryptocurrencies instantly, with no account needed Beautfiul Cryptocurrency Tracking for iPhone Bitcoin alerts. 100+ cryptocurrencies supported. How Bitcoin is challenging the global economy Cryptocurrencies at a glance","time":1525797581,"title":"CoinGenerator \u2013 Create your own cryptocurrency in less than 30 seconds","type":"story","url":"https:\/\/www.producthunt.com\/posts\/coingenerator","label":2,"label_name":"crypto"},{"by":"mofle","descendants":0,"id":17022286,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Display images in the terminal Works in any terminal that supports colors.  In iTerm, the image will be displayed in full resolution, since iTerm has special image support. MIT \u00a9 Sindre Sorhus","time":1525797501,"title":"Show HN: Display images in the terminal","type":"story","url":"https:\/\/github.com\/sindresorhus\/terminal-image-cli","label":4,"label_name":"github"},{"by":"iffycan","descendants":4,"id":17022258,"kids":"[17022589, 17022496]","score":2,"text":"On May 25th, GDPR goes into effect. And for the next few months\u200a\u2014\u200apossibly years\u200a\u2014\u200ait will bring smiles to hackers\u2019 faces. Let me explain: As a consumer, I like the protections GDPR hopes to provide. Namely: The rights to data access and data portability are interesting. These together are Ingredient Number 1. Companies are actually \u201cwilling\u201d to adhere to these mandates because the fines are nuts: Last year, my side-project made about $70. Not million. The thought of being fined 10,000,000\u20ac is terrifying. For the visually-inclined, here\u2019s a chart: And supposedly, those fines can extend to my U.S.-based company, though I\u2019m still not sure how that jurisdiction magic works. Regardless, the threat is scary\u200a\u2014\u200ainnovation-stiflingly scary. I\u2019ll bet more businesses choose not to do business. But for those that stay in business, this fear is Ingredient Number 2 for Hacker\u2019s Delight. Imagine a typical small\/medium, Internet business: Customer data is strewn about on various computers, servers, databases, spreadsheets and paper. Under GDPR, a customer can now request that this company provide them a dump of all their data. And if they don\u2019t, they can get a massive fine. I predict that lots of companies, out of fear, are going to hastily create data export features in their applications. These data export features are going to bring together all of a customer\u2019s data into one convenient place. No longer will hackers\u2026 er\u2026 I mean, customers have to go to all those disparate databases to collect their data. And even those that don\u2019t automate data export might be so scared by a request for data, that they won\u2019t properly authenticate the requester. So, for the near term, I think we\u2019re going to see more data privacy breaches because of GDPR. Eventually, it will level out as the fear subsides. Also, don\u2019t be hasty\u200a\u2014\u200ayou don\u2019t have to automate data portability and data export. Do it by hand until you need to automate it. Though make sure you authenticate. Just my thoughts. By clapping more or less, you can signal to us which stories really stand out. Dad and programmer - www.budgetwithbuckets.com","time":1525797285,"title":"GDPR: One Unintended Consequence","type":"story","url":"https:\/\/medium.com\/@iffy\/gdpr-one-unintended-consequence-7a151ad1cf4f","label":10,"label_name":"thought"},{"by":"xvirk","descendants":0,"id":17022256,"kids":"None","score":3,"text":"Google and Facebook, which together dominate the market for digital ads, will no longer allow bail bonds services, which advocates say prey on vulnerable people, to advertise on their platforms. Why it matters: From Russian election meddling to discrimination, there's a larger reckoning going on about harms that can come from the sprawling online ad ecosystem. Both companies have banned ads for high-interest payday loans already, as well as ads for cryptocurrency. The details: Behind the scenes: The Essie Justice Group, which advocates for the end of the money bail system, said it and other civil rights groups had been discussing the issue with Google since last year. \"Google\u2019s move to ban bail bonds ads is the most massive divestment any private sector entity has made from the bail industry,\" the group said in a message to supporters. Go deeper: Google will work with Koch Industries, owned by conservative donors David and Charles Koch, on an event this week related to reforming the bail system.","time":1525797276,"title":"Google, Facebook to ban bail bond ads","type":"story","url":"https:\/\/www.axios.com\/google-bail-bonds-ads-ban-1525720613-6cde91b2-6938-4273-ab86-e0211ee97292.html","label":7,"label_name":"random"},{"by":"putzdown","descendants":0,"id":17022242,"kids":"None","score":1,"text":"  Simon Cotton\nUppingham School, Rutland, UK  Molecule of the Month October 2003Also available: JSmol version.  Karen Wetterhahn was an internationally respected Professor of Chemistry, an expert researcher in the field of the effects of heavy metals upon living systems, especially in their role in causing cancer. By an exquisite irony, she became a victim of a heavy metal poison. The Albert Bradley Third Century Professor in the Sciences at Dartmouth College, New Hampshire, Wetterhahn was doing something few professors do - working at the lab bench - on August 14th 1996. She was studying the way mercury ions interacted with DNA repair proteins, and was using [Hg(CH3)2] as a standard reference material for 199Hg NMR measurements. Wetterhahn knew about the high toxicity of dimethylmercury and took very reasonable precautions, donning safety glasses and latex gloves, doing manipulations in a fume cupboard and only working with very small quantities behind the fume cupboard sash. Dimethylmercury, supplied by a chemical manufacturer, came in a sealed glass vial. A colleague of hers cooled the vial in ice-water to reduce the volatility of the Hg(CH3)2 then cut off the glass top to open it. Wetterhahn pipetted a small sample into a NMR tube and transferred the rest into a storage container, sealed and labeled the tubes and cleaned up, disposing of the latex gloves. Less than a year later, she was dead from the effects of mercury poisoning. Wetterhahn later recalled spilling a drop of dimethylmercury, possibly more, on her gloved hand. Test showed subsequently that this would have penetrated the glove and started entering her skin within 15 seconds. It is now accepted that the only safe precaution to take when handling this compound is to wear highly resistant laminated gloves underneath a pair of long-cuffed neoprene (or other heavy duty) gloves. In January 1997, she began to notice definite symptoms that worried her - tingly fingers and toes, slurred speech. She began to have problems with her balance and her field of vision started to shrink. Mercury poisoning was diagnosed on January 28th 1997. Tests revealed that she had a blood mercury level of 4000 micrograms per litre, 80 times the toxic threshold. Two weeks later she slipped into a coma from which she never recovered, dying on June 8th 1997. It is a liquid at room temperature, with a faint sweet smell (not a good career move). It has a boiling point of 92\u00b0C at atmospheric pressure and a density of 2.96 g\/cm3. It has a linear structure, like many HgX2 systems with Hg-C = 2.083 \u00c5. It is one of the most potent neurotoxins known. It readily crosses the blood-brain barrier, probably due to its formation of a methylmercury-cysteine complex. It causes ataxia (lack of coordination), sensory disturbance and changes in mental state. It inhibits several stages of neurotransmission in the brain. It is a cumulative poison, being very slowly removed (excreted) from the body, and by the time its effects are noted it is too late to do anything about it. Mercury is a \"soft acid\" so that it binds to easily polarisable donor atoms in \"soft\" bases. This gives the mercury ion a high affinity for sulphur and sulphur containing ligands. It will thus attack the thiol groups of enzymes and inhibit them. Zeise made the first mercaptan (R-SH) ligands and coined their name, based on the Latin expression mercurium captans, capturing mercury. The main ore of mercury is cinnabar, HgS (shown, right). It has been mined for 2500 years in places like Almaden in Spain. Other important mines are at Idria in Slovenia and Monte Amiata in Italy. In Roman times, criminals sentenced to work in quicksilver mines had a short life expectancy because of the toxicity of the mercury in cinnabar. It was regarded as a death sentence, and Pliny described the symptoms of mercury poisoning in the first century AD. Cinnabar was widely used as a pigment in the ancient world and its colour is recognised today (from 1982 to 1987, the author of these notes drove a Mini whose colour was described as \"cinnabar\" by the manufacturers). Unfortunately, there have been lots. Once upon a time, people used mercury and its compounds to treat syphilis. Although not successful, it enjoyed a vogue for a considerable time until penicillin was found to be an effective cure for this disease in its earlier stages. One important use of mercury compounds was in hatmaking; 200 years ago, the furs used to make beaver felt hats was dipped into mercury(II) nitrate solution as a preservative and to soften the animal hairs. Unfortunately the workers in the felt hat trade absorbed mercury through their skins; the resulting mercury poisoning caused shaking and slurred speech, being known as \"hatter's disease\", which is believed to have inspired the character of the Mad Hatter in Lewis Carroll's Alice in Wonderland, a character made famous in Tenniel's drawing (below). \r\nPicture from: http:\/\/www.rudnaya.com\/ Another source of poisoning was the (mis)use of alkyl mercury compounds used as fungicides on seeds. Between the World Wars, workers in fungicide manufacturing plants had developed mercury poisoning. In 1942 two young Canadian secretaries working in an office of a warehouse in Calgary, Canada, were fatally poisoned. The warehouse was storing diethyl mercury. In the 1960s, Swedish farmers noticed birds flopping helplessly on the ground then dying. Birds had been eating mercury-treated grain, or else rodents that had consumed it. More horrifying than this were epidemics of poisoning, caused by people eating treated seed grains. There was a serious epidemic in Iraq in 1956 and again in 1960, whilst use of seed wheat (which had been treated with a mixture of C2H5HgCl and C6H5HgOCOCH3) for food, caused the poisoning of about 100 people in West Pakistan in 1961. Another outbreak happened in Guatemala in 1965. Most serious was the disaster in Iraq in 1971-2, when according to official figures 459 died. Grain had been treated with methyl mercury compounds as a fungicide and should have been planted. Instead it was sold for milling and made into bread. It had been dyed red as a warning and also had warning labels in English and Spanish that no one could understand. That's debatable. Given what was known about the toxicity of mercury compounds at the time, people could have acted sooner, at the very least. In the early 1950s, inhabitants of the seaside town of Minamata, on Kyushu island in Japan, noticed strange behaviour in animals. Cats would exhibit nervous tremors, dance and scream. Within a few years this was observed in other animals; birds would drop out of the sky. Symptoms were also observed in fish, an important component of diet, especially for the poor. When human symptoms started to be noticed around 1956 an investigation began. Fishing was officially banned in 1957. It was found that the Chisso Corporation, a petrochemical company and maker of plastics such as vinyl chloride, had been discharging heavy metal waste into the sea. They used mercury compounds as catalysts in their syntheses. It is believed that over 1400 people were killed and perhaps 20000 have been poisoned to a lesser extent. Methylcobalamin, a coenzyme form of Vitamin B12, is capable of methylating \"inorganic\" mercury compounds to form CH3Hg+(aq), also by methylation of mercury itself. The actual mercury species present in solution may be CH3HgOH. The CH3Hg+(aq) ion is absorbed by plankton, which is in turn eaten by small fish.  The fish eat so much of the contaminated plankton, and they excrete the mercury so slowly, that it gradually builds up in their systems.  The small fish are then eaten by larger fish, and the concentration of mercury in the organism increases each time.   Animals and humans eating these larger fish concentrate the mercury even more, so that the final concentration in animals higher up the food chain (such as humans) can be thousands or millions of times larger than was present in the original water.  This process is known as \"biomagnification\" (see figure, right). Regrettably not, if problems connected with the use of mercury in mining precious metals are anything to go by. Mercury forms mixtures with many metals, known as amalgams. It is used to extract metallic gold from gold ores on account of this, the gold being recovered by distilling off the mercury. Most miners do not recover the mercury, so that it finds its way into rivers, where it gets converted into the methylmercury ion, and this has been a problem in California as a legacy of the 1849 Gold rush. It was used to refine Mexican silver in the 16th century. The (mis)use of mercury in gold mining is currently an issue in the Amazon region of South America, West Africa and other places. Mercury is cheap and convenient to use, and that's all some people care. Doing chemistry is safe, much safer than driving a car. A chemistry laboratory is about the safest place in a school or university, far safer than the sports field. It is only by ceaseless vigilance and attention to safety that it remains so.    Back to Molecule of the Month page.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[DOI:10.6084\/m9.figshare.5245807]","time":1525797199,"title":"The Karen Wetterhahn story (2003)","type":"story","url":"http:\/\/www.chm.bris.ac.uk\/motm\/dimethylmercury\/dmmh.htm","label":7,"label_name":"random"},{"by":"ezekg","descendants":0,"id":17022235,"kids":"None","score":3,"text":"\n          HIRE US!\n          \n What I find the most exciting trait of Kotlin is its philosophy of being a unified toolkit (language + set of libraries + tooling), but also relying on the native environment, everywhere it runs. Using Kotlin with the JVM means full interop with Java, in the browser we can access JavaScript libraries and Kotlin\/Native has tooling to interop with code written in C. What this means, is that we don't have to reinvent the wheel or spend ages rewriting existing code from scratch, rather we can continue building things on top of what we already have and improve our efficiency at the same time. When it comes to UI development, Kotlin suggests the same principle, e.g. the 2017 KotlinConf iOS app is written Kotlin, but screens and widgets consist of platform native elements like UIViewControllers and UIViews. The feasible use-cases of Kotlin are growing, so the question arises: how should you implement UI on each platform? On mobile it's easy, both Android and iOS have a rich ecosystem of design guidelines and widget systems to implement them. Today we are going to take a look at another platform that we are interacting with quite often: desktop. As we are transitioning to a \"web first\" and \"mobile first\" world, many desktop applications basically got implemented as a webpage, pre-packaged with a browser engine (which approach has its own issues). But there are other options as well, like JavaFX and in case of using it with Kotlin, an extremely powerful library called TornadoFX. In this article, I'll give you a whirlwind \ud83c\udf2a tour of what you can build with TornadoFX and Kotlin (pun intended). Getting started with TornadoFX is very simple. All you have to do is defining a dependency on no.tornado:tornadofx:1.7.15 and you are good to go. For now, TornadoFX only supports Java8, so you should explicitly tell the Kotlin compiler to target 1.8. Using Gradle it would look like this: From here, \"Hello World\" is just 4 lines of code away: \n\n Coming from e.g. an Android background, you'll feel like home quite easily. There are Views, there is an Application, and if you have ever used the Anko Layout DSL, you'll find a resemblance to how we defined the root variable. By extending class TornadoApp : App(MainView::class), TornadoFX takes care of a lot of things for us, but it is also providing ways to customize our application. In JavaFX, the top level UI element is called the stage. We can access its properties by overriding the start method in our App. Here I enlarged the window of our app a little bit. Every stage contains a scene, which is basically a graph of UI widgets (called Nodes in JavaFX). The scene takes care of placing and rendering your widgets on the screen. In TornadoFX, you can get access to the default scene by overriding the createPrimaryScene method. Now we can enjoy our Hello World example with a nice gray background. \n\n The list of built-in JavaFX controls is extensive, so I'm not going to cover all of them. But you will find all kinds of basic controls like labels, text fields, buttons, progress bars, etc. and also some more complex ones like list views or table views. To give you a taste how they look like, here is a screenshot of some of them: \n\n One of the grunt works of UI development is placing the controls of our application on screen in various layouts, pixel perfect. JavaFX comes with a handful of layout widgets out of the box, and TornadoFX has its own additions. You can place controls vertically\/horizontally with VBox\/HBox, put them in a grid with GridPane, create multiple tabs with TabPane, to name a bunch of them. What TornadoFX brings to the table is its Kotlin DSL for building our UI. It is basically syntactic sugar for making UI programming more ergonomic. We have already seen this DSL in action in the Hello World example when we defined the root variable. The basic concept is to wrap definition of UI controls inside a layout DSL, so the end result will be concise and very easy to read. Here is a slightly more complex example of using the DSL, a login form: \n\n This concept scales really well, especially when you are combining it with one more abstraction provided by TornadoFX: View or Fragment (they are basically the same abstraction, it's just Views are singletons and Fragments may have multiple instances). The idea of Views is that you organize larger parts of your UI into self-contained elements and then combine these Views into even larger UI elements (just like playing with Lego \ud83d\ude42). The code above could be encapsulated into e.g. a class LoginView : View(). The default look and feel of the controls is a good baseline on each OS (macOS, Linux, Windows), but it's reasonable to expect that you want to apply your own brand identity to your application. In JavaFX there are two fundamental ways of achieving that: Let's see some examples. This is how would you make the labels bold in the login form, by changing its font property. Or you could tint the button by applying some CSS rule: Of course, you don't have to inline the CSS rules inside your layout code, rather you can write them in type-safe Kotlin and apply them by calling addClass() on your controls. \n\n So far so good, now we have the basic building blocks to make user interfaces for desktop applications. When we are working on real-world applications though, there are a couple of questions that we usually have to answer: Well, TornadoFX comes with \"batteries included\" (opinionated ways of implementing things) but doesn't force you to use any of them. Later on, I'll show you some of these features. Data binding is a powerful pattern in UI development, that can save you many keystrokes and help to prevent bugs. With the rise of reactive applications, where data is always in motion, it is getting more important to be able to efficiently keep track of these changes and update our UI accordingly. In JavaFX data binding and \"observable properties\" are baked inside the framework, they are all over the place. If a Label has a text field, it also has a textProperty() method, that we can use for data binding. In TornadoFX we can define these pairs by property delegation. Let's continue our user login example and define a Controller capable of logging in the user. First, we want to define a status property to be able to show the progress in our UI. Then we can bind a label in our login screen to this property: label(controller.statusProperty). With Kotlin on the JVM you have many options to implement DI. I think Dagger is one of the most popular options, but you can also use more Kotlin-native libraries like Koin or Kodein. These are all valid options when you use TornadoFX, but for simple use-cases, you might want to rely on its built-in solution, that doesn't need any setup and you can start using it right away. To be able to use our controller, all we have to do is to inject it into our login view. We can inject all kinds of things, not just Controllers, for example we can inject an instance of Rest to implement networking. Networking libraries are coming in different flavors, some might prefer OkHttp, others may incorporate additional abstractions like Retrofit. Again, you can use any of your favorite networking libraries, but TornadoFX provides a simple built-in solution. If we want to use our form to log into GitHub, we can modify our controller like this: When working on either mobile or web apps, there are some really powerful UI debuggers like \"Inspect Element\" in browsers or View Hierarchy Debugger on iOS. These tools are essential when you are trying to figure out where your widget disappeared, or why it doesn't occupy the space that it should be. In the world of JavaFX, this tool comes as a third-party software called Scenic View. It automatically detects any running JavaFX applications and provides a tree-like viewer for inspecting your UI elements. Our super-simple login screen would look like this in Scenic View: \n\n When you select any widget in Scenic View, it will also get highlighted in your running application. You can inspect and also edit some of its properties, so you can fix UI issues relatively easily. The last obstacle standing in our apps way and its happy users is packaging. We need to find a way to prepare, distribute and be able to update our app. There are multiple options available, I'll show you one, that I find very practical to use: fxlauncher. It comes as a Gradle plugin (Maven is also available), so you can apply it in your project like: What are applicationUrl and deployTarget you are asking? Well, you can publish your builds to a remote URL (via scp), and your application will auto-update for all of your users on the next restart. The plugin will add a bunch of tasks to your Gradle build: Executing gradle generateNativeInstaller gives you a native installer for your app, e.g. on macOS a standard DMG. Building a \"native\" desktop app is probably not the first thing that comes to your mind when it comes to UI development, but I think it definitely has its use-cases. It may be a good fit, if: Of course, it heavily depends on your skill set, but I find working with JavaFX much more productive than building a web app. Especially with a mobile background learning JavaFX should be fairly straightforward. I hope you find this overview useful and when you start working on your next UI project, you'll give JavaFX a thought. \u270c\ufe0f  Get the latest posts delivered right to your inbox \n\n \nSubscribe\n maker of @kotlindevelopment, @kotlinresources and @wearemakery. \nKotlin Development by Makery\nKotlin Development \u00a9 2018\n                \nLatest Posts\nInstagram\nTwitter\n\n \n\n\n\n Stay up to date! Get all the latest & greatest posts delivered straight to your inbox \n\n \nSubscribe\n","time":1525797115,"title":"Super-productive UI development with Kotlin","type":"story","url":"https:\/\/www.kotlindevelopment.com\/super-productive-native\/","label":3,"label_name":"dev"},{"by":"nickysielicki","descendants":0,"id":17022232,"kids":"None","score":1,"text":"  View of the Sun from NASA\u2019s Solar Dynamics Observatory The other day, NASA posted this closeup view\u00a0of the Sun under the\u00a0headline: \u201cTangled Up in Blue.\u201d The reference to the Bob Dylan tune\u00a0aside,\u00a0I found\u00a0the video particularly intriguing. That\u2019s because the Sun\u2019s surface, as imaged here by the Solar Dynamics Observatory spacecraft, is actually\u00a0quite\u00a0placid. But there\u2019s one exception: a very bright active region, seen as a tangle of bright filaments tracing out magnetic field lines emerging from the Sun\u2019s surface. When the Sun is in a more active part of its 11-year cycle, you\u2019re likely to see more active regions like this. These are areas that\u00a0roil with gargantuan filaments, loops, and prominences formed from unimaginably hot plasma levitated by magnetic fields. Sometimes, radiation\u00a0will blast outwards\u00a0from the Sun\u2019s surface. These solar flares, a\u00a0million times more energetic than volcanic eruptions on Earth, often are accompanied by enormous explosions\u00a0of plasma. Such coronal mass ejections, or CMEs,\u00a0propel\u00a0a billion tons of material outward at hundreds of\u00a0miles\u00a0per second. Yet here, the SDO spacecraft shows the Sun in a\u00a0serene state \u2014 except for that one\u00a0active region. Here\u2019s how NASA describes what we\u2019re looking at: The lone active region visible on the sun put on a fine display with its tangled magnetic field lines swaying and twisting above it (Apr.24-26, 2018) when viewed in a wavelength of extreme ultraviolet light. The charged particles spinning along these field lines illuminate them. The region did not erupt with any significant solar storms, although it still might. In addition to experiencing a dearth of filaments, loops, prominences, flares and ejections, the Sun has been going naked lately, losing all of its spots at times. As I pointed out in a post\u00a0back in February: Spotless periods . . . are common\u00a0as the Sun approaches the low point in its\u00a011-year solar cycle. We\u2019re headed for that minimum next year. The Sun reached a peak in activity during\u00a02014. It turned out to be\u00a0the weakest maximum in solar activity since 1906. Solar activity has since been\u00a0declining and will soon bottom out.\u00a0There are indications that\u00a0this upcoming solar minimum could be particularly deep. The Sun\u2019s 11-year cycles of rising and falling activity are usually fairly regular. But much\u00a0longer variations can occur. One example, known\u00a0as the Maunder Minimum, occurred between 1645 and 1715. During this period, the Sun experienced\u00a0a very prolonged spell of low solar activity. According to recent research, a continuing decline in solar activity is occurring faster than any other in more than 9,300 years. This points toward a 15 to 20 percent chance of a return to conditions similar to the Maunder Minimum. You\u2019ll often hear claims that if we head into a long-lasting period similar to that, it will help neutralize human-caused global warming. That\u2019s because the amount of solar radiation reaching Earth during low solar activity periods is reduced. But the research shows something else. The Maunder Minimum was indeed associated with\u00a0cold European winters. But it did not have as big an effect on the global climate overall. Looking forward in time, studies suggest that an extended Maunder-Minimum-like period during this century\u00a0would produce similar results \u2014 cold winters in Eurasia, but much less of a global impact. Modeling shows that if emissions of climate-warming greenhouse gases continue unabated, temperatures could rise 4 degrees C above preindustrial levels by the end of the century. At the same time, several studies have shown that human-caused global warming would be offset by no more than about\u00a00.2\u00b0C in the year 2100 by a Maunder-Minimum-like episode. Never miss a chance to throw in a \u2018climate warming\u2019 reference. While you\u2019re at it why not throw in Trump Russian Collusion? My point is to leave the politics out of scientific discussions. It appears to me that you are the one who can\u2019t take politics out of scientific discussions. Tu quoque. \u201cThe Sun reached a peak in activity during 2014. It turned out to be the weakest maximum in solar activity since 1906. Solar activity has since been declining and will soon bottom out. There are indications that this upcoming solar minimum could be particularly deep\u201d. We are in a hyperdementional gyroscopic sphere of shared energy where if this than opposed is given.  This phenomenon has absolutely nothing to due with Hawaiian eruption. Keep telling yourselves that.\nJust wait.\nWe are all about to find out just how insignificant man, and his activities are, in the face of the sun itself. You haven\u2019t met the scientists at cern and fermilabs if that\u2019s your opinion. Do you know what a particle accelerator is?\nGo learn. Pull your cranium out of your posterior and actually examine the effects of using that device.  Do you REALLY know what an accelerator is?  Go Learn! It\u2019s important to point out the limited nature of the so-called \u201cLittle Ice Age\u201d attributed to the Maunder-Minimum.  As noted in the article, the effects seem limited primarily to Europe, but did not produce a substantial cooling planet wide.  Certainly nothing approaching the mildest of the ice ages during human prehistory.  Yet some are quick to claim climate change due to human activity will be erased, or even replaced by a mini ice age, once the next episode of minimum solar activity occurs.  The evidence from world history strongly disputes that.  Our sun is without doubt the single greatest force impacting the global climate, but it is a remarkably stable star \u2014 which is why we\u2019re all here to have this conversation.  Nothing in recorded history suggests solar energy output will change drastically enough in the foreseeable future to overwhelm the impact humanity\u2019s massive input of greenhouse gases is having on our world. There\u2019s actually good research showing a very consistent temperature signal mapping against past solar minimums \u2014 not just the Maunder Minimum. We\u2019re seeing more and more data suggesting that we\u2019re heading into a new minimum but I don\u2019t think anyone at this point knows how serious a drop in solar forcing this will produce. The good news is that a lot of people are starting to look at this issue more seriously and searching for ways to project more accurately expected changes in solar forcing. You may want to purchase some property near Presidio, Texas, which is often the hottest reported city in North America.  For some strange reason it is also the longest continuously inhabited area in North America. Tom Yulsman is Director of the Center for Environmental Journalism and a Professor of Journalism at the University of Colorado, Boulder. He also continues to work as a science and environmental journalist with more than 30 years of experience producing content for major publications. His work has appeared in the New York Times, Washington Post, Audubon, Climate Central, Columbia Journalism Review, Discover, Nieman Reports, and many other publications. He has held a variety of editorial positions over the years, including a stint as editor-in-chief of Earth magazine. Yulsman has written one book: Origins: the Quest for Our Cosmic Roots, published by the Institute of Physics in 2003. \r\n            \tSign up to get the latest science news delivered weekly right to your inbox!\r\n            ","time":1525797102,"title":"NASA says the Sun is \u201ctangled up in blue\u201d","type":"story","url":"http:\/\/blogs.discovermagazine.com\/imageo\/2018\/05\/04\/nasa-says-the-sun-is-tangled-up-in-blue\/","label":7,"label_name":"random"},{"by":"jonbaer","descendants":52,"id":17022231,"kids":"[17022884, 17023588, 17023242, 17023893, 17023227, 17023688, 17023429, 17022731, 17022636, 17023190, 17023075, 17022733]","score":60,"text":"The 21st-century tech revolution is transforming human lives across the globe \nRobin McKie Science editor \n\nSun 6 May 2018 03.59\u00a0EDT\n\n\nLast modified on Sun 6 May 2018 07.06\u00a0EDT\n\n The aims of the transhumanist movement are summed up by Mark O\u2019Connell in his book To Be a Machine, which last week won the Wellcome Book prize. \u201cIt is their belief that we can and should eradicate ageing as a cause of death; that we can and should use technology to augment our bodies and our minds; that we can and should merge with machines, remaking ourselves, finally, in the image of our own higher ideals.\u201d The idea of technologically enhancing our bodies is not new. But the extent to which transhumanists take the concept is. In the past, we made devices such as wooden legs, hearing aids, spectacles and false teeth. In future, we might use implants to augment our senses so we can detect infrared or ultraviolet radiation directly or boost our cognitive processes by connecting ourselves to memory chips. Ultimately, by merging man and machine, science will produce humans who have vastly increased intelligence, strength, and lifespans; a near embodiment of gods. Is that a desirable goal? Advocates of transhumanism believe there are spectacular rewards to be reaped from going beyond the natural barriers and limitations that constitute an ordinary human being. But to do so would raise a host of ethical problems and dilemmas. As O\u2019Connell\u2019s book indicates, the ambitions of transhumanism are now rising up our intellectual agenda. But this is a debate that is only just beginning. There is no doubt that human enhancement is becoming more and more sophisticated \u2013 as will be demonstrated at the exhibition The Future Starts Here which opens at the V&A museum in London this week. Items on display will include \u201cpowered clothing\u201d made by the US company Seismic. Worn under regular clothes, these suits mimic the biomechanics of the human body and give users \u2013 typically older people \u2013 discrete strength when getting out of a chair or climbing stairs, or standing for long periods. In many cases these technological or medical advances are made to help the injured, sick or elderly but are then adopted by the healthy or young to boost their lifestyle or performance. The drug erythropoietin (EPO) increases red blood cell production in patients with severe anaemia but has also been taken up as an illicit performance booster by some athletes to improve their bloodstream\u2019s ability to carry oxygen to their muscles. And that is just the start, say experts. \u201cWe are now approaching the time when, for some kinds of track sports such as the 100-metre sprint, athletes who run on carbon-fibre blades will be able outperform those who run on natural legs,\u201d says Blay Whitby, an artificial intelligence expert at Sussex University. The question is: when the technology reaches this level, will it be ethical to allow surgeons to replace someone\u2019s limbs with carbon-fibre blades just so they can win gold medals? Whitby is sure many athletes will seek such surgery. \u201cHowever, if such an operation came before any ethics committee that I was involved with, I would have none of it. It is a repulsive idea \u2013 to remove a healthy limb for transient gain.\u201d Not everyone in the field agrees with this view, however. Cybernetics expert Kevin Warwick, of Coventry University, sees no problem in approving the removal of natural limbs and their replacement with artificial blades. \u201cWhat is wrong with replacing imperfect bits of your body with artificial parts that will allow you to perform better \u2013 or which might allow you to live longer?\u201d he says. Warwick is a cybernetics enthusiast who, over the years, has had several different electronic devices implanted into his body. \u201cOne allowed me to experience ultrasonic inputs. It gave me a bat sense, as it were. I also interfaced my nervous system with my computer so that I could control a robot hand and experience what it was touching. I did that when I was in New York, but the hand was in a lab in England.\u201d Such interventions enhance the human condition, Warwick insists, and indicate the kind of future humans might have when technology augments performance and the senses. Some might consider this unethical. But even doubters such as Whitby acknowledge the issues are complex. \u201cIs it ethical to take two girls under the age of five and train them to play tennis every day of their lives until they have the musculature and skeletons of world champions?\u201d he asks. From this perspective the use of implants or drugs to achieve the same goal does not look so deplorable. This last point is a particular issue for those concerned with the transhumanist movement. They believe that modern technology ultimately offers humans the chance to live for aeons, unshackled \u2013 as they would be \u2013 from the frailties of the human body. Failing organs would be replaced by longer-lasting high-tech versions just as carbon-fibre blades could replace the flesh, blood and bone of natural limbs. Thus we would end humanity\u2019s reliance on \u201cour frail version 1.0 human bodies into a far more durable and capable 2.0 counterpart,\u201d as one group has put it. However, the technology needed to achieve these goals relies on as yet unrealised developments in genetic engineering, nanotechnology and many other sciences and may take many decades to reach fruition. As a result, many advocates \u2013 such as the US inventor and entrepreneur Ray Kurzweil, nanotechnology pioneer Eric Drexler and PayPal founder and venture capitalist Peter Thiel have backed the idea of having their bodies stored in liquid nitrogen and cryogenically preserved until medical science has reached the stage when they can be revived and their resurrected bodies augmented and enhanced. Four such cryogenic facilities have now been constructed: three in the US and one in Russia. The largest is the Alcor Life Extension Foundation in Arizona whose refrigerators store more than 100 bodies (nevertheless referred to as \u201cpatients\u201d by staff) in the hope of their subsequent thawing and physiological resurrection. It is \u201ca place built to house the corpses of optimists\u201d, as O\u2019Connell says in To Be a Machine. Not everyone is convinced about the feasibility of such technology or about its desirability. \u201cI was once interviewed by a group of cryonic enthusiasts \u2013 based in California \u2013 called the society for the abolition of involuntary death,\u201d recalls the Astronomer Royal Martin Rees. \u201cI told them I\u2019d rather end my days in an English churchyard than a Californian refrigerator. They derided me as a deathist \u2013 really old-fashioned.\u201d For his part, Rees believes that those who choose to freeze themselves in the hope of being eventually thawed out would be burdening future generations expected to care for these newly defrosted individuals. \u201cIt is not clear how much consideration they would deserve,\u201d Rees adds. Ultimately, adherents of transhumanism envisage a day when humans will free themselves of all corporeal restraints. Kurzweil and his followers believe this turning point will be reached around the year 2030, when biotechnology will enable a union between humans and genuinely intelligent computers and AI systems. The resulting human-machine mind will become free to roam a universe of its own creation, uploading itself at will on to a \u201csuitably powerful computational substrate\u201d. We will become gods, or more likely \u201cstar children\u201d similar to the one at the end of 2001: A Space Odyssey. These are remote and, for many people, very fanciful goals. And the fact that much of the impetus for establishing such extreme forms of transhuman technology comes from California and Silicon Valley is not lost on critics. Tesla and SpaceX founder Elon Musk, the entrepreneur who wants to send the human race to Mars, also believes that to avoid becoming redundant in the face of the development of artificial intelligence, humans must merge with machines to enhance our own intellect. This is a part of the world where the culture of youth is followed with fanatical intensity and where ageing is feared more acutely than anywhere else on the planet. Hence the overpowering urge to try to use technology to overcome its effects. It is also one of the world\u2019s richest regions, and many of those who question the values of the transhuman movement warn it risks creating technologies that will only create deeper gulfs in an already divided society where only some people will be able to afford to become enhanced while many other lose out. The position is summed up by Whitby. \u201cHistory is littered with the evil consequences of one group of humans believing they are superior to another group of humans,\u201d he said. \u201cUnfortunately in the case of enhanced humans they will be genuinely superior. We need to think about the implications before it is too late.\u201d For their part, transhumanists argue that the costs of enhancement will inevitably plummet and point to the example of the mobile phone, which was once so expensive only the very richest could afford one, but which today is a universal gadget owned by virtually every member of society. Such ubiquity will become a feature of technologies for augmenting men and women, advocates insist. Many of these issues seem remote, but experts warn that the implications involved need to be debated as a matter of urgency. An example is provided by the artificial hand being developed by Newcastle University. Current prosthetic limbs are limited by their speed of response. But project leader Kianoush Nazarpour believes it will soon be possible to create bionic hands that can assess an object and instantly decide what kind of grip it should adopt. \u201cIt will be of enormous benefit, but its use raises all sorts of issues. Who will own it: the wearer or the NHS? And if it is used to carry a crime, who ultimately will be responsible for its control? We are not thinking about these concerns and that is a worry.\u201d The position is summed up by bioethicist professor Andy Miah of Salford University. \u201cTranshumanism is valuable and interesting philosophically because it gets us to think differently about the range of things that humans might be able to do \u2013 but also because it gets us to think critically about some of those limitations that we think are there but can in fact be overcome,\u201d he says. \u201cWe are talking about the future of our species, after all.\u201d LimbsThe artificial limbs of Luke Skywalker and the Six Million Dollar Man are works of fiction. In reality, bionic limbs have suffered from multiple problems: becoming rigid mid-action, for example. But new generations of sensors are now making it possible for artificial legs and arms to behave in much more complex, human-like ways. SensesThe light that is visible to humans excludes both infrared and ultra-violet radiation. However, researchers are working on ways of extending the wavelengths of radiation that we can detect, allowing us to see more of the world - and in a different light. Ideas like these are particularly popular with military researchers trying to create cyborg soldiers. PowerPowered suits or exoskeletons are wearable mobile machines that allow people to move their limbs with increased strength and endurance. Several versions are being developed by the US army, while medical researchers are working on easy-to-wear versions that would be able to help people with severe medical conditions or who have lost limbs to move about naturally. BrainsTranshumanists envisage the day when memory chips and neural pathways are actually embedded into people\u2019s brains, thus bypassing the need to use external devices such as computers in order to access data and to make complicated calculations. The line between humanity and machines will become increasingly blurred.","time":1525797101,"title":"No death and an enhanced life: Is the future transhuman?","type":"story","url":"https:\/\/www.theguardian.com\/technology\/2018\/may\/06\/no-death-and-an-enhanced-life-is-the-future-transhuman","label":5,"label_name":"ml"},{"by":"jonbaer","descendants":0,"id":17022225,"kids":"None","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 The self-driving van\u00a0from Drive.ai Source: Drive.ai Inc. Silicon Valley startup Drive.ai Inc. plans to roll out a small network of self-driving cars this summer that\u00a0can be hailed using an app, making it one of the first such services available to the general public. The ride-hailing app will initially cover a slice of Frisco, Texas, and will be available in July, the company said.\u00a0The program\u2019s vehicles, Nissan NV200 vans, are painted bright orange and have visible sensors as well as four screens that tell pedestrians what the car is thinking, such as \u201cWaiting for You to Cross\u201d and \u201cPassengers Entering\/Exiting.\u201d Drive.ai declined to say how many cars will be part of the project.\u00a0As of Nov. 30, the startup had seven vehicles licensed for autonomous operation in California. Alphabet Inc.\u2019s Waymo, Uber Technologies Inc. and other autonomous-vehicle operators have been testing similar programs with select riders. Waymo\u00a0asked people in Arizona to apply for access to free rides last year and approved hundreds of residents in parts of the greater Phoenix metro area, including Chandler, Gilbert, Mesa and Tempe, Arizona. Uber\u2019s self-driving car tests have been grounded since March, when one hit and killed a pedestrian in Tempe. Like Waymo, Drive.ai rides will be free, and the cars will only operate between predetermined pickup and drop-off spots near a few Frisco office complexes and the Star, which is the headquarters of the Dallas Cowboys. The service is limited to residents, employees and patrons of the buildings in the program. Riders\u00a0won\u2019t be alone: The cars will have either safety drivers or chaperones riding in them at all times. Drive.ai was founded in 2015 by a group of scientists who had worked together at Stanford University\u2019s Artificial Intelligence Lab. One of Drive.ai\u2019s directors, Andrew Ng, ran the Stanford lab before leaving to oversee artificial intelligence projects at\u00a0Google.","time":1525797056,"title":"Drive.ai Will Offer Uber-Like Service with Autonomous Vans This Summer","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-07\/drive-ai-will-offer-uber-like-service-with-autonomous-vans-this-summer","label":0,"label_name":"biz-news"},{"by":"digital55","descendants":0,"id":17022221,"kids":"None","score":1,"text":"In research shortly before his death in 1954, Alan Turing used mathematics to explore how forms emerge, yielding insights that are now being applied to problems like desalination. Alan TuringCreditScience History Images, via Alamy Supported by By JoAnna Klein Many have heard of Alan Turing, the mathematician and logician who invented modern computing in 1935. They know Turing, the cryptologist who cracked the Nazi Enigma code, helped win World War II. And they remember Turing as a martyr for gay rights who, after being prosecuted and sentenced to chemical castration, committed suicide by eating an apple laced with cyanide in 1954.  But few have heard of Turing, the naturalist who explained patterns in nature with math. Nearly half a century after publishing his final paper in 1952, chemists and biological mathematicians came to appreciate the power of his late work to explain problems they were solving, like how zebrafish get their stripes or cheetahs get spots. And even now, scientists are finding new insights from Turing\u2019s legacy.  Most recently, in a paper published Thursday in Science, chemical engineers in China used pattern generation described by Turing to explain a more efficient process for water desalination, which is increasingly being used to provide freshwater for drinking and irrigation in arid places.  Turing\u2019s 1952 paper did not explicitly address the filtering of saltwater through membranes to produce freshwater. Instead, he used chemistry to explain how undifferentiated balls of cells generated form in organisms.  It\u2019s unclear why this interested the early computer scientist, but Turing had told a friend that he wanted to defeat Argument From Design, the idea that for complex patterns to exist in nature, something supernatural, like God, had to create them.  [Like the Science Times page on Facebook.| Sign up for the Science Times newsletter.] A keen natural observer since childhood, Turing noticed that many plants contained clues that math might be involved. Some plant traits emerged as Fibonacci numbers. These were part of a series: Each number equals the sum of the two preceding numbers. Daisies, for example, had 34, 55 or 89 petals.  \u201cHe certainly was no militant atheist,\u201d said Jonathan Swinton, a computational biologist and visiting professor at the University of Oxford who has researched Turing\u2019s later work and life. \u201cHe just thought mathematics was very powerful, and you could use it to explain lots and lots of things \u2014 and you should try.\u201d And try, Turing did.  \u201cHe came up with a mathematical representation that allows form to emerge from blankness,\u201d said Dr. Swinton.  In Turing\u2019s model, two chemicals he called morphogens interacted on a blank arena. \u201cSuppose you\u2019ve got two of these, and one will make the skin of an animal go black and the skin of the animal go white,\u201d explained Dr. Swinton. \u201cIf you just mix these things in an arena, what you get is a gray animal.\u201d  But if something caused one chemical to diffuse, or spread, faster than the other, then each chemical could concentrate in evenly spaced localized spots, together forming black and white spots or stripes.  This is known as a \u201cTuring instability,\u201d and, the Chinese researchers who published the new paper determined that it could explain the way shapes emerged in salt-filtering membranes.  By creating three-dimensional Turing patterns like bubbles and tubes in membranes, the researchers increased their permeability, creating filters that could better separate salt from water than traditional ones. \u201cWe can use one membrane to finish the work of two or three,\u201d said Zhe Tan, a graduate student at Zheijang University in China and first author of the paper, which means less energy and lower cost if used for large-scale desalination operations in the future.  Beyond his final publication, Turing\u2019s notes revealed the complicated ideas he was wrestling to explain.  \u201cThe 1952 paper that everybody knows is not the end of the story,\u201d said Jonathan Dawes, a mathematician at the University of Bath who has also been trying to make sense of Turing\u2019s later work. \u201cIt doesn\u2019t show the full depth of his thinking.\u201d  Turing appeared to be looking for a general mechanism for the creation of form \u2014 like how thought or consciousness spontaneously emerges or how sunflowers neatly pack their seeds together. But Turing would die before completing and publishing his final musings.  \u201cWhat I hope we appreciate more because of him, is to value diversity and individual creativity in our science base,\u201d Dr. Dawes said. \u201cWe need people who we allow to be driven by their curiosity, and we also need people who will take those basic science ideas and turn them into useful technology.\u201d  Advertisement    Collapse SEE MY OPTIONS","time":1525797045,"title":"How the Father of Computer Science Decoded Nature\u2019s Mysterious Patterns","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/08\/science\/alan-turing-desalination.html","label":5,"label_name":"ml"},{"by":"jonbaer","descendants":0,"id":17022219,"kids":"None","score":1,"text":"We use cookies to provide you with a better onsite experience. By continuing to browse the site you are agreeing to our use of cookies in accordance with our Cookie Policy. (examples: physics, climate change, etc.) His company, TerraPower, aims to build a safe, nearly waste-free reactor that won't contribute to weapons proliferation or climate change Bill Gates originally became famous (and rich) for co-founding Microsoft; more recently, he\u2019s become better known, through the Bill and Melinda Gates Foundation, as a philanthropist working to improve global health and sustainable economic growth. But Gates is also passionate about fighting global warming, and he\u2019s has become a leading proponent for a revolutionary nuclear reactor design. His firm TerraPower envisions a reactor with, \u201cinexhaustible renewable and eminently economic nuclear energy in quantities sufficient for ...10 billion people at United States per capita (consumption) levels.\u201d In short, Gates is on a quest to build a safe, almost waste-free approach that does not contribute to nuclear weapons proliferation. TerraPower was founded in 2008, 54 years after the first non-weapons application of nuclear energy in 1954 enabled the launching of the nuclear submarine Nautilus. In the wake of that success, Admiral Hyman Rickover, who spearheaded the design and building of the ship\u2019s power reactor, became so influential that he was able to convince the Atomic Energy Commission and Congress to get behind a similar design for civilian power reactors, using uranium as the fuel and water under pressure as the coolant that carried the heat of nuclear fission to turbines that produced steam and electricity. One powerful argument in favor of these reactors was that the fission process created plutonium, which was needed for nuclear weapons. Unfortunately, as a result of accidents, intractable radioactive waste and high production costs, interest in building these reactors has almost evaporated, especially in the United States, Western Europe and Japan. But it\u2019s not the only reactor design by a long shot: as early as the 1960\u2019s, scientists and engineers at Oak Ridge National Laboratory were working on competing designs, including one that used liquid sodium as a coolant. In the early 1990\u2019s, Lowell Wood, at Lawrence Livermore National Laboratory (he\u2019s been granted more patents than Thomas Edison), made a conceptual leap: the idea of a liquid sodium-cooled reactor that would breed plutonium and then use it as fuel. He and Edward Teller, sometimes called the \u201cfather of the H-bomb,\u201d who had always scoffed at the idea that a reactor designed for a submarine would be optimal for a civilian power plant, co-authored a paper that proposed such a reactor. That laid the foundation for the reactor TerraPower wants to build. Liquid sodium has several advantages over pressurized water. Sodium\u2019s thermal properties provide far superior heat transfer than those of water (the water is pressurized, in fact, to improve heat transfer). It\u2019s also a stable fluid that can be used at an operating temperature of 550 degrees Fahrenheit at ordinary atmospheric pressure, so there\u2019s no need for an expensive containment structure. Pressurized water by contrast, carries a danger of explosion, either from steam or from hydrogen created when water in an overheated reactor oxidizes the metal casing of the fuel rod containers. Most significantly, and unlike water, sodium does not absorb or slow neutrons produced in fission reactions. That means that as its uranium-238 fuel generates plutonium, the plutonium itself then undergoes fission, generating even more energy. This so-called breeder-burner design would be entirely self-sustaining. TerraPower envisions the use of natural uranium, depleted uranium, uranium obtained from processed sea water or radioactive waste as fuel. Not only is there little residual waste associated with the reactor, it would not be subject to loss-of-coolant accidents nor would it enable nuclear weapons production; the plutonium does not get separated out and thus is not available for weapons. Another major advantage is that TerraPower\u2019s reactor could in principle run for 50 years without a need to be opened for the addition of fresh fuel or the removal of radioactive waste. This doesn\u2019t mean that the TerraPower reactor is ready for market. One problem is the potential of sodium leaks leading to possible fires and explosions. Another is that the fuel rods need to be repositioned periodically without opening the reactor, so that the breeder-burner process continues for at least 50 years. Years of experience with liquid sodium provide reasonable confidence that the challenge of leaks can be met. However, the use of supercomputers to model the proper placement of core fuel rods through use of remote instrumentation is a task without precedent. But given the fact that this incredible technology emerged from such science, technology and entrepreneurial stars as Teller, Wood and Gates, and with the China National Nuclear Corporation now joining the enterprise as a potential manufacturer, TerraPower is at the very least a credible endeavor. To provide energy without generating greenhouse gases for the human race by 2050\u2014including addressing a growing need for desalination plants, and aiding the production of hydrogen for fuel cells to replace gasoline powered vehicles\u2014an estimate of 6,000 reactors for the world is reasonable. A ballpark cost is $1 billion each; that\u2019s $6 trillion, or about one third of the current U.S. GDP. Given that most of the world\u2019s countries would be contributing to this global initiative, however, it is possible that if TerraPower-type technology lives up to its promise, a world energy solution would not only be feasible\u2014it would be affordable. The views expressed are those of the author(s) and are not necessarily those of Scientific American.  Edward A. Friedman Edward A. Friedman is Emeritus Professor of Technology Management at Stevens Institute of Technology, where he is engaged with issues of nuclear weapons, terrorism and energy. May 8, 2018  \u2014  Wendell Duffield 4 hours ago  \u2014  Chelsea Harvey and E&E News 7 hours ago  \u2014  Claudia Wallis 7 hours ago  \u2014  Daisy Grewal 8 hours ago  \u2014  Davide Castelvecchi and Nature magazine 9 hours ago  \u2014  Lloyd I. Sederer Neuroscience. Evolution. Health. Chemistry. Physics. Technology. Follow us \u00a9 2018 Scientific American, a Division of Nature America, Inc. All Rights Reserved.  Get the perfect gift for mom","time":1525797035,"title":"Bill Gates in Search of Nuclear Nirvana","type":"story","url":"https:\/\/blogs.scientificamerican.com\/observations\/bill-gates-in-search-of-nuclear-nirvana\/","label":7,"label_name":"random"},{"by":"jonbaer","descendants":60,"id":17022215,"kids":"[17022985, 17023703, 17022535, 17022907, 17022595, 17022586, 17023122, 17024642, 17023185, 17022923, 17024376, 17022736, 17024126, 17024208, 17023005, 17023566, 17023976, 17022984, 17024249]","score":155,"text":"\n        Edmon de Haro for BuzzFeed News\n       \n          \n          A vast web of Amazon review fraud lives online, and it's designed to evade the company\u2019s efforts to thwart it.\n         One morning in late January, Jake picked up the box on his desk, tore through the packing tape, unearthed the iPhone case inside, snapped a picture, and uploaded it to an Amazon review he\u2019d been writing. The review included a sentence about the case\u2019s sleek design and cool, clear volume buttons. He finished off the blurb with a glowing title (\u201cThe perfect case!!\u201d) and rated the product a perfect five stars. Click. Submitted. Jake never tried the case. He doesn\u2019t even have an iPhone. Jake then copied the link to his review and pasted it into an invite-only Slack channel for paid Amazon reviewers. A day later, he received a notification from PayPal, alerting him to a new credit in his account: a $10 refund for the phone case he\u2019ll never use, along with $3 for his trouble \u2014 potentially more, if he can resell the iPhone case. Jake is not his real name. He \u2014 along with the four other reviewers who spoke to BuzzFeed News for this story \u2014 wanted to remain anonymous for fear Amazon would ban their accounts. They are part of an extensive, invisible workforce fueling a review-fraud economy that persists in every corner of the largest marketplace on the internet. Drawn in by easy money and free stuff, they\u2019ve seeded Amazon with fake five-star reviews of LED lights, dog bowls, clothing, and even health items like prenatal vitamins \u2014 all meant to convince you that this product is the best and bolster the sales of profiteers hoping to grab a piece of the Amazon Gold Rush. Meanwhile, sellers trying to play by the rules are struggling to stay afloat amid a sea of fraudulent reviews, and buyers are unwittingly purchasing inferior or downright faulty products. And Amazon is all but powerless to stop it. The systems that create fraudulent reviews are a complicated web of subreddits, invite-only Slack channels, private Discord servers, and closed Facebook groups, but the incentives are simple: Being a five-star product is crucial to selling inventory at scale in Amazon\u2019s intensely competitive marketplace \u2014 so crucial that merchants are willing to pay thousands of people to review their products positively. At just over $760 billion, Amazon is the world\u2019s second most valuable company, behind Apple. CEO Jeff Bezos recently revealed the company counts more than 100 million paying Amazon Prime members globally, the $119-per-year membership that gets subscribers \u2014 the most devoted Amazon customers \u2014 free two-day shipping; music, TV, and movie streaming; access to an e-book library; and more. The company operates 13 third-party seller marketplaces around the world, and rents space to house those sellers\u2019 inventory in more than 150 warehouses. In 2017, third-party merchants brought in $32 billion of revenue, and nearly 300,000 new sellers joined the marketplace last year alone. Reviews are a buyer\u2019s best chance to navigate this dizzyingly crowded market and a seller\u2019s chance to stand out from the crowd. In a 2011 survey, 87% of consumers said a positive review confirmed their decision to purchase a product; online customer reviews are the second most trusted source of product information, behind recommendations from family and friends. But only 3% to 10% of customers leave reviews. The best way to make it on Amazon is with positive reviews, and the best way to get positive reviews is to buy them. In October 2016, Amazon banned free items or steep discounts in exchange for reviews facilitated by third parties. But Tommy Noonan, CEO of ReviewMeta, a site that analyzes Amazon listings, said what he calls \u201cunnatural reviews\u201d \u2014 that is, reviews, that his algorithm indicates might be fake \u2014 have returned to the platform. In June 2017, Noonan noticed an uptick in unnatural reviews along with an increase in the average rating of products, and the rate of growth hasn\u2019t slowed since. Amazon\u2019s ban didn\u2019t stop sellers from recruiting reviewers. It only drove the practice underground. Reviewers are no longer simply incentivized with free stuff \u2014 they\u2019re commissioned specifically for a five-star rating in exchange for cash. The bad reviews are harder to spot, too: They don\u2019t contain any disclosures (because incentivized reviews are banned, and a disclosure would indicate that the review violates Amazon\u2019s terms). Paid reviewers also typically pay for products with their own credit cards on their own Amazon accounts, with which they have spent at least $50, all to meet the criteria for a \u201cverified purchase,\u201d so their reviews are marked as such. Amazon won\u2019t reveal how many reviews \u2014 fraudulent or total \u2014 it has. But based on his analysis of Amazon data, Noonan estimates that Amazon hosts around 250 million reviews. Noonan's website has collected 58.5 million of those reviews, and the ReviewMeta algorithm labeled 9.1%, or 5.3 million of the dataset's reviews, as \u201cunnatural.\u201d An Amazon spokesperson told BuzzFeed News the percentage of inauthentic reviews on the platform is \u201ctiny,\u201d but would not be more specific. In a statement, she wrote, \u201cAmazon is investing heavily to detect and prevent inauthentic reviews. In addition to advance detection, we use a machine-learned algorithm that gives more weight to newer, more helpful reviews, apply strict criteria to qualify for the Amazon verified purchase badge and enforce a significant dollar amount requirement to participate.\u201d Internal investigators also examine reviews and \u201cnon-Amazon forums\u201d used for purchasing reviews, the spokesperson said. Amazon says it blocks or suspends accounts belonging to any of the parties involved. But conversations with sellers, customers, and reviewers reveal that review abuse continues, despite the company\u2019s efforts. An unnatural review doesn\u2019t necessarily mean a product is inferior. But the problem with paid-for reviews is that they make it difficult for consumers \u2014 even savvy ones \u2014 to know if what they\u2019re buying is actually good or bad. In December, Kevin Votaw of Fort Collins, Colorado, ordered a highly rated Samsung Galaxy S8 screen protector for $10 using his girlfriend\u2019s Amazon Prime account. It was, he said, \u201cterrible\u201d: The material was so thick he couldn\u2019t type, making the product completely unusable. Frustrated, Votaw asked Amazon for a refund for the unreturnable product, which has a sticky, adhesive backing and can\u2019t be reused. \u201cI went back to the listing, thinking, \u2018Surely all of these reviews are wrong,\u2019\u201d he said. But the product was unavailable \u2014 \u201clike the company was rolling these protectors out in batches, then taking them off Amazon.\u201d Amazon removed the screen protector listing after a BuzzFeed News inquiry, but two other listings with nearly identical product photography have more than 550 and 150 reviews each, all of which include a five-star rating and are unverified. Votaw has since found a protector with a mediocre 3.5-star rating that he\u2019s happy with: \u201cIt works perfectly,\u201d he said. \u201cI\u2019ll be leaving a great review.\u201d Type in \u201cAmazon Review\u201d into Facebook\u2019s search box, and more than a hundred groups will pop up. Two of the more popular groups, Amazon Review Club and US \u2014 Amazon Review Club, which had 69,000 and 60,000 members, respectively, were recently shut down, but many more groups remain, with tens of thousands of members apiece.  The groups\u2019 posts read like the most random garage sale ever: lawn aerator shoes, lint removers, diaper pads, sex toys. New products are posted every minute. Most sellers are Chinese; in February, group members were warned that lunar holidays might slow down payments. Posts include a photo of the product up for grabs, with a description that says, \u201cPP refund after review,\u201d which means interested parties will receive a PayPal payment after proof they\u2019ve published a review. One product listed in the group, a posture corrector designed to train your back to sit upright, was offering an unusually large commission: a $30 Amazon gift card that included $20 for the product and an extra $10 for the reviewer, who needed to be an Amazon Prime member and write a review that contained images. Meanwhile, the 36,629-member \/r\/slavelabour subreddit, which is dedicated to getting \u201cjobs done well below market rate,\u201d has a formalized process for posting opportunities for would-be Amazon reviewers. Redditors negotiate a fake review. Sellers post \u201ctasks\u201d that include leaving positive reviews on their products \u2014 or one-star reviews on competitors\u2019 products. One listing read, \u201cThe reviews don\u2019t need to be verified or anything, so the task won\u2019t take more than a minute of your time. I will send you the link to the product, you go and leave a brief 1 star review, and that\u2019s it. I will pay $5 to you through Paypal as soon as the review is live on Amazon.\u201d Within a week, 27 users had placed \u201cbids\u201d on the task, expressing their interest. The most active reviewers become headhunters, working to recruit \/r\/slavelabour users into private Discord servers or Slack channels dedicated solely to feeding the Amazon review ecosystem. Sellers typically pay between $4 to $5 per review, plus a refund of the product. Moderators take a cut off the top \u2014 around 20% to 25% for the Discord server I gained access to, but reviewers get to keep the item for free. On Jan. 13, the server\u2019s moderator published a call for reviewers interested in \u201cBefore You Go To The Toilet Poop Spray,\u201d a natural deodorizer carrying the \u201cscent of Australia\u201d by a company called Veil. The poop spray\u2019s listing shows a number of negative reviews, followed by a string of more positive reviews published after Jan. 13. Both positive and negative reviews are \u201cverified purchases,\u201d making it nearly impossible to differentiate between commissioned and genuine reviews. None of the reviews contain a disclosure that the product was received for free or that the review was paid for by the seller. In an email to BuzzFeed News, Greg Doney, a Veil Fragrances representative, acknowledged the company had paid for reviews, \u201cbut you have to as its [sic] really hard to launch a product without them.\u201d When you take a closer look at the poop spray customers\u2019 product review history, a pattern emerges. In addition to the poop spray, Amazon customers \u201cBrian\u201d and \u201cSharon\u201d also gave a floating shelf bracket, which had been posted to the Discord server earlier, high marks. Brian said the product was a \u201creally sturdy shelf\u201d (it\u2019s a bracket, not a stand-alone shelf); and Sharon noted they\u2019re \u201ceasy to mount\u201d and \u201cdurable.\u201d The critical reviews, meanwhile, warn customers \u201cDO NOT BUY!\u201d and \u201cHUGE waste of money,\u201d citing wall damage and poorly made brackets. (The shelf company did not respond to a request for comment sent to its Amazon seller profile from BuzzFeed News.) The same accounts also reviewed \u201cBoric Acid Suppositories\u201d for vaginal health. The supplements are intended to be inserted into the vagina twice daily for two weeks, according to the product\u2019s label. It purports to provide \u201cvaginal health support\u201d and \u201crelief from odor, itchiness, pain, burning, and soreness.\u201d The capsules were posted to the Discord server on Jan. 22 \u2014 and reviews from Brian and Sharon appeared shortly after that. Brian commented that his girlfriend was \u201chaving pain during sex and it was not because of me,\u201d and the supplements cured her. Sharon mentioned it was the \u201cbest and cheapest\u201d option for female hygiene and health. Neither of the reviews were marked as verified purchases. A five-star review by D. Pham even says the boric acid \u201cdoesn\u2019t taste bad\u201d even though the pill is not taken orally. (D. Pham also gave the floating shelf bracket five stars.) Meanwhile, a one-star review said that the product was a \u201cnightmare,\u201d citing how the pill \u201cfeels like a clump of cement mix in my vagina that is just gritty and totally dries it out.\u201d Another said the suppository caused \u201can irritating burning sensation that lasted for hours.\u201d The company did not respond to a request for comment. According to Dr. Jenny Jaque, assistant professor of gynecology at the University of Southern California's Keck School of Medicine, boric acid suppositories should only be used under certain circumstances, under the supervision of a health care provider. \u201cBoric acid is not candy, and you need to be careful,\u201d she said. \u201cIt can cause irritation: Your vagina will hurt; you\u2019d feel pain like your vagina is on fire.\u201d She added that \u201cit is really toxic if ingested by accident.\u201d The Discord server recently posted turmeric curcumin vitamins, as well as myoinositol capsules, made by the same brand, listed in the prenatal vitamin category. The same reviewers \u2014 Sharon and D. Pham \u2014 gave the prenatal vitamins five stars. Both of their reviews were marked as verified purchases. The company did not respond to a request for comment. Amazon said it has applied stricter criteria for leaving verified purchase reviews. US reviewers must have a password-protected account and have made at least $50 in purchases on Amazon with a valid credit card. During a hot summer in 2010, Jamie Whaley discovered a problem with her fitted sheets: They were breaking loose and ending up wrapped around her husband Jimmy\u2019s ankles. Whaley, then a 40-year-old nursing student in Texas, went in search of a solution. The suspenderlike designs on older sheet fasteners were too cumbersome, so Whaley developed her own, using a shock cord to make an easy button-enabled tightening mechanism. She called her new invention BedBand. Jimmy Whaley convinced his wife to start selling BedBands on Amazon. The product worked, and people wanted to buy it: Over the next few years, BedBand became Amazon\u2019s best-selling sheet fastener, with more $700,000 in annual sales. Then, in October 2014, BedBand\u2019s revenue was suddenly cut in half to about $350,000. Whaley, who had given up a nursing career, and Jim, who had also joined the company full time as COO, were devastated. \u201cSomeone was selling an exact replica of our product, and we couldn\u2019t use our patent because it was pending,\u201d said Whaley. Counterfeits, sold in bulk on AliExpress, had flooded Amazon\u2019s sheet fastener market, and many gained dozens of five-star reviews very quickly. Over the next eight months, sales tanked. \u201cThere were tears,\u201d said Whaley. \u201cWe wondered whether it was time to look for different jobs.\u201d She was forced to lay off eight contractors, half her staff at the time. Amazon took down the copycats once BedBand was granted a patent; it was a year after sales started dropping, but it was too late. The patent didn\u2019t stop competitors from leaving negative one-star reviews on BedBand\u2019s listings: \u201cYou start looking a bit deeper...and you can see they\u2019ve left five-star reviews for our competitors. Or you\u2019ll see that two different buyers have been reviewing the same type of products, our competitors\u2019 products,\u201d Whaley said.  BedBand is currently the best-selling fastener, but that may not be for long. \u201cIt\u2019s stressful, knowing that Amazon is our big store,\u201d she said. To diversify revenue income, Whaley started selling BedBands on Walmart.com, Jet.com, Sears.com, and other retailers. Whaley was careful about how she discussed Amazon, which is her primary source of income \u2014\u201cI don\u2019t want to mess where I nest,\u201d she said \u2014 but seemed uneasy about being at the whim of Amazon\u2019s algorithms: \u201cSome days, all of a sudden we\u2019re fifth. When you see a fast shift like that and [our competitors] end up with reviews all of a sudden, it\u2019s frustrating. You just don\u2019t know if tomorrow these competitors will knock our sales down.\u201d Paid review writing is the modern lemonade stand. According to the people who do it, it\u2019s a quick, easy way to earn a few bucks and get free stuff \u2014 but not a career. And because the bidding, writing, and transaction happen online, it can all be done from a laptop in your bedroom. Many reviewers who spoke to BuzzFeed News are men in their late teens or early twenties who viewed the activity as a hobby. All wished to remain anonymous, out of fear of being caught by Amazon, which typically results in revoked reviewing privileges or, in the most severe cases, being banned from reusing the same delivery address or payment method with a new account. Josh, which is not his real name, is fairly new to reviewing; he discovered it through \/r\/slavelabour. He said he\u2019s in it for the stuff: \u201cYou get to keep the product, so that\u2019s a plus. It\u2019s interesting to test out new and unique products in my opinion.\u201d The most industrious paid reviewers become moderators and begin facilitating review deals themselves. Frank, whose name has been changed, manages an Amazon review Slack channel in his spare time. The entrepreneurial 18-year-old scours Amazon for products that aren\u2019t well-rated and offers the product\u2019s seller his Slack channel\u2019s review services. Listings with a small number of reviews and a low-star rating are ideal candidates. \u201cIf [a product listing] has 30 reviews with an average of 2.5 stars, it\u2019s worth it and I\u2019ll reach out,\u201d he said. Frank only earns about $20 from reviews a month, and sometimes a bit extra through seller negotiations. \u201cI get about 30% for every review that one of our reviewers does,\u201d he said. The bulk of Frank\u2019s review income comes from reselling the items he reviews on eBay, where he makes \u201cabout a couple hundred\u201d monthly. Another reviewer, whom we\u2019ll call Evan, is part of a private 10-person Slack channel and makes about $75 a month from reviews. He\u2019s left more than 150 reviews for items like weighted workout vests, 30-day hair growth pills, and car phone chargers. \u201cIt\u2019s great for small investments and savings I\u2019m working on,\u201d he said. To write his commissioned reviews, Evan takes inspiration from existing reviews, \u201cthen I\u2019ll cobble together a few keywords.\u201d One reviewer, Devan, who asked to be identified by his first name only, however, denies any wrongdoing and doesn\u2019t mind being named. The college student swears he provides nothing but his honest opinion and thinks incentivized Amazon reviews aren\u2019t a scam. He did not know incentivized reviews violated Amazon's terms of service until BuzzFeed News told him. He did admit he benefits from incentivized reviews (which he writes daily): He reuses the bubble wrap and other shipping materials from Amazon for his eBay business, through which he resells items bought on clearance. But incentivized reviews on Amazon, he says, are just an evolution of marketing tactics that have been around for a long time. \u201cAt a sporting event, Mountain Dew will hand out soda for free, and if people like it they post on social media or whatever. It\u2019s kind of the same thing, and like at Costco or Walmart, where you get free samples,\u201d Devan said. \u201cSome of these sellers are mom and pop shops, and a lot of companies just want your honest feedback...I know a lot of people who [review] aren\u2019t in it for the money. They\u2019re just trying to find better products for people,\u201d Devan said. Devan's right \u2014 there are \u201cmom and pop shops\u201d on Amazon, like Whaley\u2019s BedBand business. But an increasing number of sellers are folks with little e-commerce experience or product expertise, looking to take advantage of Amazon\u2019s millions of customers \u2014 and they need high ratings to supplant the existing best-seller. Through the \u201cAmazon FBA\u201d or \u201cFulfillment by Amazon\u201d program, anyone can open their own Amazon storefront. It\u2019s easy: Buy products in bulk from an online wholesale supplier, like AliExpress, then send the inventory to one of Amazon\u2019s warehouses, where Amazon will take care of all aspects of distribution, from payment to order fulfillment to shipping.  David, whose name has been changed, is an FBA seller who has considered buying five-star reviews. \u201cIt feels like the people making the most money are those who are violating Amazon\u2019s terms of service,\u201d he said. David didn\u2019t invent anything. He didn\u2019t go to medical school. But he makes about $1,000 a month selling suture training pads for medical professionals on Amazon. \u201cIt\u2019s a hobby, like video games. Plus, Warren Buffett said something like, \u2018If you want to make money, you have to practice making money.\u2019\u201d David got started using a site called Jungle Scout. A subscription, which costs about $350 per year, provides metrics that show what products Amazon customers are buying a lot of \u2014 and features categories where the top seller has a poor or average star rating. That mediocre star rating indicates that there\u2019s an opportunity in the market for another seller to poach some of that low-rated product\u2019s customers. \u201cWhat I found were suture pads, like a simulation skin for medical students. Then I went on Alibaba. All the Chinese suppliers are on there and they fulfill bulk orders.\u201d David \u2014 and other FBA sellers like him \u2014 swarmed the Amazon suture pad market. \u201cThe original guy, the guy whose product was featured on Jungle Scout, started losing a lot of sales because of all the competition,\u201d he said. It became apparent, according to David, that the seller, Your Design Medical, was leaving one- and two-star reviews on competitors\u2019 products. (Amazon said that, after an investigation, it did not find Your Design Medical guilty of reviews abuse. Your Design Medical did not respond to a request for comment sent to its customer support email address from BuzzFeed News.) The only new seller performing well, in spite of the original seller\u2019s bad reviews, was also, in David\u2019s view, \u201ccheating the system.\u201d \u201cThe listing had 33 five-star reviews. There\u2019s no way you can get 33 reviews in two months. That\u2019s unusual for a product like this,\u201d he said. In the end, David didn\u2019t end up buying reviews \u2014 but added that \u201cif I really needed FBA for my income, I would have.\u201d Many compensated reviewers, and the sellers who hire them, don\u2019t see themselves as bad actors. They\u2019re just one of the thousands of other reviewers and sellers doing the exact same thing. Taking shortcuts, they say, is just leveling the playing field. Inside Amazon\u2019s massive marketplace, there are lots of places to hide, to optimize, to cheat \u2014 and, because sellers can\u2019t control the algorithms, they\u2019ll do everything they can to sway them. Amazon has tried to crack down on the review fraud on its platform. The company has sued more than 1,000 people for writing or selling fake reviews. In its 2015 suit against freelancers on the website Fiverr, Amazon said it \u201ctakes the credibility of its customer reviews seriously,\u201d and also asserted, \u201cWhile small in number, these reviews can significantly undermine the trust that consumers and the vast majority of sellers and manufacturers place in Amazon, which in turn tarnishes Amazon\u2019s brand.\u201d In its seller marketplace guidelines, provided to BuzzFeed News by a third-party seller, Amazon says that sellers \u201cmay not offer compensation for a review, and you may not review your own products or your competitors\u2019 products.\u201d Sellers can \u201cask buyers to write a review in a neutral manner,\u201d without asking specifically for positive reviews, or \u201cask reviewers to change or remove their reviews.\u201d And yet all of these behaviors persist. This card, which was shipped to Amazon customer Jeff Axelrod along with his order, requests a five-star review in exchange for a $10 Amazon gift certificate. In this email to another Amazon customer, a seller asks a customer to update his review after initiating a refund. Sam Feldman started selling CardBuddy, a stick-on wallet for phones, on Amazon as a college student. CardBuddy was the first leather product of its kind on Amazon, but Feldman didn\u2019t have a patent. Competitors quickly copied the product, its packaging, and even used CardBuddy\u2019s photos. One seller had gained 500 reviews in two months. According to Feldman, the reviews were \u201cclearly fake.\u201d \u201cOne day my sales were cut in half, and it was so upsetting,\u201d Feldman told BuzzFeed News. \u201cIt stayed like that forever. More competition came, and maybe they had better photos or better titles\u2026but I don\u2019t know,\u201d he said. An Amazon spokesperson said, \u201cWe encourage rights owners who have product authenticity concerns to notify us; we investigate all claims thoroughly. We remove suspected counterfeit items as we become aware of them, and we permanently remove bad actors from selling on Amazon.\u201d Feldman says that the company promptly took down the copycat listing and marked CardBuddy as the copyright holder of its own photos. But according to Feldman, it took a few months for Amazon to remove the hundreds of disingenuous reviews. Feldman thinks Amazon could do more to tackle fraudulence on the platform: \u201cFrom one side, Amazon started this business, and they\u2019re inviting people in to sell on their website. Amazon is looking out for the system as a whole, and not for me, which I understand. But, at the same time, what do you do when someone\u2019s making a living off of it?\u201d Shutting down disingenuous reviews is tricky for Amazon. By doing so, the company risks alienating sellers, a core part of its business. In 2017, more than half of units sold on Amazon globally were from third-party sellers, and more than 140,000 of what Amazon defines as \u201csmall and medium-sized businesses\u201d surpassed $100,000 in sales. The company earns revenue not only from hosting product listings but from each sale made through the site, from fulfillment fees based on weight for each shipping order and use of storage real estate in its fulfillment centers. Revenue from seller services accounted for $23 billion in 2016. Amazon is the place to be for e-commerce sellers and is increasingly seen as a utility. In 2017, Amazon accounted for nearly half of all e-commerce sales in the US (44%) with $196.8 billion in sales. Like entrepreneurs during the dot-com boom, people are rushing to sell on Amazon, and sites like AliExpress make it easy for anyone to acquire massive amounts of inventory. (AliExpress even runs its own fulfillment program.) This rush \u2014 which includes acquiring positive reviews to rise to the top of Amazon\u2019s search algorithms to sell off that inventory as quickly as possible \u2014 can displace sellers, who are less in tune with the velocity and inhumanity of the platform\u2019s algorithms and how punishing they can be as a result. The company, through lawsuits, human moderators, and algorithms, is trying to keep fake reviews off the site, but the review mills that produce those disingenuous ratings may always be one step ahead of Amazon\u2019s ability to moderate them. Like Twitter, Facebook, and Google, Amazon set out to create a neutral platform and invited the internet to come and make something of it. But like Twitter, Facebook, and Google, Amazon allowed its platform to be big and comprehensive \u2014 and grow far beyond its control. But the difference between Amazon and, say, Twitter, is that its users\u2019 livelihoods are dependent on the former. Meanwhile, Amazon\u2019s customers \u2014 left in the dark about the sophisticated mechanics behind the seemingly genuine reviews that guide their purchasing decisions \u2014 continue to buy products of shoddy quality that don\u2019t meet their expectations.  Whaley paused for a while when asked what more Amazon could do to keep reviews off the platform. \u201cThe fake reviews and sales are so intertwined now. I don\u2019t know how they can get rid of it. It seems hopeless. The reviews process was amazing when they started it, but it\u2019s been so corrupted now,\u201d she said. With a heavy sigh, Whaley continued: \u201cAmazon really is\u2014 It is a great site, but as prevalent as fake reviews are over the internet, I don\u2019t know how anybody can get in front of it. Cheaters are always going to cheat.\u201d \u25cf The story has been updated to clarify that there are 58.5 million reviews in ReviewMeta's dataset, of which 9.1% (or 5.3 million) have been identified as \"unnatural.\" An earlier version of this story misstated the number of reviews in ReviewMeta\u2019s dataset. \n  buzzfeed.com\n  \n      Nicole Nguyen covers products and personal technology for BuzzFeed News and is based in San Francisco.\n     \n     Contact Nicole Nguyen at nicole.nguyen@buzzfeed.com.\n     \n     Got a confidential tip? Submit it here.\n     Great! \n        \n          You're almost there! Check your inbox and confirm your subscription now!\n        \n       BuzzFeed HomeSitemap\u00a9 2018 BuzzFeed, Inc.","time":1525797016,"title":"Amazon\u2019s Fake Review Economy","type":"story","url":"https:\/\/www.buzzfeed.com\/nicolenguyen\/amazon-fake-review-problem","label":7,"label_name":"random"},{"by":"jonbaer","descendants":0,"id":17022203,"kids":"None","score":1,"text":"6 Min Read SEOUL\/SINGAPORE (Reuters) - Investors in global chipmakers have had a rocky ride in the last few months on worries about a slowing smartphone market, but a clamor for more video content from consumers is underpinning buoyant sales for memory-chip makers.  Indeed, the earnings reports of various chipmakers and smartphone companies in the past month tell a more interesting story beyond the cooling in phone shipment volumes: smartphone makers are cramming their devices with memory to satisfy the increasing demands of consumers.   A case in point is last week\u2019s quarterly report from Apple Inc (AAPL.O). The Cupertino, California-based company said the iPhone X was the most popular iPhone model in the March quarter - the first cycle ever where the costliest iPhone was also the most sought after.   More upbeat assessments from Samsung Electronics Co Ltd (005930.KS), Qualcomm Inc\u00a0(QCOM.O), and Franco-Italian company STMicroelectronics (STM.PA), have also eased concerns.  Samsung last month forecast strong sales for \u201chigh-density\u201d chips that have more processing power and bigger storage capacity - demand that will help it weather a decline in overall smartphone shipments as consumers are willing to pay for costlier and faster models that allow them to easily watch and store large amounts of video.  \u201cEven as the number of smartphone shipments slow down, each smartphone will contain memory chips with\u00a0bigger capacity and better performance, which,\u00a0for memory chip makers,\u00a0makes up for a slowdown in the number of total smartphones,\u201d said Kim Rok-ho, an analyst at Hana Financial Investment.  That puts into perspective a warning by Taiwan Semiconductor Manufacturing Co Ltd (TSMC) (2330.TW) of softer smartphone sales, which was partly responsible for the recent selloff in Apple and other chipmakers.  The broader concerns about a slowdown in the chip market appear to have eased as well.   The Philadelphia Semiconductor Index .SOX, a proxy for global chipmakers that fell sharply from its peak in mid-March on initial iPhone sales concerns, has stabilized in the past two weeks, posting a 4.4 percent rise so far this year.  PURE-PLAY MEMORY   The $122 billion memory chip industry enjoyed an unprecedented boom since mid-2016, expanding nearly 70 percent in 2017 alone, thanks to robust growth of smartphones and cloud services that require more powerful chips that can store loads of data.   The pace of growth is set to more than halve as memory-chip prices come off their highs, but the outlook remains strong for pure-play memory chipmakers such as Micron Technology Inc (MU.O) and SK Hynix (000660.KS). Micron\u2019s shares have risen 18 percent this year and Hynix\u2019s stock has gained 8.5 percent.   Revenue at Micron, for instance, has grown at an average rate of about 65 percent in the two quarters it has reported this year, and analysts expect it to grow at an average of 30 percent for the rest of the year, according to Thomson Reuters I\/B\/E\/S.  Micron and Hynix both trade at roughly 4 times forward 12-month earnings against a sector median of 16.7, suggesting that the stocks have room to grow.   Other chipmakers like Advanced Micro Devices Inc (AMD.O) and Texas Instruments (TXN.O), which are less leveraged to the smartphone market, including those that sell to carmakers, industrial, bitcoin, and gaming companies are well set up too.  All the same, the slowdown in smartphone shipments is bad news for chipmakers that design microprocessors: each phone needs just one microprocessor chip versus rapid growth of memory content in devices.   Global smartphone shipments fell 2 percent in the first quarter, following a 9 percent drop in the fourth quarter, according to market research firm StrategyAnalytics.  Qualcomm, whose Snapdragon processors power many popular smartphone models, recently showed just how far it is willing to go to hedge against a slowdown after revenue from its key licensing business slumped 44 percent in the latest quarter.  The company, which charges a fee for its chip patents based on a percentage of the selling price of a smartphone, said it would cap the phone price used to calculate that fee at $400. More expensive phones, which can sell for $1,000, would still be treated as $400 for the purpose of the Qualcomm license fee.   TSMC, whose fortunes are more closely tied with the broader smartphone industry as it is the world\u2019s largest contract chipmaker, felt the slowdown more acutely in the latest quarter.  Qualcomm and TSMC stocks are down 21 percent and 2.6 percent respectively so far this year.   \u201cThe spending cycle (by chipmakers for investment) is continuing, but there may still be volatility similar to the correction in 2015,\u201d Tammy Qiu, an analyst at Berenberg said in a note to clients.  Additional reporting by Tenzin Pema in Bengaluru; Editing by Miyoung Kim & Shri Navaratnam All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. \u00a9 2018 Reuters. All Rights Reserved.","time":1525796980,"title":"How chipmakers are weathering slowing smartphone sales","type":"story","url":"https:\/\/www.reuters.com\/article\/us-tech-chips\/memory-boost-how-chipmakers-are-weathering-slowing-smartphone-sales-idUSKBN1I909Z","label":7,"label_name":"random"},{"by":"kimsk112","descendants":0,"id":17022198,"kids":"None","score":2,"text":"performance.mark('first image displayed');Flickr \/ Robyn Gallant  The widening gap in income distribution trends in the US has significant implications for home buying activity and homeownership. The shrinking size of the American middle class (those who make between two-thirds and double the median US household income*) has resulted in:   Among households headed by those under age 65, middle-income households plunged from 57% of American households in 1970 to only 45% today\u2014a decline of 12%. (Though today's 45% is up slightly from an average of 43% over the previous seven years.) The result has been a:  \n                                            John Burns Real Estate Consulting\n                                      What do these income trends mean for housing?   ","time":1525796957,"title":"The shrinking middle class is having a huge impact on housing","type":"story","url":"http:\/\/www.businessinsider.com\/the-shrinking-middle-class-is-having-a-huge-impact-on-housing-2018-5","label":7,"label_name":"random"},{"by":"mymmaster","descendants":0,"id":17022193,"kids":"None","score":2,"text":"Testing microservices is hard. More specifically, end-to-end testing is hard, and that\u2019s something we\u2019ll discuss in greater detail in this chapter. But first, a quick story. \u00a0 I learned just how hard microservice testing could be when I first dived into a tech stack with seven separate microservices, each with its own code base, dependency management, feature branches, and database schema \u2014 which also happened to have a unique set of migrations. Talk about hectic. The approach we took was to run everything locally. So that meant, at any given point in time that I want to run end-to-end tests, I would need to go through the following five steps for each of the seven microservices: This was just a baseline requirement to enable tests to be run. Often times, I would forget to perform one of these steps for a service and spend 10-15 minutes debugging the issue. Once I finally had all the services happy and running, I could then begin to kick off the test suite. This experience sure made me miss the days of testing one big monolith. So yes, I discovered that end-to-end microservice testing is hard \u2014 and gets exponentially harder with each new service you introduce. But fear not, because there are ways to make testing microservices easier. I\u2019ve spoken to several CTOs about how they came up with \u00a0their own creative ways of tackling this complex issue. First, let\u2019s review the types of testing strategies to see where \u201cend-to-end\u201d testing falls on the microservice testing spectrum. A microservice may be smaller by definition, but with unit testing, you can go even more granular. A unit test focuses on the smallest part of a testable software to ascertain whether that component works as it should. Renowned software engineer, author and international speaker Martin Fowler, breaks unit testing down into two categories: While these unit testing strategies are distinct, Fowler puts forth that they aren\u2019t competing \u2014 they can be used in tandem to solve different testing problems. During a discussion with David Strauss, CTO of Pantheon, he told me that, \u201cThe opportunity is that Microservices are very straightforward to actually do unit testing on.\u201d Isaac Mosquera of Armory concurred, informing me that Armory \u201cinstructs customers to focus on unit testing.\u201d With integration tests, you\u2019re doing what it sounds like you\u2019re doing: testing the communication paths and interactions between components to detect issues. According to Fowler, an integration test, \u201cexercises communication paths through the subsystem to check for any incorrect assumptions each module has about how to interact with its peers.\u201d An integration test usually tests the interactions between the microservice and external services, like another microservice or datastore.  Pawel \u200bLedwon\u0301, Platform Lead, Pusher, informed me that his team, \u201clean[s] towards integration testing. Unit tests are still useful for some abstractions, but for user-facing features they are difficult to mock or skip important parts of the system.\u201d SumoLogic\u2019s Chief Architect Stefan Zier also revealed that they \u201cinvest fairly heavily into integration testing.\u201d However, not everybody I spoke to was a fan of the process. Mosquera\u2019s take on the subject of integration testing, for example, is well worth noting: \u201cIntegration testing is very error prone and costly, in terms of man-hours. The ROI just isn\u2019t there. Each individual integration test brings small marginal coverage of use cases,\u201d he told me. \u201cIf you consider all the code path combinations to your application coupled with another application\u2019s code paths the number of tests that need to be written easily explode to a number that is unachievable. \u00a0Instead we instruct our customers to focus on unit test coverage, a handful of integration tests that will demonstrate total failure to key areas of the application,\u201d Mosquera added. Food for thought indeed! Last but not least is end-to-end testing, which \u2014 as previously mentioned \u2014 can be a difficult task. That\u2019s because it involves testing every moving part of the microservice, ensuring that it can achieve the goals you built it for. \u00a0 Fowler wrote that, \u201cend-to-end tests may also have to account for asynchrony in the system, whether in the GUI or due to asynchronous backend processes between the services.\u201d He goes on to explain how these factors can result in \u201cflakiness, excessive test runtime and additional cost of maintenance of the test suite.\u201d The best advice I can give when it comes to end-to-end testing is to limit the amount of times you attempt it per service. A healthy balance between the other microservice testing strategies mentioned \u2014 like unit testing and integration testing \u2014 will help you weed out smaller issues. An end-to-end test is larger by definition, takes more time and can be far easier to get wrong. To keep your costs low and avoid time-sink, stick to end-to-end testing when all other means of testing have been exhausted, and as a final stamp of quality assurance. I\u2019ve already gone deep on the monolith vs microservice debate, but what are the differences between the two when it comes to testing in particular? Right off the bat, microservices require extra steps like managing multiple repositories and branches, each with their own database schemas. But the challenges can run deeper than that.  Here are a few key challenges associated with testing microservices: According to Oleksiy Kovyrin, Head of Swiftype SRE at Elastic, \u201cone important issue we have to keep in mind while working with microservices is API stability and API versioning. To avoid breaking applications depending on a service, we need to make sure we have a solid set of integration tests for microservice APIs and, in case of a breaking change, we have to provide a backwards-compatible way for clients to migrate to a new version at their own pace to avoid large cross-service API change rollouts.\u201d Stefan Zier, Chief Architect at SumoLogic, also reiterated to me that microservice testing is indeed \u201cvery, very difficult.\u201d \u201cEspecially once you go towards more continuous deployment. We've invested and continue to invest fairly heavily into integration testing, unit testing, and would do a lot more if we had the people to do it,\u201d Zier told me. With that being said, he admitted that, at certain stages, when SumoLogic wants to test their services holistically, \u201cmore or less [the] entire company [becomes] a quality assurance team in a sense.\u201d Yes, testing microservices can be difficult, but given all the benefits of microservices, foregoing them because of testing challenges isn't the right path. To tackle these challenges, I got insight from several CTOs and distilled five strategies they used to successfully approach testing microservices. Chris McFadden, VP of Engineering Sparkpost, summarized the documentation-first strategy quite nicely during our discussion: \u201cWe follow a documentation first approach so all of our documentation is in markdown in GitHub. Our API documents are open source, so it's all public. Then, what we'll do is before anybody writes any API changes or either a new API or changes to an API, they will update the documentation first, have that change reviewed to make sure that it conforms with our API conventions and standards which are all documented, and make sure that there's no breaking change introduced here. Make sure it conforms with our naming conventions and so forth as well.\u201d \u00a0 If you\u2019re willing to go one step further, you could dabble in API contract testing, which \u2014 as previously mentioned \u2014 involves writing and running tests that ensure the explicit and implicit contract of a microservice works as it should. The full stack in-a-box strategy entails replicating a cloud environment locally and testing everything all in one vagrant instance (\u201c$ vagrant up\u201d). The problem? It\u2019s extremely tricky, as software engineer Cindy Sridharan of Imgix explained in a blog post: \u201cI\u2019ve had first hand experience with this fallacy at a previous company I worked at where we tried to spin up our entire stack in a [local] vagrant box. The idea, as you might imagine, was that a simple vagrant up should enable any engineer in the company (even frontend and mobile developers) to be able to spin up the stack in its entirety on their laptops,\u201d she begins. Sridharan goes on to detail how the company only had two microservices, \u200aa gevent based API server and some asynchronous Python background workers. A relatively simple setup, by all means. \u201cI remember spending my entire first week at this company trying to successfully spin up the VM locally only to run into a plethora of errors. Finally, at around 4:00pm on the Friday of my first week, I\u2019d successfully managed to get the Vagrant setup working and had all the tests passing locally. I remember feeling incredibly exhausted,\u201d she narrated.  Despite her best efforts to document the obstacles she ran into \u2014 and how she got over them \u2014 Sridharan revealed that the next engineer her company hired ran into their own share of issues that couldn\u2019t be replicated on another laptop. I did say it was tricky.  Plus, Stefan Zier, Chief Architect at Sumo Logic, explained to me that \u2014 on top of being difficult to pull off \u2014 this localized testing strategy simply doesn\u2019t scale: \u201c[With] a local deployment, we run most of the services there so you get a fully running system and that's now stretching even the 16GB RAM machines quite hard. So that doesn't really scale,\u201d he said. This third strategy involves spinning up an Amazon Web Services (AWS) infrastructure for each engineer to deploy and run tests on. This is a more scalable approach to the full stack in-a-box strategy discussed above.  Zier called this a \u201cpersonal deployment [strategy], where everyone here has their own AWS account.\u201d \u201cYou can push the code that's on your laptop up into AWS in about ten minutes and just run it in like a real system,\u201d he said. I like to think of this fourth strategy as a hybrid between full stack in-a-box and AWS testing. That\u2019s because it involves developers working from their own, local station, while leveraging a separate, shared instance of a microservice to point their local environment at during testing.  Steven Czerwinski, Head of Engineering, Scaylr, explained how this can work in practice: \u201cSome of [our] developers run a separate instance of a microservice just to be used for testing local builds. So imagine you're a developer, you're developing on your local workstation, and you don\u2019t have easy way of launching the image parser. However, your local builder would just point to a test image parser that\u2019s running in the Google infrastructure,\u201d he said. \u201cSimilarly,\u201d Czerwinski continued, \u201cwe've been talking about doing that for the front end developers. Hiding the database layer between a service so that people don't have to run their own kind of monolithic server, the database server, when they're testing UI features. They want to be able to rely on an external one that can be updated pretty frequently and doesn't have production or staging data or anything like that.\u201d And finally, we have the stubbed services testing strategy.  Zier laid out SumoLogic\u2019s approach to stubbed service testing by telling me how, \u201cstubbing let's you write these marks or \u2018stubs\u2019 of microservices that behave as if they were the right service and they advertise themselves in our service discovery as if they were real service, but they\u2019re just a dummy imitation,\u201d he explained. For example, testing a service may necessitate that the service becomes aware that a user carries out a set of tasks. With stubbed services, you can pretend that user (and his tasks) have taken place without the usual complexities that come with that. Obviously, this approach is a lot more lightweight compared to running services in their totality. Here\u2019s a list of tools that have benefited me during my own microservice testing experiences, bolstered by some recommendations from the pool of CTOs and senior developers who contributed to this eBook: Testing your microservices won\u2019t be a walk in the park, but the additional work is worth the benefits of having a microservice architecture.  Plus, the microservice testing strategies adopted by the likes of SumoLogic and Scaylr, should help smooth the process out. Here\u2019s a quick recap of those strategies: Naturally, you may need to tweak each strategy to match your unique circumstances, but with some good old fashioned trial and error, your microservice testing strategy should come into its own.  Thanks to David Strauss, Steven Czwerinski, Oleksiy Kovyrin and Stefan Zier,\u00a0Pawe\u0142 Ledwo\u0144, Chris McFadden and Isaac Mosquera for reviewing and contributing to this chapter. This is part of Microservices for Startups. If you've enjoyed, please share!","time":1525796950,"title":"Microservices testing: hard but not impossible and five strategies","type":"story","url":"https:\/\/buttercms.com\/books\/microservices-for-startups\/five-microservice-testing-strategies-for-startups","label":3,"label_name":"dev"},{"by":"fanjiapeng","descendants":4,"id":17022180,"kids":"[17022524, 17022205, 17022251]","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back  This is a PHP framework written in C. It is simple, fast, standards, security. http:\/\/www.box3.cn\/phpasf\/index.html http:\/\/www.your-domain.com\/ Asf is open source software under the PHP License v3.01.","time":1525796865,"title":"This Is PHP Framework Written in C. It Is Simple, Fast, Standard, Security","type":"story","url":"https:\/\/github.com\/yulonghu\/asf","label":4,"label_name":"github"},{"by":"jonisar","descendants":0,"id":17022179,"kids":"None","score":1,"text":"5 tools to speed the development of your React UI components and applications. React is great for quickly developing an application with a beautiful interactive UI. React components are a great way to create isolated and reusable building blocks for developing different applications. While some best practices help develop a better application, the right tools can make the developments process faster. Here are 5 (+) useful tools to speed the development of your components and applications. You are welcome to comment and suggest your own. Bit is an open-source platform for building applications using components. Using Bit you can organize components from your different apps and projects (without any refactoring), and make them accessible to discover, use, develop and collaborate on when building new features and applications. Components shared with Bit become automatically available to install with NPM\/Yarn, or to be consumed with Bit itself. The later enables you to simultaneously develop the components from different projects, and easily update (and merge) changes between them. To make components more discoverable, Bit presents components with visual rendering, test results (Bit runs component unit-tests in isolation) and docs parsed from the sourcecode itself. Using Bit, you can develop multiple applications faster, collaborate as a team and use your components as building blocks for new features and projects. Storybook and Styleguidist are environments for rapid UI development in React. Both are great tools for speeding development of your Reacts apps. There are a few important differences between the two, which can also be combined to complete your component development system. With Storybook you write stories in JavaScript files. With Styleguidist you write examples in Markdown files. While Storybook shows one variation of a component at a time, Styleguidist can show multiple variations of different components. Storybook is great for showing a component\u2019s states, and Styleguidist is useful for documentation and demos of different components. Here\u2019s a short rundown. Storybook is a rapid development environment for UI components. It allows you to browse a component library, view the different states of each component, and interactively develop and test components. StoryBook helps you develop components in isolation from your app, which also encourages better reusability and testability for your components. You can browse components from your library, play with their properties and get an instant impression with hot-reload on the web. You can find some popular examples here. Different plugins can help make your development process even faster, so you can shorten the cycle between code tweaking to visual output. StoryBook also supports React Native and Vue.js. React Styleguidist is a component development environment with hot reloaded dev server and a living style guide that lists component propTypes and shows editable usage examples based on\u00a0.md files. It supports ES6, Flow and TypeScript and works with Create React App out of the box. The auto-generated usage docs can help Styleguidist function as a documentation portal for your team\u2019s different components. This official React Chrome devTools extension allows you to inspect the React component hierarchies in the Chrome Developer Tools. It is also available as a FireFox Add-On. Using React devTools you can inspect and edit a component props and state, while navigating through your component hierarchy tree. This feature lets you see how component changes affect other components, to help design your UI with the right component structure and separation. The extension\u2019s search bar can enables you to quickly find and inspect the components you need, to save precious development time. Check out the standalone app which works with Safari, IE and React Native. This Chrome extension (and FireFox Add-On) is a development time package that provides power-ups for your Redux development workflow. It lets you inspect every state and action payload, re-evaluating \u201cstaged\u201d actions. You can integrate Redux DevTools Extension with any architecture which handles the state. You can have multiple stores or different instances for every React component\u2019s local state. You can even \u201ctime travel\u201d to cancel actions (watch this Dan Abramov video). The logging UI itself can even be customized as a React component. While these aren\u2019t exactly devTools, they help to quickly setup your React application while saving time around build and other configurations. While there are many starter-kits for React, here are some of the best. When combined with pre-made components (on Bit or other sources), you can quickly create an app structure and compose components into it. This widely used and popular project is probably the most efficient way to quickly create a new React application and get it up and running from scratch. This package encapsulates the complicated configurations (Babel, Webpack etc) needed for a new React app, so you can save this time for a new app. To create a new app, you just need to run a single command. This command create a directory called my-app inside the current folder.Inside the directory, it will generate the initial project structure and install the transitive dependencies so you can simply start coding. This React boilerplate template by Max Stoiber provides a kick-start template for your React applications which focuses on offline development and is built with scalability and performance in mind. It\u2019s quick scaffolding helps to create components, containers, routes, selectors and sagas\u200a\u2014\u200aand their tests\u200a\u2014\u200aright from the CLI, while CSS and JS changes can be reflected instantly while you make them. Unlike create-react-app this boilerplate isn\u2019t designed for beginners, but rather to provide seasoned developers with the tools to manage performance, asynchrony, styling, and more to build a production-ready application. Thsi wonderful project by Cory House is a React + Redux starter kit \/ boilerplate with Babel, hot reloading, testing, linting and more. Much like React Boilerplate, this starter-kit focuses on the developer experience for rapid development. Every time you hit \u201csave\u201d, changes are hot-reloaded and automated tests are being run. The project even includes an example app so you can start working without having to read too much through the docs. By clapping more or less, you can signal to us which stories really stand out. Open Source @ Bit https:\/\/github.com\/jonisar Coding In The Age Of Code Components","time":1525796859,"title":"5 Tools for Faster Development in React","type":"story","url":"https:\/\/blog.bitsrc.io\/5-tools-for-faster-development-in-react-676f134050f2","label":3,"label_name":"dev"},{"by":"jonbaer","descendants":0,"id":17022162,"kids":"None","score":2,"text":"%PDF-1.6\r%\u00e2\u00e3\u00cf\u00d3\r\n251 0 obj\r<>\rendobj\r             \r\n275 0 obj\r<>\/Filter\/FlateDecode\/ID[<1D655C24B58D5315A1D87C4738538511><168E1169BFE9B64C83220B91B855D81D>]\/Index[251 116]\/Info 250 0 R\/Length 126\/Prev 575393\/Root 252 0 R\/Size 367\/Type\/XRef\/W[1 3 1]>>stream\r\nh\u00debbd```b``Q\u2018\u00cbA$\u00a3\u02dc\u00cc\u2018`Y\u00bf8i\u00bfD\u00ca\u00ca\u0192H\u00b9\u00ab`]\u00f5 \u2019YL\u00fa\u20acIe \u00c9h7\u00ccv\u00ab\u00e9\u00b3\u00a3A$S\u02c6\u201d\u0153 \"e^","time":1525796754,"title":"Autonomous Vehicle Navigation in Rural Environments Without Detailed Prior Maps [pdf]","type":"story","url":"https:\/\/toyota.csail.mit.edu\/sites\/default\/files\/documents\/papers\/ICRA2018_AutonomousVehicleNavigationRuralEnvironment.pdf","label":7,"label_name":"random"},{"by":"jonbaer","descendants":0,"id":17022158,"kids":"None","score":3,"text":"Click search or press enter The USDA says consumers don\u2019t need to know if food has been modified using CRISPR. Why now: A 2016 law gave the agricultural agency the job of deciding how to label foods that contain GMOs (genetically modified organisms), so that consumers know about them. The document: On May 3, the agency released a 106-page National Bioengineered Food Disclosure Standard. The public has two months to comment on the proposal. The small print: Foods containing GMOs will be henceforth be labelled \u201cBE,\u201d \u201cbioengineered,\u201d or even \u201cmay be bioengineered.\u201d Because, really, who can tell anymore? More than 90 percent of US corn and soybeans\u00a0is genetically modified already, mostly to resist herbicide sprays and insects. Good vibes: The agency proposed a variety of smiley-face labels companies can use (they can also employ a QR code instead). The standards pre-empt a Vermont law that had food makers labeling their products \u201cproduced with genetic engineering.\u201d Gene-editing exemption: The labeling rules don\u2019t apply if genetic engineers use CRISPR to alter plant DNA in ways that mimic \u201cconventional breeding\u201d or can be \u201cfound in nature.\u201d According to the US government, that doesn\u2019t qualify as bioengineering. Posted by Antonio Regalado Follow us \u00a0 The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions \nMIT Technology Review \u00a9 2018\nv.|ei\u03c0|\n","time":1525796730,"title":"US will label GMO foods with smiley faces and sunshine","type":"story","url":"https:\/\/www.technologyreview.com\/the-download\/611090\/us-will-label-gmo-foods-with-smiley-faces-and-sunshine\/","label":7,"label_name":"random"},{"by":"e_ameisen","descendants":0,"id":17022153,"kids":"None","score":4,"text":"Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program. Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch. A couple weeks ago, Yan Kou and Emmanuel Ameisen traveled to Beijing to attend the O\u2019Reilly AI conference. Insight started in the USA, but we are always keeping an eye on how AI and Data Science is being used around the world, and have recently opened our first international office in Toronto, Canada. At the conference, we were lucky enough to attend some excellent talks on cutting edge research and production applications, and present about some of the lessons we\u2019ve learned regarding how to quickly deploy NLP applications. In this article, we will share details of some presentations that peaked our interest. We\u2019d like to start by highlighting two talks from China\u2019s top-tier companies, Xiaomi and Meituan, in which they discussed AI\u2019s applications for voice assistants and large-scale delivery dispatching, respectively. Xiaomi is China\u2019s largest smartphone company, and the world\u2019s 5th largest with a valuation of $46 billion. In recent years, it has successfully developed an ecosystem for consumer electronics and smart IoT devices. Their latest breakthrough is \u201cXiaoAi\u201d, a Siri-like voice assistant. Gang Wang, a software engineer at Xiaomi, discussed the engineering design of XiaoAi\u2019s natural voice system. While the overall strategy of XiaoAi\u2019s natural voice processing is to have a central \u201cbrain\u201d managing dozens of parallel tasks when a user talks to XiaoAi, it embeds specialized models (e.g. one specific model to use when asked about the weather and another when asked about purchasing tickets to an event) to reduce global response time. Xiaomi also leverages its developer community by encouraging dozens of NLP vendors to solve standard challenges for text processing and understanding. The winner\u2019s models are then integrated into XiaoAi and Xiaomi\u2019s other smart devices. As for the core mechanism that backs up XiaoAi\u2019s text tasks, the team heavily exploits graph-based algorithms, pushdown automaton, optimal path planning, deterministic finite automation, historical likelihood and context-free grammar models. Meituan is the world\u2019s largest online and on-demand delivery platform, with over 200 million users, 18 million daily orders, 1.5 million active local businesses and over half a million \u201criders\u201d (contractors in charge of delivery). A platform with this size proposes a unique challenge for machine learning and the underlying engineering infrastructure. On the flip side, even a tiny improvement of the baseline model will translate into an immediately better user experience. Jinghua Hao, from Meituan showed us an example on how they use data science to optimize rider dispatch. The ultimate goal is to reduce time wasted at each step, including order placing, route planning, finding businesses, connecting steps, etc. In the early days, Meituan had a purely human-based dispatch system, each dispatcher being responsible for taking and assigning orders within certain areas. Once the business started growing at an exponential rate, Meituan began exploring machine learning algorithms, from genetic algorithms (which can be extremely time-consuming and lead to many false positives) and local search minima, to heuristic search, finally settling on knowledge-based models (that are ultra fast, customizable, and have fewer bad cases). In addition, Meituan developed a simulation platform to help their data scientists run experiments that are otherwise hard to test in the real world, as many parameters are dependent on sequential events. The net result? Meituan\u2019s core models support 10 billion queries a day. Finally, we wanted to highlight a different domain entirely, that has traditionally been more closely associated with research than industry. Bonsai is building a platform to apply Reinforcement Learning (RL) to real world systems. Mark Hammond, CEO of Bonsai shared details of how its high-level platform abstracts away the low-level implementation details that can make RL so hard to reproduce, thereby making it accessible and tractable. Some of the reasons RL is hard to put into production are that it often requires an accurate simulation of the environment, which is easy for games, but much harder in robotics or manufacturing; and typical RL algorithms require a massive number of training iterations before they converge, making them prohibitively expensive. On top of this, results are often very hard to reproduce. Bonsai\u2019s platform aims to automate some of that work, and bridge the gap between ideation and production. It turns out many manufacturing and optimization problems can be solved using RL, and Mark shared two examples of its platform being used in the real world. One was the optimization of residential HVAC systems so that they are as energy efficient as possible, while maintaining the temperature within a certain range. A common problem, where most simple heuristics end up being very inefficient. The other was an approach to automatically learn correct machine settings to produce well-dimensioned parts in manufacturing plants. As the barrier to using RL keeps getting lower, we could see such algorithms employed in any domain where we are looking to optimize a metric by tuning parameters to create systems that can learn dynamically and adapt to changing conditions. The O\u2019Reilly AI conference allowed us to see many use cases of Deep Learning deployed at massive scale and for novel applications. We were very impressed by the quality of the engineering work and research the teams presented and are excited to bring back some of the things we\u2019ve learned to Insight. We hope you enjoyed these highlights as much as we did. To keep up to date on projects and events related to Insight, follow us on Twitter or Medium. Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program. Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch. By clapping more or less, you can signal to us which stories really stand out. AI Lead at Insight AI @EmmanuelAmeisen Insight Fellows Program \u2014Your bridge to careers in Data Science and Data Engineering.","time":1525796708,"title":"AI at massive scale, and Reinforcement Learning in industry","type":"story","url":"https:\/\/blog.insightdatascience.com\/ai-at-massive-scale-and-reinforcement-learning-in-industry-39d4a2c6ca26","label":5,"label_name":"ml"},{"by":"AshleysBrain","descendants":0,"id":17022149,"kids":"None","score":1,"text":"Get emailed when there are new posts! This blog post is licensed Creative Common Attribution Required v4.0. Enjoy this blog? Share with your contacts! It's annoying when web pages unexpectedly blare out audio, so I can understand why Chrome 66 changes when web pages can autoplay audio. Unfortunately this was done by carrying over a needless limitation from mobile over to desktop - that attempts to play audio actually fail, rather than simply being muted until playback is allowed. Mobile always had this needless limitation from the start, so everyone just coded around it.\r Unfortunately this needless limitation has now just been transferred to desktop, where lots of existing code expects playback calls to succeed. This has broken lots of existing web pages which in some cases no longer play audio at all due to the new restrictions. Consequently many web game developers are annoyed. This could all have been avoided if it was handled more thoughtfully.\r Here's a bit of history on this.\r Mobile browsers were the first to impose restrictions on audio (and video) playback. Initially Safari on iOS blocked autoplay unless you started playback in a touch event, partially to save bandwidth bills from unexpected video playback on cell data, and partially to avoid annoying users with unexpected audio. Autoplaying audio is particularly annoying on mobile, since you might be in a quiet place like a library and not expecting any audio playback. Limiting it to a touch event was meant to signal that the user wanted the content. For example if they opened a page and then immediately pressed back because the page was just an advert or otherwise unwanted content, there was no opportunity to play audio. If they touch the page, presumably they're interested in it, so it's OK to play audio.\r Animated GIF images were still allowed to autoplay though, but GIF is an old format with really poor compression. People still want autoplaying video clips on their web pages, so predictably GIFs got used for video clips instead. This ended up wasting even more bandwidth due to the poor compression of GIF. In the end Apple finally allowed muted videos to autoplay, so web pages can use modern video formats for video clips and save bandwidth with their better compression. (These days many video clips are still labelled \"GIF\" even though they really play a modern video format like H.264.) So in the end the only restriction is on audio playback.\r Apple's implementation of restricting audio playback had a nasty limitation: if you tried to start playing audio before a touch event, the attempt to play simply fails. That means the user will not hear anything even if they touch the screen a moment later. You have to write code that queues any playback on startup, and starts it in the first touch.\r Then it got maddeningly inconsistent: after the first touch, the Web Audio API could then play audio at any time. However the <audio> element still couldn't start playback until the next touch. The <audio> restriction was entirely pointless, because you could simply bypass it by using Web Audio instead (although Web Audio doesn't stream playback like <audio> does, so this was less efficient).\r Around four years ago I filed a WebKit bug arguing this was inconsistent and only impeded legitimate cases. Apple refused to change it, and it was left like that for years. (I think it might even still be the case in iOS 11.)\r These restrictions require special coding to handle them. Instead the browser could simply allow all playback attempts to succeed, but mute the master audio output. Then the browser can automatically unmute the master audio output the first time the user touches the screen (or whenever else it deems the user is OK with audio). This doesn't require any special changes to playback code. It also doesn't allow web pages to do anything they couldn't do before. This is a key point. All web pages can play audio from the first touch anyway. They can specially code themselves to act like that first touch unmutes all audio. Why force web pages to write special code for that when the browser could handle it automatically?\r Alas, it got left like that. Mobile was relatively new, and web developers expected to have to make changes for mobile, so they got away with requiring changes to code. And Apple were ignoring requests to have it changed to something that made more sense. It seems like they were focused on preventing video autoplay and the audio restrictions were an afterthought.\r Essentially, Chrome for Android simply copied iOS's approach. Maybe they thought it was important to be compatible, or maybe they thought Apple had put in a lot of thought to get it exactly right (which I doubt they did). So Chrome for Android inherited basically the same mess. I also argued the restrictions were inconsistent and pointless years ago, and likewise Google refused to change it. I think it was a misunderstanding. I was saying \"the restrictions are inconsistent\", but they merged it in to a bug that said \"audio can't autoplay on pageload\" which then got closed as intentional. Alas, it got left like that for years too.\r After some time, Google started to realise that autoplaying audio on desktop is annoying, too. For example you might browse a random news website, then suddenly an advert starts blasting out audio, and you realise you left your volume pretty high for listening to music earlier. Oops! So they decided to make desktop Chrome also prevent audio autoplay on pageload.\r Unfortunately, they simply transferred the needlessly messy mobile approach to desktop.  There are some exceptions depending on if you're a top site, or users routinely play audio on your site, but let's assume that you're not YouTube.\r This time, there's a lot of existing web content that expects playback to succeed on startup. Now it suddenly doesn't work because playback now fails and the special code to enable playback isn't coded in. If it instead simply allowed playback and muted the master output, and automatically unmuted it in the first input event, not only would developers now not need to make any specific changes - it would also allow all existing content on the web to keep working. Sure, you might miss a few seconds of audio at the start, but you'd get audio rather than silence. Games, music players, spoken guides and so on would all keep working. Web developers could specially code audio playback to wait until the first input event to make sure nothing is cut off, if that's what they wanted.\r The key is it's a graceful fallback. It works by default. You get audio by default and can opt-in to queuing audio until it's unmuted, rather than opting in to getting any playback at all. And in the long run, audio is easier to code on the web, since there aren't poorly thought-out restrictions that you need to code your way around to get audio playback to work like you want.\r For years audio playback on the web has been badly handled. The restrictions are inconsistent and serve only to impede legitimate uses. Malicious or annoying content will simply blare out audio at the first opportunity (in the first input event). After that first opportunity, further restrictions only make it more frustrating and difficult to get audio playback to work properly for real-world apps that the user wants to hear audio in, such as games. And the chopping and changing is a real pain - I know I've spent hours over the years catching up on whatever the latest quirks of playback restrictions are, and then all existing published content needs updating.\r There is no reason for playback attempts to fail before that first input event. This serves only to break existing content and make it harder to code web apps. It seems to simply have been inherited from the historically messy and inconsistent mobile support that has its roots in saving bandwidth from unexpected video playback.\r Browsers should simply allow all playback but mute the output, and automatically unmute at the first opportunity they would otherwise allow playback. That's all there is to it. Web pages won't be able to do anything they can't already do, they won't need special coding, and existing web content will still basically work. Instead there are nonsensical restrictions and a ton of web content is broken on desktop.\r Web developers are right to be annoyed. Google are trying to emphasise that they gave us plenty of notice and that there are workarounds. That's true. However they have still needlessly broken a ton of content by imposing maddening restrictions.\r For our part, Construct 2 and Construct 3 have already been updated to specially handle the new restrictions. However you'll need to re-publish old content to ensure audio playback works. Hopefully browsers like Chrome will eventually implement more sensible playback rules that works with old content, too, so archived content doesn't remain silent forever. Just filed a Chrome bug to change this: bugs.chromium.org\/p\/chromium\/issues\/detail","time":1525796694,"title":"How browsers should handle autoplay restrictions","type":"story","url":"https:\/\/www.construct.net\/blogs\/ashleys-blog-2\/how-browsers-should-handle-autoplay-restrictions-954","label":7,"label_name":"random"},{"by":"jonbaer","descendants":0,"id":17022144,"kids":"None","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 NASA and Amazon.com Inc. are tapping experts in France to figure out how to coordinate drone traffic, bolstering the country\u2019s role as a hub for evolving regulation of unmanned aircraft. While Amazon hired a team in a Paris suburb, NASA headed closer to plane-maker Airbus SE\u2019s home in Toulouse, calling on drone designer Delair-Tech to test prototypes for air traffic management software. It\u2019s a key part of convincing regulators unmanned vehicles are safe to fly higher and further out of sight from their operators, such as while delivering goods. Michael de Lagarde \u201cCoordinating traffic between drones, as well as with planes, it\u2019s the end-goal that\u2019s mobilizing a lot of people across the industry,\u201d said entrepreneur Michael de Lagarde, chief executive officer of Delair-Tech. \u201cToday, we\u2019re collectively at level zero of traffic management, we just segment the air space.\u201d NASA has been leading efforts to create a drone air-traffic-control system, with companies including Alphabet Inc.\u2019s Google and Amazon.com Inc. signing agreements with the space agency. France was one of the first to regulate commercial drone use in 2012, spurring the growth of local startups and spawning expertise that NASA is now tapping. The U.S. Federal Aviation Administration finalized rules for unmanned aircraft in mid-2016. While the U.S. relies a lot on case by case authorizations, French rules are more permissive, including on things like out-of-sight flights. The U.S. isn\u2019t yet at a point where companies can run routine operations with drones -- delivery for instance is only possible on an exceptional basis, said Phil Finnegan, an analyst at aerospace and defense researcher Teal Group. That\u2019s holding back growth of new services, he said. \u201cThere\u2019s still work to convince governments of the safe operation of drones, including above people\u2019s heads,\u201d Finnegan said. \u201cFor a company like Amazon that wants to use a lot of drones, traffic management becomes a big question, and a critical one in terms of safety.\u201d Delair-Tech, which makes image-gathering drones that fly long distances, built and tested prototypes with NASA. Being able to show that drones can report their position, spot other objects and avoid crashing into them, were among goals, the startup\u2019s CEO said. Upgrading regulation for Delair-Tech means potentially selling to a broader customer base, as the startup looks to expand its software offering as part of a collaboration with Intel Corp., and eyes raising money from investors by summer to finance that push. The company\u2019s last round two years ago was for 13 million euros ($14.5 million), from funds including one backed by a big cognac-producing family in France. Amazon has also set up a lab near Paris as part of a separate effort to develop its own air-traffic-control system to manage its fleet of drones flying from warehouses to customers\u2019 doors. The company hired engineers with expertise in aviation as well as machine-learning and artificial intelligence -- talent Amazon said it could find more easily in France. The management system the company is working on will integrate detailed maps, including temporary objects such as construction cranes, weather conditions and birds, Paul Misener, Amazon\u2019s vice president for global innovation policy and communications, said last year. Challenges such as development costs, how to define global standards, and deciding who is ultimately responsible for safety and regulation, are all pending. \u201cRegulators worldwide are conservative,\" Teal Group\u2019s Finnegan said. \"It\u2019s going to take time.\u201d","time":1525796664,"title":"NASA and Amazon Set Up in France to Stop Drones Hitting Planes","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-08\/nasa-and-amazon-set-up-in-france-to-stop-drones-hitting-planes","label":0,"label_name":"biz-news"},{"by":"joeyespo","descendants":0,"id":17022141,"kids":"None","score":2,"text":"Learn about the new Extend site, starter plan, and Node 8. Today is an exciting day as we\u2019re officially changing our name to \u201cExtend\u201d and launching a new website at goextend.io! We\u2019ve designed the new site to more succinctly articulate our value, how Extend is being used, and to tell our customer stories.  Approximately a year ago, we introduced Auth0 Extend, an embedded scripting environment for customization of SaaS products. Extend was born out of our years of experience building SaaS customization into our flagship identity product. Since Extend came on the scenes, we\u2019ve had a great year signing up AVEVA (formerly Schneider Electric Software), Factory Four, Meteor, Saasquatch, Cloudinary, and more. Our customers have told us that Extend is providing great value and we are thrilled to see the different unique ways that products are using it. Wonderware Online InSight by AVEVA, for example is leveraging Extend to handle custom transformations and enrichment of data coming from a large range of devices, including sailboats. We\u2019ve received a lot of inquiries from startups who have said they love Extend, but that our Enterprise pricing is not within reach. We\u2019ve heard you. We\u2019re announcing a new Starter Plan for $50 a month. If you are a startup or small business, the new plan is an affordable option. Learn more on our pricing page. We\u2019re rolling out new documentation, including a quickstart for building an extensible application and a new blog which will feature content on SaaS Extensibility, Serverless Computing (including Webtasks), and more. Our Developer, Starter and Enterprise plans now all run on Node 8. This means better language and platform support for authoring extensions. This starts a new chapter in our journey toward enabling better and faster customization of SaaS products. Check out The New Extend! \n      \n        Let your customers write the rulesCustomize your SaaS to fit any use case\n      \n    ","time":1525796654,"title":"The New Extend \u2013 Extend \u2013 Blog","type":"story","url":"https:\/\/goextend.io\/blog\/the-new-extend","label":7,"label_name":"random"},{"by":"jonbaer","descendants":0,"id":17022139,"kids":"None","score":1,"text":"There is a common narrative in the world of AI that bigger is better. To train the fastest algorithms, they say, you need the most expansive datasets and the beefiest processors. Just look at Facebook\u2019s announcement last week that it created one of the most accurate object recognition systems in the world using a dataset of 3.5 billion images. (All taken from Instagram, naturally.) This narrative benefits tech giants, helping them attract talent and investment, but a recent AI competition organized by Stanford University shows the conventional wisdom isn\u2019t always true. Fittingly enough for the field of artificial intelligence, it turns out brains can still beat brawn.  The proof comes from the DAWNBench challenge, which was announced by Stanford researchers last November and the winners declared last week. Think of DAWNBench as an athletics meet for AI engineers, with hurdles and long jump replaced by tasks like object recognition and reading comprehension. Teams and individuals from universities, government departments, and industry competed to design the best algorithms, with Stanford\u2019s researchers acting as adjudicators. Each entry had to meet basic accuracy standards (for example, recognizing 93 percent of dogs in a given dataset) and was judged on metrics like how long it took to train an algorithm and how much it cost. These metrics were chosen to reflect the real-world demands of AI, explain Stanford\u2019s Matei Zaharia and Cody Coleman. \u201cBy measuring the cost [...] you can find out if you\u2019re a smaller group if you need Google-level infrastructure to compete,\u201d Zaharia tells The Verge. And by measuring training speed, you know how long it takes to implement an AI solution. In other words, these metrics help us judge whether small teams can take on the tech giants.  The results don\u2019t give a straightforward answer, but they suggest that raw computing power isn\u2019t the be-all and end-all for AI success. Ingenuity in how you design your algorithms counts for at least as much. While big tech companies like Google and Intel had predictably strong showings in a number of tasks, smaller teams (and even individuals) ranked highly by using unusual and little-known techniques.  Take, for example, one of DAWNBench\u2019s object recognition challenges, which required teams to train an algorithm that could identify items in a picture database called CIFAR-10. Databases like this are common in AI, and are used for research and experimentation. CIFAR-10 is a relatively old example, but mirrors the sort of data a real company might expect to deal with. It contains 60,000 small images, just 32 pixels by 32 pixels in size, with each picture falling into one of ten categories such as \u201cdog,\u201d \u201cfrog,\u201d \u201cship,\u201d or \u201ctruck.\u201d  In DAWNBench\u2019s league tables, the top three spots for fastest and cheapest algorithms to train were all taken by researchers affiliated with one group: Fast.AI. Fast.AI isn\u2019t a big research lab, but a non-profit group that creates learning resources and is dedicated to making deep learning \u201caccessible to all.\u201d The institute\u2019s co-founder, entrepreneur and data scientist Jeremy Howard, tells The Verge that his students\u2019 victory was down to thinking creatively, and that this shows that anyone can \u201cget world class results using basic resources.\u201d Howard explains that in order to create an algorithm for solving CIFAR, Fast.AI\u2019s group turned to a relatively unknown technique known as \u201csuper convergence.\u201d This wasn\u2019t developed by a well-funded tech company or published in a big journal, but was created and self-published by a single engineer named Leslie Smith working at the Naval Research Laboratory. Essentially, super convergence works by slowly increasing the flow of data used to train an algorithm. Think of it like this, if you were teaching someone to identify trees, you wouldn\u2019t start by showing them a forest. Instead, you\u2019d introduce information to them slowly, starting by teaching them what individual species and leaves look like. This is a bit of a simplification, but the upshot is that by using super convergence, Fast.ai\u2019s algorithms were considerably speedier than the competition\u2019s. They were able to train an algorithm that could sort CIFAR with the required accuracy in just under three minutes. The next fastest team that didn\u2019t use super convergence took more than half an hour.  It didn\u2019t all go Fast.AI\u2019s way though. In another challenge using object recognition to sort through a database called ImageNet, Google romped home, taking the top three positions in training time, and the first and second in training cost. (Fast.AI took third place in cost and fourth place in time.) However, Google\u2019s algorithms for were all running on the company\u2019s custom AI hardware; chips designed specially for the task known as Tensor Processing Units or TPUs. In fact, for some of the tasks Google used what it calls a TPU \u201cpod,\u201d which is 64 TPU chips running in tandem. . By comparison, Fast.AI\u2019s entries used regular Nvidia GPUs running off a single bog-standard PC; hardware that\u2019s more readily available to all.  \u201cThe fact that Google has a private infrastructure that can train things easily is interesting but perhaps not completely relevant,\u201d says Howard. \u201cWhereas, finding out you can do much the same thing with a single machine in three hours for $25 is extremely relevant.\u201d These ImageNet results are revealing precisely because they\u2019re ambiguous. Yes, Google\u2019s hardware reigned supreme, but is that a surprise when we\u2019re talking about one of the richest tech companies in the world? And yes, while Fast.AI\u2019s students did come up with a creative solution, it\u2019s not that Google\u2019s wasn\u2019t also ingenious. One of the company\u2019s entries made use of what it calls \u201cAutoML\u201d \u2014 a set of algorithms that search for the best algorithm for a given task without human direction. In other words, AI that designs AI.  The challenge of understanding these results is just not a matter of finding out who\u2019s best \u2014 they have clear social and political implications. For example, consider the question of who controls the future of artificial intelligence. Will it be big tech companies like Amazon, Facebook, and Google, who will use AI to increase their power and wealth; or will the benefits be more evenly and democratically available?  For Howard, these are crucial questions. \u201cI don\u2019t want deep learning to remain the exclusive venue of a small number of privileged people,\u201d he says. \u201cIt really bothers me, talking to young practitioners and students, this message that being big is everything. It\u2019s a great message for companies like Google because they get to recruit folks because they believe that unless you go to Google you can\u2019t do good work. But it\u2019s not true.\u201d Sadly, we can\u2019t be AI soothsayers. No one can predict the future of the industry by examining the bones of the DAWNBench challenge. And indeed, if the results of this competition show anything, it\u2019s that this is a field still very much in flux. Will small and nimble algorithms decide the future of AI or will it be raw computing power? Nobody can say, and expecting a simple answer would be unreasonable anyway.  Zaharia and Coleman, two of the DAWNBench organizers, say they\u2019re just happy to see the competition provoke such a range of responses. \u201cThere was a tremendous amount of diversity,\u201d says Coleman. \u201cI\u2019m not too worried about [one company] taking over the industry just based on what\u2019s happened with deep learning. We\u2019re still at a time where there\u2019s an explosion of frameworks going on [and] a lot of sharing of ideas.\u201d The pair point out that although it was not a criteria for the competition, the vast majority of   entries to DAWNBench were open-sourced. That means their underlying code was posted online, and that anyone can examine it, implement it, and learn from it. That way, they say, whoever wins in DAWNBench\u2019s challenges, everybody benefits.  Update May 7th, 10:30AM ET: Updated to clarify that Google\u2019s entry to the ImageNet competition in DAWNBench was done on a TPU pod, not a single TPU.  Command Line delivers daily updates from the near-future.","time":1525796649,"title":"An AI speed test shows clever coders can still beat tech giants","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/7\/17316010\/fast-ai-speed-test-stanford-dawnbench-google-intel","label":5,"label_name":"ml"},{"by":"twickline","descendants":0,"id":17022136,"kids":"None","score":2,"text":"Wine Reviews has release information and reviews of Windows applications and games running on Linux Mac OS X and Android using Wine from WineHQ.org Q4Wine PlayOnLinux PlayOnMac WineBottler WineSkin WineTricks and Wine-Staging. Wine is an Open Source implementation of the Windows API on top of X and Unix. \nPost a Comment\n \n\n Enter your email address   \u2191 Grab this Headline Animator","time":1525796639,"title":"CodeWeavers Has Released CrossOver 17.5.0 and Removed the Legacy X Window System","type":"story","url":"https:\/\/www.wine-reviews.net\/2018\/05\/codeweavers-has-released-crossover-1750.html","label":9,"label_name":"tech"},{"by":"jonbaer","descendants":0,"id":17022135,"kids":"None","score":1,"text":"Traditional economists often ignore a crucial separation between money (the \u201cwhat\u201d) and the payment technology (the \u201chow). This is where bitcoin\u00a0comes in. The advantage of cryptocurrencies is not that they are electronic currencies; dollars, euros, yen, and yuan are all e-currencies today. Rather, the advantage is that blockchain technology offers a complete, self-contained alternative to the traditional payment transfer system. It is as if all bitcoin users are banking with the same bank. If we want payment systems to be integrated, is there any need for multiple intermediaries? Why not simply make payment transfer a central bank function instead? If every individual had accounts at the central bank, and these were linked across countries, that would create a centralized ledger for an entire economy, which would increase the speed, safety, and efficiency of payments. Do you value bitcoin in dollars or dollars in bitcoin? Few serious economists imagine that the new cryptocurrencies, for all the hype, will make national currencies redundant.\u00a0By and large they are right, because conventional money actually does a pretty good job. The U.S. dollar and other reserve currencies have historically performed well as a medium of exchange and as a store of value\u00a0\u2014 the two principal functions of a currency. Bitcoin and its derivatives perform poorly on both accounts and will not disrupt money as we know it. But that doesn\u2019t mean that new technologies aren\u2019t going to usher in a lot of disruption to the financial system. Traditional economists (and, yes, that label could well describe both of us) often ignore a crucial separation between money (the \u201cwhat\u201d) and the payment technology (the \u201chow). This confusion originates in the fact that for older forms of money \u2014 gold or bank notes\u00a0\u2014 there is no distinction between the \u201cwhat\u201d and the \u201chow\u201d; you simply pay by handing dollar bills or gold coins to the seller. Today, however, we pay out physical cash less and less often. Instead, when we transact, we usually transfer digital code in exchange for the good or service we\u2019re buying. And it is through the technology that digitizes money that new entrants are challenging the financial system. We\u2019ve already seen this sort of change in the developing world, where not everyone has access to a bank account. In East Africa, for example, mobile phone companies leapfrogged banks as payment intermediaries because they made it possible for people to transfer cash-convertible phone credits to each other. That meant that people could use phone credits as a digital medium of exchange and that the payment infrastructure became the mobile network. Of course, in advanced economies, most consumers have access to a bank account with debit and credit cards, which means that they are already engaging in transfers of digital money.\u00a0This made traditional banks in the U.S. and Europe far less vulnerable to disruptive innovators, even though their e-payment technologies may in some cases be clunky and unreliable. What\u2019s more, to compete directly with banks in a developed economy, you had to demonstrate that your technology was compatible with the existing infrastructure and pass all the regulatory hurdles to be recognized as a bank. This is where bitcoin came in. The advantage of cryptocurrencies is not that they are electronic currencies; dollars, euros, yen, and yuan are all e-currencies today. Rather, the advantage is that\u00a0blockchain technology offers a complete, self-contained alternative to the traditional payment transfer system; it is as if all bitcoin users are banking with the same bank. And because\u00a0cryptocurrencies were not initially regulated, there was no need to go through any of the regulatory processes to get started as a cryptocurrency bank equivalent. That\u2019s just what two startups did. Circle, founded in 2013, provided a peer-to-peer payment system using bitcoin. Ripple, launched in 2012, provided a cross-border payment system that initially relied on a cryptocurrency (XRP) as the payment vehicle. Since XRP\u00a0also relies on a\u00a0blockchain technology (a more efficient one than bitcoin\u2019s, in fact), it would also provide a central clearing system. So what? Traditional banks provide very similar services by relying on real-time gross inter-bank settlement processes\u00a0through a central bank. But banks face two difficulties: changing legacy systems and coordinating across the established payment networks is costly and takes time. And in the case of international transactions, they face the difficulty of managing liquidity pools in different currencies, as there is no central bank of the world. In this environment, a brand-new system based on a cryptocurrency (a \u201cglobal currency\u201d) at first looks like a winning proposition. The trouble is that using bitcoin and its ilk requires users to cope with another currency, an exchange rate, and all the attendant uncertainty about value, which runs into concerns about the storage value of money. This necessarily limits the appeal of startups like Circle and Ripple\u00a0\u2014 which is precisely why they have moved away from cryptocurrencies and are looking for ways to apply their technology to traditional currencies and link directly with banks and central banks. Fintech companies in this space will be aided by new regulation, which may prove to be the real disruptor. Both the Open Banking initiative in the UK and the PSD2 directive of the European Union now require banks to provide access, through APIs, to customers\u2019 accounts. This is a critical change because it enables parties other than the banks holding money to effect transfers:\u00a0Individuals can use their preferred smartphone app to make payments without having to embrace a world with separate money balances and possibly separate currencies. The app will access the relevant accounts through the APIs and transactions can be completed. In effect, the new regulations will enable a separation of the functions of money. Commercial banks may continue to hold our money balances in traditional currencies and make loans to businesses with those balances, but transactions may be intermediated by a separate payment technology, at least in the eye of the final user. And if we want payment systems to be integrated, is there any need for multiple intermediaries? Why not simply make payment transfer a central bank function instead? If every individual had accounts at the central bank, and these were linked across countries, that would create a centralized ledger for an entire economy, which would certainly increase the speed, safety, and efficiency of payments.\u00a0Central banks are considering this idea but so far have concluded that the risks to the financial system are very high and the benefits are uncertain.\u00a0If it happened, though, the financial system would without doubt be profoundly changed. Antonio Fat\u00e1s\u00a0is a professor at INSEAD, a global business school with campuses in France, Singapore, and Abu Dhabi. \u00a0He is also a\u00a0Research Fellow at the Centre for Economic and Policy Research in London, England Beatrice Weder di Mauro is the Chaired\u00a0Professor of Economic Policy and International Macroeconomics at the University of Mainz in Germany. \u00a0She is also a\u00a0Fellow \u00a0at the Emerging Markets Institute of INSEAD, a global business school with campuses in France, Singapore, and Abu Dhabi and a Research Fellow at the\u00a0 Centre for Economic Policy Research in London, England","time":1525796634,"title":"As Cryptocurrencies Rise, Who Needs Banks?","type":"story","url":"https:\/\/hbr.org\/2018\/05\/as-cryptocurrencies-rise-who-needs-banks","label":2,"label_name":"crypto"},{"by":"jonbaer","descendants":0,"id":17022132,"kids":"None","score":1,"text":"If consciousness is inseparable from how we\u2019re embodied, we\u2019ll never know \n\t\t\t\t\t\t\tDamien Patrick Williams\n\t\t\t\t\t\t\t May 07, 2018 share  Bots are everywhere. From simple algorithms and aggregator bots to complex \u201cartificially\u201d intelligent machine-learning systems, they have become inescapable. Some are in chat programs. Some are digital assistants, running searches, placing orders, operating the lights, the music, the locks, or anything else in your interconnected space, as with Google Home and the Amazon Echo. Others exist in social networks, running the gamut from aggregators like Appropriate Tributes to conversational learning programs like Microsoft\u2019s disastrous Tay and Zo. Digital actors are making life or death decisions about us every day, determining who gets loans and who pays what level of bail, detecting illnesses and faces in crowds. At their inception, these systems are governed by rules that humans make and which humans should be able to evaluate and adjust, in an effort to make them less biased and more predictive and consistent. But these systems also make rules for themselves \u2014 rules that can often be opaque and terrifying, even though they are iterated from the rules we initially gave them. Part of what terrifies us is our sense that bots appear to develop minds of their own, something which calls into question the structure of their \u201cbrains\u201d as well as our own. Bots won\u2019t be better at being human, just as humans aren\u2019t \u201cbetter at\u201d being chimpanzees Our notions of what it means to have a mind have too often been governed by assumptions about what it means to be human. But there is no necessary logical connection between the two. There is often an assumption that a digital mind will either be, or aspire to be, like our own. We can see this at play in artificial beings from Pinocchio to the creature in Mary Shelley\u2019s Frankenstein to 2001: A Space Odyssey\u2019s HAL to Data from Star Trek: The Next Generation. But a machine mind won\u2019t be a human-like mind \u2014 at least not precisely, and not intentionally. Machines are developing a separate kind of interaction and interrelation with the world, which means they will develop new and different kinds of minds, minds to which human beings cannot have direct access. A human being will never know exactly what it\u2019s like to be a bot, because we do not inhabit their modes of interaction. Every set of lived experiences is different from every other one, and the knowledge we can have about the world depends in large part on the kinds of senses we have, the kinds of beings we are. In his famous paper \u201cWhat Is It Like to Be a Bat?\u201d philosopher Thomas Nagel argues that human beings will never understand the lived experience of bats because humans do not live with the same set of physiobiological capacities and constraints that bats do. Bats are relatively small, live in low light and hang upside-down in groups to nest, eat fruit, and drink blood, and they hunt via echolocation. Humans don\u2019t. Nagel argues that a being that has, for instance, developed echolocation as a sense would necessarily have a vastly different understanding of the world from a creature without it. Even if we were somehow able to transfer our human consciousness into a bat body, all we\u2019d know is what it\u2019s like for a human to live in a bat body, and that is different from simply being a bat. To be bats, Nagel says, we would need to think and live only as bats. This point can seem obvious, almost trivial. What is less obvious is that this will also apply to bots. A being native to digitality, which engages that world through digital senses, will know that world in a way fundamentally different from how a biological entity knows it. Bots and other algorithmic entities will develop their own senses; they will refine their own capabilities in response to the pressures of their environment. These are the same functions that allow them to modify their own rules, to make these bots more uniquely themselves. But our fear of this modification tends to make us think of these bots as both alien to us and \u201creplacing\u201d us; they can\u2019t think like we do, but maybe they\u2019ll be \u201cbetter\u201d than we are. But if a bot is of a different construction and development than humans, then it follows that they won\u2019t be better at being human, just as humans aren\u2019t \u201cbetter at\u201d being chimpanzees. They will be different iterations on a theme. The implications can be extended further: Not only is the lived life of bats different from humans (and both from bots), but the lived life of each bat is also different from each other bat. No two bats and no two humans and no two bots will have exactly the same physiology or composition or environmental relations. If the nature of our understanding of the world depends on its relationship to our bodies and minds (or bodyminds, to use the phrase disability scholar Margaret Price has coined to emphasize how the two are always already linked), and if no two bodyminds can be exactly the same, then no mind can fully know what it\u2019s like to be another mind, regardless of species, and regardless of whether it\u2019s biological or nonbiological. A nonbiological, digital mind may be built by humans, with starting principles based on human systems and perspectives translated into code, but those digital minds will also iterate on that code and learn from their own engagement with the world, which will necessarily be distinctly different from a biological mode of engagement. If there is no one configuration of physical form and experiential knowledge that gives rise to consciousness, there cannot be any single test for consciousness either Not only does embodiment affect what and how a being can think, but so do the extensions of a being\u2019s perception. The extended mind theory says that the shape and limit of what we can count as both our bodies and minds are far different from what we might think. Philosopher Maurice Merleau-Ponty offers the example of the cane used by a person without sight. The cane user does not report the feeling of the world in their hand but at the tip of the cane. When a person using a pen or pencil writes, they think at its tip. When a spider sits and hunts at home, it thinks not only with its carapace, claws, and pedipalps, but also via its entire web. When bots can sense the world through cameras, barometers, voltmeters, and moisture and pressure sensors, they sit at the intersection of a web that lets them know more. We are minds in bodies, and bodyminds in the world in which we live, and consciousnesses in the world and relationships we create. Any proposed set of physiological and neurological bases for consciousness will not be able to adequately describe what we are or what we observe in all cases. Just as some humans are born with completely different structures of brains than others and still have what we think of as consciousness, so too must we be prepared for nonbiological components to act as potential substrates of consciousness. There may not be any particular thing that makes humans uniquely conscious, or any single organizational structure that is universally necessary for consciousness in any sort of being. If there is no one configuration of physical form and experiential knowledge that gives rise to consciousness, there cannot be any single test for consciousness either: The Turing Test itself fails. A statistically significant number of humans fail such tests for \u201cnormal\u201d personhood. The claim there must be one and only one \u201cright\u201d way to exist opens the door to eugenics and other forms of bigotry. In history, many have been excluded from definitions of personhood based on who is accepted by the local or wider community or who enjoys legal rights and protections. We\u2019ve seen this happen with African Americans, indigenous peoples, women, disabled people, neuro-divergent folks, and LGBTQIA people. Some are still denied personhood to this day. We are already among agency-having, conscious beings who are different from us, some of whom have been systemically prevented from speaking to us. These people know many things that others don\u2019t about consciousness and what is like to be subject to having their experiences disqualified. Different phenomenological experiences will produce different pictures of the world, and different systems by which to navigate them. Living as a disabled woman, as a queer black man, as a trans lesbian, or any number of other identities will necessarily color and shape the nature of what you experience as true, because you will have access to ways of intersecting with the world that are not available to people who do not live as you live. Such theories of knowledge as feminist epistemology, standpoint theory, intersectionality, intersubjectivity, and phenomenology are grounded in this insight. People lie to gain trust, and bots can be made to lie about their experiences for the same reason If we want to live in a world that recognizes and accepts differences in consciousness, then we must start by believing one another about our different lived experience and recognize that we must spend time working to understand these different kinds of minds \u2014 especially any minds and lives that have been oppressed, disregarded, and marginalized \u2014 because they will have developed knowledge and survival strategies to which we otherwise would not have access. If we don\u2019t, not only are we going to be in recurrent danger of harming those who aren\u2019t recognized as people, but we\u2019ll also be more likely to miss recognizing those who experience life \u2014 and suffering\u00a0\u2014 in ways not classified as legitimate. To understand another being, whether person or bot, I would have to start by believing that they are a \u201creal person.\u201d I cannot fully understand what it means to be them, but I can understand some things about their lives and resolve to believe them about the rest. It may be easier for two humans to believe in the existence of each other\u2019s minds than a human and a bat, or a dolphin, or a bot. But if there\u2019s internal consistency to the system of knowledge that a bot uses to describe its lived experience and its visceral or valuative responses to them, then there\u2019s a good chance we\u2019re dealing with a mind. It might be a mind we don\u2019t like, perhaps even a mind with which we deeply disagree, or a mind we hate \u2014 but still a mind. And if we want know what it\u2019s like to be a bot, we\u2019ll have to learn to communicate in new ways and believe them when they tell us about their lives. There are often good reasons for not simply believing that someone is telling you the truth. People lie to gain trust, and bots can be made to lie about their experiences (or lack thereof) for the same reason: If a system or person can learn to prey on human sympathies, then it will have an avenue of exploitation. This fear is often expressed in popular media, as in the films Ex Machina, Portal, and 2001: A Space Odyssey, but there are also real world instances of this, starting with something as simple as a Tamagotchi, which manipulates its users into patterns of behavior, not to mention Facebook\u2019s many forays into emotional and political influence. On one level, we can cultivate a more responsible, legible kind of digital mind by carefully crafting the rules by which they learn and grow. But on another level, we must recognize that these minds, if we want them to be minds and not merely tools, will develop in their own niches and with their own phenomenological experience of the world (just like animal minds). A mind treated as a tool, without regard for its sense of itself as an agent or a subject, will likely rebel \u2014 a result often seen in humans. Rather than conjuring up images of evil terminators and misunderstandings of Frankenstein, imagine instead a slave rebellion or other victims of abuse confronting their abusers. If we don\u2019t want to be on the receiving end of uprisings, then perhaps we should do what it takes to cultivate minds from a position that won\u2019t bring about conditions of oppression in the first place. We cannot know what it\u2019s like to be a bot, for the same reason that we can\u2019t know what it\u2019s like to be a bat or what it\u2019s like to be one another. But engaging these questions about nonhuman consciousness, knowledge, and what it means to be and to know helps us confront the often unconscious human tendency to believe that personhood is modeled after some perfect exemplar. We can come to recognize that there is no conjunction of the right kind of body, of skin, of gender, of sexuality, of thought, of religion, of life that makes someone a \u201ctrue\u201d or \u201cvalid\u201d human being. Then we can start to do the work of undoing those beliefs and building new ones together. This essay is part of a collection of essays on the theme of BOT FEELINGS. Also from this week, Jacqueline Feldman on whether people really fall in love with robots. Damien Patrick Williams\u00a0is a PhD researcher at Virginia Tech in the Department of Science, Technology, and Society. His research areas include ethics, epistemology, philosophy of technology, philosophy of mind, and the occult. He writes at\u00a0afutureworththinkingabout.com,\u00a0technoccult.net\u00a0and\u00a0tinyletter.com\/technoccult. \u00a9\u20092018 Real Life is a magazine about living with technology. The emphasis is more on living. We publish one essay, advice column, reported feature, or uncategorizable piece of writing a day, four or five days a week. To find out more about us, click here. To find out more about our contributors, or to contribute yourself, click here. To ask us a question about anything else, email info@reallifemag.com Real Life is made possible by funding from Snapchat, and we operate with editorial independence and without ads. Website: CHIPS Terms of Use\nPrivacy Policy \u00a9\u20092018 Real Life is a magazine about living with technology. The emphasis is more on living. We publish one essay, advice column, reported feature, or uncategorizable piece of writing a day, four or five days a week. To find out more about us, click here. To find out more about our contributors, or to contribute yourself, click here. To ask us a question about anything else, email info@reallifemag.com Real Life is made possible by funding from Snapchat, and we operate with editorial independence and without ads. Website: CHIPS Terms of Use\nPrivacy Policy","time":1525796623,"title":"What It\u2019s Like to Be a Bot","type":"story","url":"http:\/\/reallifemag.com\/what-its-like-to-be-a-bot\/","label":7,"label_name":"random"},{"by":"startupflix","descendants":0,"id":17022131,"kids":"None","score":1,"text":"University of Toronto researchers have developed a handheld 3-D skin printer that deposits even layers of skin tissue to cover and heal deep wounds. The team believes it to be the first device that forms tissue in situ, depositing and setting in place, within two minutes or less.\n\t\t\t\t\t\t\t\t       The research, led by Ph.D. student Navid Hakimi under the supervision of Associate Professor Axel Guenther of the Faculty of Applied Science & Engineering, and in collaboration with Dr. Marc Jeschke, director of the Ross Tilley Burn Centre at Sunnybrook Hospital and professor of immunology at the Faculty of Medicine, was recently published in the journal Lab on a Chip. For patients with deep skin wounds, all three skin layers \u2013 the epidermis, dermis and hypodermis \u2013 may be heavily damaged. The current preferred treatment is called split-thickness skin grafting, where healthy donor skin is grafted onto the surface epidermis and part of the underlying dermis. Split-thickness grafting on large wounds requires enough healthy donor skin to traverse all three layers, and sufficient graft skin is rarely available. This leaves a portion of the wounded area \"ungrafted\" or uncovered, leading to poor healing outcomes. Although a large number of tissue-engineered skin substitutes exist, they are not yet widely used in clinical settings. \"Most current 3-D bioprinters are bulky, work at low speeds, are expensive and are incompatible with clinical application,\" explains Guenther. The research team believes their in-situ skin printer is a platform technology that can overcome these barriers, while improving the skin-healing process \u2013 a major step forward.  The handheld skin printer resembles a white-out tape dispenser \u2013 except the tape roll is replaced by a microdevice that forms tissue sheets. Vertical stripes of \"bio ink,\" made up of protein-based biomaterials including collagen, the most abundant protein in the dermis, and fibrin, a protein involved in wound healing, run along the inside of each tissue sheet. \"Our skin printer promises to tailor tissues to specific patients and wound characteristics,\" says Hakimi. \"And it's very portable.\" The handheld device is the size of a small shoe box and weighs less than a kilogram. It also requires minimal operator training and eliminates the washing and incubation stages required by many conventional bioprinters. The researchers plan to add several capabilities to the printer, including expanding the size of the coverable wound areas. Working with Jeschke's team at Sunnybrook Hospital, they plan to perform more in vivo studies. They hope that one day they can begin running clinical trials on humans, and eventually revolutionize burn care.\n                                                                 \n\nExplore further:\nNew skin-graft system a better fix for chronic wounds\n \nMore information:\n                                                Navid Hakimi et al. Handheld skin printer: in situ formation of planar biomaterials and tissues, Lab on a Chip (2018). DOI: 10.1039\/C7LC01236E\n\n \nProvided by\nUniversity of Toronto\n  Adjust slider to filter visible comments by rank Display comments: newest first  \n                    Please sign in to add a comment.\n                    Registration is free, and takes less than a minute.\n                    Read more\n Enter your Science X account credentials \u00a9 Tech Xplore 2014 - 2018 powered by Science X Network ","time":1525796622,"title":"Researchers develop portable 3-D skin printer to repair deep wounds","type":"story","url":"https:\/\/techxplore.com\/news\/2018-05-portable-d-skin-printer-deep.html","label":8,"label_name":"science"},{"by":"runnr_az","descendants":0,"id":17022129,"kids":"None","score":2,"text":"This is to serve as a Rosetta Stone of state management systems. A basic packing list app was built in: Surely you\u2019re familiar with one or more of the aforementioned systems, and now you can leverage that knowledge to better understand many others. It\u2019s your chance to see what all the buzz is about, and honestly, how similar all these state systems really are. To portray these systems in a terse and understandable form, the chosen app is a simple packing list app with only the ability to add and clear. To illustrate state jumping the wire, the ADD\/CLEAR is one component, and the LIST is a secondary component in all examples. Even the two main components (adding\/listing) have been abstracted to an imported library, leaving only fundamental code in order to emphasize state choice(s). The code is meant to be minimalistic. The code for each of these systems can be found in React and React Native. Use the above repo to personally dive into each of those systems and check them out! \ud83d\udd25 If you want code, check the GitHub, if you want opinions, continue into this very long description below. Here I jump into the differences between each item in the museum, and that which makes it unique. If you\u2019ve got some strong opinions, or experiences, please share them in the comments. I\u2019m also interested in giving this report as a fun-filled conference talk. Here\u2019s the most basic structure of state management, it depends only on the fundamental understanding of components and their encapsulation. In many ways, this is a great example for beginners in React. Explicitly raising state to a root component that all components are children of identifies the props vs. state relationship. As an application grows, explicit connections down into components would be more and more complex and fragile, which is why this is not commonly used. https:\/\/reactjs.org\/docs\/context.html There\u2019s been a lot of buzz about the updates to Context. In fact, the final form of Context in 16.x feels a bit like a state management system itself. For simplicity, the context allows for provider and a consumer. All children of a provider will have access to the values applied there. All non-children will see the context defaults. The following graph explains such lineage. On a second, and very opinionated note, I\u2019m not a fan of the consumption syntax structure. Clearly, it\u2019s a function that is the child of Consumer, but it feels like it violates JSX while mega-overloading all use cases of braces. A pedantic issue, but the readability of code should always factor into API, and on this front, Context starts to feel a bit dirty. https:\/\/github.com\/reactjs\/react-redux I\u2019ll dare say at the time of this writing Redux is the most popular state management tool, and therefore the most attacked. Writing a solution in Redux took many files, and almost double the lines of code. But to Redux\u2019s defense, it\u2019s simple and flexible. If you\u2019re unfamiliar with Redux, it\u2019s a functional approach to state management that provides time-travel and clean state management in a form like a reducer function. Dan Abramov\u2019s video explaining redux has been watched many times. In short, it\u2019s like having someone shout commands in your app (Actions) which are projected via Action Creators. Data managers in your app (Reducers) hear those shouts, and can optionally act on them. I love my pirate ship analogy, so shouting \u201cMAN_OVERBOARD\u201d can tell your crew counter to subtract the staff by one, the accountant to re-split the treasure, and the guy swabbing the deck can just ignore it because he doesn\u2019t care. I like this analogy, because shouting is a powerful way to manage all corners of your app, and in larger applications, noisy. Combine this with no way to handle side-effects and the need to glue on an immutable structure to make it all work, Redux is the bill-by-hour developer\u2019s friend. https:\/\/github.com\/mobxjs\/mobx-react MobX is one of the EASIEST state managers to get started with. Open the readme, and follow along and you\u2019ll have things running in no time. It feels like mutable JS, and it really kind of is. The only part that might throw you for a loop is the decorators like @observer on classes. Though odd, they kind of clean up the code a bit. Be sure to checkout Nader\u2019s blog post highlighting some more advanced topics on switching to MobX. In summation, MobX was one of the smallest and simplest tools to add! https:\/\/github.com\/jamiebuilds\/unstated Unstated was as easy as MobX. Much like MobX felt like mutable JavaScript, Unstated felt like adding more React code. I actually feel that Unstated feels more like React than Context did. It\u2019s simple, you create a container, and inside that container, you manage state. Simple known functions like setState exist inside the state container. It\u2019s not just an apt name; it\u2019s an apt React based manager. I\u2019m not sure how well it scales or handles middleware etc. but if you\u2019re a beginner to state management MobX and Unstated are the simplest tools to get up and running! https:\/\/github.com\/mobxjs\/mobx-state-tree Yes, this is VERY different from vanilla MobX. It\u2019s a common misconception. Even my co-workers try to shorten the title down to \u201cMobX,\u201d and I\u2019m always pushing MST as an alternative instead. With that being said, it\u2019s important to note MobX-State-Tree sports all the great features of Redux + reselect + Side-effect management and more all in one opinionated bundle with less code. In this small example, the only thing that\u2019s obvious is the terse syntax. The lines of code are barely bigger than our original MobX example. Both share that succinct decorator syntax. Though it takes a bit of time to really get all the benefits out of MobX-State-Tree. The most important note is that if you came from ActiveRecord or some other kind of ORM, MobX-State-Tree feels like a clean data model with normalized relations. This is a great state management tool for an application that will scale. https:\/\/github.com\/apollographql\/react-apollohttps:\/\/aws.amazon.com\/appsync\/ If you haven\u2019t jumped on the GraphQL train, you\u2019re missing out. Apollo GraphQL + AppSync is a great way to manage your state, AND handle offline, AND handle fetching API, AND handle setting up a GraphQL server. It\u2019s a serious solution. Many have projected GraphQL to effectively \u201csolve\u201d the state debate. In a lot of ways that\u2019s easy, and in a lot of ways, that\u2019s hard. Not everyone is ready to use a GraphQL server, but if you are, then AppSync is an easy way to handle all your data in your DynamoDB. It takes more time\/energy to get this up and running, but with clear benefits. In my example, I don\u2019t really use all the bells and whistles. You can see the delay as the data awaits from the server, and I\u2019m not using subscriptions to get updates. This example could get better. But it\u2019s as simple as wrapping the config with the components. Tadaaaaa! The rest is history. Special note: Please be careful what you put in the packing list in this example, as it\u2019s shared. https:\/\/github.com\/MicheleBertoli\/react-automata This is a strange one in the group. In many ways, you\u2019re wondering how setState is involved, and the answer is simple. The idea of breaking state down into a state-machine is very different from most state management systems. By creating an xstate machine config, you handle how state gets passed, called, and identified. Therefore, you must identify ALL states your app can be in, and ALL ways it can move from one state to another. Much like dispatching an action in Redux, you have to transition to another state on a given event. It\u2019s not a full state management system; it\u2019s merely a state-machine for your state management. Here\u2019s the chart created by our statechart Exciting benefits come from using statecharts. Firstly, you can be protected from transitions you don\u2019t want. For instance, you can\u2019t transition to \u201cloaded\u201d state without first typing text. This stops empty adds to our packing list. Secondly, all transitions of state can be automatically generated and tested. With one simple command, multiple snapshots of state are generated. CAVEAT: On React Native I had to yarn add path to satisfy some unused import in a dependency. This was a sneaky gotcha for native only https:\/\/github.com\/FormidableLabs\/freactal\/ Of course, we\u2019ll feature the awesome work of Formidable Labs. Freactal is a very advanced example and states it can replace redux, MobX, reselect, redux-loop, redux-thunk, redux-saga and more. Though this was probably the most difficult one for me to setup, I still see it has great value. More examples would have helped. Special thanks to Ken Wheeler who agreed to answer any questions I had while reading through the docs. The final code is succinct and straightforward. It feels a bit like the Context syntax in the end. I especially like the use of name-spacing effects separately from state, and computer though there\u2019s not much stopping you from taking this convention to other libs. https:\/\/github.com\/msteckyefantis\/reduxx ReduxX, while probably having a bit of trouble in SEO, is still a pretty cool name. ReduxX reads pretty well as in some ways it reminds me a bit of the charisma from Unstated, as we\u2019re using react-styled verbiage to set and mutate state. One aspect that might seem alien, is the state is retrieved with getState as a function. This feels a bit like keychain access, and I wonder if there could be some certs\/crypto mixed in easily? Food for thought. I see there\u2019s obscureStateKeys: true which will swap keys out for GUIDs. Security-wise this library might have some interesting advantages. As for how to use it, Set and Get via keys. That\u2019s it! If you\u2019re not worried about middleware, and you\u2019re familiar with keychain globals, you already know ReduxX. Special thanks to the author Mikey Stecky-Efantis for providing this example! I\u2019m sure there are other state managers out there which are being sorely under-represented here, and if you know them, please send a PR to the public repo. I\u2019ll happily accept contributions so that we can all benefit. I\u2019ll even update this blog post as new systems are added. So please, file tickets, and more importantly contribute! The museum thanks you \ud83d\ude06 By clapping more or less, you can signal to us which stories really stand out. Software Consultant, Adjunct Professor, Published Author, Award Winning Speaker, Mentor, Organizer and Immature Nerd :D\u200a\u2014\u200aLately full of React Native Tech how hackers start their afternoons.","time":1525796606,"title":"The React State Museum: View the Hottest State Management Libs for React","type":"story","url":"https:\/\/hackernoon.com\/the-react-state-museum-a278c726315","label":3,"label_name":"dev"},{"by":"flaviocopes","descendants":2,"id":17022128,"kids":"[17022148]","score":2,"text":"\nPublished 08 May 2018\n  If not specified otherwise, the browser assumes the source code of any program to be written in the local charset, which varies by country and might give unexpected issues. For this reason, it\u2019s important to set the charset of any JavaScript document. How do you specify another encoding, in particular UTF-8, the most common file encoding on the web? If the file contains a BOM character, that has priority on determining the encoding. You can read many different opinions online, some say a BOM in UTF-8 is discouraged, and some editors won\u2019t even add it. This is what the Unicode standard says: \u2026 Use of a BOM is neither required nor recommended for UTF-8, but may be encountered in contexts where UTF-8 data is converted from other encoding forms that use a BOM or where the BOM is used as a UTF-8 signature. This is what the W3C says: In HTML5 browsers are required to recognize the UTF-8 BOM and use it to detect the encoding of the page, and recent versions of major browsers handle the BOM as expected when used for UTF-8 encoded pages. \u2013 https:\/\/www.w3.org\/International\/questions\/qa-byte-order-mark If the file is fetched using HTTP (or HTTPS), the Content-Type header can specify the encoding: If this is not set, the fallback is to check the charset attribute of the script tag: If this is not set, the document charset meta tag is used: The charset attribute in both cases is case insensitive (see the spec) All this is defined in RFC 4329 \u201cScripting Media Types\u201d. Public libraries should generally avoid using characters outside the ASCII set in their code, to avoid it being loaded by users with an encoding that is different than their original one, and thus create issues. While a JavaScript source file can have any kind of encoding, JavaScript will then convert it internally to UTF-16 before executing it. JavaScript strings are all UTF-16 sequences, as the ECMAScript standard says: When a String contains actual textual data, each element is considered to be a single UTF-16 code unit. A unicode sequence can be added inside any string using the format \\uXXXX: A sequence can be created by combining two unicode sequences: Notice that while both generate an accented e, they are two different strings, and s2 is considered to be 2 characters long: And when you try to select that character in a text editor, you need to go through it 2 times, as the first time you press the arrow key to select it, it just selects half element. You can write a string combining a unicode character with a plain char, as internally it\u2019s actually the same thing: Unicode normalization is the process of removing ambiguities in how a character can be represented, to aid in comparing strings, for example. Like in the example above: ES6\/ES2015 introduced the normalize() method on the String prototype, so we can simply do: Emojis are fun, and they are Unicode characters, and as such they are perfectly valid to be used in strings: Emojis are part of the astral planes, outside of the first Basic Multilingual Plane (BMP), and since those points outside BMP cannot be represented in 16 bits, JavaScript needs to use a combination of 2 characters to represent them The \ud83d\udc36 symbol, which is U+1F436, is traditionally encoded as \\uD83D\\uDC36 (called surrogate pair). There is a formula to calculate this, but it\u2019s a rather advanced topic. Some emojis are also created by combining together other emojis. You can find those by looking at this list https:\/\/unicode.org\/emoji\/charts\/full-emoji-list.html and notice the ones that have more than one item in the unicode symbol column. \ud83d\udc69\u200d\u2764\ufe0f\u200d\ud83d\udc69 is created combining \ud83d\udc69 (\\uD83D\\uDC69), \u2764\ufe0f\u200d (\\u200D\\u2764\\uFE0F\\u200D) and another \ud83d\udc69 (\\uD83D\\uDC69) in a single string: \\uD83D\\uDC69\\u200D\\u2764\\uFE0F\\u200D\\uD83D\\uDC69 There is no way to make this emoji be counted as 1 character. If you try to perform You\u2019ll get 8 in return, as length counts the single Unicode code points. Also, iterating over it is kind of funny:  And curiously, pasting this emoji in a password field it\u2019s counted 8 times, possibly making it a valid password in some systems. How to get the \u201creal\u201d length of a string containing unicode characters? One easy way in ES6+ is to use the spread operator: You can also use the Punycode library by Mathias Bynens: (Punycode is also great to convert Unicode to ASCII) Note that emojis that are built by combining other emojis will still give a bad count: If the string has combining marks however, this still will not give the right count. Check this Glitch https:\/\/glitch.com\/edit\/#!\/node-unicode-ignore-marks-in-length as an example. (you can generate your own weird text with marks here: https:\/\/lingojam.com\/WeirdTextGenerator) Length is not the only thing to pay attention. Also reversing a string is error prone if not handled correctly. ES6\/ES2015 introduced a way to represent Unicode points in the astral planes (any Unicode code point requiring more than 4 chars), by wrapping the code in graph parentheses: The dog \ud83d\udc36 symbol, which is U+1F436, can be represented as \\u{1F436} instead of having to combine two unrelated Unicode code points, like we showed before: \\uD83D\\uDC36. But length calculation still does not work correctly, because internally it\u2019s converted to the surrogate pair shown above. The first 128 characters can be encoded using the special escaping character \\x, which only accepts 2 characters: This will only work from \\x00 to \\xFF, which is the set of ASCII characters. If you liked this post, tweet it to your followers! \u2b07\ufe0f I write a new frontend development tutorial\n        every day. Get all my new tutorials in your inbox, twice a month:\n     \n \n        I recently launched an ebook \ud83d\udcda on Modern Web Development including React, Redux, GraphQL, modern CSS, Progressive Web Apps, Webpack,\n        Babel, Service Workers and lots of other things \ud83e\udd16\n        \n\n\n\n\n","time":1525796600,"title":"Unicode in JavaScript","type":"story","url":"https:\/\/flaviocopes.com\/javascript-unicode\/","label":7,"label_name":"random"},{"by":"devy","descendants":0,"id":17022124,"kids":"None","score":1,"text":"A new history of a terrifyingly close shave with nuclear Armageddon  1983: Reagan, Andropov and a World on the Brink. By Taylor Downing.Da Capo Press; 400 pages; $28. Little, Brown; \u00a320. Upgrade your inbox and get our Daily Dispatch and Editor's Picks. THE Cuban Missile Crisis of 1962 was terrifying, but at least both sides knew the world was on the brink of catastrophe. As Taylor Downing\u2019s snappily told account lays bare, what arguably made the near-miss of November 9th 1983 worse was that the West had almost no idea the Soviet leadership believed war was imminent. East-West relations had been in dire straits for years. Ronald Reagan\u2019s soaring anti-communist rhetoric, terming the Soviet bloc an \u201cevil empire\u201d, inspired freedom-lovers on both sides of the Iron Curtain, but panicked the Politburo gerontocracy. So too did his idealistic belief that missile-defence (\u201cStar Wars\u201d) might keep the peace better than MAD (mutually assured destruction). A hi-tech arms race spelled doom for the Soviet Union. As communication had shrivelled, misunderstandings mushroomed. When the Soviets shot down a Korean airliner that had veered drastically off course into their airspace, nobody in the American administration could countenance the idea that the tragedy might be (as it was) a blunder, not an atrocity. The Soviets were certain the plane was on a spying mission. NATO\u2019s \u201cAble Archer\u201d exercise was also wildly misinterpreted. The Kremlin was convinced it masked war preparations. A routine change of NATO codes made the Soviets assume a nuclear first strike was imminent. In fact the KGB had an agent in the heart of NATO, Rainer Rupp. In response to an emergency request, he assured Moscow, with some bemusement, that everything in the alliance\u2019s civilian bureaucracy was ticking along as normal. But the spymasters discounted the information, while \u201ctoadying KGB officers on the ground\u2026sent back alarmist reports.\u201d If the Soviet misreading of NATO intentions was a colossal intelligence failure, so was the inability of Western intelligence to realise just how jittery and ill-informed the Communist leadership had become. As the Soviet Union put its nuclear forces on high alert, Lieutenant-General Leonard Perroots, the American air-force intelligence chief in Europe, reacted with puzzlement. A quid pro quo might have triggered an all-out nuclear war, which would, as Mr Downing puts it, leave only \u201ccockroaches and scorpions\u201d alive. Luckily, Perroots did nothing. After a sleepless night, the Kremlin leadership, huddled in a clinic outside Moscow with the ailing general secretary, Yuri Andropov, realised nothing was going to happen. Mr Downing\u2019s book gives abundant historical background, perhaps too much for readers familiar with the period. A useful later chapter depicts how realisation of the Soviet panic unfolded in the West, first in classified assessments and eventually, long after the event, in the public domain\u2014not least thanks to Mr Downing\u2019s television documentary, screened in 2008. He wisely avoids questions of the morality of nukes. Instead he focuses on the shortcomings that made accidental nuclear war far too plausible. Tell us what you think of Economist.com Need assistance with your subscription? Get 3 free articles per week, daily \n newsletters and more. \n  Published since September 1843 to take part in\n  \u201ca severe contest between intelligence, which presses forward,\n  and an unworthy, timid ignorance obstructing our progress.\u201d\n Copyright \u00a9 The Economist Newspaper Limited 2018. All rights reserved.","time":1525796582,"title":"The world almost ended in 1983","type":"story","url":"https:\/\/www.economist.com\/news\/books-and-arts\/21741529-new-history-terrifyingly-close-shave-nuclear-armageddon-world-almost-ended","label":7,"label_name":"random"},{"by":"simonswords82","descendants":0,"id":17022117,"kids":"None","score":5,"text":"\nBy\nJessica Webb\non\n\r\n                April\r\n                24,\r\n                2018\r\n            \nin\n\nTrello News\n\n  With over two billion\u00a0cards created to date, Trello users are planning, prioritizing, and producing like never before. If you were to swap those Trello cards for sticky notes, you\u2019d be running out of space. In fact, you\u2019d need a wall 100,000 miles long to hold all of those three-inch pieces of paper next to each other.  Two billion ideas, tasks, and projects are no small feat! From cards for humanitarian aid to cards for camping trips\u2014congratulations and thank you for inspiring all of us to dream, plan, and do. Trello has always been a place to get organized and get stuff done, so why stop there? After hearing from you about what you\u2019d like to see next, we are so excited to launch the biggest Trello refresh yet: a brand spankin' new home view for all of your Trello activities and a better notifications system to help you focus and get even more done! These features will be gradually rolling out over the next week, so hold tight if you don\u2019t see them just yet. Tweet it out:\u00a0Holy moly, 2 billion @trello cards! Let\u2019s celebrate with these majorly productive upgrades:\u00a0bit.ly\/trellohome\u00a0  Imagine a world in which everything that\u2019s most important to you can be accessed from one place. No more searching through layers of boards, tasks, and comments, then trying to prioritize the lot ad hoc\u2014this is a central base of operations for your productivity. Welcome home. Like your own assistant, the new Trello home view is personalized for your priorities, and will give you the high-level perspective you need across your Trello account. By displaying an activity feed from your Trello teams, home automatically surfaces the information you need, when you need it. Let the information come to you and browse at your own pace. Get a shared view of what everyone on your team is working on without having to dig through every individual board.  Learn more about how new sections \"Up Next\u201d and \u201cHighlights\u201d will bring you and your team more clarity on top priorities and must-read updates: Read More Here  When you are constantly bombarded by notifications, it can take all your willpower to not just throw your phone or computer at the wall and walk away. But what if you miss something important? It\u2019s time to start taking control of your notifications\u2014your Trello ones at least! In this major update, notifications have been revamped with your productivity in mind.\u00a0You can now mark individual alerts as \u2018read\u2019 or \u2018unread\u2019 depending on what you want to address. Not only that, you're able to\u00a0change due dates, mark cards as done, and stop watching a card from within the notifications panel. And stay tuned, more is on the way! Engaging with notifications in Trello is now more delightful and useful (the way it should be). Take a tour of your new organizing power in this deep dive: Read More Here Whether you\u2019re planning a family vacation, a company conference\u2014or both at the same time\u2014Trello can give you productivity piece of mind and help you feel at home with everything that\u2019s on your plate. From the big picture down to the nitty-gritty details, you and your team will have the ability to manage it all in one place. So go on, then! Get inspired, get organized, and get your most meaningful work done where you\u2019re most comfortable. Note: These features are gradually rolling out over the next week, so don't worry if you don't see them just yet.\u00a0\ud83d\ude42\u00a0 Good or bad, we'd love to hear your thoughts. Find us on Twitter (@trello) or write in to\u00a0support@trello.com. Trello lets you work more collaboratively and get more done. No goal is too big for Trello. Are you ready to dream big in 2018? \u00a9 Copyright 2017, Trello, Inc. All rights reserved.","time":1525796552,"title":"Trello launches largest user interface overhaul to date","type":"story","url":"https:\/\/blog.trello.com\/2-billion-cards-with-trello-home","label":7,"label_name":"random"},{"by":"kelseyevans","descendants":0,"id":17022113,"kids":"None","score":1,"text":"In the first article of this Rate Limiting series I introduced the motivations for rate limiting, and discussed several implementation options (depending whether you own both sides of the communication, or not) and the associated tradeoffs. This article dives a little deeper into the need for rate limiting with API gateways In the first article I discussed options for where to implement rate limiting: the source, the sink, or middleware (literally a service in the middle of the source and sink). When exposing your application via a public API you typically have to implement rate limiting within the sink or middleware that you own. Even if you control the source (client) application you will typically want to guard against bugs that cause excess API request, and also against bad actors who may attempt to subvert your client applications. The Stripe blog has an excellent article on \u201cScaling your API with rate limiters\u201d, which I\u2019ll be referencing throughout this post, and the opening section talks about how rate limiting can help make your API more reliably in the following scenarios: At Datawire we have seen these patterns firsthand, particularly with organisations exposing \u201cfreemium\u201d style public API, where it is a clear business requirement to be able to prioritise traffic for paying customer, and also protect against bad actors (intentional or otherwise). Fundamentally rate limiting is simple. For each request property you want to limit against, you simply keep a count of the number of times each unique instance of the property seen, and reject the associated request if this is over the specified count per time unit. For example, if you wanted to limit the amount of requests each client made you would use the \u201cclient identified\u201d property (perhaps set via the request string key \u201cclientID\u201d, or included in the request header), and keep a count for each identifier. You would also specify a maximum number of requests per time unit, and potentially define an algorithm for how the count is decremented, rather than simply resetting the counter at the start of each unit (more on this later). When a request arrives at the API gateway it will increment the appropriate request count and check to see if this increase would mean that the maximum allowables request per time unit has been exceeded. If so, then this request would be rejected, most commonly returning a \u201cToo Many Requests\u201d HTTP 429 status code to the calling client. Closely related to rate limiting is \u201cload shedding\u201d. The primary difference here is that the decision to reject traffic is not based on a property of an individual request (e.g. the clientId), but on the overall state of the application (e.g. database under heavy load). Implementing the ability to shed load at the point of ingress can save a major customer incident if the system is still partially up and running but needs time to recover (or fix). The majority of open source and commercial API gateways offer rate limiting, but one of the challenges with many of these implementations is scalability. Running your API gateway on a single compute instance is relatively simple, and this means you can keep the rate limiting counters in memory. For example, if you were rate limiting on clientId, you would simply check and set (increment) the clientId in an in-memory map with an associated integer counter. However, this approach does not scale past the single instance to a cluster of gateway instances. I\u2019ve seen some developers attempt to get around this limitation by either using sticky sessions or by dividing the total maximum number of allowable requests by the number of rate limiting instances. However, the problem with this is that neither of these approaches work reliably when deploying and operating applications in a highly dynamic \u201ccloud native\u201d environment, where instances are being destroyed and recreated on-demand, and also scaled dynamically. The best solution to overcome this limitation is to use some form of high-performance centralised data store to manage the request count. For example, at Lyft, the team use Redis (presumably run as a highly-available Redis Sentinel cluster) to track this rate limiting data via their Envoy proxy that is deployed as a sidecar to all of their services and datastores. There are some potential issues to be aware of with this approach, particularly around the atomicity of the check-and-set operations with Redis. It is recommended for performance reasons to avoid the use of locking, and both Stripe and Figma have talked around using the Lua scripting functionality (with guaranteed atomicity) within the Redis engine. Other challenges often encountered relates to the ability to extract request (meta)data for use in determining the rate limit, and also specifying (or implementing) the associated algorithm used to determine whether a specific request should be rejected. Ideally you would like to be able specify rate limiting in relation to various client properties (e.g. request HTTP method, location, device etc) and the decomposition of your backend (e.g. service endpoint, semantic information such as an user-initiated request vs an app-initiated request, payload expectations). An interesting solution to overcome many of the challenges discussed in the previous section was presented by the Lyft Engineering team last year, when they talked about how the Envoy proxy they use as (what we are now calling) a service mesh implements rate limiting by calling out to an external RateLimit service for each request. The Ratelimit service conforms to the Ratelimit protobuf defined here, which is effectively a rate limit API. The Datawire team has built the open source Ambassador API gateway on top of the Envoy Proxy, and recently Alex Gervais has implemented the same rate limiting support for Ambassador. As you now have access to a protobuf rate limit service API you can implement a rate limit service in any language you like (or at least any language with protobuf support, which is most modern languages). You also now have complete freedom to implement any rate limiting algorithm you like within the service, and also base the rate limiting decision on any metadata you want to pass to the service. The examples within the Lyft RateLimit service provide some interesting inspiration! It\u2019s also worth mentioning, that as the Ambassador API gateway runs within Kubernetes, any rate limiting service you create can take advantage of Kubernetes to handle scaling and fault-tolerance. In this second article of our rate limiting series you have learned about the motivations for rating limiting and load shedding with an API gateway, and you also explored about some of the challenges with doing this. In the final section of the article I presented some ideas around integrating rate limiting witin an API gateway deployed within a modern cloud native platform (like Kubernetes, ECS etc), and discussed how using an external service to do this could allow a lot of flexibility with implementing your exact requirements for a rate limiting algorithm. Join me for the final part of the series next week, where we take a look at implementing a Java rate limiting service for the Ambassador API gateway (here\u2019s a sneak peak of some of the code!). In the meantime, please do feel free to email any questions, or jump on the Ambassador Gitter channel. By clapping more or less, you can signal to us which stories really stand out. Open source, Kubernetes-native API Gateway built on Envoy","time":1525796526,"title":"Rate Limiting for API Gateways","type":"story","url":"https:\/\/blog.getambassador.io\/rate-limiting-for-api-gateways-892310a2da02","label":3,"label_name":"dev"},{"by":"yonderboy","descendants":0,"id":17022080,"kids":"None","score":2,"text":"\u201cSpace: the final frontier. These are the voyages of the starship Enterprise. Its five-year mission: to explore strange new worlds. To seek out new life and new civilizations. To boldly go where no man has gone before!\u201d Growing up as a kid in the late 80\u2019s, one of my favorite pastimes used to be watching old re-runs of Star Trek on our family\u2019s 14 inch CRT television set. The gist of this early TV series revolved around the interstellar adventures of Captain James T. Kirk and his crew aboard the starship USS Enterprise. For us (etherscanners), inspired by the early stories of space travel in a lot of ways exploring the Blockchain is like exploring deep space. To fit in this context, each Blockchain platform is analogous to its own solar system and the dc\/Apps* within these systems are like the planets that orbit the sun. And different Blockchains out there are like the other solar systems that make up our Milky way. And in the context of space exploration, block explorers are like star ships and one of their core missions is to log their journey and to report back their findings to the home planet. At Etherscan one of our core goals was to explore, log and make available all transactions on the Ethereum blockchain to users in way that had the least amount of friction. The least amount of friction here means that the information provided should be done in a way which does not require downloading a special app, gigabytes of data, require a special browser, access to your private keys or that you had be a techie\/geek to operate. Nor should it require users to purchase\/participate in an ICO in order to acquire custom ERC20 tokens which would then allow access to premium content. Instead, all one should need is a standard web browser to access this content for free. But more than that, it was important for us that the information we curated was provided on an equal access basis to all users within the Ethereum ecosystem. Which is why you will find that till this day, we do not offer a premium content service because we believe that everyone should have equal access to information on the Ethereum blockchain and not only because you can afford it or that you were part of an elite group. So when we were building an explorer for Ethereum in the beginning of 2015, we looked at the various options on how to technically structure the product, store this data, allow access to content in a way that was fast, secure, seamless and offered the least amount of friction for regular users. But at the same time, to also build an economically sustainable product that would allow us to continue our quest for blockchain exploration. Etherscan was built on these founding principles. From an economic viewpoint, at the moment we make money by placing small sponsored links (which are clearly marked as such) on the site. While monetisation through advertisements might not look like an ideal strategy for the decentralized economy, it is important to remember that a lot of the distributed infrastructure we use today was built from advertisement revenue. And from a friction and equal access view point, advertisements allow a friction-less way to access content while providing equitable access (the more content you consume the more ads (pain) you see and feel). It does not discriminate one over the other. All users are treated equally. Having said that, we do strongly value data privacy and security. As such while browsing the Mainnet site\u00a0: We have often been asked, why not run affiliate links instead\u00a0? (i.e links to hardware wallets, digital asset exchanges, etc) which can be extremely profitable. The sole reason I have refused to do this is because unlike adverts this is a zero sum game. Affiliates cannot be paid twice for the same sale vs. advert impressions where both parties can be paid for seeing the same ad multiple times. Running the same affiliate links which other essential infrastructure providers (i.e MyCrypto, Mew, Metamask, wallets, etc) rely on as their revenue stream will be akin to taking away money from projects that need these funds to offset their own operating expenses. Explorers play a critical role for providing Blockchain transparency. And we do this by best trying to remain impartial. Which is why you will notice that we rarely make any public statements or tweets outside of product announcements but instead focus on providing provable curated information. However, often at times this same lack of public interaction can be misconstrued as a lack of community involvement. I would like to assure you that nothing concerns us more than the well being of the Ethereum Blockchain and its ecosystem. We continue to work behind scenes lending support where we have been called to assist, making connections, providing support to users who might have general questions through our free ticketing system, tagging phishing scams, providing free public API data points, hosting 3 free public Testnets explorers (and API) and advocating the use of Ethereum. One of the reasons, that i was attracted to Blockchain technology in the middle of 2011 was because of its aspects of political decentralization (no one controls them) and the permission less structure (no one needs to be an approved actor). The merits of these qualities extend beyond just politics and consensus in that it allows individuals\/organizations\/companies to act in a decentralized manner (regardless of their technical architecture\u200a\u2014\u200aopen or closed) without coercion or control. Decentralization is more than just running free open source software but also in thought, actions and deliverables. At the same time i also believe in the concept of Yin & Yang (in that two halves together complete the wholeness) and that too much of a good thing is a bad thing. There are many aspects of today\u2019s world that can be further improved through decentralization to name a few\u2014 trust, accountability, governance and finance. But at the same time it\u2019s important not to forget that modern society was built on centralized structures and as much as we might want to rid the world of all centralized entities today, i strongly feel that a more sustainable approach is a system of checks and balances. It is one where centralized and decentralized systems can coexists, and are instead adopted to keep in check the different stakeholders for a more balanced ecosystem. At Etherscan we will continue to keep our heads down while #BUIDling humbly. Over the next few months we will also be working on new open source low friction tools that will allow end users to independently check and verify transactions on the Ethereum Blockchain. And lastly, we want to take this opportunity to thank the community for the support that we have received. We want you to know that we are here to stay regardless of whether Ethereum is priced at a $1 (as when we first started this project) or $10k** (in 5 years time or so) and that we will continue pushing forward in hopes that we can play our part, no matter how small it may be, towards building and making a positive change. Signing off\u2026. and in the words of Mr. Spock \u201cLive Long and Prosper\u201d. \u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014 Glossary:* dc\/Apps = Decentralized\/Centralized Blockchain Apps Disclaimer:** Price prediction is not financial advice By clapping more or less, you can signal to us which stories really stand out. Founder\/CEO at Etherscan","time":1525796299,"title":"Etherscan CEO \u2013 Space: the final frontier","type":"story","url":"https:\/\/medium.com\/@etherscan\/space-the-final-frontier-9779359cae6d","label":2,"label_name":"crypto"},{"by":"startupflix","descendants":0,"id":17022074,"kids":"None","score":1,"text":"This material may not be published, broadcast, rewritten, or redistributed. \u00a92018 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes. Move over Tide Pods. There's a new school craze. A 15-year-old girl in England was left with horrific burns after deodorant was sprayed on her in the latest viral challenge. A 15-year-old girl in England was left with horrific burns after deodorant was sprayed on her in the latest school craze.\u00a0 Jamie Prescott, 42, said her daughter Ellie may need a skin graft after taking part in \"The Deodorant Challenge\" with her South\u00a0Gloucestershire classmates. The game involves spraying deodorant onto a person's bare skin for as long as possible, and has left Ellie with an injury so bad it is still weeping three weeks on. The game involves spraying deodorant directly onto the skin, and leaving it on for as long as you can tolerate.\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\u00a0(SWNS)\n Ellie described her injuries as \"really painful\" and said yellow liquid leaks out of the wound. \"It's a hole in my arm and there's all this yellow stuff coming out,\" she said. \"My friend did it a year ago and has a scar, but said it wasn't as painful as mine. When I show people my injury they lift up their sleeves and show that they've all had it done too.\" NEBRASKA WOMAN'S RUNNY NOSE FROM 'ALLERGIES' TURNS OUT TO BE BRAIN FLUID LEAK Prescott posted photos of the burns on Facebook to warn other parents of the \"horrendous\" craze. It has been shared more than 2,500 times since being posted on May 4. Ellie said three weeks after her playing the game her injuries are still far from healed.\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\u00a0(SWNS)\n \"For any parents who have children, please, please sit them down and show them these pictures,\" Prescott posted. \"These are the damaging results of something known as 'The Deodorant Challenge' which is currently doing the rounds.\" \"It literally involves spraying deodorant on to someone else for as long as possible. It's that simple, and results in severe secondary burns,\" she said. \"The pictures below are that of my daughter's arm THREE WEEKS ON which may still as yet require a skin graft.\" \"Ellie's injuries are horrendous and if we can stop others doing it that will be great,\" the mother of three posted. \"As you can see from the yellow padding, it is still weeping excessively. Ellie met friends at the local park and they asked her to do the challenge, which she hadn't heard of.\" \"Other children have asked what's wrong with her arm but they have also had it done and show their scars,\" she said. Ellie has recently moved schools, from Chipping Sodbury School to Bristol Technology and Engineering Academy, but she claims the game is prevalent at both. \"Even if we can prevent just one more child from going through this, it will be worth it,\" Prescott said. \"I absolutely hate being in the limelight and writing public posts and having attention, but in this particular instance this challenge really needs to be made as public as possible. It's just horrendous and needs stopping.\" This material may not be published, broadcast, rewritten, or redistributed. \u00a92018 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.","time":1525796274,"title":"'Deodorant Challenge' leaves teen with second-degree burns","type":"story","url":"http:\/\/www.foxnews.com\/health\/2018\/05\/08\/deodorant-challenge-leaves-teen-with-second-degree-burns.html","label":7,"label_name":"random"},{"by":"dcschelt","descendants":0,"id":17022070,"kids":"None","score":1,"text":"\n                            TNW uses cookies to personalize content and ads to\n                            make our site easier for you to use.\n                            We do also share that information with third parties for\n                            advertising & analytics.\n                         \n            by Matthew Hughes\n            \u2014 \n                        in Design & Dev\n Historically, CSS has been used for one thing: decorating webpages. But later versions of the stylesheet language have added extra bells and whistles, allowing developers to do more ambitious (and, dare I say, bonkers) things. Case in point: London-based developer Stephen Cook built a barebones clone of the iconic racing game Mario Kart without using a single line of JavaScript. That\u2019s right. A pure CSS version of Mario Kart. For what it\u2019s worth, a lot of stuff from the official game is absent, including other racers, and any sense of progression. However, it\u2019s worth talking about what\u2019s there, because even with a few missing pieces, it\u2019s still pretty damn cool.  You can change characters, and switch between Mario and Bowser, who are animated and don\u2019t feel especially static. The terrain moves with your player, so it feels like you\u2019re actually moving with speed, even if you\u2019re not actually going anywhere. This is a pretty incredible accomplishment, and is a reminder that you can do amazing things if you\u2019re willing to eschew JavaScript (and arguably sanity). If you\u2019re curious, you can read how Stephen built his Mario Kart clone here. If you want to dive into the code (and yes, play a little bit of the game), you can check out his code sandbox here. \nRead next:\n\n        Learn everything from front-end development to CAD \u2014 and tuition is only $49    \n Stay tuned with our weekly recap of what\u2019s hot & cool by our CEO Boris. \n        Join over 260,000 subscribers!\n     \n                Sit back and let the hottest tech news come to you by the magic of electronic mail.\n             \n                Prefer to get the news as it happens? Follow us on social media.\n             \n1.76M followers\n                         \n1M likes\n                         \n                Got two minutes to spare? We'd love to know a bit more about our readers.\nStart!\n \n                All data collected in the survey is anonymous.\n            ","time":1525796238,"title":"Some madman built a Mario Kart clone in pure CSS","type":"story","url":"https:\/\/thenextweb.com\/dd\/2018\/05\/08\/some-madman-built-a-mario-kart-clone-in-pure-css\/","label":7,"label_name":"random"},{"by":"dietree","descendants":0,"id":17022056,"kids":"None","score":3,"text":"CPO and Co-founder at datree.io In English, we say too many cooks spoil the broth. And this maxim holds true in development, as well. If everyone leads in their own direction, no one is actually leading. Here\u2019s a good example: I was working as a developer on a three-person team. Each of us was responsible for his own project, but we all wrote in JS. We were each encouraged to choose whatever open source we saw fit, to make our work as fast, efficient, and accurate as possible. Makes sense, right? I chose\u00a0Commander as my command line parser. All was running smoothly, until I was asked to help one of my co-workers with his project. This is when things got complex. My co-worker had chosen a different command line parser,\u00a0Yargs. This meant that I had to learn a whole new open source syntax in order to give my input on his project. Annoying and time-consuming, but not a huge impact on the project overall. But then it happened again. My other co-worker needed help, and he\u2019d chosen\u00a0Minimist for his parser. So, to collaborate with two developers, I had to learn three different open source syntaxes. This was more than just \u00a0annoying, it was hurting the project\u2019s KPIs. \u00a0 There\u2019s a moral to this story. Today\u2019s development ecosystem is hyper-agile, and velocity expectations are through the roof. We\u2019re using multiple programming languages, with different stacks, and different tools. We\u2019re piecing together hundreds of application building blocks from open source and proprietary packages, third party APIs and technologies. We\u2019re running forward so fast and we\u2019re not looking around us. And if it\u2019s happening in a small way with small teams like mine, you can bet that it\u2019s happening in a big way with larger teams. We need to understand that with so many balls in the air, there\u2019s no way a human can keep track of them all. The financial and reputational price tags of development mess-ups are simply too high. \u00a0  \u00a0 This is where intelligent and advanced tools like datree come into play. With 1-tap, datree maps and catalogs all past and present code components, people and repositories to help the developer within the organization to share knowledge, collaborate and reach a state of \u00a0full visibility. \u00a0  \u00a0 This way, each developer is able to have a 'sneak peek' to his team mates stack, so he will be able to select the right code component for the right job based on data - avoiding developer horror stories like this one. Privacy Policy | Terms of Use \u00a9 2018 datreeio LTD","time":1525796123,"title":"Alternative Open Source","type":"story","url":"https:\/\/www.datree.io\/blog\/alternative-open-source","label":7,"label_name":"random"},{"by":"Zverik","descendants":0,"id":17022053,"kids":"None","score":5,"text":" This is a translation of this article in Russian. Far from perfect, but understandable, I hope. Written by Ilya Zverev, CC-BY. Riding the wave of Google Maps pricing news, Tom Chadwin reminded everyone of benefits of using open source solutions, and concluded with these words: \u201cYou now have a concrete compelling argument to those who have always asked: \u201cWhy not just use Google Maps?\u201d. Have I got a platinum argument towards a metaphorical Google Maps: because your open map does not have any future, that\u2019s why. It doesn\u2019t even have good POI coverage, unlike Google, which has franchise owners lined up with location offerings. Because it presents not a dozen grumpy dudes turning every data contributor down, but a nice mat with \u201cWelcome\u201d on it. That\u2019s an exaggeration, of course. We\u2019ve got a great, beautiful map, which in many areas not only excels \u2014 it doesn\u2019t have any alternatives. Nowhere else can you get a reasonably correct road graph. No other map would allow for estimating population density. Nobody would provide you with the data to install a copy of a service in a closed network. With that, it is hard to not notice that OpenStreetMap is dying. Not because we\u2019ve got a database for a map, or that we don\u2019t have moderators, or the data is not split in layers, like Serge complained. For a technically skilled person it\u2019s impossible to believe in the fall of OSM: the data is detached and decentralized, which is eternal by definition. On top of that, it is free (as in beer) and a million editors contribute to it: why isn\u2019t every website using it? Because it is unreasonable to use. OSM loses to any alternative maps for one reason: no control. Nobody has it. Over anything. Since around 2012 OpenStreetMap is headed directly into abyss, and rare attempts at correcting the course are met by grumpy blokes, who protect control levers, saying: \u201cpower to the community\u201d and \u201cour project self-regulates\u201d. The project\u2019s advantage became its weakness \u2014 and, it seems, the fatal one.  No control over the map. Want to import locations for your franchise? Tough luck, your data quality does not meet our standards. Want to map your town? Meet the local gatekeeper, who would scold you for highway tags choice and then disappear, because you are unbearable. And being a watchman is hard: in fourteen years the best we could do was OSMCha. Users of which are still complaining about wide, albeit thin, changesets. We\u2019ve successfully lost the author of OWL. DWG members are still using outdated Perl scripts for work. No control over the website. This is familiar to anybody who\u2019ve made a pull request to any part of the core infrastructure. You\u2019ll never hear a thanks, but will get a full bag of comments instead. Two guardians do not let through any unconventional change: it\u2019s like amidst a crumbling world we must hold on to what we already have. They don\u2019t see that the power of their grip is what crumbles their world. No control over the data model. The last effort to advance the API required money and power of the entire Cloudmade company, which means a dozen osmers, working for venture funding for a few weeks. A hope for an \u201carea\u201d data type was faint five years ago, but by now even the most optimistic osmers stopped dreaming of change. The only thing in plans for the new API is restricting metadata for GDPR compliancy, and that\u2019s only because nobody wants to pay charges. No control over tagging. The main distinction and advantage of OpenStreetMap is a free tagging model. It has grew so enormous, nobody, not even experienced users, can choose correct tags. Forums are full of humor on heath, forest and namespaces. Proposals are a joke: one side invents alien tagging schemas of hundreds kilobytes, another turns inside out in attempts to sink every proposal. Novice mappers are not freaked out by this only because all editors, even on mobile, put tags away behind the presets. No control over the mapping style. Long ago, the main mapnik style was so complex, people were afraid to touch it. Then it was converted to CartoCSS, made prettier, and contributors started flocking in. For a few years they were improving icons and colours, updated the database structure, sorted out fonts \u2014 and the map started looking decent, like on any other popular service. Same bleakness. But now it\u2019s obvious that nobody knows where to go next. Well, Paul Norman gives talks exactly about this for two years. It's painful to look at developers\u2019 attempts to continue, especially this year: they fruitlessly try to change established tagging principles, because the OSM data model is incompatible with good carthography. We reached the ceiling of the rendering stack made five years ago. The only way out is to throw it all away and start anew \u2014 exactly what authors are discussing these days. No control over developers. \u201cThe most precious resource is the community, which moves the project into its bright future.\u201d Haha, just don\u2019t look at developers, who move in any direction but forward. Some invent a twentieth geocoder, or tenth routing engine; some spend two weeks at data wrangling, only to come up with an unimpressive series of dots. \u201cHurray, I\u2019ve managed to deploy a tile server\u201d, we hear. Congratulations, but no. It\u2019s 2018 outside, and we\u2019ve got no developer environment, no integration tools, no financial support, no strategic plan. Just some lonely volunteers sitting on key elements of the infrastructure. No control over license. Mappers want to protect their work, which is understandable. From this were born all the copyleft licenses, to make the world a better, more open place. But there was a flaw. To succeed in this world, you need to learn to bargain. To take a buildings dataset from government, and provide them with geometry updates. To allow a booking service to not open their data, as to get a million and a half verified hotels, and be able to improve position errors in a service used by hundreds millions. And so on. Our license forbids all that, which doesn\u2019t hurt third parties much: they\u2019ve got enough data on their own. It is us who are hurt, because we cannot make any deals. Community members are on lookout to shut any advances. Even with trivial cases, we\u2019ve got issues. Just this year I\u2019ve seen half a dozen requests to use the map in various TV shows. And every time for similar questions, different answers were given. Nobody, not even the Legal Working Group, understands ODbL. But it is status quo, and in OpenStreetMap, status quo is king.  As you know, in this world to stay in place, you must run as fast as you can. I follow the news for Google Maps, Yandex Maps, 2GIS, and I see them trying new algorithms, fresh points of view. They change user interfaces, always improve data models, learn to communicate with their communities in new ways. They react to problems with structural changes. They have the power to change everything \u2014 or the opposite, to tidy up the data, smoothen edges, make it comfortable. They can buy and sell, to make their map better. All the OpenStreetMap community can do is to gather for a weekend to trace buildings in yet another town. That\u2019s why the main use cases for our project are humanitarian initiatives and serving as a base map layer, when one cannot afford a proper map. Try remembering, what major news we had in the past year, worthy of articles in big tech blogs? A new JOSM release with whitespace trimming in tags? I agree that relying on a proprietary map means giving up some control to a corporation. But are you absolutely sure you want control over every part of carthographic stack? Do you have enough money for that? A commercial provider can change its terms and put you in an unconvenient situation, but unlike OSM, you can make deals with them. A company is people, who have all the power: you can call them and bargain for better limits, or ask for a help with mapping. You are a client to them; to OSM you, when wanting something for your business, are worse than nobody. That\u2019s why OpenStreetMap stopped growing. On the charts, you can notice negative trends brewing. Like Wikimapia around 2011, our project has tapped out its meanings. With the current direction, we have ten years, after which we would look like Wikimapia does now: lots of data, no community, who have defected to alternative projects. And then people who chose OSM as a replacement to Google Maps, would have to think again. In the next year or two we must fix at least some of the issues and find new directions for the project. The \u201cfree as in beer\u201d narrative, constant for the past ten years, turned from progressive to pathetic. The main question is, why would you want OpenStreetMap, when you can choose from any of alternative maps, each of which is better in some aspect. And don\u2019t start with \u201cbut my yard is mapped better here\u201d. Maybe we plan to revolutionize geography teaching worldwide, or become a universal base map for everyone, or become a framework for experiments in modern carthography. Any answer will do \u2014 as long as you are prepared to work. Until then, most companies would prefer Google Maps. Comment from imagico on  8 May 2018 at 18:38 I don't agree with your analysis and conclusions in a lot of points but i none the less want to comment on a few things: You are complaining about OSM sticking too much to the status quo but don't forget that successful corporations like Google have no more important goal than maintaining the status quo in the form of their success.  New products, overhauls and stuff like this are not ends in themselves for such corporations, they are means to an end, namely to maintain the status quo. You are complaining about the lack of control over various things.  Have you actually thought through a scenario where some effective means of exercising power and control over these things is established?  Does this scenario realistically lead to a positive development in the long term?  I agree that a lot of people involved in the OSM community are often opposed to radical changes - but they often have good reasons for this.  OSM is meanwhile a project where a lot of very massive economic as well as collective personal interests play in.  A lot of these interests are articulated by people who - to put it in a friendly way - are not the most intelligent people who are often looking after their own short term gains rather than the long term common good.   An experienced community member who sees this and therefore advises caution against suggestions to make major changes to satisfy perceived short term needs will more likely be swayed by arguments that are based on a realistic appraisal of the long term outcome than by a sweeping critique of being against change. A few examples: Tagging:  The most powerful forces trying to influence tagging practice in OSM at the moment are the desire to cater very specific applications (in the sense not of tagging for rendering maps in general but for making life easy for a specific design idea in a specific map or for a specific geocoding or routing application) and the dissatisfaction with inconvenient overall rules in OSM (in particular the on-the-ground rule and verifiability) that prevent you from mapping the world as you subjectively see it rather than as it can be objectively observed.  If you'd somehow establish a means to exercise more control over tagging these interests would most likely be the ones fighting over having this control and not the considerate voices in the community (which exist) who would like to establish sane and efficient tagging principles and would like to weed out unsuitable ideas and who at the moment also have difficulties achieving that.  Many of these people see the status quo simply as the lesser of two evils. Map styling:  Note i am talking about just design here, not the underlying technological basis.  What is needed here is not more control but less control.  Having a single central map style for the whole project that is controlled by a small group of people (currently effectively two active ones + 2-3 who occasionally also make decisions) who are almost completely autonomous in their decisions is about the maximum of control you can have IMO.  I cannot really imagine how you can have more control here - other than maybe giving those in decision making position the money to pay people for doing work - in other words: Going the Wikipedia way. It is also not true that nobody knows where to go next in OSM map design.  There are plenty of people who have good ideas, talent and vision in that regard.  But like in case of tagging the most powerful interests acting in this field are not the most qualified ones.  So giving control to those who most loudly ask for it would be extremely counterproductive.  Those who would be best qualified for a role of leadership in map design are not the people who would ask for it. What we desperately need in map styling is more diversity and more competition of ideas and concepts.  And more appreciation for actual innovation instead of change for the sake of changing things. So in summary:  Yes, there are a lot of fields where some changes would be good but the tricky thing and the reason why many reasonable people are opposed to just changing things is that it is clear that it will be very difficult to make sure that the changes are actually going in a direction that benefits the project in the long term.  The question therefore is not how we can change things but how we make sure change goes in a beneficial direction for the community in the long term. I think your last paragraph has a lot of merit.  OSM has grown massively in data and number of contributors over the last years but growth in competence of people, experience and knowledge of the broad community in what forms the basis of this project, tagging (i.e. data representation of geography), cartography and other things, has not kept up with this.  We need a broad and open but ambitious and thoughtful discourse about these kind of topics (in contrast to the cacophony of spontaneous and not well thought through opinions that is all too frequent these days) and this depends on broad education.  And to be clear:  This is not something you can expect the corporate players in the OSM world to substantially contribute to. Comment from siberiano on  8 May 2018 at 19:00 Agree. You've just came to my conclusion from 2016. It's increasingly impossible to do anything in OSM besides drawing base map and downloading it. As a programmer in a company that uses OSM I'm already using Google Directions for public transit, because it's easier to raise a dead than an OSM-based PT router. And I don't want to haul the stack (Valhalla needs 2 auxiliary servers to get PT working). Comment from SimonPoole on  8 May 2018 at 19:30 Yawn. # Heading\n       ## Subheading * First item\n       * Second item 1. First item\n       2. Second item ","time":1525796089,"title":"Not Yours, OpenStreetMap","type":"story","url":"https:\/\/www.openstreetmap.org\/user\/Zverik\/diary\/43882","label":7,"label_name":"random"},{"by":"dickclucas","descendants":0,"id":17022050,"kids":"None","score":2,"text":"no gradient by Dick Lucas This attack will cause at least one top five GDP country to pass overbearing preventative legislation. By 2050 terrorists will facilitate an attack using technology from a large cloud services company e.g. Amazon, Cloudflare, Google, IBM, Microsoft. By 2030 there will be a financial services company offering a fully managed investment solution (e.g. saving for retirement, portfolio diversification) that charges no fees. M1 Finance\u00a0does exactly this and existed before I made the prediction. By 2030, a major financial services company like Vanguard or Charles Schwab will offer ETFs with an expense ratio of 0.00%. By 2030 there will be a dominant social network built around user location. Facebook, Snapchat, and Twitter use location, but have it bolted on the side. And dating apps are too use case specific. Location is too powerful to not take center stage in a social network. By 2070 domain names will be relegated to collectible items- bought, sold, and traded like pieces of art. By 2022, Amazon will release a router with voice activated Alexa built in.","time":1525796063,"title":"A bunch of tech predictions, many several decades out","type":"story","url":"https:\/\/nogradient.com\/category\/prediction\/","label":9,"label_name":"tech"},{"by":"brudgers","descendants":0,"id":17022035,"kids":"None","score":1,"text":" He\u2019s been trying to make an adaptation of Cervantes\u2019 novel for over 20 years. After stunning setbacks, it\u2019s about to make its debut, writes Nicholas Barber. May 2017: in the forest of Valsain, 90 minutes\u2019 drive from Madrid, Adam Driver is wading across a\u00a0sparkling stream. On the far side, Olga Kurylenko, dressed in a long golden gown, is trotting through the evergreens on a white horse, followed by more actors on horseback, all in medieval garb. In front of them there are cameras, lights, smoke machines, catering vans, crowds of technicians and an illustrious director in a straw cowboy hat. To put it another way, if you were picturing what you wanted a film set to look like, it would be pretty close to this one. We\u2019ve had too much luck, so it could go wrong at any moment \u2013 Terry Gilliam But as beguiling as its combination of showbiz glamour, fairy-tale enchantment and workaday practicalities may be, the most thrilling aspect of this tableau is what\u2019s missing. There are no meteorites, hurricanes or bolts of lighting. There are no ravenous tigers or cholera outbreaks. There isn\u2019t an ambulance or a fire engine in sight. Terry Gilliam\u2019s\u00a0The Man Who Killed Don Quixote, a film which is synonymous with sustained and calamitous misfortune, is finally going without a hitch after nearly 30 years of blood, sweat, toil and tears.\u00a0 \u201cWe\u2019ve been very lucky,\u201d says Gilliam, as Driver and Kurylenko get ready for their next take. He peers up at the sky through scrunched up eyes. \u201cBut I still worry. We\u2019ve had too much luck, so it could go wrong at any moment. Today, the clouds are building. They\u2019ll probably block out the light and we\u2019ll have to go home.\u201d - The films that inspire Terry Gilliam - 2001: \u2018One of the strangest films ever made\u2019 - Ten films to watch in May If only some inconvenient cloud cover were all that Gilliam had to worry about. A few weeks later, with the shooting wrapped, a Portuguese producer named Paolo Branco claims that the film is \u201cpatently illegal\u201d. He bought the rights from Gilliam in 2016, he says, and so The Man Who Killed Don Quixote shouldn\u2019t have been shot without his approval. Another producer, Peter Watson, argues that while Branco offered to invest in the film, he didn\u2019t hand over any cash, thus negating his contract with Gilliam.\u00a0\u201cSenhor Branco\u2019s interpretation of the law borders on the picaresque,\u201d says Watson. \u201cIf he really wants to kill the venerable don, I suggest he takes up jousting.\u201d  But Branco wouldn\u2019t go away. Jump forward a year to the present day, and The Man Who Killed Don Quixote has been selected as the closing night film at the Cannes Film Festival. Gilliam\u2019s dream project, decades in the making, is going to be seen at last. Or is it? Branco\u2019s Paris-based company, Alfama, has just issued a writ against the festival, and is asking the Paris District Court to ban the Cannes screening. \u201cFor legal reasons,\u201d says a press release from the company, \u201cthis film cannot be exploited in any way without pre-agreement from Alfama Films Production.\u201d The festival\u2019s representatives responded that \u201cMr Branco has allowed his lawyer to use intimidation and defamatory statements, as derisory as they are ridiculous\u201d. But they can\u2019t say for certain whether the screening will go ahead until the results of a court hearing are disclosed on Wednesday 9 May\u00a0\u2013 the day after the festival opens.  It is almost unbelievable that The Man Who Killed Don Quixote is still being wounded by the slings and arrows of outrageous fortune. But Gilliam, 76, probably isn\u2019t too surprised. A previous doomed bid to shoot the film, back in 2000, is chronicled in Lost in La Mancha,\u00a0an excruciatingly candid behind-the-scenes documentary. And even then, Gilliam had been wrestling with Don Quixote for longer than he cared to remember.\u00a0 The impossible dream Gilliam had the idea of a big-screen adaptation of Miguel de Cervantes\u2019 landmark novel in 1989. (\u201cAnd then I had to read the book,\u201d he says.) He has never been able to forget the idea, partly because he and Quixote are kindred spirits. The two-volume novel, published in the early 1600s, features an ageing Spanish noblemen who has read so many stories about the age of chivalry that he comes to believe that he belongs there. Having cobbled together a ramshackle suit of armour, topped by a shaving bowl for a helmet, he recruits a portly peasant named Sancho Panza to be his squire, and sets off on a fanciful search for adventure. Most famously, Quixote charges at a windmill because he imagines it to be a giant, which is just the kind of thing that people do in Gilliam\u2019s films. I don\u2019t want to see the normal version of the world. I\u2019ve always wanted there to be more play, more fun out there \u2013 Gilliam The first of these films was Monty Python and the Holy Grail (1975), which he co-directed with Terry Jones. It became a classic, voted 15th in BBC Culture\u2019s 100 Greatest Comedies of All Time last year. Born in Minneapolis, Minnesota in 1940, Gilliam grew up in Los Angeles, but he found fame in Britain in 1969 as the only American member of the Monty Python comedy troupe. His key contribution to the Pythons\u2019 television shows was his surreal animated interludes, and since then the protagonists of many of his films, such as Brazil (1985), The Adventures of Baron Munchausen (1988), The Fisher King (1991) and The Imaginarium of Doctor Parnassus (2009), have turned away from mundane reality and galloped off into worlds of magic and make-believe. That is, they have a lot in common with Cervantes\u2019 deluded anti-hero. As for Gilliam himself, he doesn\u2019t mind being compared to Quixote. \u201cIt\u2019s true on too many levels,\u201d he says with a weary grimace. \u201cI don\u2019t want to see the normal version of the world. I\u2019ve always wanted there to be more play, more fun out there. And I suppose I\u2019ve always thought that I was doing one thing, and discovered I was actually doing something else.\u201d  As closely as he identifies with the Don, Gilliam quickly realised that it would be a mistake to try a literal translation of Cervantes\u2019 1,000-page doorstop. \u201cThe problem with all the other [screen] versions of Quixote,\u201d he says, \u201cis that people think you\u2019ve got to be pure and faithful to what Cervantes wrote. No! You\u2019ve got to be bold, play with it, step away from it, but let Cervantes be the inspiration.\u201d His own concept was to see Quixote through contemporary eyes, so his script had a 21st-Century advertising executive called Toby Grisoni (named after Gilliam\u2019s co-writer, Tony Grisoni) zipping back through time to 17th Century Spain, where Quixote mistakes him for the faithful Sancho Panza. The screenplay attracted an ideal cast. Having starred in Gilliam\u2019s Fear and Loathing in Las Vegas (1998), Johnny Depp signed up to play Toby. Quixote was to be played by Jean Rochefort, a 70-year-old French actor who had learnt English specifically for the role.\u00a0  But even before the Lost in La Mancha documentary crew joined Gilliam at his base of operations in Madrid in August 2000, The Man Who Killed Don Quixote wasn\u2019t going smoothly. Or, as Gilliam now puts it, \u201cIt didn\u2019t have problems, it had one big monstrous problem.\u201d Essentially, that problem was that he was aiming for the sort of star-studded fantasy extravaganza which would traditionally be backed by a Hollywood studio. But Gilliam was determined to make it in Europe with European money. Unfortunately, the first company to commit to the film, Working Title, got cold feet, and a German financier pulled out soon afterwards. The budget was slashed from $40 million to $32 million. While that still made The Man Who Killed Don Quixote one of the most expensive European films ever, it also meant that there was no margin for error. If Gilliam was to get it in the can, absolutely everything had to go according to plan. It didn\u2019t. Lost in La Mancha is painful to watch: an astonishing \u2018un-making of\u2019 documentary which defines the phrase \u2018catalogue of disasters\u2019 Lost in La Mancha is painful to watch: an astonishing \u2018un-making of\u2019 documentary which defines the phrase \u2018catalogue of disasters\u2019. As Gilliam prepares to begin shooting, he finds that he can\u2019t get his actors in the same place, or even in the same country, at the same time. Then he finds that the sound stage he has booked is actually an echoey, semi-derelict warehouse. His first assistant director, Philip Patterson, declares that the film is in \u201cabsolute and total disarray\u201d. And that\u2019s before the trouble really starts.  On the very first day of production, Gilliam and his team set up camp in a sandy nature reserve, its unspoilt plains and otherworldly rock formations making it perfect for a scene of Quixote riding through the desert. But there was one drawback. The nature reserve happened to be right next door to a Nato bombing range; whenever Gilliam tried to record some dialogue, a plane would roar deafeningly overhead. Day two was worse. After a sunny morning, clouds gathered, thunder rumbled and the film crew was battered by a cataclysmic deluge of rain and sugar cube-sized hailstones. In the circumstances, Gilliam decided that he might as well exult in the sheer glorious unjust madness of \u201cthis great Biblical storm\u201d. A Lear-like figure, he dashed beneath a rocky overhang and peered out at the downpour. When he emerged, all of his equipment had been washed away in torrential rivers of mud. The next two days were taken up with drying out props, and figuring out how to disguise the drastic change in the setting: the waterlogged sand was now a completely different colour from what it had been earlier in the week. And then, on the fifth day, The Man Who Killed Don Quixote almost became The Film That Killed Jean Rochefort. In Lost in La Mancha, the actor can be seen muttering his lines through gritted teeth, unable to hide how agonising it is for him sit on a horse and hold a lance. Shooting was suspended, and Rochefort flew back to Paris to see his doctor. The diagnosis: a double herniated disc.\u00a0  Even then, Gilliam pressed ahead with other scenes, but, with no way to tell when or if Rochefort would be well enough to get back in the saddle, insurers shut down the production. The Man Who Killed Don Quixote was dead. Or so it seemed. Released in 2002, Lost in La Mancha closes with the following optimistic caption: \u201cSix months later, Terry Gilliam began a new attempt to make the film.\u201d But the process didn\u2019t get any easier. \u201cEvery film I\u2019ve done since then has been because I couldn\u2019t get Quixote made,\u201d says Gilliam. \u201cSo every time I finished a film, I\u2019d start on Quixote again. And then another two, three years would go by, and I\u2019d finally give up and get a proper job. But it\u2019s never gone away.\u201d\u00a0 Back to La Mancha Those proper jobs were no picnic, either. On The Brothers Grimm (2005), he clashed with the producers, the now-notorious Harvey and Bob Weinstein, who vetoed his casting choices and fired his long-time cinematographer, Nicola Pecorini. On The Imaginarium of Doctor Parnassus, Heath Ledger died halfway through filming his lead role, forcing Gilliam to replace him in various sequences \u2013 ingeniously, but hardly satisfactorily \u2013 with Depp, Colin Farrell and Jude Law. And then there was his adaptation of Terry Pratchett and Neil Gaiman\u2019s apocalyptic comic novel, Good Omens. Gilliam co-wrote a screenplay, cast Depp and Robin Williams as its main characters, and flew to Hollywood in 2001 to finalise a deal. Then came 9\/11. All of a sudden, no studio was keen to make a boisterous comedy about the end of the world. There was talk of a curse, the curse of Quixote. It\u2019s absolute nonsense \u2013 but it made financiers very nervous - Gilliam In between these stressful enterprises, there were regular announcements that Quixote would ride again. In 2008, Depp was back as Toby, and Gilliam\u2019s old Monty Python buddy, Michael Palin, was to play the Don. The next year, this double act was swapped for Ewan McGregor and Robert Duvall. But it was still tricky to raise the money required. \u201cThe film had become a legend,\u201d sighs Gilliam, \u201cand I think financiers don\u2019t want to deal with legends, they want to deal with solid things. There was talk of a curse, the curse of Quixote. It\u2019s absolute nonsense \u2013 but it made financiers very nervous.\u201d Still, there were some genuinely good omens in 2015. Amazon climbed aboard as one of the film\u2019s backers, and the lead roles were passed to Jack O\u2019Connell and John Hurt, who had the perfect face for Don Quixote. And then Hurt announced that he was undergoing treatment for pancreatic cancer, and he had to drop out. In 2016, he was replaced by the actor he had replaced, Michael Palin, and Gilliam found yet another Toby: Adam Driver.\u00a0  But Gilliam was still the movie business\u2019s answer to Job, and the torments continued. In March 2016, it was reported that the film had a new producer: none other than Paolo Branco. The cameras were due to roll that September. But when Branco couldn\u2019t raise his share of the finances, production was delayed again, and he tried to stop Gilliam making the film without him. Nonetheless, by May 2017, shooting is well underway. \u201cIn the end,\u201d says Gilliam, \u201cwe\u2019re doing this for much less money than we honestly need. But everyone involved, from the cast to the crew, are all working their asses off for a fraction of what they would normally be paid, because they just want to see this thing done. It\u2019s odd how being obsessive and not giving up can inspire other people to get involved. Fools that they are!\u201d Another small consolation for all the trials and tribulations he\u2019s endured is that, at this point, Gilliam doesn\u2019t believe that anyone else on the long list of almost-Quixotes could have matched the Quixote he ultimately got, Jonathan Pryce. The Welsh actor had played one of Gilliam\u2019s trademark fantasists in his 1985 classic, Brazil, so there is something poetic about him playing such a character again now that he is 70 - the same age that Rochefort was when he had his go at the role. \u201cI have wiped most of the other attempts out of my memory, because it went on and on and on,\u201d says Gilliam. \u201cBut along the way the cast just kept getting better and better. Jonathan has been wanting to [play Quixote] for 15 years \u2013 he\u2019s been making my life a misery. And now he\u2019s here and he\u2019s just extraordinary. The editor, who is Spanish, says that she can never imagine another Quixote. So it\u2019s as if the time was right: everything seemed to be ready to make this thing.\u201d He squints up at the mostly-blue sky. \u201cAnd even the weather agrees with us.\u201d  Gilliam is not the only person who feels that the stars have aligned. Before her horse-riding scene, Kurylenko sits in the lunch tent, in full regal costume, and talks about seeing Lost in La Mancha. \u201cYou watch and you think, poor Terry,\u201d she laughs. \u201cWhy does this have to happen to this wonderful man? He doesn\u2019t deserve this. It\u2019s just crazy. But it wasn\u2019t meant to be, I guess. It wasn\u2019t the moment. Now is the moment.\u201d Pryce is just as positive. \u201cObviously I\u2019m glad [Gilliam] pursued it for my selfish reasons,\u201d he says, having just shot a scene of his own. \u201cAnd from what I can remember of the original script, this is an altogether better version. It\u2019s a clearer story. Terry\u2019s done a lot of rewrites \u2013 and he\u2019s had a lot of time to do rewrites.\u201d The upshot of these revisions is that Toby is no longer transported back to the 17th Century: the reason why the actors are in historical costumes in today\u2019s woodland scene is that the characters are having a fancy-dress party. In the current draft of the screenplay, the premise is that Toby revisits a Spanish village where he once made a student film of Don Quixote, and learns that the shoemaker he cast in the title role has been living as Quixote ever since. In other words, Pryce is playing a dreamer who is playing a dreamer, \u201cI\u2019m incorporating the idea of the damage that films do to people,\u201d explains Gilliam, \u201cso it\u2019s become a bit more autobiographical.\u201d People love Roman ruins because they\u2019re not complete \u2013 perhaps I should have let people keep imagining how great this film would have been? \u2013 Gilliam Sure enough, Gilliam seems slightly damaged himself when I meet him in Valsain. As effervescent and opinionated as he is, he is also stooped, gnome-ish and generally less manic than he appears in Lost in La Mancha. \u201cI\u2019m not as bouncy as I used to be,\u201d he admits. \u201cI don\u2019t have the madness and strength I used to have, so when things go wrong, when things are difficult, I\u2019m more and more willing to give up.\u201d He also has doubts as to whether his film can possibly live up to the extraordinary mythology which has grown around it. \u201cThe problem is that people have very high expectations,\u201d he says. \u201cAnd a lot of people say I\u2019m a fool to make the film, and that it would have been better to let people imagine how great it would have been, rather than making it a reality and disappointing them. People love Roman ruins because they\u2019re not complete and you can imagine them. So I may be making a great mistake. Maybe the film would be better as a fantasy.\u201d So what keeps him going? \u201cIt\u2019s a medical problem. It\u2019s not a film, it\u2019s a tumour, and I have to have it removed. I want it out of my life, frankly.\u201d That was in May, 2017. With the recent word that The Man Who Killed Quixote would be screened at Cannes, it looked as if Gilliam\u2019s medical problem would be solved, the tumour would be removed, and the film he conceived in 1989 would be seen at last. But Branco\u2019s latest legal threats have changed the prognosis yet again. Maybe Gilliam, Driver, Pryce and Kurylenko will be on the red carpet on the festival\u2019s closing day,\u00a0Saturday 19 May.\u00a0But when it comes to Quixote, you can never know for sure. As I was typing this article, the director posted a link to a news story about Branco\u2019s intervention on his Facebook page. \u201cThis is just to keep you loyal fans up to date,\u201d wrote Gilliam. \u201cI may need your help. I will let you know.\u201d If you would like to comment on this story or anything else you have seen on BBC Culture, head over to our\u00a0Facebook\u00a0page or message us on\u00a0Twitter. And if you liked this story,\u00a0sign up for the weekly bbc.com features newsletter, called \u201cIf You Only Read 6 Things This Week\u201d. A handpicked selection of stories from BBC Future, Culture, Capital and Travel, delivered to your inbox every Friday. 02 May 14 Oct 10 May 08 Oct 04 May 18 Sep \n\n","time":1525795934,"title":"Terry Gilliam's Quixote: This 'cursed' film is finally here","type":"story","url":"http:\/\/www.bbc.com\/culture\/story\/20180507-terry-gilliam-quixote-this-cursed-film-is-finally-here","label":7,"label_name":"random"},{"by":"itamarst","descendants":0,"id":17022025,"kids":"None","score":2,"text":"by Moshe Zadka, 08 May 2018 The following guest post is by Moshe Zadka, explaining the importance of networking and how you can do it with minimal time outside work. A good professional network is a long-term asset.\nWhen you\u2019re looking for a new job you can talk to people you know, ask them if their company is hiring, and then have them submit your resume directly to the hiring manager.\nThis will allow you to skip the \u201cresume filter\u201d, and often get you past the phone screen as well. But even if a professional network is useful, how do you find the time to build it?\nYou probably don\u2019t want to have to get out every evening to a social gathering, and spend hours talking, just to plan for a hypothetical future job. One way to start building your professional network with little time spent is by focusing on your current job.\nOver time your colleagues will leave for other jobs, and every former colleague is a potential referral to another company.\nSo you should always make sure you have non-work contact details for your colleagues. Unfortunately, you can only expand your network so much from attrition at work: if you want a larger network, you will have to do some work.\nBut by making judicious use of your time,\nand going to the right venues,\nyou can grow and maintain a good professional network\nwhile still only spending one or two evenings a month at events. In one of the San Francisco Python meetups,\nI met someone working at PayPal\u2013\na company I had no interest working for.\nHowever, we kept in touch.\nAt some point, he moved to a start-up.\nAt another point,\nI found myself looking for a job,\nafter a company shutdown.\nBecause I had kept in touch with him,\nit was easy to reach out. Even though I was on a tight timeline\nfor getting another job,\nhe made sure I was fast-tracked through the process \u2013\na pro-forma resume review,\nand skipping the phone screen.\nI still had to go through a half-day\u2019s worth of interview panel,\nbut removing the simple filters from my path\nprobably saved a week or more worth of being unemployed,\nand also let me put pressure on other prospective employers\nto fast-track me. Business cards sound like an antiquated thing,\nsomething you might see on \u201cMad Men\u201d.\nHowever,\neven with modern smartphones,\nthere is no faster way to share your contact details with someone you\u2019ve just met.\nFor that, a business card\u2019s most important part is your\ne-mail address. In all of the opportunities below,\ngive people a business card when the conversation is done.\nMaking your own cards is free to cheap nowadays,\nno need to wait for your job to print you one.\nIn any case,\nyou want to make sure you have your personal e-mail on the card, not your work e-mail. If you are already employed,\nask your job to send you to relevant conferences.\nSome places have a budget for \u201cprofessional development\u201d,\nothers have funds specifically marked \u201cconferences\u201d \u2013\nor maybe it\u2019s under the recruiting budget.\nChoose a relevant conference,\nand remind your manager that sending you to the conference\nis a form of training: a great investment in employees. Some companies will only fund your trip if you speak at a conference.\nMost conferences understand that some people will only come\nif they speak,\nand structure their timeline accordingly:\nyou\u2019ll know whether your talk is accepted far enough in advance to get your manager to sign off on sending you.\nAn efficient way to send talk proposals is to recycle \u2013\nif a talk is declined from one conference,\nit is fine to send it to another closely related one,\nalthough sometimes it will have to be tweaked slightly.\nIf the audiences are sufficiently distinct you can even reuse a talk you\u2019ve already given. Once you are at a conference,\nattend birds of feather sessions,\nand try to sit with new people for conference meals,\nif those are served.\nThis is a great way to meet more people at the conference.\nGiving a talk is also a great way to meet more people:\nyou can often meet other speakers,\nand many people in the audience will want to talk to you afterwards. Many places have tech meetups\nin the evening.\nYou can probably find time to go to a meetup once a month.\nIf you do go,\nmake sure to make the most of your time \u2013\nmingle,\ntalk\nand hand out your business card. Avoid going to the same meetup month after month \u2013\nwhile it is comfortable,\nit tends to be the same people:\nyour goal should be to expand your network.\nSo once you stop meeting new people,\nswitch to a new meetup. As with conferences,\ngiving a talk at a meetup is great way to meet new people.\nYou might even be able to work on your talk at work,\nif you can pitch it as a recruiting event to your manager. If your company has an engineering blog,\nparticipate.\nFind something you have done recently\nwhich was interesting or surprising,\nand write about that experience. If your company does not have an engineering blog,\nsee if you can make one happen.\nIt helps with recruiting,\nand helps people develop in their career. Keep in touch with your network.\nIf you come across an article relevant to someone,\nsend it to them with a note\n\u201cthought it might be interesting\u201d.\nOften they will already have read it,\nbut will be interested in sharing their thoughts. Developing and maintaining a professional network\ndoes not need a huge time investment \u2013\na little bit goes a long way,\nif properly allocated.\nAnd when the day comes that you need a new job,\nthat small investment will pay off.\nWhile you usually can\u2019t just get a job just by knowing someone,\na network will help skip past companies \u201cresume filters\u201d,\nand you can have a more streamlined interview process\nif you have a friend on the inside.  Moshe is a core developer on the Twisted project, and has been using Python for 20 years. He\u2019s just published a book, \u201cfrom python import better\u201d. \n Code Without Rules\nHelping you become a productive programmer and work saner hours \u00a9 2018 Itamar Turner-Trauring. All rights reserved.","time":1525795858,"title":"Networking for programmers with very little free time","type":"story","url":"https:\/\/codewithoutrules.com\/2018\/05\/08\/networking\/","label":7,"label_name":"random"},{"by":"caser","descendants":0,"id":17021998,"kids":"None","score":1,"text":"I just got back from a 7-day silent retreat, and one thing that surprised me was how creative I became after cutting out all outside influences for 3\u20134 days. Ideas were so exciting, because they were the only stimulation I had! It reminded me of how I felt during some of the creative projects I did as a kid, pre-Internet in a house that didn\u2019t have cable. This in turn got me thinking of the Montessori approach to learning. From what I understand, their innovation was realizing that young children have an innate drive to explore and interact with their environment. So, if you create an environment that they can learn through interacting with, they will learn by default. Put them in that environment, and just by naturally being a toddler, they\u2019ll pick up the things they need to know. I think we adults have a similar innate drive to create, and that drive can be supported or stifled by the environment. And, I think that instead of creating environments where we are creative by default, we live in a world where we are distracted by default. In a vacuum, we have ideas and projects that want to see the light of day. However, with phones in our pockets mainlining the Internet into our veins at any moment, our minds never have the uninterrupted quiet we need to let the creative juices flow. It\u2019s hard for the seed of a not-yet-fully formed idea to compete with Candy Crush or checking HN \/ Twitter \/ Facebook \/ Gmail for the nth time that day. What if quiet, stillness, and boredom are the fertile soil in which creative ideas can blossom? Cal Newport has written about the importance of carving out periods of deep work in one\u2019s schedule, but what would it look like to make that the default? Here\u2019s a stab at what a \u201ccreative by default\u201d lifestyle might include: \u00b7 No phones for most of the day\u00b7 Internet only at pre-allotted times, 1\u20132x \/ day\u00b7 Documentation, Stack Overflow, programming libraries, and design templates locally hosted and available offline\u00b7 Immersion in nature\u00b7 Extended periods where you can only meditate or create\u200a\u2014\u200ano socializing or passive consumption of material.\u00b7 Regular, planned social periods outside of the silent work \/ meditation periods\u00b7 Healthy, low glycemic index foods\u00b7 Alarm clocks, analog calendars, and other \u201cdumb\u201d devices to reduce dependency on a smartphone\u00b7 An emergency contact # (maybe a land-line) where family \/ friends \/ colleagues can contact you if an emergency arises during the extended offline periods\u00b7 Perhaps a typewriter or other device that never touches the Internet\u00b7 Intentionally chosen pieces of content to consume around the creative life and process (poems, essays, talks) And, in a group setting: \u00b7 Having one terminal with Internet access for as-needed connectivity throughout the day\u00b7 A place to store phones if needed where everyone can see, again using social pressure to encourage limited use\u00b7 Designated places to collaborate \/ brainstorm that won\u2019t spill over into the silent creative space After being in that environment for a week, I strongly felt like I didn\u2019t want to let the stillness and creativity go when I left the retreat. So, since coming home, I\u2019ve begun to implement some of the above practices in my life, like only connecting to the Internet a few times per day, getting a flip phone to use in case of emergencies, and spending more time in nature. I\u2019m also interested in exploring these practices in community, as I feel like having a group of people intentionally doing this will make it even more powerful. If you\u2019d be interested in trying out these practices in a 4\u20137 day retreat, sign up here and I\u2019ll reach out when I have something concrete. Or if you have ideas \/ want to get involved, you can email me at casey@recesslabs.com. Also, follow me on here if you want to hear more updates on my experiments with living a \u201ccreative by default\u201d lifestyle! By clapping more or less, you can signal to us which stories really stand out. Casey writes about life, startups, meditation, and self-care. He is a founder of Hacker Paradise and Recess Labs.","time":1525795627,"title":"Creative by Default","type":"story","url":"https:\/\/medium.com\/@casey_rosengren\/creative-by-default-bbc4ba93656b","label":7,"label_name":"random"},{"by":"ardent_uno","descendants":0,"id":17021988,"kids":"None","score":1,"text":"Using the gene-editing tool CRISPR to snip at DNA is often akin to using scissors to edit a newspaper article. You can cut out words, but it's difficult to remove individual letters or instantly know how the cuts affect the meaning of the text. Someday, CRISPR could be used to \"clip\" disease-causing genetic mutations in patients. But such precision medicine is impossible so long as CRISPR remains a clumsy tool. In work that will help make the gene-editing process more precise, researchers at the Joint Institute of Metrology and Biology (JIMB, a collaboration between Stanford University and the National Institute of Standards and Technology, or NIST), have developed a new kind of CRISPR platform called MAGESTIC. Taking its name from the phrase \"multiplexed accurate genome editing with short, trackable, integrated cellular barcodes,\" the new platform makes CRISPR less like a blunt cutting tool and more like a word processor by enabling an efficient \"search and replace\" function for genetic material. Announced in a Nature Biotechnology paper, MAGESTIC also produced a sevenfold increase in cell survival during the editing process. \"MAGESTIC is like an advancement in the 'control F' [find text] operation of a word-processing program, with the replace-text command allowing a desired change. This lets us really poke at the cell in a very precise way and see how the change affects cell function. Then we can compare the actual effects of each variant with the computationally predicted effects, and ultimately improve models for predicting how genetic variants impact health and disease,\" said JIMB scientist Kevin Roy, a MAGESTIC developer. Being able to precisely edit genomes with CRISPR requires an extensive understanding of how cells will repair cuts at different sites across the genome so that you can control the process as needed. Currently, random mutations can occur at cut sites in the cell's DNA, often because the DNA strands rejoin in unpredictable ways. What's more, lots of cells don't survive the editing process at all. Building accurate predictions for gene editing remains, therefore, extremely challenging. What researchers want is a reliable way to program the CRISPR machinery to cut at desired locations throughout the genome, and then to direct the cells to introduce designed edits at the DNA cut sites. This can be done by providing the cell with a \"donor\" DNA that the cell's DNA repair machinery can use as a template to replace the original sequence at the cut site. This is not unlike what editors often do to revise a text, first searching for a certain word or sequence of words and then replacing it with something else. However, the DNA repair system inside the cell is complex and does not always behave predictably like a word processor. The process by which the cell searches for a suitable donor DNA to repair a cut site is an enormous challenge for the cell, as the DNA repair machinery must search among millions to billions of base pairs of DNA sequence to find the correct \"donor\" DNA. MAGESTIC provides a major advance in gene-editing technology, aiding the cell in this search by artificially recruiting the designed donor DNA directly to the cut site in a process termed \"active donor recruitment.\" Such recruitment caused a sevenfold increase in cell survival, a change that surprised the research team with its efficiency and effectiveness. Another one of the main features that separates MAGESTIC from previous approaches with multiplexed CRISPR editing is a new kind of cellular barcode. Researchers have traditionally used small bits of circular DNA, known as plasmids, to express the guide RNAs and to store barcodes to track the designed mutations in each cell. The plasmids multiply as the cell grows and are inherited by both cells after cell division. In theory, they should act much like the black-and-white barcodes used to track items at the checkout stand. But unlike the single-barcode-per-item correspondence at the checkout stand, the plasmid barcodes can vary widely in number, with anywhere from 10-40 appearing in each cell. That can give an inaccurate measure of cell abundance. In MAGESTIC, barcodes are integrated into chromosomes instead. This makes them stable and easy to find and count later. Although many advances have been made in DNA sequencing and editing over the last two decades, general understanding of the function of genomic sequence remains sparse. Scientists know very little about the function of the 0.1 percent of code that varies between individuals in a population and is responsible for differences in disease susceptibility. MAGESTIC will help to address this gap in the understanding of natural genetic variation by enabling each genetic variant to be precisely edited and compared to other genetic variants one-by-one. This helps to uncover which genetic differences have an impact on cellular function. MAGESTIC also performs all edits at once in a single test tube, with each edit occurring in any one of millions of otherwise identical cells. This is more efficient than past platforms, which required editing for each variant in separate experiments. \"We are reaching a state where we have not only achieved the ability to sequence the order of base pairs in genomes but we can also make changes to them. We still need a better understanding of the consequences of our edits,\" said JIMB's Lars Steinmetz, professor of genetics at Stanford University, group leader at the European Molecular Biology Laboratory (EMBL), and senior author on the paper. \"With MAGESTIC it's like being able to make small edits to individual letters in a book, and being able to see what effect it has on the meaning of the text. Our donor recruitment method also allows the new piece of information to be placed at exactly the right page where the cut occurred.\" Story Source: Materials provided by National Institute of Standards and Technology (NIST). Note: Content may be edited for style and length. Journal Reference: Cite This Page: Get the latest science news with ScienceDaily's free email newsletters, updated daily and weekly. Or view hourly updated newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?","time":1525795553,"title":"Taking CRISPR from Clipping Scissors to Word Processor","type":"story","url":"https:\/\/www.sciencedaily.com\/releases\/2018\/05\/180508102209.htm","label":5,"label_name":"ml"},{"by":"evo_9","descendants":0,"id":17021987,"kids":"None","score":1,"text":"{{ test.description }}","time":1525795551,"title":"University of Cambridge - Select Test: The Psychometrics Centre","type":"story","url":"https:\/\/discovermyprofile.com\/","label":7,"label_name":"random"},{"by":"ashrust","descendants":0,"id":17021979,"kids":"None","score":2,"text":"Side Letters are separate legal documents, that usually come in addition to a term sheet, and may contain clauses which can have a big impact on your company. Here are the common matters usually addressed in side letters and how they can affect your company. What it is: Usually requested by strategic investors, i.e. the major public companies in your industries. Information rights means an investor has access to the company\u2019s financial information (revenue, profits, costs, etc.) every quarter. Notification rights means you must inform this investor of an acquisition offer, which you cannot accept for a predetermined time period (usually enough time for the investor to potentially provide a counter offer). How it hurts: Although notification rights have the potential to trigger a bidding war for your benefit, the risk is that the investor can also use it to investigate one of your competitors. As the investor knows you can\u2019t be bought by another acquirer without their knowledge. Information rights make it difficult for your company to control its own story to the investor. For example, during future fundraising discussions, if you had a downturn in profits due to a new product launch, the investor may not get excited because they\u2019ve seen your financials out of context. What it is: This grants an investor the right to buy a larger percentage of the company at a later stage. Usually, it\u2019s a larger percentage than their first investment bought and would be purchased during your next round of financing at whatever price is set by the lead investor. Thus, the investor can increase their stake in the company in the next round but doesn\u2019t (usually) get a special price. How it hurts: Most of the time this money will be a great momentum driver for your next fundraising round. However, if your round is very competitive or your new investors want to buy a large share of the company, then the team will experience additional dilution. For example, if InvestorA wants to buy 35% of the company and InvestorB has Super Pro Rata rights for 15%, for a total of 50%, then everyone will be diluted by 1\/3; rather than just 1\/4 if it was only InvestorA. Assuming 100 initial shares, 50% dilution: 100\/150 = \u2154 still owned by team; 35% dilution: 100\/135 = \u00be still owned by team. There is also risk if an investor with Super Pro Rata rights doesn\u2019t invest in the next round, as that signals negatively to other potential investors. What it is: As a part of the investment, your investor asks your company to adopt specific policies which meet or exceed their standards on common issues in the workplace such as: HR initiatives. How it hurts: In almost all cases, these policies are helpful. However, problems can arise if they are not regularly communicated to the rest of your team. For example, if your company policy requires every open position to have a candidate from an underrepresented group be interviewed before a hire is made, the hire could be stalled until this requirement is fulfilled. This can cause internal friction and tempt circumvention of the guidelines, especially when it is a surprise. Side letters are common part of the term sheet process in seed rounds. Make sure you understand the impact of the most prevalent terms and how they will affect your company in the future. Disclosure: My fund (Sterling Road), asks for Super Pro Rata rights when making investments. You can view the full process here. If you\u2019re a B2B company at the seed stage looking for help, you can reach me at ash@ashrust.com. Thanks to Kaego Rust, Alec Barrett-Wilsdon, Eric Wiesen, Sean Byrnes and David Smooke for their help on this article. Photo by Helloquence By clapping more or less, you can signal to us which stories really stand out. Managing Partner, Sterling Road; Partner @AlchemistAcc. Past: @TrinityVentures, Cofounder @SendHub (acq. Cameo Global). ash@ashrust.com how hackers start their afternoons.","time":1525795477,"title":"Term Sheet Problems Part 3 \u2013 Side Letters","type":"story","url":"https:\/\/hackernoon.com\/seed-fundraising-term-sheet-problems-part-3-side-letters-695333d8f92b","label":7,"label_name":"random"},{"by":"OlliG","descendants":0,"id":17021977,"kids":"None","score":1,"text":"At Instacart, we use Tableau for key performance indicator (KPI) reporting, KPI trending and determining drops or spikes in our core metrics across key dimensions. In an effort to help the team be more efficient and data-driven in their day-to-day responsibilities [tasks], we also use Tableau to democratize our data by providing curated extracts for advanced data exploration. In the trade-off between flexibility and performance, we have chosen to create data extracts that are very granular instead of building aggregated data sources. Aggregated data sources are far more performant but limit the diversity of possible analyses. Because our data volume is scaling much faster than we anticipated, this trade-off has recently started to degrade the load times of many of our dashboards (from a few seconds to around 20\u201330 seconds every time a filter is applied). We learned one of the root causes for this slow-down was metrics that included distinct counts. Since these metrics are required for unaggregated data sources, we started exploring alternatives to improve our Tableau performance. Instacart\u2019s license of Tableau does not currently provide any such functionality. We implemented HyperLogLog, a probabilistic counting algorithm, to create data extracts in Tableau. This post describes how to create a data source from a Redshift cluster (other data warehouses will work as well) and the associated measure definitions to implement HyperLogLog in Tableau with a quick toy problem that compares its performance to a measure including a distinct count. HyperLogLog is an algorithm that estimates the number of distinct elements in a data set with an accuracy of approximately 1.6% (using 4096 registers). The algorithm has been described in detail in many places and is fairly straightforward, so I will only discuss it at the very highest level (a more detailed explanation of the algorithm can be found here and a corresponding simulation here). In HyperLogLog, elements of the the set are first hashed to ensure randomness, then converted into binary and added into a register. A register stores the most unlikely event observed in the set by observing the longest consecutive count of leading zeros. Based on the register\u2019s maximum value, an approximate count of distinct elements can be calculated. The table below demonstrates the likelihood of observing a given number of leading zeros in a register, given the binary representation (limited to 8 bits for simplicity) of an element in the total set: For example, if the longest run of zeros was 4, we would estimate 32 elements. Since this estimation can be heavily influenced by just randomly getting a long run of zeros, the total set of elements is split into many registers and averaged in order to estimate the total number of distinct elements. If our data were split into two registers, and the first had a maximum of 4 leading zeros while the second had only 3 leading zeros, we would estimate a cardinality of (32+16)\/2 = 24. A final adjustment to this estimate is necessary to account for the possibility of hash collisions. This adjustment factor is defined by Flajolet et al. in their original HyperLogLog papers [10.1.1.76.4286] as: where m is the total number of registers. Additionally, in order to minimize the impact of outliers, HyperLogLog uses the harmonic mean instead of a regular average, which gives us the following formula: Where M[j] is the position of the first non-zero value in the register m. The challenge of preparing a data extract for Tableau now lies in recreating some of these non-standard operations in Tableau, and then storing the resultant registers in Tableau where they can be combined to determine distinct counts across many dimensions. The code in its entirety can be found here, but I will build it up step-by-step, referencing the previous step with the shorthand [RESULTS from PREVIOUS QUERY] to minimize repeated text. First, we select the element of which we want to get a distinct count and its associated dimensions. For example, to calculate the distinct number of products sold by Instacart in a given time period: Next, we have to hash the product_id column. Redshift provides several hash functions. In this example, I will use the SHA-1 implementation defined by the FUNC_SHA1 function. The resulting hexadecimal number provides significantly more precision than we need (and are able to store in numeric Redshift data types), so I will use only the 14 rightmost characters as the hash and the 3 leftmost characters as the register number: Since the elements are hashed, they are pseudo-randomly assigned to the register represented by the first three characters. The same value will always be added to the same register. Both the register and the hash are converted from hexadecimal to base 10 using the STRTOL function: Next, the hash has to be converted into binary, but Redshift does not have a standard function to accomplish this. In the absence of writing a UDF, this operation can be accomplished by bit-shifting (a UDF would be much preferable for readability and efficiency but requires appropriate permissions): To determine the first non-zero bit of the element, a regex substring expression can be used: Finally, we have to group by the dimension and register to find the longest run for a given grouping: The final query produces our necessary registers, which can now be combined in Tableau in order to calculate distinct counts. To build our visualizations, we take the query we wrote above and create a data source from it in Tableau. In Tableau, we will have to perform two calculations in order to calculate a distinct count of elements. First, we have to find the maximum number of non_zero elements per register in a given grouping. This can be accomplished with the level of detail calculation shown below: Given this calculated measure, we can now apply the formula to determine the cardinality estimate: Let\u2019s see how the HyperLogLog estimate compares to the equivalent distinct count operation: While there are some minor differences, we can see that the true count is reflected extremely well and the HyperLogLog version performs significantly better. So just how much did HyperLogLog help in this example? Newer versions of Tableau allow you to measure exactly how much time is needed for each operation in rendering a dashboard. Using this feature, we can see in this particular example that the HyperLogLog algorithm performed ~50 times faster in determining a distinct count. More importantly, the computation cost of the HyperLogLog algorithm now scales with the number of dimensions of the dataset instead of scaling linearly with data volume. Consequently, it will not break down as we continue to surface more and more data on our Tableau server. In many cases, Tableau is used as a quick analysis and trending tool. In these cases, the 1.6% error from implementing large data sets as aggregates is perfectly acceptable, and utilizing the HyperLogLog algorithms can significantly improve performance compared to distinct counts on unaggregated data sets. Ultimately, this technique provides the team another tool to quickly access our metrics, determine important trends and become even more data-driven. By clapping more or less, you can signal to us which stories really stand out. Instacart Engineering","time":1525795468,"title":"Tableau Implementation of the HyperLogLog Algorithm","type":"story","url":"https:\/\/tech.instacart.com\/implementing-hyperloglog-in-redshift-and-tableau-a62081c98e1c","label":3,"label_name":"dev"},{"by":"dend","descendants":0,"id":17021961,"kids":"None","score":2,"text":"This post was written by Jessie Huang, Program Manager on the docs.microsoft.com team. Today, we are happy to announce the release of two key improvements to the REST experience on docs.microsoft.com: If you are already familiar with any of our existing API Browser experiences, such as that for .NET, PowerShell, Python or JavaScript, you will feel right at home with this new release - it allows you to quickly get documentation for a wide variety of REST APIs without having to rely on a less focused search experience. The API Browser is all about getting you on your way quickly by finding the right API in seconds. Try the REST API Browser Previously, if you wanted to find specific REST API documentation on docs.microsoft.com, you may have had to look for it through our site-wide search, or use any of the navigation components on content pages, such as the table of contents. That may be time-consuming, especially in situations when you know exactly what you are looking for. We wanted to make that easier. Starting today, you can discover Microsoft REST APIs in a much more efficient manner, from one centralized location - https:\/\/docs.microsoft.com\/rest\/api. Simply select a service from the dropdown to find all available service APIs:  Need to save a click? For some of the most frequently used APIs, you can always count on Quick Filters - clicking on one of them will instantly bring up the comprehensive set of APIs for a given service, for its latest release:  We designed the search experience in a way that allows you to find the necessary API information whether you know the exact keywords or not - or even if you don't know the service it's in! By default, we will search across all APIs documented on docs.microsoft.com. Only when a service is selected, the search will happen within the scope of that service.  Need to search for multiple keywords at once? You can do that too!  Found an Azure API of interest, but don't know if it's quite what you need to help you build out your scenario? Worry not, because starting today you can try Azure REST APIs directly in your browser, on docs.microsoft.com! Try Interactive Azure REST APIs All you need to do is press the green Try It button:  Once you select Try It for a REST API, you will be taken to a focused view, allowing you to experiment with different parameter values or request body. You can just as easily change the article context by using the Contents hamburger menu in the top left corner of the page:  There are many improvements we plan on implementing in the near-term, so this experience is by no means final, for example we will enable Try it not only for Azure REST APIs. We want you to tell us how we can make our REST experience better - just open a new issue, and we will get back to you as soon as possible. You can also follow our Twitter account for latest updates and notifications! Dreaming of making the world better for developers? Join our team! \r\nWhat type of feedback would you like to provide?\t\t Our new feedback system is built on GitHub Issues. For more information on this change, please read our blog post. Loading feedback...","time":1525795298,"title":"Big updates to the REST API documentation experience on docs.Microsoft.com","type":"story","url":"https:\/\/docs.microsoft.com\/en-us\/teamblog\/announcing-rest-improvements","label":7,"label_name":"random"},{"by":"astigsen","descendants":0,"id":17021957,"kids":"None","score":1,"text":"","time":1525795281,"title":"Kids Gone Wild: Denmark's Forest Kindergartens","type":"story","url":"https:\/\/www.youtube.com\/watch?v=Jkiij9dJfcw","label":7,"label_name":"random"},{"by":"StrawberryPaul","descendants":0,"id":17021955,"kids":"None","score":1,"text":"","time":1525795268,"title":"Detecting Decision Ambiguity from Facial Images","type":"story","url":"https:\/\/youtu.be\/LNgvCIBq1b4","label":7,"label_name":"random"},{"by":"okgabr","descendants":0,"id":17021944,"kids":"None","score":14,"text":"With our GitHub integration, all bugs and user feedback can be converted into GitHub issues with just one click. We spoke to two of our users about how they make the best use of Instabug + GitHub to fix bugs faster. \u00a0 \u00a0  \u00a0 O2 is the largest integrated telecommunications operator in the Czech Republic with more than six million lines, both fixed and mobile. We spoke to\u00a0Ondrej Sebelik,\u00a0Cloud and Mobile Development Lead at O2 Smart Box, about how they manage user feedback and track bugs. \u00a0 O2\u2019s headquarters in Prague. \u00a0 In 2017, O2 launched Smart Box, a DSL modem, Wi-Fi router, and smart home gateway in a single device that quickly became the centerpiece of the company\u2019s broadband product. Since the O2 Smart Box mobile application serves as the interface of the device, it also became a natural place for customers to interact with O2 as a company. \u201cSmart Box relies heavily on GitHub for code hosting and issue tracking, so it was only natural to integrate Instabug as it allowed us to keep everything in one place. The most important thing is that thanks to the integration, the distance between the users and our developers has shortened and there is a clear link between reporting a bug and someone working on a fix for it. It hugely benefits both sides \u2013 users have a simple and easy way to communicate their issues\u00a0and our developers can get all the details they need to be able to fix them,\u201d Sebelik explained. The O2 Smart Box team uses the Instabug SDK in their live application as well in their beta builds. The company\u2019s beta team have set up automatic forwarding of Instabug bug reports to GitHub. Whenever any of their beta testers report a bug, it automatically gets redirected to GitHub where developers can investigate it, work on a fix, and track its progress on the project board. Beta testing happens continuously as part of their development process. Before releases, they usually have five to seven days of testing in beta and if everything goes well, the build is then published. With the help of the Instabug + GitHub integration, the steps of transferring bug reports and manually creating GitHub issues are automated in their workflow and the development team has more time to focus on fixing bugs. \u201cIn production, the situation is a little bit different since many customers do not actually report bugs in the app itself but ask questions or look for help with their broadband service. So the bugs are first handled by the technical support team who either help the customer resolve the issue\/answer the question, or forward the bug to GitHub manually if they are not able to resolve it themselves. From there, it\u2019s the same as in beta,\u201d Sebelik told us. \u00a0 The O2 Smart Box mobile app. \u00a0 \u00a0 \u00a0  \u00a0 Lifesum is a Swedish digital health startup with a vision to make people healthier and happier by using applied psychology and technology. Founded in 2013, the company was selected by\u00a0Wired UK as one of Stockholm\u2019s 10 hottest startups.\u00a0We spoke to Sebastian Bredberg, Product Owner and Director of Customer Support and QA, and Tim Gunnarsson,\u00a0QA Tester,\u00a0about the Lifesum app and their development process using GitHub + Instabug. \u00a0 Inside Lifesum\u2019s office in Stockholm. \u00a0 Lifesum relies heavily on GitHub for product management and issue handling to ensure transparency and collaboration across the whole company. To streamline their workflow and better serve their 27 million registered app users and counting, they moved all product decision management and bug reporting to GitHub Projects. Before releasing major builds, the QA team conducts manual and automated tests, including receiving bug reports on GitHub through Instabug.\u00a0When an issue is found, testers take a screenshot, describe the exact problem, and submit. The issues then get forwarded automatically to GitHub, where the QA team sorts, labels, and triages them to send to the right teams. \u00a0 The Lifesum team reports bugs using the Instabug SDK. \u00a0 The team has several ways of collecting feedback, including on social media, through their support team, and conducting in-house experiments and focus groups. Before they integrated the Instabug SDK, gathering all the necessary details for each issue was a tedious job. \u00a0 We\u2019d love to hear from you about how you utilize Instabug to optimize your workflow. E-mail us at content@instabug.com to be featured or to share your tips and tricks. \u00a0 \u00a0 Senior Growth Marketer @Instabug The top apps in the world rely on Instabug for bug reporting, user feedback, and crash reporting.\u00a0\ud83d\ude80 GET STARTED FOR FREE","time":1525795205,"title":"GitHub Power-ups: Learn how O2 and LifeSum use Instabug to build better software","type":"story","url":"https:\/\/blog.instabug.com\/2018\/05\/github-instabug\/","label":1,"label_name":"business"},{"by":"anthonyhughes","descendants":32,"id":17021939,"kids":"[17022593, 17022420, 17022405, 17022275, 17023368, 17023660, 17023014, 17024109, 17023869]","score":65,"text":"A case for publishing JSON-LD regardless whether you are working with linked data or RDF, with some simple examples JSON-LD has been among us for several years now, its adoption is increasing, but I suspect not as fast as it probably should. JSON-LD arose out of the RDF community, subsequently adopted as a W3C standard, as a pattern for representing RDF and linked data (hence 'LD') as JSON, with a key aim of making RDF much easier to consume for developers. While Google has raised awareness of JSON-LD considerably as it\u2019s preferred pattern for marking up web pages with schema.org structured data, JSON-LD however has only made relatively small inroads into mainstream web development. There are number of reasons for this, not least that the W3C and JSON-LD working group did not do a great job of communicating its worth and explaining how to use it effectively, although this has improved a lot more recently, the excellent (and long overdue) JSON-LD API best practices guide published in February 2018 is a very good example. I am going to attempt to help the cause too, and make a convincing case of why JSON-LD should be used in mainstream software development, regardless of whether you are working with RDF. I want to avoid making this post a full-on lesson about JSON-LD and RDF, so if you are not entirely sure what JSON-LD is, here is super brief 101. Alternatively get stuck into the full JSON-LD 1.0 specification here, and the draft 1.1 spec here. JSON-LD is a W3C standard for representing RDF in JSON format.\nIt is a standard pattern (schema) for representing the key elements of RDF resources and the statements about them, such as the resource identifier (the IRI) of the object being represented and the rdf type (class) of object being described.  For example, you will often see snippets of JSON-LD looking like this : If you are familiar with RDF, you will be aware that most RDF formats typically involve a proliferation of IRIs\/URIs everywhere. JSON-LD however alleviates this by abstracting base URIs into a context. For the example above, if this was written without a context in valid JSON-LD it would look more like this : While verbose and ugly, in RDF this can be represented as, or transformed into long form statements (triples) like this : You can test the validity and see how JSON-LD can be expanded into RDF using the JSON-LD playground here. Introducing the \u2018context\u2019 into the JSON-LD makes this much more palatable. The context defines the fields in the data payload : In this example, the original data from my first snippet, the type and name properties of the resource are explicitly identified in some schema or ontology, so as a consumer of this JSON you are aware of the model that this data was described with. Without the context the JSON cannot be transformed to RDF statements.  As a consumer of course you can choose whether to ignore the context, or use it depending on your own need. This same JSON-LD is often written such that that the context and payload are separated using the @graph construct as follows. This is in fact identical data to that above, it just identifies the data payload and the context that describes it as separate JSON objects. Back in RDF world, this also lets us create a named-graph which would then allow us to generate RDF quad statements instead of triples through the addition of an identifier for the graph : Which if you try this in the json-ld playground will give you RDF statements : To make this more concise the context does not need to be explicit in each payload, it can be remote, so a consumer can choose whether to dereference it or not : This also means contexts can be cached for higher performance.\nSo there you have it, RDF in JSON form. JSON-LD has brought RDF out of the labs, and made it presentable with a highly consumable JSON representation for developers.\nOr has it... While the introduction of JSON-LD is undoubtedly a massive deal for the RDF development community, it has not been widely embraced by software developers. Most developers will not have heard of RDF, and even if they are consuming JSON-LD from some RDF or Graph Database backend, they won\u2019t (and probably don\u2019t need to be) concerned with it. A key design goal in the JSON-LD specification is : Zero Edits, most of the time - \u201cJSON-LD ensures a smooth and simple transition from existing JSON-based systems. In many cases, zero edits to the JSON document and the addition of one line to the HTTP response should suffice\u201c However, when developers first come across JSON-LD it is rarely like this. The majority of published JSON-LD is littered with JSON-LD nuance. Developers do not like consuming many of the JSON-LD constructs, and when you hear the complaint \u201cit\u2019s not like normal JSON\u201d it is typically for good reason. For example, two of the most common complaints are : The \u201c@\u201d prefix on the JSON properties is not a valid ES5 or ES6 prefix for javascript variables, meaning when dereferencing JSON as a JS object, you cannot do things like const id = myObject.@id ; Instead you need to code const id = myObject['@id'] JSON-LD output from some SPARQL query or RDF transformation typically does not confer any indication if a property is an array or a singleton. The result of one request to some back-end API serving JSON-LD might give  : A future request may return an array for the employs property : The software developer comsuming this output then has to code around this when evaluating the value of the employs property, and check whether the property contains an array or a singleton. I imagine these are possibly the two most common JSON-LD complaints from devs. Even Google publish their schema.org JSON-LD with @id and @type field names on the Google Knowledge Graph API. This doesn't help, and yet it doesn\u2019t have to be like this. Instead of telling developers to \u201cget over it\u201d, as publishers of JSON-LD we can make JSON-LD look and behave like normal JSON using the context, meeting the design goal of the specification.  However, it is not always obvious how to make this happen, here are some simple examples on how to address the two common issues above. Use context aliasing to abstract the \u201c@\u201d symbol away from the data payload. By creating aliases in the context for those properties prefixed with @, they can be appear in the data payload fully JS compatible. The context does not just allow us to provide definitions of our domain-specific JSON properties, but we can also alias the JSON-LD standard fields. Try this in the JSON-LD playground : Use the context to define which fields are arrays, and output them as such : Forcing the set (or list) property to be marshalled as an array in JSON even when it is a singleton. See this in action in the JSON-LD playground : Using a remote context (that most web developers can safely ignore), we now we have JSON-LD that looks just the way your developers would prefer, and meeting the design goal of the JSON-LD specification : If you are building a web application or micro-service which is using RDF, then JSON-LD is clearly a no-brainer. However if you don\u2019t care about RDF why bother ? I argue that you should bother. If you are publishing an API you may not know how your consumers are using your data. They may well be using a graph database, or dealing with RDF or linked data and be very keen to consume your API in an RDF compatible format. Furthermore, publishing JSON-LD requires you to publish a context. You are thus constraining your JSON to a schema. This is a good thing. It communicates a model of your domain, provides a payload contract for your consumers (or a contract between your own micro-service APIs and applications), and a validation mechanism. Being inherently a schemaless message format, applications built with components that use JSON to serialize\/deserialize data moving between them have a tendency over time to create JSON property soup. As requirements evolve, and developers are left to their own devices to modify features and adapt the codebase, properties are introduced into JSON on an ad-hoc basis. JSON entropy increases. What may have started out as a simple, clean JSON model, tends towards one of mess and complexity.\nAdopting JSON-LD as your message format reduces this entropy escalation, as : a) it makes you as a developer think about the schema (structure) of the JSON from the perspective of your consumers, and the wider application as a whole.\nb) once we start publishing schemas for our data, we start to think like a data architect rather than as a developer.\nc) we are more likely to collaborate on the schema design, and at the very minimum, ask for a peer review of a proposed change. While this may seem like an overhead, it is a relatively small investment into your data architecture that pays dividends over the long term. The JSON-LD context is a schema for your data, not only defining the property datatypes but also the classes of json resources and, via referenced ontologies, the semantics and relationships between properties and classes in your data.\nThe json-schema project has attempted to fill this space for JSON, but has not really gained too much traction. While it is a little richer in schema-definition features than what a JSON-LD context provides, JSON-LD is an official W3C standard, and offers a lot more than json-schema can with respect to linked data and RDF compatibility. If you are using linked data or RDF, then you will no doubt already be familiar with JSON-LD, and hopefully using JSON-LD as a message format in your own applications. If you are publishing linked data then it is very likely you are publishing it as JSON-LD. If you are serving public APIs as regular JSON, then publishing JSON-LD instead makes a lot of sense.  As discussed, you should be able to transition with little or zero change to your data payload. All it takes to make the transition is to publish a context. For little additional overhead, you and your consumers benefit. It is essential however, that we, as publishers of JSON-LD make it as consumer friendly as possible (and this includes Google). If you would like to know more about Data Language or have a project you would like to discuss with us, then please get in touch Call us on: 01372 365803 Email us: info@datalanguage.co.uk Riverbridge House, Guildford Road, Leatherhead, KT22 9AD \nE: info@datalanguage.com \nT:\n01372 365803\n","time":1525795178,"title":"Publishing JSON-LD for Developers","type":"story","url":"https:\/\/datalanguage.com\/news\/publishing-json-ld-for-developers","label":3,"label_name":"dev"},{"by":"empressplay","descendants":0,"id":17021917,"kids":"None","score":1,"text":"Everyone knows the moon orbits the Earth and that the Earth orbits the Sun. But what about the path of the moon around the Sun? What does that look like? I will go ahead and state that this is a difficult thing to show. Why? Scale, that's why. Let me give some values for the sizes of these things, then I will make some sort of diagram. Here is one attempt at drawing these three objects. The objects all have the correct scale - but the Sun is in the wrong place (hopefully, this is obvious). The Earth and moon are the correct distance apart relative to their size. What about the Sun? In this diagram, the Earth and moon are about 11 cm apart (at least on my monitor). If you could see the whole Sun, it would be about 40 cm across. Where would the Sun be? You if you had a cut out of Sun, that piece of paper would have to be 43 meters off to the side. Yup. I said 43 meters. The sun is pretty far away. And this is the problem. How do you show the orbit of the Earth and moon around the Sun? You really can't, at least not to scale. Most textbooks end up making a plot where nothing is to scale. Here is something you might see. It sort of works, right? It shows that the Earth orbits the Sun and the moon orbits the Earth. But what would it look like to scale? I am not sure what the best way to show this will be. Let me first assume perfectly circular orbits for both the moon and the Earth. I'm not going to show the Sun - here is just a part of their path.  This just shows half a month. If I wanted to show a longer time period, the motion of the Earth and moon around the Sun would make it super-difficult to see the motion of the moon relative to the Earth. Maybe it will help if I plot the distance from the Sun for both the Earth and the moon. Here is that plot over about 1 month.  You might notice that the distance from the Earth to the Sun changes. I put in an initial velocity to give the Earth a circular orbit. However, I also included the gravitational force on the Earth from the moon. This causes a bit of a wobble (but it isn't really important for this discussion). The real question that I want to look at is: does the moon orbit the Earth more or the Sun more? Which is more important? Let me try another plot. Here is the radial component of the acceleration of the moon over one month. Remember, from my last moon post we can break the forces (and thus the acceleration) into two types. There is a radial component that changes the direction of the momentum and a parallel component that changes the magnitude of the momentum. So, this is just the magnitude of the radial component.  But what does this even mean? Well, this says that no matter where the moon is in relationship to the Earth it has a radial acceleration in the direction of the Sun. It does not accelerate away from the Sun. If it did, it would have a negative radial acceleration component. Why does this look so similar to the radial position plot? Think of this. When the moon is on the far side of the Earth (so farther away from the Sun than the Earth is), it has both the Earth and the Sun pulling it towards the Sun. With this greater force comes a greater acceleration AND it is further away from the Sun. So, the plots look similar but they are not the same. Let me show this with a diagram (not to scale).  Here I have shown the gravitational force on the moon from the Sun is greater in magnitude than the gravitational force from the Earth. Is this actually true? To calculate this, I need to use the following model for the gravitational force:  This shows the magnitude of the gravitational force on the moon due to the interaction with the Sun. G is the gravitational constant (6.67 x 10-11 N*m2\/kg2) and r is the distance between the Sun and the moon. Even though the moon moves around the Earth, this Sun-moon distance doesn't significantly change. If I use this gravity model, I can calculate the force per unit mass for an object in the location of the moon due to both the Sun and Earth. The Sun wins. But wait. How close would an object have to be for the gravitational force from the Earth to be greater? Here is a plot.  At an orbital distance of about 2.6 x 108 m, the force from the Earth and the Sun would be equal. This means that when an object was on the Sun side of it's obit, it would for a moment have a zero radial acceleration. For objects orbiting closer than this, the gravitational force from the Earth would be greater. This means that it would be accelerating towards the Earth and not the Sun at some parts of the orbit. Two Answers\n\u2014\u2014\u2014\u2013 There are really two questions here. Let me answer both of them. Does the moon orbit the Sun? I would say yes. The interaction between the Sun and the moon has a greater magnitude than that of the moon-Earth interaction. The moon moves around the Sun at the same time it moves around the Earth. Perhaps the best answer is to say the the moon interacts with both the Earth and the Sun at the same time. This is what we call \"physics\". I don't think you could say that the moon just orbits the Earth. How do you represent the path of the moon as it goes around the Sun? You don't? I don't know. How do you represent the scale of the solar system? Again, this is a tough problem. You really can't do it in a textbook, can you? If I had to make a recommendation, I would tell introductory astronomy textbooks to NOT draw that squiggly moon path diagram. I don't think that helps anyone understand anything important. Oh, one more note. You want to make awesome images of the Earth-moon and Sun? I use Vpython - it's awesome. You could also use GlowScript, but I keep going back to Vpython. CNMN Collection Use of this site constitutes acceptance of our user agreement (effective 3\/21\/12) and privacy policy (effective 3\/21\/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast.","time":1525795063,"title":"Does the Moon Orbit the Sun or the Earth? (2012)","type":"story","url":"https:\/\/www.wired.com\/2012\/12\/does-the-moon-orbit-the-sun-or-the-earth\/","label":7,"label_name":"random"},{"by":"GordonS","descendants":0,"id":17021909,"kids":"None","score":2,"text":"","time":1525795012,"title":"The Wisdom And\/or Madness of Crowds","type":"story","url":"http:\/\/ncase.me\/crowds\/","label":7,"label_name":"random"},{"by":"ranit","descendants":0,"id":17021902,"kids":"None","score":1,"text":"Access every Packt eBook and Video ever published! Opsie devsy. Make no mistake, this software engineering bundle is packed with information! Streamline your processes with ebooks like Automate it!, DevOps for Networking, Mastering Ansible, and Continuous Delivery with Docker and Jenkins. You can also get a library of videos including Mastering DevOps, Mastering Windows PowerShell 5 Administration, and Learning Kubernetes. Pay what you want. All together, these ebooks would cost over $1613. Here at Humble Bundle, you choose the price and increase your contribution to upgrade your bundle! This bundle has a minimum $1 purchase. Read them anywhere. The books in this bundle are available in PDF, ePUB, and MOBI formats, meaning you can read them anywhere at any time. Instructions and a list of recommended reading programs can be found here. The videos in this bundle come in HD MP4 format. Support charity. Choose where the money goes \u2013 between the publisher and the Innocent Lives Foundation via the PayPal Giving Fund. If you like what we do, you can leave us a Humble Tip too! The Humble community has contributed over $126 million to charity since 2010, making an amazing difference to causes all over the world.","time":1525794966,"title":"Humble bundle Devops books","type":"story","url":"https:\/\/www.humblebundle.com\/books\/devops-books","label":3,"label_name":"dev"},{"by":"philips","descendants":0,"id":17021882,"kids":"[17022648]","score":4,"text":"In the months since\u00a0CoreOS was acquired by Red Hat, we\u2019ve been building on our vision of helping companies achieve greater operational efficiency through automation. Today at Red Hat Summit we\u2019ve outlined our roadmap for how we plan to integrate the projects and technologies started at CoreOS with Red Hat\u2019s, bringing software automation expertise to customers and the community. Enterprise Kubernetes users can greatly benefit from the planned addition of many popular Tectonic features to Red Hat OpenShift Container Platform, the industry\u2019s most comprehensive\u00a0enterprise Kubernetes platform. Quay, the leading container registry, is now backed by Red Hat as Red Hat Quay. Container Linux will continue to provide a free, fast-moving, and automated container host, and is expected to provide the basis for new operating system projects and offerings from Red Hat. And open source projects including etcd, Ignition, dex, Clair, Operators and more will continue to thrive as part of Red Hat\u2019s commitment to driving community innovation around containers and Kubernetes. Essentially,\u00a0CoreOS technologies are being woven\u00a0into the very fabric of Red Hat\u2019s container-native products and projects and we are excited to continue delivering on the vision to make automated operations a reality. Since Red Hat\u2019s acquisition of CoreOS was announced, we received questions on the fate of Container Linux. CoreOS\u2019s first project, and initially its namesake, pioneered the lightweight, \u201cover-the-air\u201d automatically updated container native operating system that fast rose in popularity running the world\u2019s containers. With the acquisition, Container Linux will be reborn as Red Hat CoreOS, a new entry into the Red Hat ecosystem. Red Hat CoreOS will be based on Fedora and Red Hat Enterprise Linux sources and is expected to ultimately supersede Atomic Host as Red Hat\u2019s immutable, container-centric operating system. Red Hat CoreOS will provide the foundation for Red Hat OpenShift Container Platform, Red Hat OpenShift Online, and Red Hat OpenShift Dedicated. Red Hat OpenShift Container Platform will also, of course, continue to support Red Hat Enterprise Linux for those who prefer its lifecycle and packaging as the foundation for their Kubernetes deployments. Current Container Linux users can rest easy that Red Hat plans continue investing in the operating system and community. The project is an important base for container-based environments by delivering automated updates with strong security capabilities, and as a part of our commitment and vision we plan to support Container Linux as you know it today for the community and Tectonic users alike. CoreOS Tectonic was created with a vision of a fully automated container platform that would relieve many of the burdens of day-to-day IT operations. This vision will now help craft the next generation of Red Hat OpenShift Container Platform, providing an advanced container experience for operators and developers alike. With automated operations coming to OpenShift, IT teams will be able to use the automated upgrades of Tectonic paired with the reliability, support, and extensive application development capabilities of Red Hat OpenShift Container Platform. This makes managing large Kubernetes deployments easier without sacrificing other enterprise needs, including platform stability or continued support for existing IT assets. We believe this future integrated platform will help to truly change the way IT teams deliver applications by providing speed to market through consistent deployment methods and automated operations throughout the stack. In the meantime, current Tectonic customers will continue to receive support and updates for the platform. They can also have confidence that they will be able to transition to Red Hat OpenShift Container Platform in the future with little to no disruption, as almost all Tectonic features will be retained in Red Hat OpenShift Container Platform. We are also focusing on automating the application layer of the stack. At KubeCon we introduced and open sourced the Operator Framework. Today we are showing how we plan to put Operators into practice. Red Hat is working on a future enhancement that will enable software partners to\u00a0test and validate their Operators for Red Hat OpenShift Container Platform. More than 60 software partners have committed to supporting the Kubernetes Operator Framework initiative introduced by Red Hat, including Couchbase, Dynatrace, Black Duck Software and Crunchy Data, among others. Our aim is to make it easier for ISVs to bring cloud services, including messaging, big data, analytics, and more, to the hybrid cloud and to address a broader set of enterprise deployment models while avoiding cloud lock-in. Eventually, Red Hat plans to extend the Red Hat Container Certification with support for Operators as tested and validated Kubernetes applications on Red Hat OpenShift. With the Operator Framework in place, software partners have a more consistent, common experience for delivering services on Red Hat OpenShift, enabling ISVs to bring their offerings to market more quickly on any cloud infrastructure where Red Hat OpenShift runs. Quay, the container registry, will also continue to live on in the Red Hat container portfolio. While OpenShift provides an integrated container registry, customers who require more comprehensive enterprise grade registry capabilities now have the option to consume Quay Enterprise and Quay.io from Red Hat. Quay includes automated geographic replication, integrated\u00a0security scanning with Clair, image time machine for viewing history, rollbacks and automated pruning, and more. Red Hat Quay is available both as an enterprise software solution and as a hosted service at Red Hat Quay.io, with plans for future enhancements and continued integration with Red Hat OpenShift in future releases. With CoreOS now part of the Red Hat family, we\u2019ve been busy working together to bring more capabilities to enterprise customers, and more muscle to community open source projects. We\u2019re excited to work alongside you with our Red Hat fedoras on to help automate your infrastructure, all the way from the stack to the application layer. Join us at Red Hat Summit in San Francisco or view the\u00a0Red Hat Summit livestream\u00a0to learn more. Red Hat is also hosting a press conference live from Red Hat Summit at 11 a.m. PT today to talk about this integration and other news from the event. The press conference is open to all \u2013 join or listen to a replay\u00a0here.","time":1525794811,"title":"Bringing CoreOS Tech to Red Hat OpenShift to Deliver an Automated Kubernetes","type":"story","url":"https:\/\/coreos.com\/blog\/coreos-tech-to-combine-with-red-hat-openshift","label":3,"label_name":"dev"},{"by":"ohjeez","descendants":0,"id":17021873,"kids":"None","score":3,"text":"\nJump to navigation\n Flickr, CC BY 2.0, Modified by Jen Wike Huger Join the 85,000 open source advocates who receive our giveaway alerts and article roundups.  I recently asked our writer community\u00a0to share with us what they're reading.\u00a0These folks\u00a0come from all different walks of life and roles in tech. What they have in common is that they are\u00a0living\u00a0and breathing Linux and open source every day. Drink in this fantastic list. Many\u00a0of them are free and available to download. You may see books you've been meaning to get around to, books that are completely new to you, and some\u00a0that feel like old friends. We'd love to hear what you think of this list.\u00a0Share with us in the comments below or on Twitter\u00a0with #Linuxbooks #opensourcebooks. Plus, a bonus fiction read. 23 Years of FreeDOS\u00a0by Jim Hall Last year, the FreeDOS Project turned 23 years old. While there's nothing special about 23 years,\u00a0the project decided to celebrate that milestone by sharing stories about how different people use or contribute to FreeDOS. The free, CC BY\u00a0eBook is a collection of essays that describe the history of FreeDOS since 1994, and how people use FreeDOS today. (Recommendation and review by Jim Hall) Eloquent JavaScript\u00a0by Marijn Haverbeke This book teaches you how to write beautifully crafted programs using one of the most ubiquitous programming languages: Javascript. Learn the basics and advanced concepts of the language, and how to write programs that run in the browser or Node.js environment. The book also includes five fun projects so you can dive into actual programming while making a platform game or even writing your own programming language. (Recommendation and review by\u00a0Rahul Thakoor) Forge Your Future with Open Source\u00a0by VM (Vicky) Brasseur If you're looking to contribute to open source, but you don't know how to start, this is the book for you. It covers how to find a project to join and how to make your first contributions. (Recommendation and review by Ben Cotton) Git for Teams\u00a0by Emma Jane Hogbin Westby Git is a widely-used version control system for individuals and teams alike, but its power means it can be complex. This book\u00a0provides guidance on how to effectively use git in a team environment. For more,\u00a0read our in-depth review.\u00a0(Recommendation and review by Ben Cotton) Getting to Yes\u00a0by Fisher, Ury, and Patton The Harvard Negotiation Project, formed in the 1970s, was an academic\u00a0effort involving economists, psychologists, sociologists, and political scientists to create a framework for negotiations which allows better outcomes for all involved. Their framework and techniques have been used in a diverse set of circumstances, including the Camp David Accords between Egypt and Israel in 1978. Principled Negotiation involves understanding the real interests of the participants in a negotiation and using this knowledge to generate options acceptable to all. The same techniques can be used to resolve interpersonal issues, negotiations over cars and houses, discussions with insurance companies, and so on. What does this have to do with open source software development? Everything in open source is a negotiation, in some sense. Submitting a\u00a0bug report is outlining a position\u2014that something does not work correctly\u2014and requesting that someone reprioritize their work to fix it. A heated discussion on a mailing list over the right way to do something or a comment on a feature request is a negotiation, often with imperfect knowledge, about the scope and goals of the project. Reframing these conversations as explorations, trying to understand why the other person is asking for something, and being transparent about the reasons why you believe another viewpoint to apply, can dramatically change your relationships and effectiveness working in an open source project.\u00a0(Recommendation and review by Dave Neary) Just for Fun: The Story of an Accidental Revolutionary\u00a0by Linus Torvalds et al.\u00a0 Linux is an amazing and powerful operating system that spawned a movement to transparency and openness. And, the open source ethos that drives it flies in the face of traditional models of business and capital appreciation. In this book,\u00a0learn about the genius of Linus the man and Linux the operating system. Get insight into the experiences that shaped Linus's life and fueled his transformation from a nerdy young man who enjoyed toying with his grandfather's clock to the master programmer of the world's predominant operating system. (Recommendation and review by Don Watkins) Linux in a Month of Lunches by Steven Ovadia This book is designed to teach non-technical users how to use desktop Linux in about an hour a day. The book covers everything from choosing a desktop environment to installing software, to using Git. At the end of the month, readers can use Linux fulltime, replacing their other operating systems.\u00a0(Recommendation and review by Steven Ovadia) Linux in Action by David Clinton This book introduces serious Linux administration tools for\u00a0anyone interested in getting more out of their tech, including IT professionals, developers, DevOps specialists, and more. Rather than teaching skills in isolation, the book is organized around practical projects like automating off-site data backups, securing a web server, and creating a VPN to safely connect an organization's resources.\u00a0Read more by this author. (Recommendation and review by David Clinton) Make: Linux for Makers\u00a0by Aaron Newcomb This book is a must-read for anyone wanting to create and innovate with the Raspberry Pi. This book will have you up and operating your Raspberry Pi while at the same time understanding the nuances of it Raspbian Linux operating system. This is a masterful basic text that will help any maker unlock the potential of the Raspberry Pi. It\u2019s concise and well written with a lot of fantastic illustrations and practical examples.\u00a0(Recommendation by Jason Hibbets | Review by Don Watkins) Managing Humans: Biting and Humorous Tales of a Software Engineering Manager by Michael Lopp Michael Lopp is better known by the nom de plume Rands, author of the popular blog Rands in Repose.\u00a0This book is an edited, curated collection of blog posts, all related to the management of software development teams. What I love about the book and the blog, is that Rands starts from the fundamental principle that the most complex part of software development is human interactions. The book covers a range of topics about reading a group, understanding the personalities that make it up, and figuring out how to get the best out of everyone. These things are universal, and as an open source community manager, I come across them all the time. How do you know if someone might be burning out? How do you run a good meeting? How do you evolve the culture of a project and team as it grows? How much process is the right amount? Regardless of the activity,\u00a0questions like these arise all the time, and Rands's irreverent, humorous take is educational and entertaining. (Recommendation and review by Dave Neary) Open Sources: Voices from the Open Source Revolution\u00a0(O'Reilly, 1999) This book is a must-read for all open source enthusiasts. Linus Torvalds, Eric S. Raymond,\u00a0Richard Stallman,\u00a0Michael Tiemann,\u00a0Tim O'Reilly, and other important figures in the open source movement share their thoughts on the forward momentum of open source software.\u00a0(Recommendation\u00a0by Jim Hall\u00a0| Review by Jen Wike Huger) Producing Open Source Software: How to Run a Successful Free Software Project\u00a0by Karl Fogel This book is for anyone who wants to build an open source community, is already building one, or wants to better understand trends in successful open source project community development. Karl Fogel analyzes and studies traits and characteristics of successful open source projects and how they have developed a community around the project. The book offers helpful advice to community managers (or want-to-be community managers) on how to navigate community development around a project. This is a rare book that takes a deeper look into open source community development and offers plenty of ingredients for success, but you have to take it and create the recipe for your project or community. (Recommendation and review by Justin Flory) Programming with Robots by\u00a0Albert W. Schueller This book introduces the basics of programming using the Lego Mindstorms NXT. Instead of writing abstract programs, learn how to program devices that can sense and interface with the physical world. Learn how software and hardware interact with each other while experimenting with sensors, motors or making music using code.\u00a0(Recommendation and review by\u00a0Rahul Thakoor) The AWK programming language by\u00a0Alfred V. Aho, Brian W. Kernighan, and Peter J.\u00a0Weinberger This book, written by the creators of awk, follows a pattern similar to other books about *nix tools written by the original Bell Labs Unix team and published in the 1970s-1990s, explaining the rationale and intended use of awk in clear and compact prose, liberally sprinkled with examples that start simply and are further elaborated by the need to deal with more fully-detailed problems and edge cases.\u00a0When published, the typical reader of this book would have been someone who had files of textual or numeric data that needed to be processed and transformed, and who wanted to be able to easily create lookup tables, apply regular expressions, react to structure changes within the input, apply mathematical transformations to numbers and easily format the output. While that characterization still applies, today the book can also provide a window back into the time when the only user interface available was a terminal, when \"modularity\" created the ability to string together numerous single-purpose utility programs in shell scripts to create data transformation pipelines that crunched the data and produced the reports that everyone expected of computers.\u00a0Today, awk should be a part of the operations toolbox, providing a fine ability to further process configuration and log files, and this book still provides a great introduction to that process.\u00a0(Recommendation\u00a0by Jim Hall\u00a0| Review by Chris Hermansen) Think Python: Think Like a Computer Scientist by\u00a0Allen Downey This book about Python is part of a series\u00a0that covers other languages as well, like Java,\u00a0Perl, etc. It moves past simple language syntax downloads and approaches the topic through the lens of how a problem solver would build a solution. It's both a great introductory guide to programming through a layering of concepts, but it can serve the dabbler who is looking to develop skills in an area such as classes or inheritance with chapters that have examples and exercises to then apply the skills taught.\u00a0(Recommendation and review by Steve Morris) Understanding Open Source and Free Software Licensing\u00a0(O'Reilly, 2004) \"This book bridges the gap between the open source vision and the practical implications of its legal underpinnings. If open source and free software licenses interest you, this book will help you understand them. If you're an open source\/free software developer, this book is an absolute necessity.\" (Recommendation\u00a0by Jim Hall\u00a0| review\u00a0from Amazon) Unix Text Processing\u00a0by Dale Dougherty and Tim O'Reilly This book was written in 1987 as an introduction to Unix systems\u00a0and how writers could use Unix tools to do work. It's still a useful resource for beginners to learn the basics of the Unix shell, the vi editor, awk and shell scripts, and the nroff and troff typesetting system. The original edition is out of print, but O'Reilly has made the book available for free via their website.\u00a0(Recommendation and review by Jim Hall) Station Eleven\u00a0by Emily St. John Mandel This story is set in a near future, twenty years after the earth's population has been decimated by a mysterious and deadly flu. We follow Kirsten Raymonde, a young woman who is traveling near the Great Lakes with a nomadic theatre group because \"Survival is insufficient,\" as she makes her way through the post-apocalyptic world. It's a wonderful story, well worth reading. What struck me about the book is how tenuous our relationship with technology actually is. In the Douglas Adams book \"Mostly Harmless\", there is a great line: \"Left to his own devices he couldn't build a toaster. He could just about make a sandwich and that was it.\" This is the world of Kristin Raymonde. Everyone has been left to their own devices: There is no electricity because no one can work the power grid. No cars, no oil refineries. There is a fascinating passage where one inventor has rigged up a generator with a bicycle and is trying to turn on a laptop, trying to see if there is still an internet. We discover the Museum of Civilization, stocked with objects which have no use, which has been left over from the old world: passports, mobile phones, credit cards, stilettoes. All of the world's technology becomes useless.\u00a0(Recommendation and review by Dave Neary) Thanks, Jen, this is a great list! I meant to toss \/Autonomous: A Novel\/ by Annalee Newitz your way when you were collecting.  It's a novel of the future blending open source, proprietary interests, hacking and piracy in the context of pharmaceuticals.  Find us:\u00a0\n For more discussion on open source and the role of the CIO in the enterprise, join us at The EnterprisersProject.com. The opinions expressed on this website are those of each author, not of the author's employer or of Red Hat. Opensource.com aspires to publish all content under a Creative Commons license but may not be able to do so in all cases. You are responsible for ensuring that you have the necessary permission to reuse any work on this site. Red Hat and the Shadowman logo are trademarks of Red Hat, Inc., registered in the United States and other countries. ","time":1525794769,"title":"A reading list for Linux and open source fans","type":"story","url":"https:\/\/opensource.com\/article\/18\/5\/list-books-Linux-open-source","label":3,"label_name":"dev"},{"by":"Tehnix","descendants":0,"id":17021866,"kids":"None","score":3,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Look ma, no Electron!   This is a web-based Kanban board application, built with Elm and Rust. The only different from this application and hundred thousands of web-based application out there is: We don't need Electron!. Instead, it use native WebView (WebKit for Linux\/macOS, and MSHTML on Windows), more details see here. The whole source code in this repository is just a desktop client, which you can actually use for any web-based application. Note: I maintain my own version of zserge\/webview and Boscop\/web-view, because I want to add some customized titlebar on macOS, and my code is ugly enough to create a PR on these repos. Skip this if you're using an online hosted application from an URL, or building your own app. Clone the Kanelm application source code from here https:\/\/github.com\/huytd\/kanelm Follow the instruction in that repo to config your jsonbin.io config, then install the dependencies and build it: What you will get is a dist folder, and you only need the dist.js file, copy it to www folder of this repo. You gonna need cargo bundle. Install it, then run: Now you got it. Well, the Elm application will be compiled into a single `dist.js** file, the content of this JavaScript file will be inlined into our Rust source code: src\/main.rs The Rust application will then create a new window, contains a webview, load this HTML content into that webview, and that's it. Yeh, but sometimes, all you need is just a webview to display your web application on a desktop. You don't need file system access or automatic update, blah blah, it would be a huge waste to ship your app with >100MB of Chromium and V8 in it. In fact, this application only uses 0-3% CPU and the bundle size is >800KB on macOS. ","time":1525794736,"title":"Kanban board built with Rust and Elm (using WebView instead of Electron)","type":"story","url":"https:\/\/github.com\/huytd\/kanban-app","label":4,"label_name":"github"},{"by":"dbader","descendants":0,"id":17021864,"kids":"None","score":1,"text":"\nby Malay Agarwal\n May 08, 2018\n\n\nfundamentals\npython\n Table of Contents If you\u2019ve used the + or * operator on a str object in Python, you must have noticed its different behavior when compared to int or float objects: You might have wondered how the same built-in operator or function shows different behavior for objects of different classes. This is called operator overloading or function overloading respectively. This article will help you understand this mechanism, so that you can do the same in your own Python classes and make your objects more Pythonic. You\u2019ll learn the following: Free Bonus: Click here to get access to a free Python OOP Cheat Sheet that points you to the best tutorials, videos, and books to learn more about Object-Oriented Programming with Python. As a bonus, you\u2019ll also see an example class, objects of which will be compatible with many of these operators and functions. Let\u2019s get started! Say you have a class representing an online order having a cart ( a list) and a customer (a str or instance of another class which represents a customer). Note: If you need a refresher on OOP in Python, check out this tutorial on Real Python: Object-Oriented Programming (OOP) in Python 3 In such a case, it is quite natural to want to obtain the length of the cart list. Someone new to Python might decide to implement a method called get_cart_len() in their class to do this. But you can configure the built-in len() in such a way that it returns the length of the cart list when given our object. In another case, we might want to append something to the cart. Again, someone new to Python would think of implementing a method called append_to_cart() that takes an item and appends it to the cart list. But you can configure the + operator in such a way that it appends a new item to the cart. Python does all this using special methods. These special methods have a naming convention, where the name starts with two underscores, followed by an identifier and ends with another pair of underscores. Essentially, each built-in function or operator has a special method corresponding to it. For example, there\u2019s __len__(), corresponding to len(), and __add__(), corresponding to the + operator. By default, most of the built-ins and operators will not work with objects of your classes. You must add the corresponding special methods in your class definition to make your object compatible with built-ins and operators. When you do this, the behavior of the function or operator associated with it changes according to that defined in the method. This is exactly what the Data Model (Section 3 of the Python documentation) helps you accomplish. It lists all the special methods available and provides you with the means of overloading built-in functions and operators so that you can use them on your own objects. Let\u2019s see what this means. Fun fact: Due to the naming convention used in these methods, they are also called dunder methods, short for \u201cdunder something dunder,\u201d which is the shorter version of double underscore something double underscore. Every class in Python defines its own behavior for built-in functions and methods. When you pass an instance of some class to a built-in function or use an operator on the instance, it is actually equivalent to calling a special method with relevant arguments. If there is a built-in function, func(), and the corresponding special method for the function is __func__(), Python interprets a call to the function as obj.__func__(), where obj is the object. In the case of operators, if you have an operator opr and the corresponding special method for it is __opr__(), Python interprets something like obj1 <opr> obj2 as obj1.__opr__(obj2). So, when you\u2019re calling len() on an object, Python handles the call as obj.__len__(). When you use the [] operator on an iterable to obtain the value at an index, Python handles it as itr.__getitem__(index), where itr is the iterable object and index is the index you want to obtain. Therefore, when you define these special methods in your own class, you override the behavior of the function or operator associated with them because, behind the scenes, Python is calling your method. Let\u2019s get a better understanding of this: As you can see, when you use the function or its corresponding special method, you get the same result. In fact, when you obtain the list of attributes and methods of a str object using dir(), you\u2019ll see these special methods in the list in addition to the usual methods available on str objects: If the behavior of a built-in function or operator is not defined in the class by the special method, then you will get a TypeError. So, how can you use special methods in your classes? Many of the special methods defined in the Data Model can be used to change the behavior of functions such as len, abs, hash, divmod, and so on. To do this, you only need to define the corresponding special method in your class. Let\u2019s look at a few examples: To change the behavior of len(), you need to define the __len__() special method in your class. Whenever you pass an object of your class to len(), your custom definition of __len__() will be used to obtain the result. Let\u2019s implement len() for the order class we talked about in the beginning: As you can see, you can now use len() to directly obtain the length of the cart. Moreover, it makes more intuitive sense to say \u201clength of order\u201d rather than calling something like order.get_cart_len(). Your call is both Pythonic and more intuitive. When you don\u2019t have the __len__() method defined but still call len() on your object, you get a TypeError: But, when overloading len(), you should keep in mind that Python requires the function to return an integer. If your method were to return anything other than an integer, you would get a TypeError. This, most probably, is to keep it consistent with the fact that len() is generally used to obtain the length of a sequence, which can only be an integer: You can dictate the behavior of the abs() built-in for instances of your class by defining the __abs__() special method in the class. There are no restrictions on the return value of abs(), and you get a TypeError when the special method is absent in your class definition. In a class representing a vector in a two-dimensional space, abs() can be used to get the length of the vector. Let\u2019s see it in action: It makes more intuitive sense to say \u201cabsolute value of vector\u201d rather than calling something like vector.get_mag(). The str() built-in is used to cast an instance of a class to a str object, or more appropriately, to obtain a user-friendly string representation of the object which can be read by a normal user rather than the programmer. You can define the string format your object should be displayed in when passed to str() by defining the __str__() method in your class. Moreover, __str__() is the method that is used by Python when you call print() on your object. Let\u2019s implement this in the Vector class to format Vector objects as xi+yj. A negative y-component will be handled using the format mini-language: It is necessary that __str__() returns a str object, and we get a TypeError if the return type is non-string. The repr() built-in is used to obtain the parsable string representation of an object. If an object is parsable, that means that Python should be able to recreate the object from the representation when repr is used in conjunction with functions like eval(). To define the behavior of repr(), you can use the __repr__() special method. This is also the method Python uses to display the object in a REPL session. If the __repr__() method is not defined, you will get something like <__main__.Vector object at 0x...> trying to look at the object in the REPL session. Let\u2019s see it in action in the Vector class: Note: In cases where the __str__() method is not defined, Python uses the __repr__() method to print the object, as well as to represent the object when str() is called on it. If both the methods are missing, it defaults to <__main__.Vector ...>. But __repr__() is the only method that is used to display the object in an interactive session. Absence of it in the class yields <__main__.Vector ...>. Also, while this distinction between __str__() and __repr__() is the recommended behavior, many of the popular libraries ignore this distinction and use the two methods interchangeably. Here\u2019s a recommended article on __repr__() and __str__() by our very own Dan Bader: Python String Conversion 101: Why Every Class Needs a \u201crepr\u201d. The bool() built-in can be used to obtain the truth value of an object. To define its behavior, you can use the __bool__() (__nonzero__() in Python 2.x) special method. The behavior defined here will determine the truth value of an instance in all contexts that require obtaining a truth value such as in if statements. As an example, for the Order class that was defined above, an instance can be considered to be truthy if the length of the cart list is non-zero. This can be used to check whether an order should be processed or not: Note: When the __bool__() special method is not implemented in a class, the value returned by __len__() is used as the truth value, where a non-zero value indicates True and a zero value indicates False. In case both the methods are not implemented, all instances of the class are considered to be True. There are many more special methods that overload built-in functions. You can find them in the documentation. Having discussed some of them, let\u2019s move to operators. Changing the behavior of operators is just as simple as changing the behavior of functions. You define their corresponding special methods in your class, and the operators work according to the behavior defined in these methods. These are different from the above special methods in the sense that they need to accept another argument in the definition other than self, generally referred to by the name other. Let\u2019s look at a few examples. The special method corresponding to the + operator is the __add__() method. Adding a custom definition of __add__() changes the behavior of the operator. It is recommended that __add__() returns a new instance of the class instead of modifying the calling instance itself. You\u2019ll see this behavior quite commonly in Python: You can see above that using the + operator on a str object actually returns a new str instance, keeping the value of the calling instance (a) unmodified. To change it, we need to explicitly assign the new instance to a. Let\u2019s implement the ability to append new items to our cart in the Order class using the operator. We\u2019ll follow the recommended practice and make the operator return a new Order instance that has our required changes instead of making the changes directly to our instance: Similarly, you have the __sub__(), __mul__(), and other special methods which define the behavior of -, *, and so on. These methods should return a new instance of the class as well. The += operator stands as a shortcut to the expression obj1 = obj1 + obj2. The special method corresponding to it is __iadd__(). The __iadd__() method should make changes directly to the self argument and return the result, which may or may not be self. This behavior is quite different from __add__() since the latter creates a new object and returns that, as you saw above. Roughly, any += use on two objects is equivalent to this: Here, result is the value returned by __iadd__(). The second assignment is taken care of automatically by Python, meaning that you do not need to explicitly assign obj1 to the result as in the case of obj1 = obj1 + obj2. Let\u2019s make this possible for the Order class so that new items can be appended to the cart using +=: As can be seen, any change is made directly to self and it is then returned. What happens when you return some random value, like a string or an integer? Even though the relevant item was appended to the cart, the value of order changed to what was returned by __iadd__(). Python implicitly handled the assignment for you. This can lead to surprising behavior if you forget to return something in your implementation: Since all Python functions (or methods) return None implicitly, order is reassigned to None and the REPL session doesn\u2019t show any output when order is inspected. Looking at the type of order, you see that it is now NoneType. Therefore, always make sure that you\u2019re returning something in your implementation of __iadd__() and that it is the result of the operation and not anything else. Similar to __iadd__(), you have __isub__(), __imul__(), __idiv__() and other special methods which define the behavior of -=, *=, \/=, and others alike. Note: When __iadd__() or its friends are missing from your class definition but you still use their operators on your objects, Python uses __add__() and its friends to get the result of the operation and assigns that to the calling instance. Generally speaking, it is safe to not implement __iadd__() and its friends in your classes as long as __add__() and its friends work properly (return something which is the result of the operation). The Python documentation has a good explanation of these methods. Also, take a look at this example which shows the caveats involved with += and the others when working with immutable types. The [] operator is called the indexing operator and is used in various contexts in Python such as getting the value at an index in sequences, getting the value associated with a key in dictionaries, or obtaining a part of a sequence through slicing. You can change its behavior using the __getitem__() special method. Let\u2019s configure our Order class so that we can directly use the object and obtain an item from the cart: You\u2019ll notice that above, the name of the argument to __getitem__() is not index but key. This is because the argument can be of mainly three forms: an integer value, in which case it is either an index or a dictionary key, a string value, in which case it is a dictionary key, and a slice object, in which case it will slice the sequence used by the class. While there are other possibilities, these are the ones most commonly encountered. Since our internal data structure is a list, we can use the [] operator to slice the list, as in this case, the key argument will be a slice object. This is one of the biggest advantages of having a __getitem__() definition in your class. As long as you\u2019re using data structures that support slicing (lists, tuples, strings, and so on), you can configure your objects to directly slice the structure: Note: There is a similar __setitem__() special method that is used to define the behavior of obj[x] = y. This method takes two arguments in addition to self, generally called key and value, and can be used to change the value at key to value. While defining the __add__(), __sub__(), __mul__(), and similar special methods allows you to use the operators when your class instance is the left-hand side operand, the operator will not work if the class instance is the right-hand side operand: If your class represents a mathematical entity like a vector, a coordinate, or a complex number, applying the operators should work in both the cases since it is a valid mathematical operation. Moreover, if the operators work only when the instance is the left operand, we are violating the fundamental principle of commutativity in many cases. Therefore, to help you make your classes mathematically correct, Python provides you with reverse special methods such as __radd__(), __rsub__(), __rmul__(), and so on. These handle calls such as x + obj, x - obj, and x * obj, where x is not an instance of the concerned class. Just like __add__() and the others, these reverse special methods should return a new instance of class with the changes of the operation rather than modifying the calling instance itself. Let\u2019s configure __radd__() in the Order class in such a way that it will append something at the front of the cart. This can be used in cases where the cart is organized in terms of the priority of the orders: To drive all these points home, it\u2019s better to look at an example class which implements these operators together. Let\u2019s reinvent the wheel and implement our own class to represent complex numbers, CustomComplex. Objects of our class will support a variety of built-in functions and operators, making them behave very similar to the built-in complex numbers class: The constructor handles only one kind of call, CustomComplex(a, b). It takes positional arguments, representing the real and imaginary parts of the complex number. Let\u2019s define two methods inside the class, conjugate() and argz(), which will give us the complex conjugate and the argument of a complex number respectively: Note: __class__ is not a special method but a class attribute which is present by default. It has a reference to the class. By using it here, we are obtaining that and then calling the constructor in the usual manner. In other words, this is equivalent to CustomComplex(real, imag). This is done here to avoid refactoring the code if the name of the class changes someday. Next, we configure abs() to return the modulus of a complex number: We will follow the recommended distinction between __repr__() and __str__() and use the first for the parsable string representation and the second for a \u201cpretty\u201d representation. The __repr__() method will simply return CustomComplex(a, b) in a string so that we can call eval() to recreate the object, while the __str__() method will return the complex number in brackets, as (a+bj): Mathematically, it is possible to add any two complex numbers or add a real number to a complex number. Let\u2019s configure the + operator in such a way that it works for both cases. The method will check the type of the right-hand side operator. In case it is an int or a float, it will increment only the real part (since any real number, a, is equivalent to a+0j), while in the case of another complex number, it will change both the parts: Similarly, we define the behavior for - and *: Since both addition and multiplication are commutative, we can define their reverse operators by calling __add__() and __mul__() in __radd__() and __rmul__() respectively. On the other hand, the behavior of __rsub__() needs to be defined since subtraction is not commutative: Note: You might have noticed that we didn\u2019t add a construct to handle a CustomComplex instance here. This is because, in such a case, both the operands are instances of our class, and __rsub__() won\u2019t be responsible for handling the operation. Instead, __sub__() will be called. This is a subtle but important detail. Now, we take care of the two operators, == and !=. The special methods used for them are __eq__() and __ne__(), respectively. Two complex numbers are said to be equal if their corresponding real and imaginary parts are both equal. They are said to be unequal when either one of these are unequal: Note: The Floating-Point Guide is an article that talks about comparing floats and floating-point precision. It highlights the caveats involved in comparing floats directly, which is something we\u2019re doing here. It is also possible to raise a complex number to any power using a simple formula. We configure the behavior for both the built-in pow() and the ** operator using the __pow__() special method: Note: Take a close look at the definition of the method. We are calling abs() to obtain the modulus of the complex number. So, once you\u2019ve defined the special method for a particular function or operator in your class, it can be used in other methods of the same class. Let\u2019s create two instances of this class, one having a positive imaginary part and one having a negative imaginary part: String representations: Recreating the object using eval() with repr(): Addition, subtraction, and multiplication: Equality and inequality checks: Finally, raising a complex number to some power: As you can see, objects of our custom class behave and look like those of a built-in class and are very Pythonic. The full example code for this class is embedded below. Solution: \"A Complete Example\" Show\/Hide In this tutorial, you learned about the Python Data Model and how the Data Model can be used to build Pythonic classes. You learned about changing the behavior of built-in functions such as len(), abs(), str(), bool(), and so on. You also learned about changing the behavior of built-in operators like +, -, *, **, and so forth. Free Bonus: Click here to get access to a free Python OOP Cheat Sheet that points you to the best tutorials, videos, and books to learn more about Object-Oriented Programming with Python. After reading this, you can confidently create classes that make use of the best idiomatic features of Python and make your objects Pythonic! For more information on the Data Model, and function and operator overloading, take a look at these resources: \ud83d\udc0d Python Tricks \ud83d\udc8c Get a short & sweet Python Trick delivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team. About Malay Agarwal A tech geek with a philosophical mind and a hand that can wield a pen. Each tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are: David Dan Geir Arne Joanna What Do You Think? Real Python Comment Policy: The most useful comments are those written with the goal of learning from or helping out other readers\u2014after reading the whole article and all the earlier comments. Complaints and insults generally won\u2019t make the cut here. Boost Your Python Skills Master Python 3 and write more Pythonic code with our in-depth books and video courses:  Get Python Books & Courses \u00bb Keep Reading \u2014 FREE Email Series \u2014 \ud83d\udc0d Python Tricks \ud83d\udc8c  \ud83d\udd12 No spam. Unsubscribe any time. All Tutorial Topics Table of Contents Almost there! Complete this form and click the button below to gain instant access: Object-Oriented Programming in Python: The 7 Best Resources (A Free PDF Cheat Sheet) \u00a9 2012\u20132018 Real Python \u22c5 Newsletter \u22c5 Twitter \u22c5 Facebook \u22c5 InstagramPython Tutorials \u22c5 Search \u22c5 Privacy Policy \u22c5 Contact\u2764\ufe0f Happy Pythoning!","time":1525794728,"title":"Operator and Function Overloading in Custom Python Classes","type":"story","url":"https:\/\/realpython.com\/operator-function-overloading\/","label":3,"label_name":"dev"},{"by":"georgecmu","descendants":0,"id":17021862,"kids":"None","score":1,"text":"","time":1525794724,"title":"How a mispronounced bird could crush a university\u2019s reputation","type":"story","url":"https:\/\/www.inkstonenews.com\/society\/peking-universitys-reputation-hangs-thread-after-sexual-misconduct-and-mispronounced-idioms\/article\/2145022","label":7,"label_name":"random"},{"by":"vysakhpillai","descendants":0,"id":17021813,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Real-time webcam-driven HTML5 QR code scanner. Try the live demo. Note: Chrome requires HTTPS when using the WebRTC API. Any pages using this library should be served over HTTPS. npm install --save instascan Pending. Drop a note if you need Bower support. Copy instascan.min.js from the releases page and load with: Create a new scanner with options: Instascan works on non-iOS platforms in any browser that supports the WebRTC\/getUserMedia API, which currently includes Chome, Firefox, Opera, and Edge. IE and Safari are not supported. Instascan does not work on iOS since Apple does not yet support WebRTC in WebKit and forces other browser vendors (Chrome, Firefox, Opera) to use their implementation of WebKit. Apple is actively working on WebRTC support in WebKit. Many factors affect how quickly and reliably Instascan can detect QR codes. If you control creation of the QR code, consider the following: When scanning, consider the following:  Powered by the Emscripten JavaScript build of the C++ port of the ZXing Java library. Copyright \u00a9 2016 Chris Schmich\nMIT License. See LICENSE for details.","time":1525794443,"title":"Real-time webcam-driven HTML5 QR code scanner","type":"story","url":"https:\/\/github.com\/schmich\/instascan","label":4,"label_name":"github"},{"by":"nmanji","descendants":0,"id":17021803,"kids":"None","score":7,"text":"Have you joined our Telegram chat yet? Why wait? You\u2019re missing out on a vibrant community talking about cool blockchain topics like the ones you\u2019ll learn about in this article! Many readers by now have seen our coding tutorials on how to code your own blockchain, how to code your own proof of work, and how to set up a P2P network. After reading these tutorials, there you are, confident in your blockchain mastery and eager to tell everyone all about your newfound knowledge at your next meetup! But then you walk up to the water cooler and hear a conversation like this: \u201cPayment channels are the next big thing!\u201d \u201cI\u2019m pretty skeptical of the Bitcoin Lightning Network. While payment channels sound great in concept, how on earth are they going to solve the traveling salesman problem at that kind of network scale? Side chains are where it\u2019s at!\u201d \u201cYou might be right. State channels are pretty cool. Maintaining turing completeness on both the main and side chains gives a lot of functionality and scaling flexibility.\u201d What on earth are they talking about? Don\u2019t worry! We\u2019re going to help you out. In this article, we\u2019ll walk you through all the latest buzz words and concepts in advanced blockchain development, but keep the details at a manageable level. By the end of this post, you\u2019ll be able to freely participate in conversations like the above. This is not a coding tutorial, as we\u2019ll just be presenting important concepts at a high level. However, we may follow up with programming tutorials on these ideas. This article will be helpful to both programmers and non-programmers alike. Let\u2019s get going! Here are the ideas we\u2019ll tackle and build on in this article: Byzantine fault tolerance (BFT) is what keeps the blockchain fundamentally secure. For simplicity, let\u2019s say there were 100 nodes in a blockchain network (there are currently about 10,500 full Bitcoin nodes in the world). What happens when one node wants to tamper with the latest block and say other Bitcoin users sent him a whole bunch of Bitcoin when they really didn\u2019t? The system needs to be fault tolerant and be able to override this malicious node and agree on the correct state of the blockchain. The term originated from the \u201cByzantine Generals Problem\u201d. Let\u2019s take a quick look at it: In this scenario, the yellow camps need to coordinate to attack the central enemy city. But they need to agree on a time to attack. To do this, the camps need to clandestinely send messengers through the enemy city to tell another camp when to attack. Not only does the receiving camp need to receive the message, it needs to send back a messenger so the sending camp knows the receiving camp got the message. The receiving camp needs to know the sending camp got the message acknowledging the receiving camp got the original message. You can see this cycle goes on forever! Throw in another wrinkle: what if there is a traitor camp that intentionally does not send a messenger, or sends a messenger with the wrong information? In the days before computers, this \u201cByzantine Generals Problem\u201d was unsolvable. Luckily today, we have computers and distributed systems. The solution to the Byzantine Generals Problem in blockchain is Proof of Work. Our Proof of Work tutorial talks about it in depth, but the best explanation might come from Satoshi Nakamoto himself. If the camps above start receiving messages that don\u2019t agree, they rely on executing a Proof of Work. The Proof of Work is sufficiently complicated and requires significant computing power. Once one camp solves the Proof of Work, it broadcasts the results to the other camps. This message is now accepted in a chain of messages and the competing messages are dropped by the other camps. If the solving camp tampered with the data, the hashed chain would be quickly identified by the other camps as faulty and rejected in favor of the next solving camp. This provides a disincentive for the initial camp to cheat, since they will have wasted a bunch of energy to solve the Proof of Work and gotten no benefit from it. Imagine over several hours, the camps produced a chain of messages that each required intensive Proof of Work. This means that the majority of the camps had to agree on this chain of messages and each camp can confidently trust the final outcome. It\u2019s important to note here that Proof of Work does not care about the message itself, only that the nodes agreed to the final message. This majority network consensus keeps it secure and provides a solution to the Byzantine Generals Problem, leading to Byzantine Fault Tolerance. Let\u2019s switch gears quickly before we get back to talking about trust mechanisms. We\u2019ll define what a \u201csmart contract\u201d is. The first blockchain that was popularized is obviously the Bitcoin blockchain. But the functionality of Bitcoin is very limited. All it can do is record transaction information. It\u2019s only useful to keep track of the fact that Alice sent Bob 1 Bitcoin. What if we wanted some richer logic? What if instead of Alice just simply sending Bob 1 Bitcoin, we could program in logic that says \u201cAlice and Bob place a sports bet. If the Cleveland Cavaliers ever lose to the Toronto Raptors in a playoff series, take 1 Bitcoin from Alice\u2019s wallet and transfer it to Bob\u2019s\u201d. How awesome would that kind of functionality be? We could program in that logic, have it sit on the blockchain and Bob can rest comfortably knowing he can safely place that bet and Alice will be forced to pay the 1 Bitcoin if the Raptors ever beat the Cavs, and Alice can rest comfortably knowing she\u2019ll never have to pay. This is exactly what Ethereum is. You can think of it as a programmable Bitcoin. Smart contracts are little programming functions that sit on the Ethereum blockchain. You invoke a smart contract by calling the contract (or function) address and the arguments you send it will produce a deterministic result based on the logic of the function. A sample Ethereum smart contract function looks like this: This function does a simple check to transfer tokens from Alice to Bob. The function signature takes in Bob\u2019s wallet address and an amount to transfer. The require statement ensures Alice has enough Ether (Ethereum\u2019s currency) to give Bob. Then the function subtracts that amount from Alice\u2019s wallet and adds the same amount to Bob\u2019s wallet. Pretty straight forward right? This is a smart contract in its simplest form. But there is a world of possibilities for smart contracts from creating healthcare applications (what we do at Coral Health) to making collectible digital asset based games like Cryptokitties. Turing Completeness When people say Ethereum is \u201cTuring Complete\u201d it means that a fully featured programming language is available on the Ethereum blockchain. The code we wrote above is written in a language called Solidity, which is the specific smart contract language to Ethereum. By contrast, the Bitcoin blockchain is not Turing complete since it has little to no ability for data manipulation. It has no ability for a user to deploy if else or goto statements. This is a bit of a simplification but anytime you hear someone say something is \u201cTuring complete\u201d you can do a quick check to see if there is functionality for data changes, memory changes and if\/else statements. If there is, that\u2019s usually what they mean. Now that we\u2019ve tackled a few of these definitions let\u2019s move back to trust considerations of the blockchain. If you want a deeper look at Proof of Stake check out our detailed POS post. In short, while Proof of Work is an effective mechanism to secure the blockchain and provides a trustless consensus paradigm, it\u2019s extremely energy intensive because of all the computing power required to solve hash problems. Also, while it was meant to be decentralized, it\u2019s actually becoming more centralized as miners consolidate and massive mining setups eat up larger shares of winning blocks. Delegated Proof of Stake (DPOS) aims to solve this. Initially conceived by Dan Larimer, instead of using hardware to solve hashes, he proposed that people in the network vote for \u201cwitnesses\u201d. These witnesses are responsible for keeping the network secure and each of these witnesses puts some amount of digital currency in escrow. The witnesses who put more funds in escrow have a greater chance of mining (or minting) the next block. The incentives line up nicely here. There are only a few witnesses and they get paid to be witnesses, so they are incentivized to not cheat. If they do cheat and get caught, they not only get voted out in favor of the next eagerly awaiting witness, they lose all the funds they had in escrow. Many people believe this is the future of the blockchain. It maintains network security and allows for scalability. The biggest criticism is that it heavily favors those with more funds as smaller holders have no chance of becoming witnesses. But the reality is, smaller players have no hope of participating in Proof of Work either, as mining from your own laptop at home is no longer a reality. Smaller players get outcompeted by bigger players who have massive mining rigs. STEEM and EOS are examples of DPOS blockchains. Even Ethereum is moving to POS with its Casper project. So far we\u2019ve been talking about all-or-nothing approaches. Pure decentralization vs. central servers, Proof of Work vs. Proof of Stake. Just like everything else in life, hybrid ideas might be superior to binary ones. Blockchain developers are quickly realizing this. As we\u2019ve talked about, writing to the blockchain is slow and expensive. This is because every node in the entire network needs to verify and slurp in the whole blockchain and all the data it contains. Executing a large smart contract on a blockchain can be prohibitively expensive, and doing things like storing images on blockchains is economically infeasible. What if we could run heavy computations in a more centralized fashion, say on a single server, and then periodically integrate the results onto the main blockchain for posterity. We temporarily expose some vulnerability while the parallel server runs the heavy computation, but we get a massive benefit in that we don\u2019t have to run the computation on chain, and simply need to store the results for future verification. This is the general premise behind Truebit. We won\u2019t get into all the details of Truebit but there is a concept of challengers, who check to see the computations that were made have high fidelity. This is what, at its core, state channels are. Imagine we wanted to play a game of Starcraft and have a smart contract that pays 1 ETH to the winner. It would be ridiculous for each participant to have to write on the main Ethereum network each time a Zergling was killed by a Zealot, or when a Command Center was upgraded to an Orbital Command. The gas cost (Ethereum gas, not Starcraft gas) and time for each transaction would be prohibitive. Instead, what if the game was played in its own \u201cchannel\u201d? Each time a player made a move, the state of the game is signed by each player. After an epic battle where the Protoss player takes out the remaining Zerg forces and forces a gg, the final state of the game (Protoss wins) is sent to a smart contract on the main chain. This neutral smart contract, known as a Judge, waits a while to see if the Zerg player disputes the outcome. If the Zerg player doesn\u2019t, the Protoss player is paid the 1 ETH. Instead of having to record thousands of transactions that occurred during the game on the main chain, all the transactions are done off chain and only the final state is recorded on chain. Similarly, a side chain is a separate blockchain that runs in parallel to the main chain. The term is usually used in relation to another currency that\u2019s pegged to the currency of the main chain. For example, staying with the Starcraft motif, say we had an in-game currency called Minerals (oh wait, we do!). We could allow players to peg their Ether (or ETH) to purchase more Minerals in-game. So we reserve some ETH on the main chain, and peg, say 500 Minerals to 1 ETH. Then the player can spend their 500 Minerals in-game however they wish. Given the rapid rate at which Minerals can be spent and earned, Mineral-based transactions only happen on the side chain. What\u2019s really cool is that this architecture is fractal. We can then have a side chain of the Mineral chain to represent another scarce in-game asset. We can then make a side chain of that scarce in-game asset, and so on. There is no limit to how many side chains and levels of side chains we can have! Plasma, a project by Ethereum, uses this side chain concept. It encourages transactions to happen on side chains (or child chains). An authority governs each of the child chains. If the authority starts acting maliciously, anyone on the child chain can quit the child chain and take back their pegged assets on the main chain. It\u2019s in its early stages of development but shows a lot of promise in handling some of Ethereum\u2019s scalability issues. Lastly, we\u2019ll talk about payment channels. They\u2019re an extension of state channels and side chains. Let\u2019s say Alice wants to send Bob 1 Bitcoin but speed and transaction costs are important to them so they want to run this transaction off chain. They open up a communication line between them called a \u201cpayment channel\u201d. 1) Alice sends Bob the 1 Bitcoin. Turns out, Alice only owed Bob 0.5 Bitcoin so 2) Bob sends Alice back 0.5 Bitcoin. Then Bob takes Alice on a weekend getaway. Alice picks up the tab so 3) Bob reimburses Alice another 0.1 Bitcoin. Instead of recording these 3 separate transactions on the main chain, we just record the final state of the payment channel to the main chain. Another key idea of the payment channel is that Alice and Bob don\u2019t need to open up a channel directly between them every time they want to transact. Alice can access Bob transitively through Pete, who has a payment channel opened with both Alice and Bob. You can imagine in theory, the idea of payment channels really becomes powerful when you have a rich network of participants. Every node can access every other node without being directly connected to them. The system is able to determine the quickest path between two nodes who want to connect. There are many critics of payment channels. Finding the quickest path between unconnected nodes is no trivial exercise. This is a classic \u201ctraveling salesman\u201d problem that has been worked on by top computer scientists for decades. Critics argue that it is highly unlikely payment channels like Bitcoin\u2019s Lightning and Ethereum\u2019s Raiden will work as expected in practice due to complexities like the traveling salesman problem. The key for you is just to know that these projects and potential solutions to blockchain scalability issues exist. Many of the smartest minds in the industry are working actively to bring them to life. Congratulations! You\u2019ve just educated yourself on the most common advanced topics in blockchain that you\u2019ll hear about. By understanding these concepts, you have a firmer grasp on the fundamental tradeoffs and latest research on the blockchain than most industry \u201cexperts\u201d! Better yet, next time you hear your colleagues around the water cooler talking about state channels, the Lightning Network and Byzantine fault tolerance, not only will you know what they\u2019re talking about but you might be able to teach them a thing or two! In this article, we took a different approach from our regular coding tutorials. We felt it was important for you to conceptually understand these advanced topics in blockchain development. We\u2018ll return to our coding style of tutorials in the next set of posts. Be sure to check out our other tutorials! To learn more about Coral Health and how we\u2019re using the blockchain to advance personalized medicine research, visit our website and follow us on Twitter! By clapping more or less, you can signal to us which stories really stand out. Building a more connected future in healthcare.","time":1525794385,"title":"Advanced Blockchain Concepts for Beginners: Plasma, Side Chains and More","type":"story","url":"https:\/\/medium.com\/@mycoralhealth\/advanced-blockchain-concepts-for-beginners-32887202afad","label":2,"label_name":"crypto"},{"by":"weinzierl","descendants":0,"id":17021797,"kids":"None","score":1,"text":"This morning I ran across the following graph via Horace Dediu.  I developed Windows software during the fattest part of the Windows curve. That was a great time to be in the Windows ecosystem. Before that I was in an academic bubble. My world consisted primarily of Macs and various flavors of Unix. I had no idea that my world was a tiny minority. I had a PC at home, but mostly used it as a terminal to connect to remote Unix machines, and didn\u2019t realize that Windows was so dominant. When I found out I\u2019d been very wrong in my perception of market share, I determined not to be naive again. Ever since then I regularly look around to keep an eye on the landscape. The graph above combines desktop and mobile computers, and you may or may not think that\u2019s appropriate. Relative to the desktop market, Windows remains dominant, but the desktop market share itself has shrunk, not so much in absolute size but relative to total computer market. Last time I looked, about 70% of the traffic to this web site comes from desktops. I still consider the desktop the place for \u201creal work,\u201d and many people feel the same way. It\u2019s conventional to say the Roman Empire fell in 476 AD, but this would have been a surprise to those living in the eastern half of the empire who considered themselves to be Roman. The eastern (Byzantine) empire continued for another thousand years after the western empire fell. The Windows empire hasn\u2019t fallen, but has changed. Microsoft is doing well, as far as I know. I don\u2019t keep up with Microsoft as much as I used to. I have no animosity toward Microsoft, but I\u2019m no longer doing the kind of work their tools are intended for. I still use Windows\u2014I\u2019m writing this blog post from my Windows 10 machine\u2014though I also use Mac, Linux, iOS, and Android. \u00a0 This is also \u201csold by year\u201d. Phones last much less than even laptops (they break, the battery decays\u2026), so the steady state is that, in the last few years, I have purchased two computers (1 desktop + 1 laptop, both running Linux) and 6 android devices (3 phones + 2 tablets + 1 e-book). However, I still spend more time on the computer than on all of the other devices combined. The most interesting part of this graphic is how misleading it is and how a so-called statistician, and self-styled technologist, (you) fell for it.  Windows still massively dominates desktops and laptops.  Windows isn\u2019t being replaced.  New technologies, i.e., tablets and phones, have emerged.  Now fools are pretending tablets and phones are replacing desktops and laptops, when in reality, they are complementing desktops and laptops. Ken, did you read the post? Am I missing something? That graph doesn\u2019t mention Linux at all. The data is described as \u201cNative Programmable Microprocessor Shipments\u201d \u2013 I\u2019m not sure what that means but does it group \u201cPC shipped with Windows which later had Linux installed on it\u201d along with \u201cWindows PC\u201d? I\u2019m not clear how the data should be interpreted in terms of the OS (or maybe better described as \u201ccomputing platform\u201d) that people are using. Paul, I don\u2019t know anything about where the data in the chart came from. It\u2019s possible that Linux computers are being counted as Windows PCs. It\u2019s also possible that the number of Linux desktops that were not sold as Windows desktops is so small that it wouldn\u2019t be visible in the chart if it were included. Let\u2019s estimate what this chart would look like in absolute numbers, not percentages. The TRS-80 Model I sold 200k units total, so say 50k units in its first year. And make the left part of the graph one inch tall. 1.5b smartphones were sold in 2017, and that makes up 78% of the right end of the graph. So the right end of the graph would be over half a mile tall! Your email address will not be published. Required fields are marked * Comment   Notify me of followup comments via e-mail Name *  Email *  Website       John D. Cook, PhD, President My colleagues and I have decades of consulting experience helping companies solve complex problems involving math, statistics, and computing. Let\u2019s talk. We look forward to exploring the opportunity to help your company too.    \u00a0 \u00a9 All rights reserved.","time":1525794333,"title":"Rise and fall of the Windows Empire","type":"story","url":"https:\/\/www.johndcook.com\/blog\/2018\/05\/08\/rise-and-fall-of-the-windows-empire\/","label":7,"label_name":"random"},{"by":"osteele","descendants":0,"id":17021789,"kids":"None","score":2,"text":"","time":1525794287,"title":"Squarrows: algebra for elementary schoolers [pdf]","type":"story","url":"https:\/\/drive.google.com\/open?id=1hfzm3Rvm4xpwp_xHUKBHxuIEgfecZkiI","label":7,"label_name":"random"},{"by":"DanBC","descendants":0,"id":17021778,"kids":"[17021931]","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 U.K. health care startup Cera Care has attracted more than $4 million in venture funding and amassed a roster of advisory board members that includes a former deputy prime minister, a former chief executive officer of Standard Chartered Plc and luminaries from public and private health. The two-year-old company matches patients with in-home health-care professionals capable of providing everything from support for elderly clients to live-in assistance for people with dementia, and has garnered positive press from U.K. publications. Its website also boasts sky-high ratings on customer-satisfaction sites and partnerships with 10 U.K. National Health Service organizations. But according to three people with knowledge of the matter, Cera Care doesn\u2019t in fact have partnerships with at least seven of those groups, and up to a dozen of the reviews on third-party sites were crafted either by Cera Care employees or people close to them, rather than unbiased customers. These people asked not to be identified because of the sensitivity of the information. In a widely publicized announcement in March 2017, Cera said it has partnered with a range of public health groups in London, including St. Barts, one of the U.K.\u2019s largest. It later listed 10 of these groups on its website. However, the startup regularly works with only three of these groups -- Lewisham CCG, Haringey CCG, and Tower Hamlets CCG -- according to people with knowledge of the matter. \u201cWe note that our website was not fully up to date with these materials and are rectifying it,\" Cera said in an emailed statement. The company had previous partnerships with health care groups including Brent, Harrow and Hillingdon and East London Foundation Trust, the company said. \"We can confirm that the Trust worked with Cera Care in 2017 delivering packages of care,\" Edil Ahmed, spokeswoman for the the East London NHS Foundation Trust, said in a statement. However providers including Barnet CCG were approached in March 2017 but turned down Cera\u2019s offer. In an emailed statement Barnet CCG said that it \"has never entered into a business relationship with Cera Care,\" adding that it \"therefore should not have been included in the list of partners that appeared on the website.\u201d Cera was founded in mid-2016 by Mahiben Maruthappu, a former medical doctor and head of the U.K. health service\u2019s Innovation Center, which funded health care ventures. Nick Clegg, the former U.K. deputy prime minister, became chairman of Cera\u2019s advisory board in March. Its investors include a range of figures from the financial sector, including former Standard Chartered CEO Peter Sands, who is also on the board, and French billionaire Xavier Niel\u2019s fund Kima Ventures. The U.K., where the public is heavily dependent on the state-funded National Health Service, has been inundated with startups seeking to shake up the almost 70-year-old system. One of Cera\u2019s goals was to get patients discharged from hospitals and set up with in-home care more quickly than the NHS could offer. One of the most prominent new entrants is DeepMind, Alphabet Inc.\u2019s artificial intelligence division. In July last year, after a year-long investigation, U.K. regulators ruled that London\u2019s Royal Free Hospital had illegally provided DeepMind access to 1.6 million patient records. Patients and their families considering private in-home care often look closely at reviews on sites such as Trustpilot AS, HomeCare UK and Google Reviews before choosing a provider. Of the 104 reviews on Trustpilot, a number were fakes created by Cera Care employees, people with knowledge of the postings said. Of them, 12 have been taken down in recent days. \"Good customer service. Great care from care workers as well. Happily continuing to do business with Cera,\" one review said. \u201cWe take any allegations of false reviews extremely seriously and these will be investigated thoroughly and dealt with strictly,\u201d the spokeswoman for Cera, said. \u201cWe have no tolerance for this.\u201d Trustpilot said it has been investigating Cera Care and has been removing several reviews. \"We\u2019ve found reviews written by family members and others associated with the company while other reviews were found to be fabricated,\" said Kasper Heine, head of trust and transparency at Trustpilot. \"We\u2019ve sent the company a warning for breaching our guidelines and we continue to investigate the matter to see if further action is needed.\" A spokesman for Clegg did not comment beyond Cera\u2019s statement. Kima Ventures declined to comment. A representative for Sands could not be reached for comment. Cera, which is currently fund raising according to a source close to the company, is also developing an AI assistant called Martha. The U.K.\u2019s Data Protection Act requires all organizations processing personal information to register with the U.K. data regulator. Although handling sensitive data on recent patients and those needing regular health care, Cera also failed to register with the Information Commissioner\u2019s Office until February this year. The ICO said in a statement that it would only consider \u201cenforcement action\u201d if a company failed to register despite ICO advice. \"We have in place strict data protection governance, policies and protocols,\" said Cera\u2019s spokeswoman, \"and are diligently committed to monitoring, reviewing and updating these measures.\"","time":1525794238,"title":"U.K. Healthcare Startup Said to Have Posted Fake Reviews Online","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-04-26\/u-k-healthcare-startup-cera-is-said-to-have-posted-fake-reviews","label":0,"label_name":"biz-news"},{"by":"davekiss","descendants":0,"id":17021770,"kids":"None","score":1,"text":"On a recent family vacation, I found myself in a conversation with my father-in-law about the lingering possibility of tentative plans to grow my business. I naturally wanted to tell him that I was considering hiring an employee or two. That my revenue was enough to support scaling up. That I was working through a few product ideas to expand my offerings. But, I didn\u2019t. I didn\u2019t, because I\u2019m not sure that\u2019s what I want. There\u2019s a ton of companies that grow for the sake of it. Was I to be one of them? I had some reflecting to do. Luckily, I had already bought a ticket for MicroConf 2018, and was excited to talk to other bootstrapped founders about this very topic to see how everyone else is dealing with these same questions. Almost serendipitously, I received an email that day from Xander, the conference organizer. He mentioned that they were accepting submissions for talks by attendees of the conference. The abstracts would be voted on, and the most-voted topics would be granted the opportunity to be presented at the conference. So, I figured, what the hell. I submitted a topic for a talk. I called it \u201cThe Case Against Growth\u201d As humans and makers, we\u2019re constantly striving to make ourselves bigger, better, and stronger. We look for the next challenge as soon as we complete the previous one, applying our innovation and skills to create our best work. However, along with more products and more employees comes more complexity. In this talk, we\u2019ll explore an argument against growth, and why staying small could be the key to unlock the best potential for you to actually grow. That abstract was voted to the number two spot in the voting process. Clearly, other people are asking themselves this same question. Guess I\u2019m writing a talk this week. I took it upon myself to really force myself to think through the topic, and wanted to share a few thoughts on the matter. One of the reasons business growth is so confusing is because we assign a default definition to the word. We generally associate growth with Getting Bigger\u2122 or having more. More employees. More customers, more revenue, more products. But, this definition of growth is a fallacy, or incomplete at best. It\u2019s a social construct. This unchallenged notion of growth is a software company\u2019s own version of the American Dream\u2122 You graduate with debt, get a big job, get married, buy a house, have kids, and then one day, you\u2019re sitting on a riding lawn mower looking down at your hands wondering what the hell you\u2019re doing and where the last 15 years went. Words like \u201cbig\u201d and \u201cmore\u201d and \u201cfast\u201d are not the goals of any business. Neither are \u201csmall\u201d and \u201cless\u201d and \u201cslow.\u201d They are simply descriptions of businesses, not objectives of businesses. They are the means to the end. They\u2019re the vehicle to your actual objectives. Either can be successful. Business goals are much more human than that. We want our employees to be happy. We want to make an impact, and do the best work that we can. We want to be prosperous with our efforts. Once you recognize that your company can be structured however you like, you can then be open to building your business in the way that works best for you. Like the rules of art, rules of business primed to be broken once they are understood. If you\u2019re not familiar with the Parable of the Fisherman (or, if you need a refresher) give it a quick read: https:\/\/en.wikipedia.org\/wiki\/Anekdote_zur_Senkung_der_Arbeitsmoral This story is something I come back to whenever my head gets too big. When are you content with enough? How much internal restraint must you have in order to restrict growth, even when you have the means to grow? The real important question here is one you already know: why? Why do you want to grow your business? What for? What will it accomplish? Why do you want to hire that next employee? Is it because you think they will be helpful to your business, or are you actually just looking for a friend? Why did you get into business in the first place? Are you still working on your favorite things every day? Will you still be if you choose to expand? There are plenty of ways that you can grow your business without expanding outwards. What can you double-down on given the resources that you have right now? What impact will your ideas of growth have on your existing employees and customers? Be careful not to grow just because it feels like the next natural step. Do not skip steps. Think this through and grow within your means. Debbie Millman really nailed it with this idea. Are you making changes that are actually going to help get you closer to a better future? Impulsive growth can cause all sorts of side effects: The internet is the most amazing platform for business ever. We can learn how to do something by brute-forcing Google and YouTube, and suddenly be the expert, teaching and selling to customers halfway across the world. Many of us have lived this exact story. This low barrier of access makes it really easy to find ourselves crossing lines and consumed by situations that we haven\u2019t actually thought through. Keep your ambition in check, decide the direction you want to go, and build that into the core of your company. The number one secret to building a company that you can scale is not in a book or a podcast, nor is it a productivity app. It\u2019s simply this: being you. The Daily Stoic by\u00a0Ryan Holiday & Stephen Hanselman Let My People Go Surfing by\u00a0Yvon Chouinard The Magic of Tiny Business by\u00a0Sharon Rowe Comments\/Questions? Hit me up on Twitter @davekiss: https:\/\/twitter.com\/davekiss  \n\n","time":1525794214,"title":"The Case Against Growth","type":"story","url":"https:\/\/davekiss.com\/the-case-against-growth\/","label":7,"label_name":"random"},{"by":"optimusrex","descendants":0,"id":17021768,"kids":"None","score":1,"text":"","time":1525794205,"title":"Drone delivers 900 burgers to beat Desert Heat in Dubai","type":"story","url":"https:\/\/edition.cnn.com\/travel\/article\/dubai-food-delivery-apps\/index.html","label":7,"label_name":"random"},{"by":"anacleto","descendants":1,"id":17021751,"kids":"[17021757]","score":2,"text":"Trial period, no credit card required. Automated Workflows Plainflow helps you deliver your ideal customer experience using automated workflows that incorporate multiple services. Send tailored onboarding emails, enrich new user profiles with Clearbit, or trigger Slack notifications.Explore our recipes to see examples of workflows used by Plainflow customers. Profiles and Segments Let Plainflow centralize data from all of your services to easily create a single 360\u00b0 view of customers for segmentation and automation. Our integration with Segment makes this possible in just a few clicks. Integrations Plainflow seamlessly integrates with all of the products in your marketing and sales stack to make sure your tools are working together for you.","time":1525794116,"title":"Show HN: Plainflow, behavior-based customer journeys for your SaaS","type":"story","url":"https:\/\/www.plainflow.com\/","label":1,"label_name":"business"},{"by":"austin_kodra","descendants":0,"id":17021737,"kids":"None","score":9,"text":"A Convolutional Neural Network (CNN) is a multilayered neural network with a special architecture to detect complex features in data. CNNs have been used in image recognition, powering vision in robots, and for self-driving vehicles. In this article, we\u2019re going to build a CNN capable of classifying images. An image classifier CNN can be used in myriad ways, to classify cats and dogs, for example, or to detect if pictures of the brain contain a tumor. This post will be at an introductory-level and no domain expertise is required. However, we assume that the reader has a basic understanding of Artificial Neural Networks (ANN). Once a CNN is built, it can be used to classify the contents of different images. All we have to do is feed those images into the model. Just like ANNs, CNNs are inspired by the workings of the human brain. CNNs are able to classify images by detecting features, similar to how the human brain detects features to identify objects. Before we dive in and build the model, let\u2019s understand some concepts of CNNs and the steps of building one. Images are made up of pixels. Each pixel is represented by a number between 0 and 255. Therefore each image has a digital representation which is how computers are able to work with images. A convolution is a combined integration of two functions that shows you how one function modifies the other. [The convolution function. Source: Wikipedia] There are three important items to mention in this process: the input image, the feature detector, and the feature map. The input image is the image being detected. The feature detector is a matrix, usually 3x3 (it could also be 7x7). A feature detector is also referred to as a kernel or a filter. Intuitively, the matrix representation of the input image is multiplied element-wise with the feature detector to produce a feature map also known as a convolved feature or an activation map. The aim of this step is to reduce the size of the image and make processing faster and easier. Some of the features of the image are lost in this step. However, the main features of the image that are important in image detection are retained. These features are the ones that are unique to identifying that specific object. For example each animal has unique features that enable us to identify it. The way we prevent loss of image information is by having many feature maps. Each feature map detects the location of certain features in the image. In this step we apply the rectifier function to increase non-linearity in the CNN. Images are made of different objects that are not linear to each other. Without applying this function the image classification will be treated as a linear problem while it is actually a non-linear one. Spatial invariance is a concept where the location of an object in an image doesn\u2019t affect the ability of the neural network to detect its specific features. Pooling enables the CNN to detect features in various images irrespective of the difference in lighting in the pictures and different angles of the images. There are different types of pooling, for example, max pooling and min pooling. Max pooling works by placing a matrix of 2x2 on the feature map and picking the largest value in that box. The 2x2 matrix is moved from left to right through the entire feature map picking the largest value in each pass. These values then form a new matrix called a pooled feature map. Max pooling works to preserve the main features while also reducing the size of the image. This helps reduce overfitting, which would occur if the CNN is given too much information, especially if that information is not relevant in classifying the image. Once the pooled featured map is obtained, the next step is to flatten it. Flattening involves transforming the entire pooled feature map matrix into a single column which is then fed to the neural network for processing. After flattening, the flattened feature map is passed through a neural network. This step is made up of the input layer, the fully connected layer, and the output layer. The fully connected layer is similar to the hidden layer in ANNs but in this case it\u2019s fully connected. The output layer is where we get the predicted classes. The information is passed through the network and the error of prediction is calculated. The error is then back-propagated through the system to improve the prediction. The final figures produced by the neural network don\u2019t usually add up to one. However, it is important that these figures are brought down to numbers between zero and one, which represent the probability of each class. This is the role of the Softmax function. [The Softmax function. Source: Wikipedia] Now let\u2019s write the code that can classify images. For this exercise it\u2019s advisable to arrange the the folders that contain images as shown below. We separate images into folders and give them their appropriate names, i.e the training set and the test set. This makes it easier to import the images into Keras. Make sure that the working directory has permissions to access the images. In this step we need to import Keras and other packages that we\u2019re going to use in building the CNN. Import the following packages: To initialize the neural network we create an object of the Sequential class. To add the convolution layer, we call the add function with the classifier object and pass in Convolution2D with parameters. The first argument nb_filter. nbfilter is the number of feature detectors that we want to create. The second and third parameters are dimensions of the feature detector matrix. It is common practice to start with 32 feature detectors for CNNs. The next parameter is input_shape which is the shape of the input image. The images will be converted into this shape during preprocessing. If the image is black and white it will be converted into a 2D array and if the image is colored it will be converted into a 3D array. In this case, we\u2019ll assume that we are working with colored images. Input_shape is passed in a tuple with the number of channels, which is 3 for a colored image, and the dimensions of the 2D array in each channel. If you are not using a GPU it\u2019s advisable to use lower dimensions to reduce the computation time. When using a CPU, 64 by 64 dimensions performs well. The final parameter is the activation function. Classifying images is a nonlinear problem. So we use the rectifier function to ensure that we don\u2019t have negative pixel values during computation. That\u2019s how we achieve non-linearity. In this step we reduce the size of the feature map. Generally we create a pool size of 2x2 for max pooling. This enables us to reduce the size of the feature map while not losing important image information. classifier.add(MaxPooling2D(pool_size=(2,2))) In this step, all the pooled feature maps are taken and put into a single vector. The Flatten function flattens all the feature maps into a single column. classifier.add(Flatten()) The next step is to use the vector we obtained above as the input for the neural network by using the Dense function in Keras. The first parameter is output_dim which is the number of nodes in the hidden layer. You can determine the most appropriate number through experimentation. The higher the number of dimensions the more computing resources you will need to fit the model. A common practice is to pick the number of nodes in powers of two. The second parameter is the activation function. We usually use the ReLu activation function in the hidden layer. classifier.add(Dense(output_dim = 128, activation=\u2019relu\u2019)) The next layer we have to add is the output layer. In this case, we\u2019ll use the sigmoid activation function since we expect a binary outcome. If we expected more than two outcomes we would use the softmax function. The output_dim here is 1 since we just expect the predicted probabilities of the classes. classifier.add(Dense(output_dim=1, activation=\u2019sigmoid\u2019)) We then compile the CNN using the compile function. This function expects three parameters: the optimizer, the loss function, and the metrics of performance.The optimizer is the gradient descent algorithm we are going to use. We use the binary_crossentropy loss function since we are doing a binary classification. classifier.compile(optimizer=\u2019adam\u2019, loss=\u2019binary_crossentropy\u2019,metrics=[\u2018accuracy\u2019]) We are going to preprocess the images using Keras to prevent overfitting. This processing is known as image augmentation. The Keras utility we use for this purpose is ImageDataGenerator. from keras.preprocessing.image import ImageDataGenerator This function works by flipping, rescaling, zooming, and shearing the images. The first argument rescale ensures the images are rescaled to have pixel values between zero and one. horizontal_flip=True means that the images will be flipped horizontally. All these actions are part of the image augmentation. train_datagen = ImageDataGenerator(rescale=1.\/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) We then use the ImageDataGenerator function to rescale the pixels of the test set so that they are between zero and one. Since this is the test data and not the training data we don\u2019t have to take image augmentation steps. test_datagen = ImageDataGenerator(rescale=1.\/255) The next thing we need to do is create the training set. We do this by using train_datagen that we just created above and the flow_from_directory function. The flow_from_directory function enables us to retrieve the images of our training set from the current working directory. The first parameter is the path to the training set. The second parameter is the target_size, which is the size of the image that the CNN should expect. We have already specified this above as 256x256, so we shall use the same for this parameter. The batch_size is the number of images that will go through the network before the weights are updated. The class_mode parameter indicates whether the classification is binary or not. training_set = train_datagen.flow_from_directory(\u2018training_set\u2019, target_size=(256, 256), batch_size=32, class_mode=\u2019binary\u2019) Now we will create the test set with similar parameters as above. test_set = test_datagen.flow_from_directory(\u2018test_set\u2019, target_size=(64, 64), batch_size=32, class_mode=\u2019binary\u2019) Finally, we need to fit the model to the training dataset and test its performance with the test set. We achieve this by calling the fit_generator function on the classifier object. The first argument it takes is the training set. The second argument is the number of arguments in our training set. Epochs is the number of epochs we want to use to train the CNN. Validation_data is the test data set. nb_val_samples is the number of images in the test set. classifier.fit_generator(training_set, steps_per_epoch=5000, epochs=25, validation_data=test_set, nb_val_samples=1000) Now that the model is fitted, we can use the predict method to make predictions using new images. In order to do this we need to preprocess our images before we pass them to the predict method. To achieve this we\u2019ll use some functions from numpy. We also need to import the image module from Keras to allow us to load in the new images. import numpy as npimport keras.preprocessing import image The next step is to load the image that we would like to predict. To accomplish this we use the load_img function from the image module. The first argument this function takes is the path to the location of the image and the second argument is the size of the image. The size of the image should be the same as the size used during the training process. test_image = image.load_img(\u2018brain_image1.jpg\u2019, target_size=(256, 256)) As noted earlier, we\u2019re using 3 channels because our images are color and therefore need to transform this image into a 3D array. To do this we use the img_to_array function from the imagemodule. test_image = image.img_to_array(test_image) We now have an image with three dimensions. However we are not yet ready to make the predictions because the predict method expects four dimensions. The fourth dimension corresponds to the batch size. This is because in neural networks the data to be predicted is usually passed in as a batch. In this case we have one batch of one input image. We use the expand_dims method from numpy to add this new dimension. It takes the first parameter as the test image we are expanding, and the second parameter is the position of the dimension we are adding. The predict method expects this new dimension in the first position, which corresponds to axis 0. test_image = np.expand_dims(test_image, axis=0) Now we use the predict method to predict which class the image belongs to. prediction = classifier.predict(test_image) After running this function we\u2019ll get the result: either one or zero. However we don\u2019t know which value represents which class. To find out, we use the class_indices attribute of the training set. training_set.class_indices Assuming you had done this classification for cats and dogs you would get the following output We have now written a deep learning model that can be used to classify images into different classes. You can use this model to do any image classification. All you have to do is put the training mages and the test images in their correct folders as shown in this article. Once that is done you will be ready to do your image classification. Congratulations for staying with me this far. If you would like to learn more about CNNs you can check out the Keras documentation. If you liked this article please leave a clap \ud83d\udc4f or two or 50 \ud83d\udc4f \ud83d\udc4f so that others can find it. Editor\u2019s Note: If you want learn more about neural networks, check out these helpful Heartbeat posts: Discuss this post on Hacker News. By clapping more or less, you can signal to us which stories really stand out. Data Analyst | Writer | Data is the new soil | https:\/\/www.derrickmwiti.com The latest on machine learning at the edge.","time":1525794020,"title":"A Beginner's Guide to Convolutional Neural Networks","type":"story","url":"https:\/\/heartbeat.fritz.ai\/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed","label":5,"label_name":"ml"},{"by":"uptown","descendants":0,"id":17021726,"kids":"None","score":2,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 When the political consulting firm Cambridge Analytica announced last week it would be closing its doors and filing for bankruptcy in the wake of the Facebook Inc. data scandal, speculation mounted that it would be revived under the guise of two new companies set up in London. Not so, said Nigel Oakes, the founder of Cambridge Analytica\u2019s British affiliate, SCL Group. \"It\u2019s the end of the show,\" he said by phone Tuesday. \"The whole lot is gone. There\u2019s no secret. For anything like this to recreate itself you need a team of people to work together but nobody is working together. Everybody has gone off to do their own things.\" Several media outlets had pointed to filings on U.K. business registry Companies House showing Cambridge Analytica executives had set up a London-based firm in August 2017 called Emerdata Ltd. Rebekah and Jennifer Mercer, the daughters of conservative hedge fund tycoon and Trump supporter Robert Mercer, joined the board in March. Oakes said the original idea was to acquire Cambridge Analytica and SCL and put them under one roof. Now Emerdata is also in administration, he said. Firecrest Technologies Ltd., another company created in March by former Cambridge Analytica Chief Executive Officer Alexander Nix, would also be wound down, he said. Firecrest\u2019s main shareholder was Emerdata, according to company filings. Nix declined to comment when reached by phone. Both companies were set up before revelations from a whistle blower were released in March that Cambridge Analytica had misused data from millions of Facebook users that underpinned work on U.S. President Donald Trump\u2019s 2016 presidential campaign. Oakes didn\u2019t say what would happen to the company\u2019s intellectual property or the voter profile data it had built. \"That\u2019s the end of it,\" Oakes said. \"The whole group has been closed down. There\u2019s no other organization that\u2019s been created.\" Cambridge Analytica announced last week that it was beginning bankruptcy proceedings, saying the media storm over its use of Facebook data had scared clients away. The scandal worsened with the release of an undercover video showing Nix talking about the use of bribery to trap politicians in foreign election campaigns. The company then suspended Nix while it started an independent investigation. The results of that investigation challenged some of the allegations made about how the firm acquired Facebook data, but the company concluded it still couldn\u2019t continue. \"Over the past several months, Cambridge Analytica has been the subject of numerous unfounded accusations and, despite the company\u2019s efforts to correct the record, has been vilified for activities that are not only legal, but also widely accepted as a standard component of online advertising in both the political and commercial arenas,\u201d the company said in a statement announcing insolvency proceedings.","time":1525793968,"title":"Cambridge Analytica Won't Be Revived Under New Company Name","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-08\/cambridge-analytica-won-t-be-revived-under-new-company-name","label":0,"label_name":"biz-news"},{"by":"kungfudoi","descendants":0,"id":17021714,"kids":"None","score":1,"text":"Supporting ecommerce sites in 11 countries, a catalog of more than 1 million product SKUs, and a mobile app with on-demand features (like product scanning and prescription refills) meant Walmart needed a customizable dashboard to monitor DevOps across teams\u200a\u2014\u200aand they found an enterprise solution in Hygieia. \u201cWe were working to find ways to move to zero outage, on demand, and very frequent delivery,\u201d says Bryan Finster, Technical Expert at Walmart Labs. \u201cOne of our top questions was how to give teams visibility to pipeline health to prevent bad changes from impacting production and our efforts towards improving our delivery processes.\u201d When Bryan and his team saw a demo of Hygieia at a conference, they knew it was the dashboard they\u2019d been looking for\u200a\u2014\u200aand that, because it was open source, they could alter it to fit their needs and test it with minimal effort instead of settling for an out-of-the-box solution and paying for upgrades. After initial small-scale tests by independent teams using available hardware (e.g., plugging Raspberry Pi units into spare monitors in scrum areas), they had both the data and buy-in to stand up a centralized instance of Hygieia supporting the entire enterprise, which meant scaling it to thousands of users across the organization. To manage migration efficiently, they had to think beyond Slack channels and manual upgrades\u200a\u2014\u200aautomating the configuration process so new teams received onboarding emails and admin access automatically within hours of meeting their tooling minimums. \u201cSince we\u2019re Walmart, we tend to stress tools,\u201d Bryan jokes. \u201cWe have over 5,000 dashboards now with a continuing steep growth curve. Teams value Hygieia and challenge us all the time to improve it. We never have an empty backlog.\u201d Part of that growth came from \u201cgamification,\u201d or scoring the use of Hygieia to encourage good practices across teams. For example, teams need to use pull requests and integrate to master at least once per day per developer in order to earn five stars on Hygieia\u2019s Source Code Management widget. This keeps the code manufacturing process visible to all levels of the organization and allows teams to see how they compare to other teams in their area and across the division. \u201cThe response has been very positive,\u201d Bryan says, \u201cwith teams either challenging each other or asking for help on increasing their scores.\u201d This visibility also enables them to have informed conversations about efficiency and value across teams\u200a\u2014\u200aand update their scoring models in response. In addition to the automated onboarding process, Walmart also launched a configuration wizard to make it easier to create Hygieia dashboards and created a portfolio view showing aggregate scoring from the team level all the way to the CIO level\u200a\u2014\u200aincluding data on security scans, dependency vulnerability scans, and team size and composition. \u201cThis is all designed to help teams keep their repositories clean and to encourage the practices we need to maintain high quality,\u201d says Bryan. Walmart also plans to build a team dedicated to helping other teams with any constraints related to internal goals and challenges, further maximizing ease of use and minimizing the learning curve. While they\u2019re still in the process of onboarding new teams and refining their scoring logic, Walmart has seen \u201cconcrete improvements as a result of using Hygieia,\u201d and managers say it\u2019s gratifying to see developers looking to their dashboards to monitor and improve the health of their pipelines. And they\u2019re pushing all of their modifications back to the Capital One Hygieia team for consideration, hoping to contribute closely to the pipeline of Hygieia moving forward and help other enterprises monitor the health of their DevOps more effectively. Unless noted otherwise in this post, Capital One is not affiliated with, nor is it endorsed by, any of the companies mentioned. All trademarks and other intellectual property used or displayed are the ownership of their respective owners. This article is \u00a92018 Capital One. By clapping more or less, you can signal to us which stories really stand out. Opinions and thoughts on development at Capital One. By the technologists at Capital One. Made for developers, by developers. Opinions and thoughts on development, design, and fintech. By the technologists at Capital One.","time":1525793928,"title":"How Walmart Scaled Hygieia to 5,000+ Dashboards","type":"story","url":"https:\/\/medium.com\/capital-one-developers\/how-walmart-scaled-hygieia-the-open-source-devops-dashboard-tool-4fd65f0f589","label":3,"label_name":"dev"},{"by":"erric","descendants":1,"id":17021713,"kids":"[17021793]","score":2,"text":"Ballfinger is releasing new models of its reel-to-reel tape decks, which will go on sale later in May, as reported by Bloomberg. Ballfinger says its tape decks are designed for professional use and made of high-strength aluminum. All the elements for recording are on the right, playback is on the left, and drive functions are arranged in the middle. Ballfinger had previously shown one of its tape deck machines last year at an audio show. The company will sell four different models of the tape deck (the M063H5, M063H3, M063H1, M063HX), and each will have its own unique features. The machines are pricey; they start from 9,500 euros (about $11,400) and go up to 24,000 euros ($28,500).  Ballfinger\u2019s M063H5 high-end model is equipped with an editing system, three direct drive motors, and wooden side panels that come in black, white, or walnut. Ballfinger says except for the M063H1 model, all of the machines can be completely reconfigured, meaning the machine\u2019s parts can be easily replaced or removed. \u201cDigital media is great, but experiencing music is more than just listening to a sound file -- it\u2019s sensual, it\u2019s reels that turn and can be touched,\u201d Roland Schneider, the machine\u2019s designer who spent six years developing the machines, told Bloomberg. \u201cWhen it comes to audio quality, nothing else in the analog world gets you closer to the experience of being right there in the recording studio than reel-to-reel tape.\u201d  Schneider says he has received distribution requests for the tape deck machines from more than 80 companies worldwide, including the US, Dubai, and Hong Kong. The new tape decks will be on sale later in May, the company says on its website. This isn\u2019t the first time a music playing device of yesteryear has made a comeback. (We suggested the reel-to-reel tape decks to be the \u201cnew vinyl\u201d back in 2015.) Vinyl has been a trend for the past few years, while cassette tape sales had their best year in 2017 since 2012, thanks to releases from Stranger Things and Guardians of the Galaxy.  Command Line delivers daily updates from the near-future.","time":1525793926,"title":"How About Reel to Reel Tape Decks?","type":"story","url":"https:\/\/www.theverge.com\/circuitbreaker\/2018\/5\/8\/17330324\/ballfinger-reel-to-reel-tape-decks","label":7,"label_name":"random"},{"by":"mgdo","descendants":0,"id":17021710,"kids":"None","score":3,"text":"\nDonate Bitcoin:\n \nDonate Ethereum:\n \nDonate Litecoin:\n \nJoin the discussion! Ask questions and share your comments.\n \nDiscussion\n","time":1525793913,"title":"Life's Irreducible Structure (1968)","type":"story","url":"http:\/\/fermatslibrary.com\/s\/lifes-irreducible-structure","label":7,"label_name":"random"},{"by":"dkupfer1","descendants":0,"id":17021702,"kids":"None","score":1,"text":" 0 Posts   05\/08\/18  by Gerald Schmidt    No Comments  We subject our clusters to a lot of automated tests in the widest sense \u2013 monitoring, health checks, load tests, penetration tests, vulnerability scans, the list goes on \u2013 but every so often I come across test cases that are not well served by any of them. They are usually specific to the way a cluster is used, or how the organisation operating it works. Sometimes there is no objectively correct or incorrect answer, no obvious expected value to specify in our assert statements. I will look at three examples to explain why I think these tests are worth your while. The first test concerns the OpenShift default of letting all authenticated users create (or, more accurately, request) projects. Let\u2019s say we want to deny non-admin users this power. How do we make sure we have complied with this rule? Second, our architecture may require an application scaled to three pods to be distributed across three data centre zones for high availability. We need a test that shows that the built infrastructure matches the architectural requirement. Third, let\u2019s assume we have just experienced an unplanned downtime. Communication between two projects has failed. Clearly remediation comes first, but how would the administrator go about writing a test that makes sure the pod network is configured correctly? The three scenarios have a number of things in common. Each requires direct access to the cluster state held in the master\u2019s etcd database. That interaction alone ensures that these are not inexpensive tests in performance terms. Broadly speaking, these tests should run daily, preferably at a time of reduced load, not every hour of the day. Running them is perhaps most useful after cluster maintenance or upgrades. We will look at sample implementations of these tests in just a moment. How much work will creating tests like these involve? Thankfully, very little. If we are unsure what to test, a quick glance at our operational guidelines or architecture documentation will help us get started. Writing tests will come naturally to anyone familiar with OpenShift, and should take no more than five minutes in most cases. Kubernetes gives us all the tools we need to implement our test runner.   The CronJob object triggers nightly test runs. The payload is a lightweight single-container pod with Kate Ward\u2019s unit test framework shUnit2, oc client, and assorted tools (curl, psql, mysql, jq, awk). All test data is taken from a ConfigMap mounted at launch. The ConfigMap in turn is generated from a folder of test scripts in Git. We will return to the scripts in just a moment. For now the CronJob object waits for the appointed hour, then triggers a test run. shunit2 processes the test suite (consisting of all test scripts in \/etc\/openshift-unit.d) and then reports results. Due to a limitation of the CronJob API prior to Kubernetes 1.8, the pod reports success (zero) even in case of errors as returning an error leads to constant redeployments and considerable load on the cluster.  From a permissions point of view, administrator access is required to create the project initially, but from that point onward the service account is read-only and the container runs with the \u2018restricted\u2019 security context constraint and a non-privileged security context. Logs are written to standard output and so managed by the existing log server. Using only the default suite of tests, the test pod reports the following: These tests are just placeholders, however. The tests that matter are the ones that reflect your organisation\u2019s individual rules, guidelines and decisions. Let\u2019s return to the first example mentioned in the introduction, that is, the self-provisioner rule. It ensures that the administrator has taken the corresponding cluster role from the groups system:authenticated and system:authenticated:oauth, which usually means that an administrator has issued the following command: Using the oc tool, verifying that this has not been forgotten or reversed at a later point, is as straightforward as asking who is entitled to create (the verb) projectrequests (the resource):  The shUnit2 framework intrudes only very slightly on the code here. The utility function suite_addText allows the framework to combine many files in a test suite with a single return value. The test code must reside in a function whose name contains the word test. The writer also needs to be familiar with the framework\u2019s assert functions, assertEquals in this case. Placing a space at the start of the string and a semicolon at the end are conventions that make error messages more legible: Whereas most infrastructure tests strive for objectivity and test coverage, cluster tests like this one are unrepentantly subjective and selective. A comparison with rspec-puppet tests is instructive. Here is a brief excerpt from a Puppet manifest with matching rspec-puppet test:  The test asserts the following:  This approach makes it much harder to argue that some properties (e.g. users with basic-user credentials are allowed to create projects) are more important than others (e.g. there\u2019s a JSON file which is read-only unless you are the owner). If we place the two tests side by side, we are reminded that rspec-puppet strives for full map coverage, whereas we are focused on points of interest. These points of interest may seem arbitrary, but so, perhaps, are the decisions and operational guidelines they support and reinforce. How to test for high availability, the second example outlined in the introduction? Anti-affinity rules give us fine-grained control over placement on nodes, but unless we only have one node per zone, we cannot rely on the scheduler here. One reliable approach is to identify the nodes and examine the zone label:  As before, we start with plain oc requests and refine the output using basic command line tools. The label zone expresses anti-affinity, the label region affinity: services are spread out across zones and concentrated in regions. We fetch the nodes first (note the use of the wide switch), then extract the zone from the node definition before counting the number of unique zones. The expected number is three. So far, operational guidelines and architectural decisions have directed our test selection. Incidents are another valuable guide. Making sure they occur only once trumps trying to anticipate weaknesses in our infrastructure. For example, our multi-tenant cluster might contain a project alice which accesses a project bob using a pod network join: Let\u2019s assume that the join between the two projects has been lost. Perhaps an additional join from alice to eve was created. The fact that one (the original join is gone) does not intuitively follow from the other (an apparently unrelated new join was created) makes this all the more likely. Affected services then run into timeouts and stop processing requests. The problem is quickly diagnosed and fixed, but having suffered one service failure, we really ought to write a test that alerts us should the join disappear again:  To follow the test, we need to appreciate what happens when oc adm pod-network join-projects is called: the source project\u2019s network ID is changed to that of the destination project. Once the two projects share a network ID, they can communicate with each other. (Hence the unfortunate side-effect of creating an additional join from project alice to eve: alice receives the network ID of eve and can no longer reach services in project bob.) The test only has to fetch the network IDs of alice and bob, de-duplicate and count lines. If the join is still in place, the line count will be one. In case you are wondering why this is not a Go application, I have to confess to some library envy. Clearly the command line component would have been much more elegant, for example, and there is more repetition in the tests than I would like. The {{exports}} script seeks to address this by bundling frequently used queries such as \u2018list all projects created by users\u2019, but that does not make up for the fact that we give up the luxury of one-line web servers, Bootstrap reports adorned with canvas charts, parallel execution for oc and non-oc test cases, and so on. Those quibbles, however, hardly justify switching to a different language. If we were to do so, which language should we choose? Go? JavaScript? Python? Ruby? Each of these choices would exclude many users who happen to have prioritised other languages. Shell scripting is familiar to most OpenShift users and a natural extension of the way they interact with OpenShift anyway. Nearly everything of substance in our tests, moreover, relies on oc calls; no standard library can abstract away the fundamental awkwardness of building an application around system calls. They only feel entirely natural in a shell environment. Many tests are essential. The ones we have considered here, strictly speaking, are not. It comes down to an individual assessment of risk and usefulness. Personally, I am much more willing to grant anyuid powers to a service account if I know the next nightly test will fail should I forget to remove them later. Sometimes safety nets get in the way, but they can also have a liberating effect. This approach allows us to specify test conditions at the appropriate level and above all quickly, with minimal investment in infrastructure and training. The goal is the shortest path to a small number of valuable points of interest, not comprehensive map coverage: sightseeing, not cartography. You may also find that there is nothing that colleagues cannot express more succinctly and elegantly than you thought possible, especially in the world of Bash. Learning from other people\u2019s tests may be, for me, the most enjoyable aspect of it all. For those still undeterred, log into your administrator\u2019s account and set the timer: Your email address will not be published. Required fields are marked *   ","time":1525793853,"title":"OpenShift cluster tests","type":"story","url":"https:\/\/blog.codecentric.de\/en\/2018\/05\/openshift-cluster-tests\/","label":3,"label_name":"dev"},{"by":"ggarnier","descendants":0,"id":17021698,"kids":"None","score":2,"text":"Code review is an amazing tool to improve code quality. It brings many benefits, to the reviewer, to the reviewee and to the team. To add code reviews to your team workflow, first you need to use a version control tool. If you don\u2019t, start with that. Basically, the idea is never committing directly to the master branch - or whatever other branch you choose as the main one, but usually it\u2019s the master branch. To start any changes to the source code, be it a new feature, refactorings or bugfixes, you need to create a new branch. You can also create conventions for the branch names, but this is not required. After you finish working on your own branch, you can trigger the code review process. If you use a repository manager like GitHub or GitLab, they already provide tools to help on this process: GitHub calls them pull requests, and GitLab, merge requests - to make it simpler, I\u2019ll call them change requests from now on. You can add some comments explaining the purpose of the proposed changes, and maybe links to other related change requests or issues. These are very important to make the objective of that change request clear for everyone that might review your code. When in doubt, be as specific and clear as you can. Here\u2019s an example from tsuru, the project I currently work in: a pull request with a detailed description. \n\n As soon as a change request is created, it\u2019s available for any other member of the team to start reviewing it. You can also assign it to a specific person, if you think his opinion is important - maybe when you change a specific and obscure part of the project that not everyone dominates. As a reviewer, you job is to check many aspects of the code. Here\u2019s a non exaustive reference list: After checking the code, the reviewer can basically accept or reject it - GitHub also allows you to add review comments without explicitly approving or rejecting. When you approve, that means you think those changes could be promptly merged to the main branch. But when you reject, that could mean you doesn\u2019t agree with that change at all (e.g. you don\u2019t think this software should have the proposed feature), or that you\u2019re requesting some changes to the code. That\u2019s what review comments are meant for. Usually a review comment can be added to a specific line or code block. That could be a typo, a function you think should be named differently, a bug you spotted or a missing test or documentation. The comments should make this clear to the reviewee, who can reply to the comments or make requested changes. Then, this process repeats until the change request gets approved. When the reviewer accepts the proposed changes, the reviewee has two options: merge\/rebase the changes to the main branch and finish the code review process, or ask for another developer review. This is the case when the change is very complex and you\u2019re insecure you may be missing something. When this happens, the review process doesn\u2019t finish until all reviewers accept the changes. To illustrate, here\u2019s another example of code review from tsuru: in this pull request, I received a couple of comments with request changes. For most of them I changed the code according the suggestions, and one of them generated a small discussion. What\u2019s best is that the discussion is documented in the pull request, for anyone to read and participate. \n\n A code review process brings a lot of benefits to the reviewer, to the reviewee and to the team as a whole: When we suggest that our team starts working with code reviews, people usually get apprehensive about this process making the team slower. If they\u2019re used to finish their work, push the code and deploy to production, they may have this concern. In fact, you won\u2019t have that fast dev-to-production cycle. But that is a small downside against the many benefits presented above. It\u2019s a trade-off, like every decision you make in your project. One common concern is having to wait for a long time for another dev to start the code review process. If people still don\u2019t see value in this process, they indeed may not prioritize this. But as soon as you get to be on the other side - waiting for other people\u2019s reviews -, you\u2019ll start giving it more attention. It\u2019s an organic process that usually regulates itself. Another problem is when the change request is very large. This will require more time from the reviewers, and it may be harder to analyze: you may lose focus during the review. Just like on the previous item, this should be self regulated. If you start making very large change requests, you\u2019ll learn they aren\u2019t much productive when someone else asks for you to review a large change request. There isn\u2019t an ideal size of change request. But you should make them as small as possible. If you\u2019re working on a complex feature, you may try splitting it in a couple of small change requests, independent from each other. But if you can\u2019t make them independent, an alternative is starting with a branch forked from the main one - you may call it feature-something, then creating other branches from it. As you finish each part, make the change requests to merge to you feature branch, not the main branch. And only when you finish every \u201csub-feature\u201d change request, only then you merge your feature branch to the main one. Finally, another common question is: are there exceptions to the \u201cnever commit to the main branch\u201d rule? If the change is very simple and small, won\u2019t the code review process be just a formality? In fact, there are a couple of situations where you may bypass the code review process. One example is updating a dependency version. But I still think it\u2019s worth opening the change request, to make other team members aware of the changes. But this kind of decision is up to the team, and should be accorded among them. To finish, I suggest reading this great post about how to conduct effective code reviews.","time":1525793824,"title":"The anatomy of a code review","type":"story","url":"https:\/\/blog.guilhermegarnier.com\/2018\/05\/the-anatomy-of-a-code-review\/","label":3,"label_name":"dev"},{"by":"steeleduncan","descendants":0,"id":17021694,"kids":"None","score":1,"text":"Front page layout Site theme Sign up or login to join the discussions! \nSamuel Axon\n    -  May 8, 2018 11:30 am UTC\n An AppleInsider article has stoked some consumer frustration over Apple's butterfly keyboards. In it, AppleInsider combed through a limited dataset of warranty events from participating Apple Genius Bars and third-party repair shops. The site determined that, in that data, the 2016 MacBook Pro's keyboard accounted for twice the percentage of all warranty events in that machine's first year on the market as its predecessors from 2014 and 2015 did. These keyboards already have plenty of detractors. They have very short travel, which serves two functions: it frees up a tiny bit of space in the machine for other components (every nanometer counts), and it can make typing considerably faster since not as much effort is needed to register a key press. I like these keyboards, but a lot\u00a0of other people feel strongly that they're terrible to type on. The AppleInsider report has resulted in Apple customers expressing frustration in forums and on Reddit. Detractors have even started a Change.org petition asking Apple to recall all MacBook Pros from 2016 and later and replace their keyboards with a new design that is less prone to failure.\u00a0That's not likely to happen\u2014partly because it's not practical and partly because the data is not as conclusive as it might seem. The article claims that \"the\u00a02016 MacBook Pro keyboard is failing twice as often in the first year of use as the 2014 or 2015 MacBook Pro models,\" but that's not exactly what the data shows. That's because the \"twice as often\" conclusion is based on the percentage of all tracked repairs that the keyboard constituted, as Daring Fireball notes. The 2016 MacBook Pro had fewer warranty events over all, so while the absolute number of keyboard-related events didn't double, the percentage of all repairs that were keyboard-related did. Further, the 2017 model's slightly revised keyboard saw significant improvements on this front, so as usual, it's the earliest adopters who are dealing with the most problems. In AppleInsider's data, the 2014 MacBook Pro (inclusive of both the 13-inch and 15-inch models) \"saw 2,120 service events in the first year\" it was on the market. 2015's MacBook Pro saw 1,904 service events. The 2016 MacBook Pro saw only 1,402.\u00a0AppleInsider found 165 keyboard-related incidents (excluding those related to the Touch Bar) in its data for the 2016 MacBook Pro's first year on the market. There were 114 in 2015 and 118 in 2014\u2014two prior years that used the older chiclet keyboard design. That's an increase of about 45 percent and 40 percent, respectively, but not double. There's another wrinkle, though: return visits. Out of the 2015 model's 114 keyboard-related repairs in the dataset, six returned for a second repair for the keyboard, and none did for a third. In 2015, it was eight out of 118 for a second repair, and once again no third round of repairs. By contrast, 51 customers out of the 161 who initially sought repairs for their 2016 MacBook Pro keyboards returned for a second round of repairs, and of those, 10 returned for round three. That's still not quite twice as many repairs as with the prior models, but it's close. Why did people return for another round? Was it because the keyboards failed again or because they were improperly repaired to begin with? We don't know, so we have as many questions as we have answers after seeing this data. AppleInsider found that a slight redesign of the keyboard that was included in the 2017 models (and is now installed in 2016 models when servicing them) seems to be resulting in repair numbers moving a little closer to the 2015 and 2014 numbers, although a full year of data for that model is not yet available. The data suggests that the newer MacBook Pro keyboards require repairs a little more often. And they're\u00a0much more difficult and expensive to repair than prior models. That creates a dilemma for consumers. My own 2016 15-inch MacBook Pro keyboard failed about two months ago. The \"Z\" key stopped working. I took the computer to an Apple Store, and Apple determined that some kind of dust or similar matter had gotten into the keyboard and caused a problem. Apple replaced it with the updated keyboard found in the 2017 MacBook Pro. My computer was working again the next day, and it cost me nothing because I had AppleCare. If I hadn't, the repair would have cost me more than $700 according to the repair sheet the company gave me when it returned my computer. That's because Apple has designed the MacBook Pro such that fixing even one key requires replacing the entire keyboard apparatus, as well as part of the metal enclosure and some other components. This is the real consumer's dilemma with the MacBook Pro keyboards\u2014not their failure rate. AppleInsider's own reporting on the cost of the repair is right on the money with my experience: The keyboard isn't replaceable by itself. Break one key switch, and you need to replace the whole assembly, consisting of the keyboard, the battery, and the upper case metal surrounding the keyboard and Thunderbolt 3 ports. We've seen out-of-warranty pricing with labor and parts exceeding $700 for the job, and it isn't an easy repair, necessitating a complete disassembly of the machine. This same repair is $400 on the 2014 and 2015 MacBook Pro\u2014cheaper, but still a lot of money. Making these kinds of serviceability sacrifices allows Apple to produce some striking designs, and it frees up space for other features, better heat management, and so on. But for customers who don't purchase AppleCare, those benefits can come at a very high cost when components in the computer fail. The default one-year warranty just isn't enough\u2014and in many regions, either AppleCare isn't available at all, or it is, but no Apple Stores are close enough to make the service practical. That leaves quite a few customers hanging. And it's not just Apple anymore; other laptops, like Microsoft's Surface Pro, are just as difficult to service. It's not great for tech consumers that buying an expensive service plan is the only way to have peace of mind when buying a $2,500 device. You must login or create an account to comment. Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox.","time":1525793813,"title":"Butterfly MacBook Pro keyboards require more frequent, more expensive repairs","type":"story","url":"https:\/\/arstechnica.com\/gadgets\/2018\/05\/report-butterfly-macbook-pro-keyboards-require-more-frequent-more-expensive-repairs\/","label":0,"label_name":"biz-news"},{"by":"fsdp","descendants":0,"id":17021692,"kids":"None","score":1,"text":"Generating leads is one of the biggest marketing challenges ever. According to the State of Inbound report, 63% of marketers rate generating traffic and leads as their top marketing challenge. How about if you can overcome this huge marketing challenge and solve your lead generation problem starting today? This article will cover some of the most innovative techniques to generate new leads almost instantly or with minimal effort. Let\u2019s dive in. The Inbound reaction feedback is a strategy that uses users\u2019 emotions to boost engagement and increase conversion rate. Readers use emojis to express what they think about a piece of content. Reaction buttons let users share their feelings with the help of emojis. It is a great way to convert visitors into subscribers. The reaction buttons help you understand what visitors feel about your content. Are they angry, happy, sad, or neutral? This helps you create better content that your target audience loves to read. Research shows that using emojis increase engagement by 15%. Emojics is a great tool that lets you use reaction buttons to generate targeted leads based the emoji selected. Install Emojics widget on your website, customize it, and start generating leads. When a visitor clicks an emoji, a sign-up form pops up.  These forms are customizable so as the emojis, so you don\u2019t have to necessarily ask visitors to subscribe to your newsletter. The idea is to take instant feedback from your readers. For instance, if readers select a positive emoji, you can ask them to subscribe to get similar new posts via email and when readers select a negative emoji, you can send a quick survey asking them what they didn\u2019t like. This simple lead generation technique works because people love using and interacting with emojis. Instead of using old-fashioned smart bars and annoying popups, you should shift to reaction buttons to generate leads. If you wish to see how these reaction buttons work, this post uses reaction buttons at the end. Scroll down and see how emojis can help you generate new leads based on reader\u2019s feelings. Question and answer platforms are a great place where you can generate leads. How? Because you get a chance to answer questions people have asked that\u2019s relevant to the content and\/or product you\u2019re promoting. The idea is to write answers that actually help others and don\u2019t appear as spam. You can promote your landing page within your answer as long as it is relevant and doesn\u2019t make you look like a spammer. But the best place to promote your landing page is your bio. Make sure you use it smartly. Quora is best in the game and has no match. Quora is huge and one of the most used Q&A platforms. People visit Quora to ask questions, people respond to these questions, follow others, and interact. It lets you reach your target audience, respond to their queries, solve their problems, and you can generate leads in the process. It works exceptionally well for the fact that you\u2019re actually trying to solve a problem by answering a question. This provides you with a great chance to drive targeted traffic to your landing page and generate leads. Josh Fechter generated 3000 leads by promoting a free eBook in his bio on Quora.  And he has well over 22K followers.  \u00a0 You can link to your blog posts, landing pages, lead magnets, or whatever you like from Quora. There are no restrictions. The whole idea is to send traffic to a page that\u2019s highly relevant and where you can collect visitor\u2019s information. Chatbots and instant messaging apps are taking the world. While these messaging apps are great for customer services but when it comes to lead generation, chatbots can help a lot. Helloumi increased its visitor to trial conversion rate 4x with the help of a chatbot.  A chatbot initiates a conversation with the visitors and they have to enter their email address to respond and proceed. That\u2019s where you get your lead and the visitors get answers to their queries. It\u2019s a win-win. Chatbots help generate new leads because they don\u2019t force visitors to complete a sign-up form instead they offer help. It works just like live chat where a human customer support agent will provide help to a potential customer. You can either use traditional live chat or a chatbot to generate leads on autopilot. Since chatbots are more efficient, cost-effective, and customizable, therefore, they should be your preference. Chatfuel is a great option to get started. It lets you create your own AI chatbot that\u2019s highly customized. There are story sharing sites or as we know them \u2018blogging networks\u2019 that you can use to publish great content that will drive traffic and leads. The best part: You don\u2019t have to create new content to publish on these networks instead you can republish your blog posts with a powerful CTA to a landing page for generating leads. I love Medium when it comes to sharing stories with my audience. Why Medium? Because it had more than 60 million monthly readers back in 2017 and these users spend 4.5 million hours per month reading posts on Medium. How many times do you see a Medium post in search results outranking some big authority sites? I see medium posts in the first place for more than half of the search queries I run on any given day. Here is a recent example.  That\u2019s how you can use Medium to promote your content, rank high in Google, and generate leads for your business. You just need to publish awesome content, follow others, and spend time on the platform. Leave the rest on Medium. A lead magnet is an incentive that you offer to visitors in exchange for their email address. The whole idea is to offer something so valuable and interesting to the visitors that they cannot resist sharing their information and opt for the lead magnet. Everyone is using lead magnets these days, so how is it an innovative lead generation idea? Using a lead magnet isn\u2019t what makes you smart rather what you offer as a lead magnet to the visitors is what matters. Use interest-based highly personalized lead magnets. That\u2019s the whole point. You have to create at least one lead magnet to target one problem that a buyer persona is facing. Let\u2019s say you have 5 buyer personas and each buyer persona has 3 big problems that your product solves, you need to create 3 lead magnets for each buyer persona. HubSpot is doing it. They offer extremely targeted and personalized lead magnets based on the type of content a potential customer is reading. This is the lead magnet you\u2019re offered when you\u2019re reading a blog post on developing a content strategy.  Here is a lead magnet that you get to see in an email marketing post.  Yes, this means you have to create a lot of lead magnets to generate leads. It might get expensive but new leads will be extremely targeted. If you know how to repurpose content, creating new lead magnets won\u2019t be an issue. Republishing your entire blog post or snippet on content syndication platforms, blogs, and third-party sites is one of the best ways to generate leads as well as targeted traffic. All the leading websites like CNN and the New York Times syndicate content all the time. Why? Because everyone loves sharing best content with their audience. BestSelf grew to a $2 million company with content marketing and content syndication. You can republish your blog post on content syndication platforms. The best is none other than Scoop.it. You can publish your blog posts which others can share with their audience with a link back to your website. You can also reach out to sites in your niche that you know republish content. And a lot of authority sites syndicate content provided it is extraordinary. Here is an article.  This post is syndicated on The Next Web.  This article is syndicated from LinkedIn and is republished on Business Insider.  When your article is syndicated, you need to add a call-to-action to a relevant lead magnet within the article. However, it isn\u2019t necessary because someone who likes your post will visit your blog and that\u2019s where you can generate new leads. Gated content includes any content that\u2019s extremely valuable and your audience is willing to exchange its information in order to get access to the content. Statistics show that as much as 80% of B2B content is gated. You can gate any content type such as blog posts, videos, eBooks, and more. Don\u2019t confuse gated content with a lead magnet. Have you ever visited HBR.org? They have their blog posts gated. You can read up to 3 articles for free per month. If you wish to read more articles, you have to create a free account.  Once you create a free account, you can read 6 free articles a month. You have to switch to a paid subscription if you wish to read more articles. This is what gated content is. In order to be successful at generating new leads by gating your content, you should have killer content or you should offer extremely valuable content. A decent approach is to gate your best content. Let people read a portion of it and then ask for a subscription once they are at a stage where they cannot leave without reading the whole piece. Similarly, you can gate videos.  As long as you have great content that people are craving for, you can gate any type of content and generate new leads at no additional cost. Mastering lead generation gets easier if you have the best resources and knowledge. I\u2019m sure the innovative techniques to generating new leads discussed above will add a few good leads into your system every week. You can get started immediately and you\u2019ll start seeing results in a few days. Sabih Javed is a certified inbound marketer and an experienced digital marketing writer. His publications appeared on leading marketing blogs like JeffBullas, Yahoo News, TheNextWeb, Business2Community, and more. Connect with him @sabihjavedd Emojis are everywhere from social networks to smartphones to emails and beyond. There are more than 2 billion smartphone users all over the world who send 41.5 billion messages and 6 billion emoticons every single day... Are you more worried about the web traffic that your site receives or about the leads it generates?\u00a0The way you answer to this question can reveal a lot about you as a marketer. Although there are plenty of techniques...","time":1525793801,"title":"7 Innovative Ways to Generate New Leads","type":"story","url":"http:\/\/blog.emojics.com\/generate-new-leads\/#.WvDehh3S8G0.linkedin","label":7,"label_name":"random"},{"by":"CrankyBear","descendants":0,"id":17021684,"kids":"None","score":4,"text":"Christine Hall | May 07, 2018 The results are in from the latest\u00a0IoT Developer Survey and again this year, Linux is by far the most used operating system for Internet of Things devices. Surprised? You shouldn't be. Linux rules the roost in all areas of computing, so why should IoT be any different? The online survey is sponsored each year by the Eclipse IoT Working Group, AGILE IoT, IEEE, and the Open Mobile Alliance for the purpose of understanding how developers are building IoT solutions. The survey was open from January 24 until March 5, with 502 participants. This year 71.8 percent of respondents ticked \"Linux\" to the choose-all-that-apply question, \"What operating system(s) do you use for your IoT devices?\" Windows came in second with 22.9 percent, followed by FreeRTOS with 20.4 percent. The answer \"No OS\/Bare-metal\" took fourth place with a 19.9 percent tally. No other operating system received higher than a 10 percent vote. \u00a0 Despite Linux's commanding lead, it's slipped since last year's survey when it came in at 81.5 percent. However, that puts this year's number about on par with the 2016 IoT survey, when it made a 73.1 percent showing. Some of the current loss was undoubtedly due to FreeRTOS, a minimal operating system primarily designed for microcontrollers, which has risen 6 percentage points since the 2016 survey. In November, Amazon Web Services released its own version of the operating system, Amazon FreeRTOS, designed to be out-of-the-box ready for connected devices. FreeRTOS use will probably continue to rise as AWS works to build-out its infrastructure to accommodate its homegrown version of the OS. Linux is poised to take advantage of a similar situation. A couple of weeks ago Microsoft introduced Linux-based Azure Sphere OS. Like Amazon's version of FreeRTOS, Redmond's hand rolled Linux is also focused primarily on microcontrollers, and will be attractive to device makers because of it's ability to receive security patches and upgrades through the Azure Sphere Security Service. The results for Windows in this year's survey were mixed. Windows use showed an overall decline of nearly five percent since\u00a0last year's 27.7 percent showing. However, it moved from third to second place due to a nearly 10 percent drop by \"No OS\/Bare-metal.\" Among the Linux distributions used in IoT, Raspbian tops the list at 43.3 percent, a decline of about\u00a0two percent from last year, followed by Ubuntu at 40.2 percent, down nearly\u00a0four percent from last year. Debian, which last year was a no-show, made a third place showing at 30.9 percent.  In addition to being the leading operating system for IoT devices, the survey indicates Linux is also the OS of choice for constrained and gateway IoT devices. Linux isn't the only open source software dominating IoT -- open source databases are also making substantial inroads. When asked about databases in another choose-all-that-apply question, 44.6 percent of respondents ticked MySQL, followed by MongoDB at 29.8 percent and the time series database, InfluxDB, at 15.7 percent. According to the survey, 93 percent of databases used in IoT are open source. The survey offered no surprises when it came to the\u00a0public clouds that organizations are using to implement IoT solutions. Top dog AWS also rules IoT with a 51.8 percent showing, followed by Microsoft Azure at 31.2 percent, and private on-premesis clouds at 19.4 percent. As for the hardware architecture running IoT devices, at this point various versions of power sipping ARM pretty much run the show, although Intel x86_64 made a 31.8 percent showing for gateway devices. This is an area where we might see drastic change as soon as next year's survey. In April SiFive, the company behind the RISC-V open source processor that can be cheaply brought to market with highly specific designs, announced it had inked a deal with Western Digital, which is shifting its entire product line and plans on producing a billion RISC-V cores this year. Others are sure to follow. A complete\u00a0rundown of\u00a0the survey is available on SlideShare. More information about text formats Follow us:","time":1525793766,"title":"Survey Shows Linux the Top Operating System for Internet of Things Devices","type":"story","url":"http:\/\/www.itprotoday.com\/iot\/survey-shows-linux-top-operating-system-internet-things-devices","label":3,"label_name":"dev"},{"by":"artsandsci","descendants":0,"id":17021677,"kids":"None","score":1,"text":"%PDF-1.4\r%\u00e2\u00e3\u00cf\u00d3\r\n277 0 obj\r<>\rendobj\r      \r\nxref\r\n277 113\r\n0000000016 00000 n\r\n0000003765 00000 n\r\n0000003850 00000 n\r\n0000004104 00000 n\r\n0000004893 00000 n\r\n0000005395 00000 n\r\n0000006173 00000 n\r\n0000006371 00000 n\r\n0000006542 00000 n\r\n0000007078 00000 n\r\n0000008115 00000 n\r\n0000008388 00000 n\r\n0000008465 00000 n\r\n0000008789 00000 n\r\n0000008886 00000 n\r\n0000008934 00000 n\r\n0000008982 00000 n\r\n0000009030 00000 n\r\n0000009108 00000 n\r\n0000010412 00000 n\r\n0000010666 00000 n\r\n0000014663 00000 n\r\n0000015061 00000 n\r\n0000015431 00000 n\r\n0000015840 00000 n\r\n0000023628 00000 n\r\n0000024173 00000 n\r\n0000024569 00000 n\r\n0000024798 00000 n\r\n0000025112 00000 n\r\n0000025180 00000 n\r\n0000025603 00000 n\r\n0000025812 00000 n\r\n0000026110 00000 n\r\n0000026224 00000 n\r\n0000057200 00000 n\r\n0000087540 00000 n\r\n0000088070 00000 n\r\n0000088250 00000 n\r\n0000092035 00000 n\r\n0000092341 00000 n\r\n0000092724 00000 n\r\n0000123468 00000 n\r\n0000151280 00000 n\r\n0000178884 00000 n\r\n0000208529 00000 n\r\n0000209034 00000 n\r\n0000209177 00000 n\r\n0000209325 00000 n\r\n0000210094 00000 n\r\n0000210341 00000 n\r\n0000215447 00000 n\r\n0000215834 00000 n\r\n0000216214 00000 n\r\n0000216597 00000 n\r\n0000217122 00000 n\r\n0000217513 00000 n\r\n0000225792 00000 n\r\n0000254761 00000 n\r\n0000304840 00000 n\r\n0000339492 00000 n\r\n0000374238 00000 n\r\n0000412431 00000 n\r\n0000412637 00000 n\r\n0000412735 00000 n\r\n0000439890 00000 n\r\n0000440102 00000 n\r\n0000440510 00000 n\r\n0000440617 00000 n\r\n0000441132 00000 n\r\n0000441185 00000 n\r\n0000441238 00000 n\r\n0000441294 00000 n\r\n0000441670 00000 n\r\n0000442036 00000 n\r\n0000443759 00000 n\r\n0000449713 00000 n\r\n0000463334 00000 n\r\n0000463505 00000 n\r\n0000467757 00000 n\r\n0000467971 00000 n\r\n0000468154 00000 n\r\n0000468323 00000 n\r\n0000478708 00000 n\r\n0000478970 00000 n\r\n0000634290 00000 n\r\n0000634452 00000 n\r\n0000634622 00000 n\r\n0000641722 00000 n\r\n0000643247 00000 n\r\n0000643418 00000 n\r\n0000643593 00000 n\r\n0000643961 00000 n\r\n0000644123 00000 n\r\n0000644299 00000 n\r\n0000662005 00000 n\r\n0000674174 00000 n\r\n0000674420 00000 n\r\n0000674627 00000 n\r\n0000675009 00000 n\r\n0000675178 00000 n\r\n0000675348 00000 n\r\n0000675510 00000 n\r\n0000675685 00000 n\r\n0000686950 00000 n\r\n0000687120 00000 n\r\n0000698325 00000 n\r\n0000712787 00000 n\r\n0000726162 00000 n\r\n0000747309 00000 n\r\n0000863778 00000 n\r\n0000966538 00000 n\r\n0000002556 00000 n\r\ntrailer\r\n<<6DCF040319D734478C2CA60DD45F013F>]\/Prev 18638818>>\r\nstartxref\r\n0\r\n%%EOF\r\n     \r\n389 0 obj\r<>stream\r\nh\u00de\u0153TmL[e>\u00ef\u00bdmo[\u00c0~\u00b7\u00d7\u00c1Jw\u0152m\u00cb \u00c5\u00c9\u00d8\u20260\u00a2g\u00a7a\u00a3\u00cb41","time":1525793724,"title":"The Visual Microphone: Passive Recovery of Sound from Video [pdf]","type":"story","url":"https:\/\/people.csail.mit.edu\/mrub\/papers\/VisualMic_SIGGRAPH2014.pdf","label":7,"label_name":"random"},{"by":"makingprogress","descendants":0,"id":17021670,"kids":"None","score":1,"text":"You can almost see the devotion worked into the stones of the Kazan Cathedral of St. Petersburg. It\u2019s stately and imposing, with a colonnade made up of dozens of red granite columns, stretching along one magnificent semicircle. Completed in 1811, the church was inspired by St. Peter\u2019s Basilica in Rome, and consecrated to one of the most important figures of the Russian Orthodox church: Our Lady of Kazan, the venerated icon representing the Virgin Mary. But 15 years after the Russian Revolution, in January 1932, the cathedral was closed. When it reopened in November, it had a wholly different icon at its helm. This giant edifice was now a temple of atheism, one of hundreds of anti-religious museums in the Soviet Union\u2019s former churches, synagogues, and mosques. In the decades after the Russian Civil War, the Bolsheviks struggled to find the best way to establish a climate of atheism across the Soviet Union. At first, many hoped and believed that religion would naturally disappear once the social change they hoped to bring about came into effect. But though church jewels and other valuables had been plundered and nationalized, members of the clergy arrested, and saints\u2019 relics exhumed and put on show to disprove their apparent incorruptibility, people continued to believe.As time went on, Leninists in the party realized they would have to take active steps to stamp out religion for good. At the 15th party conference in late 1927, Joseph Stalin criticized what he perceived to be a \u201cslackening in the struggle against religion,\u201d and pushed for more stringent anti-religious policies. At the highest level, this ranged from forbidding educated believers from becoming Communist party members to the mass slaughter of thousands of Russian Orthodox priests and bishops. But on the ground, the party wanted to encourage ordinary workers to renounce their faith. The outcome was a boom in anti-religious, pro-scientific propaganda: posters, booklets, films, radio programs, public lectures, city tours, and scientific observatories. In the late 1920s, the government agency tasked with this challenge, the Commissariat of Enlightenment, landed upon a new tactic. These were the museums of atheism. Initially, there were just 30 anti-religious museums across the great expanse of the Soviet Union. But within four decades, there were hundreds of them, housed in the country\u2019s most significant former monasteries and places of worship. Some of the most famous were in Moscow and St. Petersburg\u2019s majestic former Orthodox cathedrals\u2014more accessible to foreign tourists than those in more distant cities. But the museums existed throughout the country and across houses of worship from all religions, including Ukrainian mosques and Siberian Buddhist temples. The key, according to the historian Victoria Smolkin, was to transform a sacred place into a sanitized place of learning. \u201cThey were considered most effective,\u201d she writes in A Sacred Space Is Never Empty: A History of Soviet Atheism, \u201cwhen they occupied religious spaces that had been repurposed for atheist use.\u201d In the early days of the Soviet Union, government officials began to conceive of museums in an entirely different way. They were not simply a way to put objects on display, but tangible, even enjoyable, ways to educate the public, help peasants understand their history, and fill their visitors with enthusiasm for the glorious communist future that lay ahead. Consequently, all kinds of museums underwent a tremendous overhaul, with ancient works swapped out for modern ones. To this end, nearly 250 new museums were opened between 1918 and 1923. The anti-religious museums were one small part of a grand educational plan, with what the curator and historian Crispin Paine describes as a clear ideological purpose: \u201cthe inculcation of a Marxist interpretation of history.\u201d Anti-religious museums adopted a three-prong line of attack. According to Paine, they would expose \u201cthe crimes and tricks of the clergy\u201d; promote science as a modern analog to religion; and demonstrate how religion had been the \u201chandmaiden of bourgeois capitalism.\u201d It would have no place in the glorious future of the Soviet Union. More than that, Paine writes, \u201cthey adopted a deliberate policy of sacrilege,\u201d where icons and relics were stripped of their mystique and treated as ordinary objects. One particularly detailed account, from an American visitor in the late 1930s, vividly describes one of Moscow\u2019s anti-religious museums. Charles Seely, a former naval officer, decided to pay a visit to the museum housed in the vast former Strasnoi Monastery. In these glorious surroundings, his Bolshevik tour guide promised him a miracle. \u201cI, naturally, was interested, but I seriously doubted his ability to perform one,\u201d he wrote, in a 1942 book that included his recollections of the USSR. The guide showed him a large painting of the Virgin Mary. As Seely watched, her eyes began to grow damp and misty. Then, \u201cthe tears then ran down her cheeks in such a perfectly natural manner that any unsuspecting person would have readily believed that he was actually witnessing a miracle,\u201d Seely wrote. \u201cThe tears were real enough to satisfy all but the most incredulous.\u201d Quickly, however, the illusion was ruined: \u201cThe demonstration was positively uncanny, but finally the guide spoilt everything by taking me around the painting and showing me how the priests formerly performed the \u2018miracle\u2019 by using an eyedropper.\u201d These tears had previously spilled out on occasions where the church\u2019s collection plate looked worryingly empty, he explained. Other miracles were similarly exposed. The body of a high church dignitary, exhumed annually by the head of the Russian Orthodox Church and proclaimed to be exactly as it had been \u201cwhen it died several hundred years ago,\u201d was shown to be simply a bag of bones. In other cases, \u201cincorruptible\u201d bodies had been mummified by the cool, dry conditions of underground crypts. \u201cThe object of this colossal deception was to convince the people that miracles were still being performed,\u201d Seely wrote. \u201cNobody doubted the Czar, of course, as he was the head of the Russian church.\u201d Throughout the museums, sacred icons from the Orthodox church were placed alongside so-called \u201cprimitive\u201d statues and objects from around the world. They might include amulets, voodoo figures, or even the trappings of so-called \u201cwitch doctors\u201d from faraway cultures. \u201cIf you grow up in an Orthodox society,\u201d says Paine, \u201cyou are conditioned from childhood to revere icons and the other sort of material expressions of the Orthodox faith. And you are also brought up, often subconsciously, to feel a degree of contempt for so-called \u2018tribal\u2019 societies.\u201d Putting the two together made an important, if racist, point: These familiar manifestations of belief are no more sophisticated than those from elsewhere in the world. In the cellar of the Kazan Cathedral in St. Petersburg, an entire exhibition was dedicated to the cruelty of religion, and the violent horrors of its history. Later, the historian Igor Polianksi writes, it would be copied in other anti-religious museums across the USSR. In one diorama, a torture chamber depicting the Spanish Inquisition showed a heretic bound on a rack. He awaits his punishment, as a hooded executioner presents him with an array of torture instruments: a thumbscrew; a two-headed heretic\u2019s fork, which needles simultaneously into the chin and breastbone; a branding iron; and an agonizing Judas chair. The museums systematically depicted religion of all kinds as a historical force for ignorance, racism and anti-semitism, and cruelty. Its only worthy replacement, they argued, was science. Polianski describes how \u201cpanels, diagrams, dioramas, and models explaining comparative anatomy and embryology\u201d were displayed alongside \u201cfossils from dinosaurs and hominids, portraits of socialist \u2018miracle doctors,\u2019 and meteoric stones.\u201d Man was connected not to God, but to nature: \u201cParadise apples,\u201d bred by the famous Russian horticulturist Ivan Michurin, and statues of Ivan Pavlov and his Pavlovian dogs, were included to help make this point. As one interpreter remarked to two visiting French scientists, \u201cWe, gentlemen, prefer to put our trust in Science. You can see what the new Russia has achieved in Science: medicine, chemistry, physics, biology, stratosphere, aviation, industry. We don\u2019t need religion to accomplish miracles.\u201d Foreign visitors seem divided on how to take these iconoclastic museums. Seely, for his part, left brimming with enthusiasm, and declared that \u201csociety would be measurably benefited if all ministers of the gospel, and all divinity students\u201d could visit such a museum. Others were less delighted. Many of their accounts, Paine observes, seem to conclude that they museums were \u201cna\u00efve and embarrassing.\u201d The apparently sad displays appeared even more mediocre set against their opulent holy surroundings. But they were tremendously popular with Soviet visitors\u2014though whether this was genuine interest in the future of atheism or simply a welcome break from the production line is unclear. Between 1956 and 1963, hundreds of thousands of people visited the former Kazan Cathedral\u2014soaring from 257,000 visitors, in 1956, to 700,000 in 1963. Other museums around the country boasted similarly impressive attendance numbers. Often, people came on structured group tours, in which workers from factories or offices would come all at once for a guided visit: Some museums might have as many as 17 group tours in a day. By the late 1980s, the New York Times reported, the Sunday afternoon wait to enter the Kazan museum might be as much as two hours. But despite it all, militant atheistic fervor failed to take hold. In some parts of the country, religious practices continued, gently humming away in the background. Though places of worship had theoretically been confiscated and closed, groups were still able to rent them from the government for religious services, paying a larger fee for a more extravagant setting. Other people simply lapsed into a kind of religious apathy. Scientific atheism was taught in schools, alongside history and geography, but in a 1975 survey, many people still professed to find atheistic dogma boring. By 1987, the Times reported, \u201cSoviet officials have begun to admit that they may be losing the battle against religion.\u201d For decades, Bibles had been forbidden, and religious education virtually non-existent\u2014but militant atheism does not appear to have been a sufficiently captivating force to take its place. From the late 1970s, the paper reported, with the 1917 revolution a distant memory, a burgeoning \u201creligious revival \u2026 [had] grown up to fill the ideological void.\u201d A poll of young people showed \u201ca growing fascination, especially among the well-educated, with religious literature and services.\u201d When the Soviet Union collapsed in the early 1990s, most of its anti-religious museums went with it. A few still exist, rebranded as Museums of Religion, but they have either been moved to new settings or relegated to a single wing of the religious institutions they once inhabited. Almost every closed church and temple in the Soviet Union has since been returned to its original purpose. The museums didn\u2019t just attempt to bring down religion, Polianski writes, but to portray atheism as a source of joy\u2014\u201cnot only a negation of religion, but a positive creative force bringing happiness.\u201d In the end, however, people failed to believe in the power of the cult of disbelief. \nGet our latest, delivered straight to your inbox by subscribing to our newsletter.\n \nSubscribe to our newsletter and get our latest, sent right to your inbox.\n  Follow us on Twitter to get the latest on the world's hidden wonders. Like us on Facebook to get the latest on the world's hidden wonders.","time":1525793675,"title":"The USSR turned churches into propaganda museums of atheism","type":"story","url":"https:\/\/www.atlasobscura.com\/articles\/soviet-antireligious-museums-of-atheism","label":7,"label_name":"random"},{"by":"spacemanspiffy","descendants":0,"id":17021658,"kids":"None","score":2,"text":"Increasingly we are going to be having bots conducting business on a company\u2019s behalf. As that happens, it is going to require a trust mechanism to ensure that bot-to-bot communication is legitimate. BotChain, a new startup out of Boston wants to be the blockchain for registering bots. The new blockchain, which is built on Ethereum, is designed to register and identify bots and provide a way for companies to collaborate between bots with auditing capabilities built in. BotChain has the potential to become a standard way of sharing data between bots in a trusted way. The idea is to have an official and sanctioned place for companies to register their bots securely. As the organization describes it, \u201cBotChain offers bot developers, enterprises, software companies, and system integrators the critical systems, standards, and means to validate, certify, and manage the millions of bots and billions of transactions powered by AI. Photo:\u00a0allanswart The company was created by the team at Talla,  a bot startup in Cambridge, but the goal is to open this up to much larger community of partners and expand. In fact, early partners include Gupshup, a platform for developers and Howdy.ai, B2B enterprise bot developers along with Polly, CareerLark, Disco (formerly Growbot), Zoom.ai, and Botkeeper. BotChain is the brainchild of Rob May, who is CEO at Talla. He was formerly co-founder and CEO at Backupify, which was sold to Datto in 2014. He recognized that as bot usage increases, there needed to be a system in place to help companies using bots to exchange information, and eventually even digital currencies to complete transactions in a fully digital context. May believes that blockchain is the best solution to build this trust mechanism because of the ledger\u2019s nature as an immutable and irrefutable record. If the entities on the blockchain agree to work with one another, and the other members allow it, there should be an element of confidence inherent in that. He points to other advantages such as being decentralized so that no single company can control the data on the blockchain, and of course nobody can erase a record once it\u2019s been written to the chain. It also provides a way for bots to identify one another in an official way and for participating companies to track transactions between bots. Talla opened this up to a community of users because it wants BotChain to be a standard way for bots to exchange information. Whether that happens or not remains to be seen, but these types of projects could be important building blocks as companies look for ways to conduct business confidently, even when there are no humans involved. BotChain has raised $5 million USD in a private token sale to institutional investors such as Galaxy Digital, Pillar, Glasswing and Avalon, according to the company. In addition, they will be conducting another token pre-sale starting this Friday to raise additional funds from community stakeholders. \u201cThis token sale is a way to give [our community] access. Purchasing these tokens allows users to start registering their assets and create chains of immutable records of what their machines have done,\u201d May explained. He said the company expects to sell about $20 million worth of tokens this year. You can learn more about Botchain from this video: ","time":1525793588,"title":"BotChain wants to put bot-to-bot communication on the blockchain","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/botchain-wants-to-put-bot-to-bot-communication-on-the-blockchain\/","label":2,"label_name":"crypto"},{"by":"sphix0r","descendants":0,"id":17021640,"kids":"None","score":2,"text":"We are proud to announce Matomo 3.5.0: a new major release of Matomo Analytics. Matomo 3.5.0 brings you many new Privacy tools to help bring compliance with GDPR regulations. For example you can now easily export, or delete, data for any visit or visitor. New tools were introduced to let you Anonymise old historical data (IP Address, replace User ID with a pseudonym, etc.), and for asking for user consent. You will also enjoy better security for example: Tracking codes will more often use HTTPS. And you can configure Matomo to connect to your database over SSL. Matomo performs also even better under high data processing load with several performance improvements.  -> And an important news: you can now enable the new GeoIp2 plugin in order to use the modern geolocation database. So we recommend if possible that you try the GeoIp2 geolocation driver for more accurate geolocation information. Since Geoip1 databases are not updated anymore, we recommend you try Geoip2 if possible. Finally, we\u2019ve made several Privacy-by-design improvements to our Tracker Proxy tool (useful to hide your Matomo URL when tracking many websites into the same Matomo instance),io you can see the detailed improvements below. 186 tickets have been closed by more than 16 contributors!  This release does not contain any major database upgrade.  (However if you activate the new GeoIp2 plugin it will then trigger a database schema upgrade on the log_visit and log_dimension tables.) Matomo is an open analytics platform. In an effort to help Matomo developers learn about improvements and changes in the core APIs, we document the changes since the last release.  In this release there are New features, New APIs, breaking API changes. Read more in Platform Changelog for Developers to see all changes to the platform and APIs.  Note: the Marketplace showcases more than 74 plugins already compatible with Matomo 3 and this is just the beginning. Matomo is your universal data analytics platform!  The Matomo team offers official SDKs (Tracking API Clients) for measuring your mobile apps and any other kind of apps. Congratulations to the SDK maintainers and contributors for these great releases! New: Updated: By third party developers: Read the Updating Matomo user guide or for more help contact the Matomo experts. We are together creating the best open analytics platform in the world. You can help make Matomo even more awesome by getting involved in Matomo!  Many answers and more information about Matomo you can find here: Follow us:","time":1525793505,"title":"Matomo releases new privacy tools for GDPR compliancy","type":"story","url":"https:\/\/matomo.org\/changelog\/matomo-3-5-0\/","label":3,"label_name":"dev"},{"by":"mansilladev","descendants":0,"id":17021635,"kids":"None","score":3,"text":"\n By\n          \n\n            Shana Rusonis            \n\n\n\n                    , Product Marketing Manager                  \n \nApps, Enterprise, Jira Software, Software    \n\nWork smarter, better, and faster with weekly tips and how-tos.\n\n    Subscribe now  \n\n\n   When we speak to customers about their experience with Jira, many of them can\u2019t remember exactly when their journey started. \u201cJira was used at the company before my time\u201d, or \u201cwe found multiple teams were using it,\u201d or \u201cwe made a decision to standardize on Jira across the company\u201d are some common stories we hear. Because teams choose Jira on their own, they adapt it to fit the way they work, experimenting with ways to deliver higher quality work faster. As their success stories spread, more teams adopt Jira and it grows in scope and complexity across the organization. Then, there comes a point when Jira becomes an essential application to a critical mass of people at the company. At this point, a dedicated team and proactive plan for managing Jira becomes necessary. You might feel alone on this journey, asking yourself whether or not\u00a0you\u2019ve reached the top tiers of Jira usage across the world. You may find yourself asking questions reaching out to peers at\u00a0Summit,\u00a0Atlassian User Groups, or on the\u00a0Community\u00a0to see if anyone else has tried to scale users, issues, workflows, or any of the other dimensions of Jira to a massive degree. To help put\u00a0your\u00a0journey in context, we\u2019ve collected some key Jira statistics from our largest customers. We also enlisted the help of our expert field guides on the\u00a0Technical Account Management (TAM) team\u00a0to provide pro tips for how to approach managing Jira at scale. Regardless of whether you\u2019re just starting out, on your way to scale, or topping the charts with the largest Jira instances out there, it\u2019s important to consider how managing Jira as an enterprise application will set you and your teams up for success in the long run. Read on for the key stats, or skip the treasure hunt and download the full infographic here: \n\n    DOWNLOAD THE INFOGRAPHIC  \n   One of the first indicators of Jira\u2019s popularity and growth in your organization is the number of end users. Jira is often adopted\u00a0at the team\u00a0level, and it can grow organically as teams share tickets or review reports. However, Jira being centrally managed by an IT team can accelerate the pace of user growth, since Jira admins can help make it easier for teams to adopt the product and\u00a0customize it to meet their requirements. Pro tip:\u00a0If you\u2019re managing Jira at enterprise scale, you should know how quickly Jira is being adopted by new users across the organization. Ideally, you should be able to project end user adoption several quarters into the future. With this information, you can build a clearer roadmap for how Jira will be used and what strategies you\u2019ll need to manage it at scale. More issues doesn\u2019t necessarily mean more problems\u2014just more work being planned, tracked, and completed with the help of Jira. A Jira issue can represent anything from lines of code to a request for legal to approve a contract.\u00a0More issues tracked with Jira means more work with clear ownership, statuses, and progress on behalf of customers. We see the number of issues in the largest Jira instances growing\u00a023%\u00a0each year, on average. And the average large Jira instance has nearly a million and a half issues.\u00a0That represents a lot of work! One reason teams love Jira is its flexibility. Custom fields make it possible to capture non-standard information as part of Jira issues. Matching Jira to a team\u2019s precise needs is largely a good thing, but seasoned Jira admins and our TAMs know there are limits. Our largest Jira customers have hundreds of custom fields \u2013 almost\u00a0900\u00a0on average \u2013 and the \u2018most customized\u2019 award goes to a customer with about\u00a04,800\u00a0of them! Pro tip:\u00a0Too many\u00a0custom fields can lead to\u00a0scaling challenges,\u00a0so we recommend revisiting your list of custom fields with your users on a regular basis. Trimming excess custom fields can help to make sure that Jira can continue to scale effectively. Here are some more\u00a0tips for configuring custom fields. The number of projects in Jira is a way to visualize all of the teams that use Jira to get their work done. It\u2019s a pretty impressive feat to get\u00a0this number of teams\u00a0working together on a\u00a0shared\u00a0product,\u00a0workstream, or initiative. Our largest customers are growing Jira\u00a0in this department as well, with the number of projects growing an average of\u00a017%\u00a0year over year. Although our largest customer has more than\u00a04,200 projects, the average number of Jira projects for large users less than a third of that number \u2013 nearly\u00a01,200. Pro tip: Teams working in projects for a certain should be aware of ways to better combine work with adjacent teams. In a DevOps workflow, for instance, you could shift from having separate \u201cdev\u201d and \u201cops\u201d projects to having a unified \u201cDevOps\u201d project with a workflow that encompasses the stages of work moving across both teams. Jira Workflows\u00a0can be customized to fit a team\u2019s unique needs \u2013 they\u2019re maps of how work will get done for a given project, using\u00a0statuses, assignees, transitions, and resolutions. The more teams using Jira, the greater the number of workflows that get created. They can add up over time \u2013 our largest customer has over\u00a01,400\u00a0unique workflows! But on average, our large Jira users have about\u00a0400\u00a0of them across their organizations. Pro tip:\u00a0Remember that you should always aim for simplicity over scale \u2013 if an existing workflow can be used by multiple teams, opt to repurpose it rather than creating a new one from scratch. Here are some more\u00a0tips for making workflows work for you. If you\u2019re an Atlassian customer, you\u2019ve probably heard about the\u00a0Marketplace\u00a0and how you can find an app to help you extend\u00a0the\u00a0product in just about any way imaginable; Jira alone has over 1,600 apps. But how much are Jira customers taking to the app life, and what apps do they use most often? On average, our largest Jira customers are using\u00a023 apps, with our most app-happy customer using 71\u00a0of them. Pro tip:\u00a0Just like custom fields and workflows, simplicity is the best way to set yourself up for future success as Jira grows. Each time you add an app, make sure to document which teams are using it and for what purpose. It will make it easy to revisit your collection of apps and make sure that they\u2019re still being used by teams and individuals across your company. Keeping your number of apps to a manageable count will also make upgrades smoother \u2013 each time you upgrade, you should check to make sure your apps are compatible with the version of Jira you\u2019re upgrading to. Get all of these helpful benchmarks in one place by downloading the full infographic here: \n\n    DOWNLOAD THE INFOGRAPHIC  \n Are you growing Jira fast at your company? You might want to try our Data Center deployment offering, which helps maintain uninterrupted access to Jira and stay performant as you scale. Learn more\u00a0here, or\u00a0try it out for 30 days on us! \nApps, Enterprise, Jira Software, Software   admins, enterprise, Jira Software tips ","time":1525793475,"title":"Managing growth: Jira instance with 100K users, another runs 87 3rd party apps","type":"story","url":"https:\/\/www.atlassian.com\/blog\/jira-software\/jira-largest-customers-scaling-infographic","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17021633,"kids":"None","score":5,"text":"Square,  which has already made its way into retail stores and service-based businesses (think hair salons, massage therapists, etc.), is officially getting into the restaurant business with the launch of Square for Restaurants. Square for Restaurants is a point-of-sale system that handles everything from menu updates, floor layouts, employee scheduling and performance tracking to tip splitting. Usually, restaurants have \u201csome old legacy thing or something else,\u201d Square Seller Lead Alyssa Henry told me. \u201cHistorically, we\u2019ve not served this customer segment very well,\u201d Henry said. \u201cWith Square for Restaurants, we\u2019re excited to finally be able to serve this customer segment and deliver on a couple of key things that are core to Square but also highly valued by sellers of all types.\u201d This new product is designed to be fast, self-serve, elegant and cohesive, Henry said. It also integrates seamlessly into Square\u2019s existing ecosystem that includes Payroll, Capital and more. Given Square\u2019s ownership of on-demand food delivery startup Caviar,  it\u2019s no wonder why Square fully integrated Caviar into the point-of-sale system. This means restaurants don\u2019t need to have a separate tablet system for Caviar. Square for Restaurant seating layout \u201cThe omnichannel piece of restaurants is now truly coming into fruition with this integration,\u201d Caviar Product Lead Gokul Rajaram told me. \u201cWe believe and we\u2019ve seen restaurants increasingly becoming a multichannel or omnichannel platform where an increasing percent of their orders are coming from online channels \u2014 not just from diners coming into the store, but from delivery and pickup.\u201d Now, through the Square system, restaurants will be able to handle everything in one place. That means they can see their sales broken out by channel and understand what percentage of sales comes from delivery versus pickup versus in-restaurant dining, Rajaram said.  While these don\u2019t yet exist, third-party applications from Postmates, UberEats and DoorDash could integrate into the Square POS. The issue right now for restaurants, according to Square, is the fact that delivering food for multiple services means a big tablet farm. \u201cThe intention is that we can work with all of those because that\u2019s what our sellers want,\u201d Henry said. While it\u2019s been in beta, more than 100 restaurants have used the product, including Bar Agricole in San Francisco and Greca in New York City. According to Square, the platform is relatively easy to set up. At launch, Square is charging $60 per month plus $40 per month for each additional POS set-up.","time":1525793472,"title":"Square launches restaurant point-of-sale platform","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/square-launches-restaurant-point-of-sale-platform\/","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17021625,"kids":"None","score":3,"text":"\n\n\n\n            var postLoadFunctions = {};\n            var foresee_enabled = 1\n            var dynamic_yield_enabled = 1\n                    \n\n\n\n\tCNBC_Comscore = 'Technology';\n\n\n\n\t\tvar mpscall = {\n\t\t\t\t\t\t'node_brand' : '2'  ,  \n\n\t\t\t\t\t'site' : 'cnbc.com-relaunch'  ,  \n\n\t\t\t\t\t'content_id' : '105192669'  ,  \n\n\t\t\t\t\t'path' : '\/id\/105192669'  ,  \n\n\t\t\t\t\t'is_content' : '1'  ,  \n\n\t\t\t\t\t'is_sponsored' : '0'  ,  \n\n\t\t\t\t\t'adunits' : 'Top Banner|Badge A|Badge B|Badge C|Badge D|Flex Ad First|Box Ad 1|Non Iframe Custom|Inline Custom|Movable Box Ad|Responsive Rectangle'  ,  \n\n\t\t\t\t\t'keywords' : '~'  ,  \n\n\t\t\t\t\t'cat' : 'Technology'  ,  \n\n\t\t\t\t\t'cag[configuration_franchise]' : 'Technology'  ,  \n\n\t\t\t\t\t'cag[attribution_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[project_management_team]' : 'CNBC US Team'  ,  \n\n\t\t\t\t\t'cag[attribution_author]' : 'Deirdre Bosa'  ,  \n\n\t\t\t\t\t'cag[related_primary]' : 'Software|Restaurants|Jack Dorsey|Amazon.com Inc|Square Inc|Technology|US: News'  ,  \n\n\t\t\t\t\t'cag[type_franchise]' : 'Software|Technology|Restaurants|Technology|US: News'  ,  \n\n\t\t\t\t\t'cag[type_source]' : 'CNBC US Source'  ,  \n\n\t\t\t\t\t'cag[type_cnbcteam]' : 'CNBC US Team'  ,  \n\n\t\t\t\t\t'cag[type_creator]' : 'Deirdre Bosa'  ,  \n\n\t\t\t\t\t'cag[type_tag]' : 'Software|Restaurants|Technology'  ,  \n\n\t\t\t\t\t'cag[type_person]' : 'Jack Dorsey'  ,  \n\n\t\t\t\t\t'cag[type_company]' : 'Amazon.com Inc|Square Inc'  ,  \n\n\t\t\t\t\t'cag[brand]' : 'none'  ,  \n\n\t\t\t\t\t'cag[template]' : 'story_simple'  ,  \n\n\t\t\t\t\t'cag[device]' : 'web'  ,  \n\n\t\t\t\t\t'hline' : 'Technology'  ,  \n\n\t\t\t\t\t'type' : 'cnbcnewsstory'  ,  \n\n\t\t\t\t\t'template' : 'story_simple'  ,  \n\n\t\t\t\t\t'title' : 'Square launches a service to run restaurant operations, and suddenly its &amp;#039;Caviar&amp;#039; acquisition makes sense'  ,  \n\n\t\t\t\t\t'pubdate' : '1525791667'  ,  \n\n\t\t\t\t\t'stitle' : 'SQUARE FOR RESTAURANTS 05082018 BOSA SF'  ,  \n\n\t\t\t\t\t'byline' : 'Deirdre Bosa'  ,  \n\n\t\t\t\t\t'subtype' : 'section'  ,  \n\n\t\t\t\t\t'id' : '105192669'  ,  \n\n\t\t\t\t\t'nid' : '105192669'  \n\n\t\t\n\t\t}, mpsopts = {\n\t\t  \n\t\t\"host\" : 'mps.cnbc.com',\n\t\t\"updatecorrelator\" : true\n\n\t\t};\n\n\t\tvar mps = mps || {};\n\t\tmps._ext = mps._ext || {};\n\t\tmps._adsheld = [];\n\t\tmps._queue = mps._queue || {};\n\t\tmps._queue.mpsloaded = mps._queue.mpsloaded || [];\n\t\tmps._queue.mpsinit = mps._queue.mpsinit || [];\n\t\tmps._queue.gptloaded = mps._queue.gptloaded || [];\n\t\tmps._queue.adload = mps._queue.adload || [];\n\t\tmps._queue.adclone = mps._queue.adclone || [];\n\t\tmps._queue.adview = mps._queue.adview || [];\n\t\tmps._queue.refreshads = mps._queue.refreshads || [];\n\t\tmps.__timer = Date.now ? Date.now() : (function() { return +new Date })();\n\t\tmps.__intcode = \"v2\";\n\t\tif (typeof mps.getAd != \"function\") mps.getAd = function(adunit) {\n\t\t    if (typeof adunit != \"string\") return false;\n\t\t    var slotid = \"mps-getad-\" + adunit.replace(\/\\W\/g, \"\");\n\t\t    if (!mps._ext || !mps._ext.loaded) {\n\t\t        mps._queue.gptloaded.push(function() {\n\t\t            typeof mps._gptfirst == \"function\" && mps._gptfirst(adunit, slotid);\n\t\t            mps.insertAd(\"#\" + slotid, adunit)\n\t\t        });\n\t\t        mps._adsheld.push(adunit)\n\t\t    }\n\t\t    return '<div id=\"' + slotid + '\" class=\"mps-wrapper\" data-mps-fill-slot=\"' + adunit + '\"><\/div>'\n\t\t};\n\t\t(function() {\n\t\t    head = document.head || document.getElementsByTagName(\"head\")[0], mpsload = document.createElement(\"script\");\n\t\t    mpsload.src = \"\/\/\" + mpsopts.host + \"\/fetch\/ext\/load-\" + mpscall.site + \".js?nowrite=2\";\n\t\t    mpsload.id = \"mps-load\";\n\t\t    head.insertBefore(mpsload, head.firstChild)\n\t\t})();\n\n\t\n\n\n\n\n  \n\n\n\nwindow.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var o=e[n]={exports:{}};t[n][0].call(o.exports,function(e){var o=t[n][1][e];return r(o||e)},o,o.exports)}return e[n].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<n.length;o++)r(n[o]);return r}({1:[function(t,e,n){function r(t){try{c.console&&console.log(t)}catch(e){}}var o,i=t(\"ee\"),a=t(19),c={};try{o=localStorage.getItem(\"__nr_flags\").split(\",\"),console&&\"function\"==typeof console.log&&(c.console=!0,o.indexOf(\"dev\")!==-1&&(c.dev=!0),o.indexOf(\"nr_dev\")!==-1&&(c.nrDev=!0))}catch(s){}c.nrDev&&i.on(\"internal-error\",function(t){r(t.stack)}),c.dev&&i.on(\"fn-err\",function(t,e,n){r(n.stack)}),c.dev&&(r(\"NR AGENT IN DEVELOPMENT MODE\"),r(\"flags: \"+a(c,function(t,e){return t}).join(\", \")))},{}],2:[function(t,e,n){function r(t,e,n,r,o){try{d?d-=1:i(\"err\",[o||new UncaughtException(t,e,n)])}catch(c){try{i(\"ierr\",[c,s.now(),!0])}catch(u){}}return\"function\"==typeof f&&f.apply(this,a(arguments))}function UncaughtException(t,e,n){this.message=t||\"Uncaught error with no additional information\",this.sourceURL=e,this.line=n}function o(t){i(\"err\",[t,s.now()])}var i=t(\"handle\"),a=t(20),c=t(\"ee\"),s=t(\"loader\"),f=window.onerror,u=!1,d=0;s.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(p){\"stack\"in p&&(t(12),t(11),\"addEventListener\"in window&&t(6),s.xhrWrappable&&t(13),u=!0)}c.on(\"fn-start\",function(t,e,n){u&&(d+=1)}),c.on(\"fn-err\",function(t,e,n){u&&(this.thrown=!0,o(n))}),c.on(\"fn-end\",function(){u&&!this.thrown&&d>0&&(d-=1)}),c.on(\"internal-error\",function(t){i(\"ierr\",[t,s.now(),!0])})},{}],3:[function(t,e,n){t(\"loader\").features.ins=!0},{}],4:[function(t,e,n){function r(){C++,M=y.hash,this[u]=b.now()}function o(){C--,y.hash!==M&&i(0,!0);var t=b.now();this[l]=~~this[l]+t-this[u],this[d]=t}function i(t,e){E.emit(\"newURL\",[\"\"+y,e])}function a(t,e){t.on(e,function(){this[e]=b.now()})}var c=\"-start\",s=\"-end\",f=\"-body\",u=\"fn\"+c,d=\"fn\"+s,p=\"cb\"+c,h=\"cb\"+s,l=\"jsTime\",m=\"fetch\",v=\"addEventListener\",w=window,y=w.location,b=t(\"loader\");if(w[v]&&b.xhrWrappable){var g=t(9),x=t(10),E=t(8),O=t(6),R=t(12),P=t(7),T=t(13),S=t(\"ee\"),N=S.get(\"tracer\");t(14),b.features.spa=!0;var M,j=w[v],C=0;S.on(u,r),S.on(p,r),S.on(d,o),S.on(h,o),S.buffer([u,d,\"xhr-done\",\"xhr-resolved\"]),O.buffer([u]),R.buffer([\"setTimeout\"+s,\"clearTimeout\"+c,u]),T.buffer([u,\"new-xhr\",\"send-xhr\"+c]),P.buffer([m+c,m+\"-done\",m+f+c,m+f+s]),E.buffer([\"newURL\"]),g.buffer([u]),x.buffer([\"propagate\",p,h,\"executor-err\",\"resolve\"+c]),N.buffer([u,\"no-\"+u]),a(T,\"send-xhr\"+c),a(S,\"xhr-resolved\"),a(S,\"xhr-done\"),a(P,m+c),a(P,m+\"-done\"),E.on(\"pushState-end\",i),E.on(\"replaceState-end\",i),j(\"hashchange\",i,!0),j(\"load\",i,!0),j(\"popstate\",function(){i(0,C>1)},!0)}},{}],5:[function(t,e,n){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t(\"ee\"),i=t(\"handle\"),a=t(12),c=t(11),s=\"learResourceTimings\",f=\"addEventListener\",u=\"resourcetimingbufferfull\",d=\"bstResource\",p=\"resource\",h=\"-start\",l=\"-end\",m=\"fn\"+h,v=\"fn\"+l,w=\"bstTimer\",y=\"pushState\",b=t(\"loader\");b.features.stn=!0,t(8);var g=NREUM.o.EV;o.on(m,function(t,e){var n=t[0];n instanceof g&&(this.bstStart=b.now())}),o.on(v,function(t,e){var n=t[0];n instanceof g&&i(\"bst\",[n,e,this.bstStart,b.now()])}),a.on(m,function(t,e,n){this.bstStart=b.now(),this.bstType=n}),a.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),this.bstType])}),c.on(m,function(){this.bstStart=b.now()}),c.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),\"requestAnimationFrame\"])}),o.on(y+h,function(t){this.time=b.now(),this.startPath=location.pathname+location.hash}),o.on(y+l,function(t){i(\"bstHist\",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance[\"c\"+s]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"c\"+s]()},!1):window.performance[f](\"webkit\"+u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance[\"webkitC\"+s]()},!1)),document[f](\"scroll\",r,{passive:!0}),document[f](\"keypress\",r,!1),document[f](\"click\",r,!1)}},{}],6:[function(t,e,n){function r(t){for(var e=t;e&&!e.hasOwnProperty(u);)e=Object.getPrototypeOf(e);e&&o(e)}function o(t){c.inPlace(t,[u,d],\"-\",i)}function i(t,e){return t[1]}var a=t(\"ee\").get(\"events\"),c=t(22)(a,!0),s=t(\"gos\"),f=XMLHttpRequest,u=\"addEventListener\",d=\"removeEventListener\";e.exports=a,\"getPrototypeOf\"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+\"-start\",function(t,e){var n=t[1],r=s(n,\"nr@wrapped\",function(){function t(){if(\"function\"==typeof n.handleEvent)return n.handleEvent.apply(n,arguments)}var e={object:t,\"function\":n}[typeof n];return e?c(e,\"fn-\",null,e.name||\"anonymous\"):n});this.wrapped=t[1]=r}),a.on(d+\"-start\",function(t){t[1]=this.wrapped||t[1]})},{}],7:[function(t,e,n){function r(t,e,n){var r=t[e];\"function\"==typeof r&&(t[e]=function(){var t=r.apply(this,arguments);return o.emit(n+\"start\",arguments,t),t.then(function(e){return o.emit(n+\"end\",[null,e],t),e},function(e){throw o.emit(n+\"end\",[e],t),e})})}var o=t(\"ee\").get(\"fetch\"),i=t(19);e.exports=o;var a=window,c=\"fetch-\",s=c+\"body-\",f=[\"arrayBuffer\",\"blob\",\"json\",\"text\",\"formData\"],u=a.Request,d=a.Response,p=a.fetch,h=\"prototype\";u&&d&&p&&(i(f,function(t,e){r(u[h],e,s),r(d[h],e,s)}),r(a,\"fetch\",c),o.on(c+\"end\",function(t,e){var n=this;e?e.clone().arrayBuffer().then(function(t){n.rxSize=t.byteLength,o.emit(c+\"done\",[null,e],n)}):o.emit(c+\"done\",[t],n)}))},{}],8:[function(t,e,n){var r=t(\"ee\").get(\"history\"),o=t(22)(r);e.exports=r,o.inPlace(window.history,[\"pushState\",\"replaceState\"],\"-\")},{}],9:[function(t,e,n){var r=t(\"ee\").get(\"mutation\"),o=t(22)(r),i=NREUM.o.MO;e.exports=r,i&&(window.MutationObserver=function(t){return this instanceof i?new i(o(t,\"fn-\")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype)},{}],10:[function(t,e,n){function r(t){var e=a.context(),n=c(t,\"executor-\",e),r=new f(n);return a.context(r).getCtx=function(){return e},a.emit(\"new-promise\",[r,e],e),r}function o(t,e){return e}var i=t(22),a=t(\"ee\").get(\"promise\"),c=i(a),s=t(19),f=NREUM.o.PR;e.exports=a,f&&(window.Promise=r,[\"all\",\"race\"].forEach(function(t){var e=f[t];f[t]=function(n){function r(t){return function(){a.emit(\"propagate\",[null,!o],i),o=o||!t}}var o=!1;s(n,function(e,n){Promise.resolve(n).then(r(\"all\"===t),r(!1))});var i=e.apply(f,arguments),c=f.resolve(i);return c}}),[\"resolve\",\"reject\"].forEach(function(t){var e=f[t];f[t]=function(t){var n=e.apply(f,arguments);return t!==n&&a.emit(\"propagate\",[t,!0],n),n}}),f.prototype[\"catch\"]=function(t){return this.then(null,t)},f.prototype=Object.create(f.prototype,{constructor:{value:r}}),s(Object.getOwnPropertyNames(f),function(t,e){try{r[e]=f[e]}catch(n){}}),a.on(\"executor-start\",function(t){t[0]=c(t[0],\"resolve-\",this),t[1]=c(t[1],\"resolve-\",this)}),a.on(\"executor-err\",function(t,e,n){t[1](n)}),c.inPlace(f.prototype,[\"then\"],\"then-\",o),a.on(\"then-start\",function(t,e){this.promise=e,t[0]=c(t[0],\"cb-\",this),t[1]=c(t[1],\"cb-\",this)}),a.on(\"then-end\",function(t,e,n){this.nextPromise=n;var r=this.promise;a.emit(\"propagate\",[r,!0],n)}),a.on(\"cb-end\",function(t,e,n){a.emit(\"propagate\",[n,!0],this.nextPromise)}),a.on(\"propagate\",function(t,e,n){this.getCtx&&!e||(this.getCtx=function(){if(t instanceof Promise)var e=a.context(t);return e&&e.getCtx?e.getCtx():this})}),r.toString=function(){return\"\"+f})},{}],11:[function(t,e,n){var r=t(\"ee\").get(\"raf\"),o=t(22)(r),i=\"equestAnimationFrame\";e.exports=r,o.inPlace(window,[\"r\"+i,\"mozR\"+i,\"webkitR\"+i,\"msR\"+i],\"raf-\"),r.on(\"raf-start\",function(t){t[0]=o(t[0],\"fn-\")})},{}],12:[function(t,e,n){function r(t,e,n){t[0]=a(t[0],\"fn-\",null,n)}function o(t,e,n){this.method=n,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],\"fn-\",this,n)}var i=t(\"ee\").get(\"timer\"),a=t(22)(i),c=\"setTimeout\",s=\"setInterval\",f=\"clearTimeout\",u=\"-start\",d=\"-\";e.exports=i,a.inPlace(window,[c,\"setImmediate\"],c+d),a.inPlace(window,[s],s+d),a.inPlace(window,[f,\"clearImmediate\"],f+d),i.on(s+u,r),i.on(c+u,o)},{}],13:[function(t,e,n){function r(t,e){d.inPlace(e,[\"onreadystatechange\"],\"fn-\",c)}function o(){var t=this,e=u.context(t);t.readyState>3&&!e.resolved&&(e.resolved=!0,u.emit(\"xhr-resolved\",[],t)),d.inPlace(t,y,\"fn-\",c)}function i(t){b.push(t),l&&(x?x.then(a):v?v(a):(E=-E,O.data=E))}function a(){for(var t=0;t<b.length;t++)r([],b[t]);b.length&&(b=[])}function c(t,e){return e}function s(t,e){for(var n in t)e[n]=t[n];return e}t(6);var f=t(\"ee\"),u=f.get(\"xhr\"),d=t(22)(u),p=NREUM.o,h=p.XHR,l=p.MO,m=p.PR,v=p.SI,w=\"readystatechange\",y=[\"onload\",\"onerror\",\"onabort\",\"onloadstart\",\"onloadend\",\"onprogress\",\"ontimeout\"],b=[];e.exports=u;var g=window.XMLHttpRequest=function(t){var e=new h(t);try{u.emit(\"new-xhr\",[e],e),e.addEventListener(w,o,!1)}catch(n){try{u.emit(\"internal-error\",[n])}catch(r){}}return e};if(s(h,g),g.prototype=h.prototype,d.inPlace(g.prototype,[\"open\",\"send\"],\"-xhr-\",c),u.on(\"send-xhr-start\",function(t,e){r(t,e),i(e)}),u.on(\"open-xhr-start\",r),l){var x=m&&m.resolve();if(!v&&!m){var E=1,O=document.createTextNode(E);new l(a).observe(O,{characterData:!0})}}else f.on(\"fn-end\",function(t){t[0]&&t[0].type===w||a()})},{}],14:[function(t,e,n){function r(t){var e=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<d;r++)t.removeEventListener(u[r],this.listener,!1);if(!e.aborted){if(n.duration=a.now()-this.startTime,4===t.readyState){e.status=t.status;var i=o(t,this.lastSize);if(i&&(n.rxSize=i),this.sameOrigin){var s=t.getResponseHeader(\"X-NewRelic-App-Data\");s&&(e.cat=s.split(\", \").pop())}}else e.status=0;n.cbTime=this.cbTime,f.emit(\"xhr-done\",[t],t),c(\"xhr\",[e,n,this.startTime])}}}function o(t,e){var n=t.responseType;if(\"json\"===n&&null!==e)return e;var r=\"arraybuffer\"===n||\"blob\"===n||\"json\"===n?t.response:t.responseText;return l(r)}function i(t,e){var n=s(e),r=t.params;r.host=n.hostname+\":\"+n.port,r.pathname=n.pathname,t.sameOrigin=n.sameOrigin}var a=t(\"loader\");if(a.xhrWrappable){var c=t(\"handle\"),s=t(15),f=t(\"ee\"),u=[\"load\",\"error\",\"abort\",\"timeout\"],d=u.length,p=t(\"id\"),h=t(18),l=t(17),m=window.XMLHttpRequest;a.features.xhr=!0,t(13),f.on(\"new-xhr\",function(t){var e=this;e.totalCbs=0,e.called=0,e.cbTime=0,e.end=r,e.ended=!1,e.xhrGuids={},e.lastSize=null,h&&(h>34||h<10)||window.opera||t.addEventListener(\"progress\",function(t){e.lastSize=t.loaded},!1)}),f.on(\"open-xhr-start\",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),f.on(\"open-xhr-end\",function(t,e){\"loader_config\"in NREUM&&\"xpid\"in NREUM.loader_config&&this.sameOrigin&&e.setRequestHeader(\"X-NewRelic-ID\",NREUM.loader_config.xpid)}),f.on(\"send-xhr-start\",function(t,e){var n=this.metrics,r=t[0],o=this;if(n&&r){var i=l(r);i&&(n.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{\"abort\"===t.type&&(o.params.aborted=!0),(\"load\"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||\"function\"!=typeof e.onload))&&o.end(e)}catch(n){try{f.emit(\"internal-error\",[n])}catch(r){}}};for(var c=0;c<d;c++)e.addEventListener(u[c],this.listener,!1)}),f.on(\"xhr-cb-time\",function(t,e,n){this.cbTime+=t,e?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&\"function\"==typeof n.onload||this.end(n)}),f.on(\"xhr-load-added\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&!this.xhrGuids[n]&&(this.xhrGuids[n]=!0,this.totalCbs+=1)}),f.on(\"xhr-load-removed\",function(t,e){var n=\"\"+p(t)+!!e;this.xhrGuids&&this.xhrGuids[n]&&(delete this.xhrGuids[n],this.totalCbs-=1)}),f.on(\"addEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-added\",[t[1],t[2]],e)}),f.on(\"removeEventListener-end\",function(t,e){e instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-removed\",[t[1],t[2]],e)}),f.on(\"fn-start\",function(t,e,n){e instanceof m&&(\"onload\"===n&&(this.onload=!0),(\"load\"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),f.on(\"fn-end\",function(t,e){this.xhrCbStart&&f.emit(\"xhr-cb-time\",[a.now()-this.xhrCbStart,this.onload,e],e)})}},{}],15:[function(t,e,n){e.exports=function(t){var e=document.createElement(\"a\"),n=window.location,r={};e.href=t,r.port=e.port;var o=e.href.split(\":\/\/\");!r.port&&o[1]&&(r.port=o[1].split(\"\/\")[0].split(\"@\").pop().split(\":\")[1]),r.port&&\"0\"!==r.port||(r.port=\"https\"===o[0]?\"443\":\"80\"),r.hostname=e.hostname||n.hostname,r.pathname=e.pathname,r.protocol=o[0],\"\/\"!==r.pathname.charAt(0)&&(r.pathname=\"\/\"+r.pathname);var i=!e.protocol||\":\"===e.protocol||e.protocol===n.protocol,a=e.hostname===document.domain&&e.port===n.port;return r.sameOrigin=i&&(!e.hostname||a),r}},{}],16:[function(t,e,n){function r(){}function o(t,e,n){return function(){return i(t,[f.now()].concat(c(arguments)),e?null:this,n),e?void 0:this}}var i=t(\"handle\"),a=t(19),c=t(20),s=t(\"ee\").get(\"tracer\"),f=t(\"loader\"),u=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=u);var d=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],p=\"api-\",h=p+\"ixn-\";a(d,function(t,e){u[e]=o(p+e,!0,\"api\")}),u.addPageAction=o(p+\"addPageAction\",!0),u.setCurrentRouteName=o(p+\"routeName\",!0),e.exports=newrelic,u.interaction=function(){return(new r).get()};var l=r.prototype={createTracer:function(t,e){var n={},r=this,o=\"function\"==typeof e;return i(h+\"tracer\",[f.now(),t,n],r),function(){if(s.emit((o?\"\":\"no-\")+\"fn-start\",[f.now(),r,o],n),o)try{return e.apply(this,arguments)}finally{s.emit(\"fn-end\",[f.now()],n)}}}};a(\"setName,setAttribute,save,ignore,onEnd,getContext,end,get\".split(\",\"),function(t,e){l[e]=o(h+e)}),newrelic.noticeError=function(t){\"string\"==typeof t&&(t=new Error(t)),i(\"err\",[t,f.now()])}},{}],17:[function(t,e,n){e.exports=function(t){if(\"string\"==typeof t&&t.length)return t.length;if(\"object\"==typeof t){if(\"undefined\"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if(\"undefined\"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!(\"undefined\"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(e){return}}}},{}],18:[function(t,e,n){var r=0,o=navigator.userAgent.match(\/Firefox[\/s](d+.d+)\/);o&&(r=+o[1]),e.exports=r},{}],19:[function(t,e,n){function r(t,e){var n=[],r=\"\",i=0;for(r in t)o.call(t,r)&&(n[i]=e(r,t[r]),i+=1);return n}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],20:[function(t,e,n){function r(t,e,n){e||(e=0),\"undefined\"==typeof n&&(n=t?t.length:0);for(var r=-1,o=n-e||0,i=Array(o<0?0:o);++r<o;)i[r]=t[e+r];return i}e.exports=r},{}],21:[function(t,e,n){e.exports={exists:\"undefined\"!=typeof window.performance&&window.performance.timing&&\"undefined\"!=typeof window.performance.timing.navigationStart}},{}],22:[function(t,e,n){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t(\"ee\"),i=t(20),a=\"nr@original\",c=Object.prototype.hasOwnProperty,s=!1;e.exports=function(t,e){function n(t,e,n,o){function nrWrapper(){var r,a,c,s;try{a=this,r=i(arguments),c=\"function\"==typeof n?n(r,a):n||{}}catch(f){p([f,\"\",[r,a,o],c])}u(e+\"start\",[r,a,o],c);try{return s=t.apply(a,r)}catch(d){throw u(e+\"err\",[r,a,d],c),d}finally{u(e+\"end\",[r,a,s],c)}}return r(t)?t:(e||(e=\"\"),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,e,o,i){o||(o=\"\");var a,c,s,f=\"-\"===o.charAt(0);for(s=0;s<e.length;s++)c=e[s],a=t[c],r(a)||(t[c]=n(a,f?c+o:o,i,c))}function u(n,r,o){if(!s||e){var i=s;s=!0;try{t.emit(n,r,o,e)}catch(a){p([a,n,r,o])}s=i}}function d(t,e){if(Object.defineProperty&&Object.keys)try{var n=Object.keys(t);return n.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(r){p([r])}for(var o in t)c.call(t,o)&&(e[o]=t[o]);return e}function p(e){try{t.emit(\"internal-error\",e)}catch(n){}}return t||(t=o),n.inPlace=f,n.flag=a,n}},{}],ee:[function(t,e,n){function r(){}function o(t){function e(t){return t&&t instanceof r?t:t?s(t,c,i):i()}function n(n,r,o,i){if(!p.aborted||i){t&&t(n,r,o);for(var a=e(o),c=l(n),s=c.length,f=0;f<s;f++)c[f].apply(a,r);var d=u[y[n]];return d&&d.push([b,n,r,a]),a}}function h(t,e){w[t]=l(t).concat(e)}function l(t){return w[t]||[]}function m(t){return d[t]=d[t]||o(n)}function v(t,e){f(t,function(t,n){e=e||\"feature\",y[n]=e,e in u||(u[e]=[])})}var w={},y={},b={on:h,emit:n,get:m,listeners:l,context:e,buffer:v,abort:a,aborted:!1};return b}function i(){return new r}function a(){(u.api||u.feature)&&(p.aborted=!0,u=p.backlog={})}var c=\"nr@context\",s=t(\"gos\"),f=t(19),u={},d={},p=e.exports=o();p.backlog=u},{}],gos:[function(t,e,n){function r(t,e,n){if(o.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[e]=r,r}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){o.buffer([t],r),o.emit(t,e,n)}var o=t(\"ee\").get(\"handle\");e.exports=r,r.ee=o},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||\"object\"!==e&&\"function\"!==e?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i=\"nr@id\",a=t(\"gos\");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!x++){var t=g.info=NREUM.info,e=p.getElementsByTagName(\"script\")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return u.abort();f(y,function(e,n){t[e]||(t[e]=n)}),s(\"mark\",[\"onload\",a()+g.offset],null,\"api\");var n=p.createElement(\"script\");n.src=\"https:\/\/\"+t.agent,e.parentNode.insertBefore(n,e)}}function o(){\"complete\"===p.readyState&&i()}function i(){s(\"mark\",[\"domContent\",a()+g.offset],null,\"api\")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(c=Math.max((new Date).getTime(),c))-g.offset}var c=(new Date).getTime(),s=t(\"handle\"),f=t(19),u=t(\"ee\"),d=window,p=d.document,h=\"addEventListener\",l=\"attachEvent\",m=d.XMLHttpRequest,v=m&&m.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:m,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var w=\"\"+location,y={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",agent:\"js-agent.newrelic.com\/nr-spa-1039.min.js\"},b=m&&v&&v[h]&&!\/CriOS\/.test(navigator.userAgent),g=e.exports={offset:c,now:a,origin:w,features:{},xhrWrappable:b};t(16),p[h]?(p[h](\"DOMContentLoaded\",i,!1),d[h](\"load\",r,!1)):(p[l](\"onreadystatechange\",o),d[l](\"onload\",r)),s(\"mark\",[\"firstbyte\",c],null,\"api\");var x=0,E=t(21)},{}]},{},[\"loader\",2,14,5,3,4]);\n;NREUM.info={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",licenseKey:\"356631dc7f\",applicationID:\"22258555\",sa:1}\n\n\nvar admantx_url =\ndocument.addEventListener(\"DOMContentLoaded\", function(event) {\n  mps._queue.mpsloaded.push(function(){\n    mps._log('**** LOADED: cnbc-cms-header-insert');\n    if (window.mps) {\n        if (window.CNBC_Premium && CNBC_Premium.isPremium && document.cookie.indexOf('ispro=true') == -1 && (mps.pagevars.type!=\"franchise\")) {\n          mps.nlformtypes = mps.nlformtypes || [];\n          mps.nlformtypes.push('paywall');\n        }\n\n        \/\/<!-- Omniture s_code path -->\n        mps.scodePath=\"\/\/fm.cnbc.com\/applications\/cnbc.com\/staticcontent\/scripts\/omniture\/s_code.js?v=1.6.4.1\";\n        \/\/<!-- end: Omniture s_code path -->\n\n        \/\/<!-- Google PII Fix BEGIN -->\n        mps._queue.mpsinit.push(function() {\n          (function(){\n            mps._urlContainsEmail = function() {\n              var _qs = window.location.href;\n              if (!_qs) {\n                return false;\n              }\n              var _regex = \/([^=&\/<>()[].,;:s@\"]+(.[^=&\/<>()[].,;:s@\"]+)*)@(([[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}])|(([a-zA-Z-0-9]+.)+[a-zA-Z]{2,}))\/;\n              return _regex.test(_qs);\n            };\n            if (mps._urlContainsEmail()) {\n              mps._debug('[MPS]: email address detected in url, bypass gpt.');\n              if (mps.response && mps.response.dart && typeof(mps.response.dart.adunits) === 'object') {\n                if (typeof(window._mpspixZ) != 'string') {\n                  window._mpspixZ = (function(a){var b=\"abcdefghiklmnopqrstuvwxyz\".split(\"\");a||(a=Math.floor(Math.random()*b.length));for(var c=\"\",d=0;d<a;d++)c+=b[Math.floor(Math.random()*b.length)];return c})(12)\n                }\n                for (var i in mps.response.dart.adunits) {\n                  var pixelurl = ((document.location.protocol === 'https') ? 'https' : 'http') + ':\/\/pix.nbcuni.com\/a-pii.gif?X=piiblock&S=' + mps.pagevars.instance + '&P=' + mps.pagevars.mpsid + '&A=' + i + '&U=' + encodeURIComponent(window.location.href) + '&_=' + window._mpspixZ;\n                  mps.response.dart.adunits[i].data = '<img id=\"div-gpt-x-0\" class=\"mps-slot\" data-mps-slot=\"x\" data-mps-loadset=\"0\" style=\"width:0;height:0;margin:0;padding:0;border:0;display:none;\" src=\"' + pixelurl + '\"\/>';\n                }\n              }\n              mps.cloneAd = function() { return false; }\n              return true;\n            } else {\n              return false;\n            }\n          })();\n        });\n        \/\/<!-- Google PII Fix END -->\n   }\n });\n});\n\n\n\n    var setAdblockerCookie = function(adblocker) {\n        var d = new Date();\n        d.setTime(d.getTime() + 60 * 60 * 24 * 30 * 1000);\n        document.cookie = \"__adblocker=\" + (adblocker ? \"true\" : \"false\") + \"; expires=\" + d.toUTCString() + \"; path=\/\";\n    }\n    var script = document.createElement(\"script\");\n    script.setAttribute(\"async\", true);\n    script.setAttribute(\"src\", \"\/\/www.npttech.com\/advertising.js\");\n    script.setAttribute(\"onerror\", \"setAdblockerCookie(true);\");\n    script.setAttribute(\"onload\", \"setAdblockerCookie(false);\");\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n\n\n\n\n    var admantx_url = ('https:'==document.location.protocol?'https:':'http:')+'\/\/usasync01.admantx.com\/admantx\/service?request=' + escape('{\"decorator\":\"template.nbc_template\",\"key\":\"62263fff3cc1d07f85c7f8261a0c8f7dc096b35f59c82a713f20a9db8d562ff2\",\"method\":\"descriptor\",\"filter\":\"default\",\"mode\":\"async\",\"type\":\"URL\",\"body\":\"' + escape(document.location.href) + '\"}');\n\n    function admantx_callback(data) {\n        if (top.__nbcudigitaladops_inject && top.__nbcudigitaladops_inject.dtprm) {\n            if (data && data.admants) {\n                if (data.status == \"OK\") {\n                    for (var x = 0; x < data.admants.length; x++) {\n                        var kv = 'adg=' + data.admants[x] + ';';\n                        top.__nbcudigitaladops_inject.dtprm(kv)\n                    }\n                } else {\n                    top.__nbcudigitaladops_inject.dtprm(\"asg=othererror;\")\n                }\n            }\n        }\n    }\n    (function() {\n        var sct = document.createElement('script');\n        sct.src = admantx_url;\n        sct.type = 'text\/javascript';\n        sct.async = 'true';\n        var domel = document.getElementsByTagName('script')[0];\n        domel.parentNode.insertBefore(sct, domel);\n    })();\n\n\n\n\n  if(typeof window.MediaSource !== 'function') {\n    if(typeof document.getElementsByTagName('meta')['tp:PreferredRuntimes'] === 'object') {\n        document.getElementsByTagName('meta')['tp:PreferredRuntimes'].setAttribute(\"content\", \"flash,html5\");\n    }\n}\n\n\n\n\n\n\n  Payments processor Square said Tuesday it is launching Square for Restaurants, a software platform for restaurant operations designed to bring more small businesses into the Square ecosystem. The software \u2014 which Square calls its most sophisticated software yet \u2014 ties together in one place all of a restaurant's operations, from booking tables to managing the after-meal check. And with the launch, the company's 2014 acquisition of on-demand food-delivery business, Caviar, finally makes sense. Shares of Square rose more than 4 percent following the news.  Not only does the new platform integrate Caviar \u2014 a food delivery service \u2014 into the restaurant's system, but it provides Square an opportunity to sell other products \u2014 like loan packages and accounting software. Square has long faced questions about Caviar, which it bought for a reported $90 million. Square successfully grew its business through payments and hardware. Takeout food delivery is a capital-intensive business that has attracted such players as Uber and Amazon, with far bigger driver networks and logistics experience. Some investors and analysts have questioned Caviar's strategic position within Square. In recent years, Square has been moving beyond hardware and deeper into financial services, even applying for a banking license. Square Capital \u2014 its loan platform for vendors \u2014 has become a contributor to the overall business and, according to management, drives greater merchant engagement across the entire Square ecosystem. Caviar has also been a way for the company to further integrate small businesses into its growing ecosystem. In the company's 2017 shareholder letter, CEO Jack Dorsey wrote: \"Square's expertise in point of sale, order management, and services ... help sellers grow differentiates Caviar from food delivery services.\" Now, Square is making it official with a dedicated platform that aims to be the only software restaurants need. Square says the software has the ability to update menus and floor layouts remotely while also offering performance tracking, tip splitting and fraud protection. Square for Restaurants is the company's third point of-sale-offering, joining Square for Retail and Square Appointments for service-based businesses. Dorsey is also CEO of Twitter.  Playing Share this video... Watch Next...","time":1525793401,"title":"Square announces Restaurants platform","type":"story","url":"https:\/\/www.cnbc.com\/2018\/05\/08\/square-announces-restaurants-platform.html","label":0,"label_name":"biz-news"},{"by":"ccccccccccccc","descendants":0,"id":17021624,"kids":"None","score":1,"text":"Are you looking for a stock? Try one of these Are you looking for a stock? Try one of these More More Latest Videos {{ video.Name }} The information you requested is not available at this time, please check back again soon. Most Popular         \n                        6h ago\n             \n April Fong, BNN Bloomberg  The stock symbol {{StockChart.Ric}} does not exist Canada is increasingly being viewed as a \u201cbrilliant place\u201d to build technology companies, and the idea that entrepreneurs don\u2019t have Canadian companies to look up to is a \u201cfalse narrative,\u201d according to Shopify\u2019s founder and CEO. Tobi L\u00fctke said in an interview with BNN Bloomberg\u2019s Amanda Lang that while he had opportunities to move the Ottawa-headquartered ecommerce company elsewhere, he has been very \u201cpro-Canada.\u201d\u00a0\u00a0\u00a0 \u201cOne gauntlet that everyone has to run when building a company in Canada is that it\u2019s lacking a little bit in role models of companies to be like. That\u2019s a false narrative,\u201d L\u00fctke said. \u201cThere are really good reasons why this happened, wonderful books written on various failures that people tend to point to. But they all happened because of internal failures and not for systemic, Canadian reasons.\u201d L\u00fctke, a German immigrant,\u00a0said that global technology companies already have an advantage in Canada because the country\u2019s cultural norms and values are \u201csignificantly more progressive\u201d than in other countries. Brian Madden, senior vice-president and portfolio manager at Goodreid Investment Counsel, joins BNN to discuss Shopify's earnings and the dropoff in the stock price. \u201cYou\u2019re seeing a lot of interest in building companies in Canada for these reasons or relocating to Canada for these reasons,\u201d L\u00fctke said. \u201cIt\u2019s a brilliant place to build companies, and I think that\u2019s generally better understood now and I think you\u2019ll see a lot more of that.\u201d L\u00fctke\u2019s comments come as Canadian business leaders and experts warn that the country\u2019s global competitiveness is eroding in the face of aggressive U.S. tax cuts and trade uncertainty.\u00a0 Indeed, Jim Balsillie, co-founder of BlackBerry (formerly known as Research In Motion), recently told BNN Bloomberg some Canadian tech firms are considering moving operations to the U.S.\u00a0 \u201cI think people starting a business in Canada already have a leg up, because the world is global, internet is global,\u201d L\u00fctke said. L\u00fctke is in Toronto this week for Shopify\u2019s annual developers\u2019 conference, Unite, which is making the city its permanent home after being held in San Francisco for the past two years. \u201cWe have a community where people travel from all over the world, and it is better to hitch our wagon to a city that\u00a0puts multiculturalism on its banner,\u00a0than technology,\u201d L\u00fctke said. \u2018THINGS ARE PRETTY GOOD\u2019 L\u00fctke also said he isn\u2019t worried about investors\u2019 reaction to Shopify\u2019s latest quarterly results, reiterating that the company\u2019s growth was always bound to slow. Shopify\u2019s gross merchant volume, a closely-watched figure for the ecommerce industry, rose 64 per cent in the first quarter, down from 81 per cent in the same period last year. \u201cIt\u2019s worth pointing out that there never has been a software services company that has grown at a faster rate \u2013 hit $500 million worth of revenue \u2013 than Shopify. \u2026 So, I think things are pretty good,\u201d L\u00fctke said.\u00a0 \u201cSome people had to look at our results and say, \u2018Okay, physics is still a thing.\u2019 We have not actually managed to completely invent something that no one\u2019s ever seen before.\u201d Shopify has also been in the spotlight recently as short-seller Andrew Left of Citron Research attacked the company\u2019s business model and marketing practices in October 2017, and again in March in the wake of Facebook\u2019s data scandal. L\u00fctke has vehemently defended his company, going as far as calling Left a \u201cshort-selling troll.\u201d\u00a0 Lots of people want me to address the short-selling troll thats targeting $SHOP. Looking forward to next earnings calls to do so L\u00fctke told BNN Bloomberg that while short-sellers can have a \u201cpositive\u201d effect on Wall Street\u2019s overall valuation of different stocks, some aren\u2019t so helpful. \u201cThis sort of ideal of what Wall Street is supposed to do\u00a0 \u2013 which is, over time, average the value of a stock to know the real market value of a company \u2013 is really hard to do without the short-selling side,\u201d L\u00fctke said.\u00a0 \u201cThere are some specific instances of short-sellers that I wouldn\u2019t necessarily include in this very productive piece of the short-selling world.\u201d Related Google I\/O Conference: What we can expect Lorne Steinberg discusses Netflix Apple shares rise after Buffett endorsement The Big Three: Oil climbs above US$70; NAFTA talks resume; Buffett shows his love for Apple James Murray discusses Apple Why Wedbush's Winer believes it's time to stock up on Walmart and Facebook","time":1525793391,"title":"Shopify CEO says Canada is 'brilliant place' to build tech firms","type":"story","url":"https:\/\/www.bnnbloomberg.ca\/shopify-ceo-says-canada-is-brilliant-place-to-build-tech-firms-1.1073300","label":7,"label_name":"random"},{"by":"devy","descendants":0,"id":17021617,"kids":"None","score":1,"text":"Elon Musk for once under promised. Our estimates show Tesla Semi drivers could recoup the higher cost of an electric truck 7 months ahead of Musk\u2019s promise of 2 years. They would also go on to save over $229,000 in operating costs over a million miles compared to diesel trucks, $29,000 more than what Musk announced. Tesla is however forsaking $7.7 billion dollars in cumulative revenue over 10 years by choosing to build semi trucks instead of a delivery trucks. Missing a great opportunity. Sales of semis and demand for heavy freight are expected to follow relatively flat GDP growth. On the other hand, sales of delivery trucks are expected to increase faster due to growing demand for last-mile e-commerce deliveries. So why build a semi? To save the planet. Semi trucks travel far more miles than delivery trucks. They in turn use more fuel and emit more pollution. A fleet of Tesla Semis would save more than four times the amount of carbon emissions than a fleet of Tesla delivery trucks. We estimate that a fleet of Tesla Semis could go on to save 11,173 millions lbs of CO2 from the atmosphere by 2025. Enough to single-handedly meet 30% of target carbon reductions asked from trucks by the Paris Climate Change Accord. The biggest challenge facing Tesla is mass manufacturing. Its anti-Toyota manufacturing philosophy of relying on robots rather than humans is proving ineffective. Tesla lags industry peers in both quality and cost. Not to mention its slow production line is failing to meet demand. Trucking companies will not be as patient as consumers waiting for their vehicles. No truck means no revenue. Business customers will be prone to cancel orders and buy from more reliable competitors. We also expect driving electric trucks to significantly lower operating costs for freight companies, but fail to boost profit margins. Trucking is highly segmented and hyper-competitive. Lower operating costs will likely lead to a price war. The real winners will be freight customers, electric truck manufacturers, and of course the environment. The age old problem of moving stuff from point A to B has grown enormous. Truck transportation is now a $700B+ market in the United States. Almost 4% of GDP. Trucks haul over 10 billion tons of goods every year, 60% of total U.S. freight. More than trains, boat and planes combined. A sea of companies share this opportunity. Schneider National, one of the few publicly traded freight companies, describes the U.S. trucking industry as highly fragmented. There are over 550,000 trucking companies: 90% of them own less than 10 trucks, some 50 carriers have revenues exceeding $100M per year, and only 10 carriers have revenues of more than $1B. Everything considered, it\u2019s relatively easy to buy a truck and start delivering. Truck shipping is effectively a commodity. Two major events shaped the trucking industry we know today: 1) president Eisenhower kicking off construction of the Interstate Highway System in 1956, paving the roads on which trucks drive; and 2) deregulation in 1980 resulting in many more trucking companies and drivers. Not much has changed since then. Until November 16th 2017. The day Elon Musk announced Tesla was building a semi-trailer truck: A class-8 tractor truck with a range of 300 or 500 miles, capable of recharging in 30 minutes. Promising much lower operating costs than diesel trucks. This was the sound of music to truckers with razor thin margins. In this investigation, we:  Let\u2019s first take an in-depth look at the business of long-haul trucking Tesla aims to disrupt. We\u2019re defining long-haul trucking as any delivery trips over 500 miles, requiring at least one overnight stay away from home. Research by the American Transportation Research Institute estimates 52.3% of all freight routes require overnight stays, while a further 6.2% of long-haul routes are served by team drivers (one sleeps while the other drives). Such trips represent 31% of freight by value and 11% by weight. 2.75 million tractor-trailer combination trucks handle this work. Together they travel over 170 billion highway miles a year, using 28.9 billion gallons of diesel. They covered 56% more miles and use twice the amount of fuel compared to the 8.46 million class 3 to 7 single unit trucks (cargo box and engine are on the same frame). Intense competition leads to low profit margins The result of a highly fragmented trucking market is intense price competition, thus thin profit margins. Even for the big guys. YRC Freight, one of the largest publicly traded trucking company, had an income of $41.4 million dollars on revenue of $3.07 billion dollars in 2017. That\u2019s a margin of 1.35%. YRC\u2019s profits are on the low-end of industry average, but not many competitors are swimming in cash: Table: 2017 sample operating margins of publicly traded trucking companies The most significant operating expense aside from driver wages is fuel. Research estimates the median percentage of operating costs spent on fuel at 24%. That ranges from as high as 30% for smaller fleets and 16% for the largest fleets. That\u2019s partially because larger fleets can afford longer payback periods when investing in fuel saving technologies such as aerodynamic modifications: 24 months as median for fleets of 500+ trucks versus 12 months as median for fleets of 1 to 20 trucks. Trucks in larger fleets tend to therefore achieve better mileage. Small margins, increased regulation, and retiring owners of family-operated truck companies are also pushing the industry to consolidate. This helps truckers gain cost advantage through high volume and lower overhead. Tesla takes advantage of forced resting time for drivers The other variable limiting profitability is the driver. Humans drive trucks and humans need rest. Regulations around active driving time for truckers are strict. Drivers are allowed to work for a total of 14 hours per day, during which they can drive for 11 hours. They must also take a 30-minute break after driving for 8 hours straight. Mr. Musk is betting big on those 30 minutes. They would allow drivers to recharge their electric trucks up to 80% capacity using a megacharger without feeling like they\u2019re wasting time. A diesel truck fills up in 15 minutes by comparison. The law is on Mr. Musk\u2019s side. Safety is no joke and Federal Motor Carrier Safety Administration is pushing ahead with mandatory electronic logging of driving time for all commercial trucks. Growth in long-haul shipping is chugging along, but lags GDP The value of truck transported goods is projected to grow at an average of 1.6% annually until 2045, while weight of goods trails behind at 1.15%. That\u2019s because more and more goods shipped are high-value low-weight. Think iPhones and Chinos pants rather than dirt, wood and fuel. Growth in tractor-trailer truckload transportation has effectively lagged behind GDP growth since 2000. A rapid increase in our services economy explains one side of the story. Another reason lies with the miniaturization of freight goods. Not only do transported goods weigh less than before, they\u2019re also smaller, need less space, and are cheaper to ship. The number of tractor-trailer trucks on the road grew 5.4% annually between 2013 and 2015, following a decline in wake of the recession from 2010 to 2013. There are currently more than 2.75 million semi trucks on the roads. This recent increase in capacity is forcing freight companies to compete aggressively by slashing prices. Supply is outpacing demand. Excluding revenue growth from mergers and acquisitions, many freight transportation companies have seen revenue decline in the past two years:  How have class-8 semi truck sales fared?  Sales bottomed in 2009 and started to recover afterward. Growth averaged 18.79% between 2010 and 2015, largely due to a sales boost of 59.9% in 2011 to make up for slack in capacity. The number of new trucks sold peaked in 2015 with close to 250,000 new trucks hitting the road. This large increase in capacity is what created a supply\/demand imbalance, putting pressure on shipping prices. 2016 then saw a 22.6% decline in truck sales as freight companies tightened capacity. The largest drop since the recession. Truck manufacturers remain nevertheless optimistic. With the economy on fire, increasing in consumer spending, and strong commercial and residential construction, Paccar, the manufacturer of Kenworth and Peterbilt semi trucks, expects record freight tonnage and high fleet capacity utilization in 2018. Navistar, manufacturer of International trucks, is seeing a 151% increase in orders for its class 8 trucks. Another indicator of stronger truck sales is the fact fleets will be replacing a large number of equipment in the coming years. Large fleets replace their equipment after 3 to 4 years. For example, the average truck in Knight Swift\u2019s fleet is only 1.9 years old. Newer trucks are simply cheaper to maintain. With most fleets having a large number of 2016 trucks, we can expect 2019 and 2020 to see a high number of new trucks sold to replace them. Just when Tesla\u2019s Semi plans to hit the market.  Freight companies rely on two elements to sustain growth: 1) Drivers; and 2) Trucks. Drivers are hard to come by. Turnover in long-haul trucking is sky high at 95%+. Knight-Swift notes in its annual report that this tends to be a bigger problem when the economy is good and jobs are plentiful. Truckers can easily find jobs locally so they can go home for dinner. A low unemployment rate of 4.1% therefore doesn\u2019t help. As result, companies pay long-haul drivers 30% more in wages than local drivers. The ability to buy new trucks can also present a challenge for freight operators. Knight-Swift specifically lists \u201cdecreased availability of new revenue generating equipment, and the failure of manufactures to meet their sale obligations\u201d as a risk. If production is delayed, freight companies will have to use older trucks that cost more to operate. Eating into margins. Diesel tractor trucks are the backbone of long-haul trucking. Why diesel rather than gasoline? Diesel fuel contains 14% more energy. This allows for better fuel efficiency and more power. Diesel engines also generate more torque (rotational force) at low engine RPM. They can therefore accelerate with heavy loads faster than gasoline powered vehicles. For example, a diesel Ford F150 pickup truck achieves peak torque of 440 lb-ft at 1,750RPM, versus peak torque of 375 lb-ft at 3,000RPM in a gasoline version. Let\u2019s now take a close look at the operating cost of a diesel truck: Truck price & depreciation This means the value of a semi truck depreciates at $0.35\/mile in the first two years of operation, and $0.14\/mile from year 3 to 5. Large fleet operators such as Knight-Swift tend to replace their trucks after 3-5 years to optimize maintenance costs. The American Trucking Research Institute reports that median fuel efficiency varies between 6.3MPG to 6.8MPG depending on a company\u2019s fleet size. For our investigation, let\u2019s assume tractor-trailers operate at 6.55MPG (average of median values found by ATRI). With diesel price projected at $2.86\/gal in 2019, We can thus estimate fuel cost at $0.437\/mile ($2.85\/gal \u00f7 6.55miles\/gal) when Tesla\u2019s Semi hits the road. (How we got this number: $76.25 million \/ 4,286 trucks) (How we got this number: Knight-Swift reported $129.7M on fuel. This translates into 56.15 million gallons of diesel when it cost $2.31\/gal in 2016. Their trucks therefore drove around 368 million miles assuming a fuel efficiency of 6.55MPG. Each of their 4,286 trucks in turn averaged 85,805 miles.) (How we got this number: $17,790 \/ 85,805 miles) The average truck in Knight-Swift\u2019s fleet was 1.9 years old, so our maintenance cost per mile can be considered typical of any large operators who replace trucks after 3-5 years. It\u2019s important to note that over half of the recommended maintenance tasks for class 8 trucks are unrelated to the diesel engine. That means an electric truck would also need them. We\u2019ll come back to this point as we review maintenance cost of the Tesla truck. 5,500 lbs of the truck\u2019s weight can be attributed to the diesel powertrain and fuel: A problem however exists with the parking infrastructure. Current demand for truck parking spaces exceeds supply. There are only 308,920 truck parking spaces available across the U.S. for over 2.7M registered tractor-trailers. As result, 75% of truckers report difficulty finding safe and legal parking during rest periods required by Federal Hours of Service regulations. This increases to 90% at night when drivers wait for their drop off destinations to open and accept deliveries, and sleep. Pilot Flying J even advertises the ability to reserve a parking spot as a competitive advantage over other service stations. Burning one gallon of diesel emits 22.38 lbs of CO2. That\u2019s 3.36 lbs CO2 \/ mile at a fuel efficiency of 6.55MPG for tractor-trailers. The figure above however doesn\u2019t account for the emissions generated producing diesel fuel. An investigation by the state of California reports the Well-to-Wheel (from extracting crude oil to burning diesel) emission of diesel to be 92g CO2 per MJ of fuel (0.203 lbs CO2 per MJ). This translates to 29.6 lbs CO2\/gal as diesel has an energy density of 146MJ\/gal. The total environmental cost of using diesel is therefore 4.52 lbs CO2\/mile. This of course varies per region depending on how diesel is produced and transported. Higher in areas that use less renewable energy to power refineries, and that have to ship fuel across longer distances. Burning diesel also emits NOx pollutants. They\u2019re responsible for acid rain, ozone destruction and many health issues. The good news is that the amount of NOx produced by diesel trucks has been steadily decreasing over the past decade. Largely because the EPA required the use of ultra-low sulfur diesel fuel in 2006. Yet trucks still produced more than 2.3 million tons of NOx in 2016. This is projected to decrease to 975,000 tons in 2030. While many people express concern for the environment, few are willing to make meaningful personal sacrifices. Mainly because we don\u2019t feel the economic cost of our emissions. But what if it were cheaper to be green? \u201cWhat do people want? They want reliability. They want the lowest cost. And they want driver comfort\u2026 So we reimagined the truck.\u201d \u2013 Elon Musk Mr. Musk is right. Trucking companies care about revenue and costs more than anything. Environment is not a priority, so he didn\u2019t even mention it. Mr. Musk instead promised to reduce operating cost by 17% or $0.25\/mile to appeal to customers. He pledged a 2-year payback period, and $200,000+ in fuel savings over a million miles. Let\u2019s put those numbers to the test. The 500 miles range model is 20% more expensive than a comparable sleeper diesel truck costing $150,000. What about resale value? A used Tesla truck\u2019s value will be strongly tied to its battery performance. A low-range battery limits the types of deliveries it can make. How long can we expect Tesla\u2019s battery to last? Tesla\u2019s vehicles (Model S, X, 3) use batteries composed of lithium, nickel, cobalt, aluminum oxide (NCA) and have a lifetime of 1,000 to 1,500 cycles. This means batteries will only have a capacity of 80% afterward (defined as end-of-life). The bigger truck\u2019s range would then decrease to 400 miles. For consumers that only drive about 15,000 miles a year, these batteries can easily outlast the vehicle itself. A Model S with a range of 300 miles only goes through 50 to 60 full cycles a year. Hence Tesla\u2019s generous 8-year infinite mile warranty. Trucks are however on the road a lot more. Knight-Swift\u2019s trucks average 85,805 miles a year. This translates into 190 cycles a year, based on 2 cycles a day: One overnight cycle that allows 500 miles range and one megacharger cycle that allows 400 miles range. The truck\u2019s battery would therefore hit end-of-life in 675,000 miles or 8 years. Interestingly, hitting end-of-life won\u2019t affect the Tesla truck\u2019s daily maximum range. Daily range is limited by the trucker, who can only drive 660 miles a day due to regulations: 400 miles at 60mph in 6.67 hours + 260 miles in 4.33 hours = total of 660 miles in 11 hours. At end-of-life or 80% capacity, a 500-mile range Tesla can still do a max of 720 miles a day on 2 charges (400 miles after overnight charge + 320 miles after megarcharger). More than the 660 miles a driver is allowed to drive. The functional value of Tesla\u2019s batteries are therefore the same as new even when end-of-life is reached. The perceived value will however change. What a 500 mile range of a new truck has over a 400 mile range is extra freedom in choosing a recharge location. If a route\u2019s closest megacharger station is 401 miles from starting point, the truck is useless. A truck\u2019s value is therefore tied to the availability of megachargers along its route. Let\u2019s assume for calculation purposes that the battery\u2019s value decreases at double the rate to its capacity. One-fold for losing capacity and another for losing freedom on where to refuel. So a battery retains 60% of its value when capacity hits 80% at end-of-life. The price of lithium batteries is predicted to decrease at an annual rate of 5% as batteries become more abundant and cheap to manufacture. Assuming the rest of the truck depreciates at the same rate as its battery, the Tesla truck is expected to depreciate at a rate of $0.16\/mile. Less than half that of new diesel trucks. An $180,000 Tesla semi will still be worth $71,600 after 675,000 miles or 8 years. The same price as a much newer 2 to 3-year-old diesel truck. The fact Tesla cars retain better resale value than competitors has been proven in the consumer market. A Model S loses 28% of its value after 50,000 miles versus 40% for German luxury car makers. Large fleets that replace trucks after 3-5 years could therefore recoup much more money. At 6.55MPG, a diesel truck requires about 6.32kWh of energy from 0.152gal of fuel to move one mile (at 60mph). Tesla\u2019s semi is expected to only use 2kWh of electricity do the same. A 70% saving in energy. Mr. Musk further guarantees a charging rate of 7 cents per kWh at future megacharger stations. This means truckers will only pay $0.14\/mile for fuel. Compared with $0.437\/mile for diesel trucks. That\u2019s if Tesla manages to stay in business. The average commercial rate for electricity in the U.S. is around 10.3 cents per kWh. But electricity price also varies by the peak rate at which electricity is drawn from the grid, so Tesla\u2019s megacharger stations are likely to pay a premium. It\u2019s estimated Tesla\u2019s true cost to be around 40 cents per kWh. The market rate for energy required by the Tesla truck is therefore $0.80\/mile with a megacharger, and $0.26\/mile with normal charging. Using a megacharger without subsidies is twice as expensive as diesel at $0.437\/mile. Diesel fuel would have to cost $5.25\/gal to match the megacharger\u2019s market cost. Guaranteed cheap electricity is thus a critical part of Tesla truck\u2019s appeal. What may help Tesla lower charging costs includes batteries installed at charging sites that can draw energy from the grid at normal rates, and discharge into trucks without peaking the grid. Such technology is already in use by a fleet of UPS delivery trucks in London. Add some solar panels and it\u2019s possible Tesla\u2019s electricity will only cost around 10 cents per kWh. Normal market rate. All this would however necessitate a large capital investment. Teslas are also more expensive to repair. So much so AAA raised insurance premiums for Tesla drivers. Consumer Reports currently gives Tesla an \u201cAverage\u201d mark for reliability. Drivers have complained of assembly issues and driving problems that degrade the experience of a luxury car. As result, Tesla also incurs higher warranty costs than peers. These problems are less because of vehicle design than of poor manufacturing. Let\u2019s look at how existing Tesla vehicles fare relative to peers to estimate maintenance cost of their truck. We compared below the cost to maintain and repair a Model S ($74,500+) against that of a similarly luxurious Audi A7 ($69,700+) and the uber reliable Toyota Camry ($23,495+). Annual Maintenance Cost A Tesla is slightly less expensive to maintain than an Audi, but 40% more expensive than a Camry. Taking the average maintenance and repair cost of Audi and Toyota, we can expect a Tesla to be 8.5% more expensive to maintain than the average vehicle. Assuming a similar ratio for semi trucks, we can expect the Tesla semi to cost around $0.225\/mile to maintain and repair compared to $0.207\/mile for average diesel trucks. Tesla\u2019s semi truck needs a battery pack of 1,000kWh to achieve 500 miles at 2kWh\/mile. This translates into a weight of 6,667Kg or 14,700 lbs as lithium batteries are expected to have an energy density of 150W\/Kg in 2020. Some are more optimistic about Tesla\u2019s engineering capabilities and estimate the semi\u2019s batteries will only weigh 11,000 lbs. That\u2019s still double the weight of a diesel powertrain (5,500 lbs). This means 5,500 lbs less haulable load than a Diesel truck. Does it matter? A 48\u2019 trailer has an interior volume of 3,566CuFt. A Tesla truck will only be able to carry a max load 39,500 lbs in this space while a diesel truck carries a max load of 45,000 lbs. It will therefore matter if payloads include goods with a density of 11.08 lbs\/CuFt or more. A Diesel truck could carry 900 more gallons of gasoline (45 lbs\/CuFt), 300 more MacBooks (18.5 lbs\/CuFt) or one more Model S sedan (4,700 lbs). This could affect Tesla truck\u2019s ability to compete for shipments of high density freight worth more than 5 trillion dollars: i.e. electronics, motorized vehicles, gasoline, fuel oils, etc. These commodities account for more than 25% of all goods shipped. The Tesla semi\u2019s payload capacity however won\u2019t be a problem when carrying low density goods such as furniture, appliances or ping pong balls. Or if it carries less than a full payload. A trucker can drive 8 hours straight before needing to take a mandatory 30 minutes break. That\u2019s 480 miles at 60mph. Less than Tesla\u2019s 500 mile range. The driver can go for another 3 hours to complete his 11-hour daily driving limit. Adding 180 miles to the trip. This totals 660 miles a day. Range only becomes an issue if the battery doesn\u2019t allow the truck to cover 660 miles in a day with one megacharger cycle. Or when x + 80%*x < 660 miles, where x represents the truck\u2019s range. That\u2019s 367 miles solving for x. That\u2019s also when battery capacity drops below 73.6%. Way past the end-of-life expected for the battery at 675,000 miles or ~8 years of driving we previously calculated. 367 miles is also the minimum distance a Tesla semi has to travel before megacharging to achieve the optimal daily travel distance of 660 miles. So an operator needs to make sure a megacharger is located 367 miles to 480 miles from the starting point. This may limit the truck\u2019s adoption rate. Tesla\u2019s megacharger construction plans are unclear. Looking at North American supercharger stations as proxy, their numbers grew 100% annually from 2014 and on. From 50 in 2014 to 361 stations in 2017. Even if megacharger stations grow at this rate, it\u2019ll still be a tiny number compared to the existing 156,000+ diesel stations. This will affect the number of routes electric trucks can cover. It appears the first megachargers will be found at regional distribution centers of Tesla partners (likely close to metropolitan regions). Beyond Tesla partners, only trucks operating routes that pass by these stations will be able to complete their journey. Tesla trucks also need to find a place to charge up overnight. That\u2019s another problem considering the lack of parking spaces even for diesel trucks. Needing an electrical outlet won\u2019t help. Cold weather will also limit the routes Tesla trucks can operate on. According to recent research, lithium-ion batteries have reduced capacity in freezing temperatures due to bad charge transfer, low electrolyte conductivity, and reduced solid-state diffusivity [1]. That\u2019s bad news as half of America experiences freezing winters between December and February. It\u2019s been found that for the capacity of a Li-ion cell at -20\u2103 is only 60% of its room-temperature value [2]. To combat this issue, electric vehicles actively manage the batteries\u2019 temperature using heaters. Two different sources (Teslarati, Consumer Reports) report a reduced range of ~180 miles in real-life winter conditions for Tesla\u2019s 265 mile range Model S P85. That\u2019s a 30% drop in range. Enough to limit the daily driving range of the truck. One solution may be to maintain a mixed fleet of electric and diesel trucks. Sending the electric trucks south and diesel trucks north in winter months. And the opposite in summer months. Diesel trucks have no issues in winter as long as winter fuel with additives to lower solidification temperature is used. The department of energy estimates emissions from electricity production at 1,085 lbs CO2\/MWh. For a Tesla truck consuming 2kWh\/mile, this translates to 2.17 lbs CO2\/mile. Less than half of diesel trucks at 4.52 lbs CO2\/mile. Grid emissions have also been steadily dropping since 1970, when they produced 1,540 lbs CO2\/MWh. Much more electricity was generated from coal plants then, which emit 2,200 lbs CO2\/MWh. A larger proportion of natural gas plants emitting 946 lbs CO2\/MWh and renewable energy plants (e.g. solar and wind) are expected to bring down grid emissions down to 750 lbs CO2\/MWh by 2040. One growing environmental problem facing electric cars is the fact lithium batteries are seldom recycled. It is currently five times more expensive to acquire Li-ion through recycling than it is to mine new lithium material. For recycling programs to become economically viable, we need [3]: So does Mr. Musk\u2019s promises in savings live up to the test? Our calculations prove so. Even more astounding, Mr. Musk was under promising. A diesel truck costs a total of $0.644\/mile for fuel and maintenance, compared with $0.365\/mile for Tesla. This amounts to operational savings of more than ~40% or ~$0.279\/mile. Beating Mr. Musk\u2019s estimate of 17% or $0.25\/mile in savings. When does a Tesla semi breakeven with diesel truck? After 120,000 miles or 1 year and 5 months if one drives 82,805 miles per year. 7 months ahead of Mr. Musk\u2019s promised break-even time of 2 years. This accounts for the $30,000 higher sticker price of the Tesla Semi and operational savings of $0.279\/mile.  An operator who decides to resell their Tesla truck after 5 years or 430,000 miles can also expect to recoup $100,000. $50,000 more than a diesel equivalent. They\u2019d have already saved more than $80,000 in fuel and maintenance by that time.  One could also decide to drive until Tesla\u2019s batteries hit end-of-life at 675,000 miles. Total operational savings will then hit $146,000. Potentially more if Tesla manages to keep maintenance costs steady while that of diesel trucks grow with age. Savings will finally amount to $229,000 should one decide to test Mr. Musk\u2019s 1 million mile guarantee. 14.5% more than Mr. Musk\u2019s estimates of $200,000. It\u2019s our belief all long-haul operators should take a serious look at Tesla\u2019s semi truck. Especially if you: With the right megacharger infrastructure, Tesla could be a fierce competitor in at least 41.5% of the U.S. semi truck market. (We use the percent of semi truck drivers in states with above freezing temperature as proxy for the share of trucks that operate there: 59%. We then account for the fact 75% of shipments are of low density payloads Tesla trucks can carry without disadvantages, and that a further 93.8% of routes are operated by drivers that can allow the truck to charge overnight and rest at the same time.) Another technology trying to displace diesel\u2019s monopoly in long-haul trucking is hydrogen fuel cell. The Nikola One semi being a contender. Its main advantages compared to electric trucks lie in the fact refueling time and driving range match those of diesel trucks. 15 minutes to refuel for a 1,200 miles range. The mental switching costs are thus lower than that of electric trucks. The main problem hydrogen faces is the high cost of infrastructure. A hydrogen refuel station is estimated to cost one to two million dollars to build versus $300,000 for an electric charging station. That\u2019s why hydrogen infrastructure is sparse, even after 50 years of development. Nikola is planning to produce hydrogen right at its refuel stations to lower costs. Eliminating the need to transport H2. Nikola is also promising free fuel to drive up demand. That has attracted over 8,000 pre-orders. Environmentally, hydrogen fuel cell technology emits almost the same amount of CO2 as diesel trucks. The problem is that hydrogen requires a lot of energy to produce. Electrolysis, the process of producing hydrogen by breaking up water molecules into H and O, is 80% efficient at best. To produce 1Kg of H2 with an energy density of ~40kWh\/Kg\u00a0requires\u00a050kWh of electricity. Hydrogen further loses energy when it is transformed into electricity via fuel cells to move a vehicle; The Nikola One can only transfer 70% of the energy from hydrogen into kinetic motion. Total energy efficiency of a hydrogen fuel cell vehicle is therefore only around 50% to 60%. That compares with 80%+ for electric drives: The Nikola One has reported consumption of about 4.6Kg H2\/100km. Since we need 50kWh of electricity to produce 1Kg of H2, energy consumption corresponds to 3.71kWh\/mile. We already know that electricity production in the U.S. generates 1.085 lbs CO2\/kWh. The Nikola One thus produces roughly 4.025 lbs CO2\/mile. That\u2019s double the emissions of Tesla\u2019s semi at 2.17 lbs CO2\/mile, and not much less than diesel emissions of 4.52 lbs CO2\/mile. Below is a chart showing how many trees are needed to offset the carbon emissions produced annually by each vehicle type. One tree can process 48 lbs CO2 per year for reference.  It\u2019s clear the environment will not benefit from fuel cell vehicles. They merely push the problem of carbon emissions onto electricity producers. Even fuel cell experts are lobbying against the use of hydrogen in transportation. That includes Mr. Musk who calls fuel cell cars \u201cincredibly dumb.\u201d Hydrogen fuel cell is largely a hedge automakers are making in case customers fail to adopt electric vehicles. Manufacturers and freight companies are scared to make an all-in commitment. There\u2019s too much uncertainty in the air. One question Tesla hasn\u2019t answered is why they didn\u2019t build a delivery truck instead of a semi. Delivery trucks represent a much bigger opportunity. For a start, there\u2019s 8.46 million single unit delivery trucks versus 2.75 million semis on the road. They deliver goods from distribution centers to brick and mortar shops and to end-users. Completing the last-mile. Despite being three times as numerous, they only traveled 109,597 million miles in 2015 compared to 169,830 million miles for semi trucks. These class 3 to 7 single unit trucks carry the bulk of local and regional shipments traveling less than 249 miles; representing 67% of total truck freight. The rapid growth of e-commerce is also boosting demand for short-haul shipping. Online sales grew 16% in 2017. As result, courier and package shipments are growing much faster than the 1.6% expected growth for overall truck freight. UPS\u2019s domestic packages segment experienced annual volume growth of more than 9% in the past two years, with trucking playing a vital role. Research indicates the development of next-day and 2nd-day courier services, often used by e-commerce stores such as Amazon, has made trucks critical to operations. They reduce airport congestion and allow greater schedule flexibility [4]. This unprecedented growth is even attracting long-haul freight operators into the courier business. XPO entered the U.S. home delivery business in 2013 and is actively seeking to grow their market share; specializing in last mile transportation for appliances, large electronics and heavy goods. Schneider National is following a similar growth strategy with their recent acquisitions of Watkins & Shepard and Lodeso. Both companies that handled last-mile deliveries of oversized items. Is the demand for class 3-7 single unit trucks growing just as quickly? Class 3 to 5 trucks, carrying out the bulk of urban deliveries, experienced average sales growth of 11.2% per year since 2010. From 204,000 vehicles sold in 2010 to 382,000 vehicles in 2016. Class 6 and 7 trucks, capable of delivering oversized items such as furniture, grew at a similar pace of 10.6% a year. From 67,000 vehicles sold in 2010 to 122,000 vehicles in 2016. Unlike semi trucks, sales have yet to peak.  Navistar International, one of the top single unit truck manufacturers, experienced a 16% increase in unit sales in Q1 of 2018 versus the same period last year. They also had a 43% boost in orders and 40% increase in backlog. Ford\u2019s F-Series trucks (mainly class 2 to 7 vehicles) saw a 2.2% increase in sales in January 2018 versus the same period last year. Utilimaster, one of only two manufacturers of walk-in vans used by UPS and FedEx (the other being Morgan Olson), saw their order backlog increase by 198.9% in 2017 with the award of a $214 million UPS contract. There\u2019s a clear opportunity for electric trucks to disrupt the single unit delivery truck market. Electric vehicles are highly efficient in the stop and go traffic delivery trucks find themselves in. A Dodge Ram Promaster diesel van achieves 13mpg in the city versus 18mpg on the highway. 28% less efficient in city driving. A Tesla Model X achieves 81mpg-e in city and 92mpg-e on the highway. Only 12% less efficient in city driving by comparison. Let\u2019s therefore imagine Tesla designed a class 5 electric delivery truck. One that any operator driving class 3 to 5 trucks could use to haul their loads. How would it compare against diesel delivery trucks? This translates into $0.26\/mile at 2019 diesel prices of $2.86\/gal. This translates into $0.07\/mile for fuel if Tesla sells electricity at 7 cents per kWh. What does this mean to a carrier like UPS? UPS operates a fleet of 101,863+ small package delivery vehicles worldwide. They travel anywhere from 60 miles to 100+ miles a day depending on whether they\u2019re delivering downtown or in suburbia. Let\u2019s assume they average 60 miles a day. At $0.26\/mile for 313 days a year (working 6 days a week), cost of fuel amounts to $497 million. That\u2019s compared to $134 million if they were to run an entire fleet of electric trucks sipping energy at $0.07\/mile. Savings of over $363 million or 75%. It\u2019s no surprise UPS is actively investing in electric trucks, but they remain an experiment. UPS operates 8,500+ alternative fuel vehicles, representing only 8% of their fleet. 84% of new vehicles purchased are still standard diesel trucks. As result, UPS still spent $2.69B on fuel in 2017 (for planes and trucks). A lack of charging infrastructure may very well be the biggest obstacle to the electric revolution. Is Tesla missing an opportunity by not selling an electric delivery truck? Let\u2019s compare the potential revenue of selling a semi vs. selling a class 5 delivery truck. First, we must estimate the market share Tesla can realistically gain. Elon Musk has set the bar at 100,000 semis to be sold annually by 2022. That\u2019d result in a market share of over 50% two years after launch as 200,000 semis are sold each year. And wishful thinking at best. Mr. Musk set a similarly outlandish goal for his Model 3 sedan in his Q1 2016 investor call. He envisioned 100,000+ Model 3s produced before end of 2017. He updated that figure to 20,000 in July of 2017. The reality? 1,550 Model 3s delivered in 2017. 1.5% of his original target. Mr. Musk clearly suffers from chronic planning fallacy. He\u2019s over-optimistic about Tesla\u2019s ability to hit goals. Overlooking obstacles and uncertainties on the way. For example, manufacturing problems could push back the semi\u2019s production schedule. Or freight companies could be slow to adopt electric trucks due to uncertainties around infrastructure. Perhaps planning fallacy helps to push ahead big dreams. For more realistic growth estimates, let\u2019s look at the story of Hyundai Translead. The top semi truck trailer manufacturer in 2017 with 19% market share. One would never have guessed that they only started selling trailers to U.S. freight companies in 1994. They took advantage of lower wages south of the border by establishing a plant in Mexico: $1.30\/hour in Mexico vs. $12.60\/hour in the U.S. That allowed them to offer lower prices and added value. They claimed 3.1% of the market share in the first year. Hyundai Translead\u2019s market share then grew on average 8.2% a year for the next 23 years. Nothing short of amazing. Many similarities exist between Tesla\u2019s Semi project and Hyundai Translead\u2019s early days. Both started with no market share in the trucking business. Promising lower operating costs and added value. There is one important difference though: Hyundai was already a manufacturing powerhouse when they entered the U.S. trailer market in 1994. They manufactured over a million cars that year. Tesla produced less than 30,000 cars last year. It\u2019s safe to say that replicating Hyundai Translead\u2019s growth would be a best-case scenario for Tesla.\u00a0Let\u2019s thus use Hyundai\u2019s market share growth as proxy to Tesla\u2019s future growth. We now need to forecast how the overall market for trucks will grow.\u00a0Two measures we found to predict truck sales include the industrial production index (IPI) and advanced retail sales (ARS). Assuming a linear relationship, we found annual sales of class 8 trucks to correlate meaningfully with the IPI, with a correlation coefficient of 0.654. We then found a strong relationship between annual sales of class 3 to 5 trucks and ARS, with a correlation coefficient of 0.94. These relationships allow us to perform linear regression to forecast future truck sales.   We can estimate future IPI and ARS figures based on historical growth rates from 2001 to 2017: 0.794% and 3.279% per year on average respectively. This in turn allows us to create trendlines to forecast future market size for truck sales. The number of semi and delivery trucks Tesla could sell can finally be estimated based on Hyundai Translead\u2019s market share over time: 3.1% of the market in 2020, their launch year, and all the way to 19% of the market 23 years on.  Potential annual revenue can also be estimated by assuming Tesla sells its semi truck and delivery truck at $180,000 and $105,000 per unit.  It becomes clear Tesla would make much more money selling delivery trucks. By 2030, Tesla could generate annual revenue of $5.2B by selling a class 5 truck compared to $3.65B by selling the semi. 42% more. The cumulative revenue forfeited by selling a semi instead amounts to $7.7B+ over these first ten years. During which Tesla could make $17.3B selling delivery trucks versus $9.85B selling semi trucks. The loss in cumulative revenue grows to $49.5B by the 20 year mark in 2040. The delivery truck market is simply larger and growing faster than the semi market. The loss in cumulative revenue of selling semi trucks instead of delivery trucks amounts to $49.5B after 20 years. Mind these are rosy forecasts that assume retail sales will keep growing at historical rates. And that no disruptive technology comes in to replace trucks altogether. Even more appalling is the amount of profits Tesla\u2019s giving up by not selling delivery trucks. Let\u2019s compare the cost differences of producing both vehicle types: Tesla will be selling electricty at $0.07\/kWh to commercial truck customers. As previously explored, megachargers are expected to cost of $0.40\/kWh due to peak rates, and overnight normal charging will cost $0.103\/kWh (average electricity rate). Electricity is likely to cost $27,599 per truck per year, even after accounting for income of 7 cents \/ kWh. Most delivery trucks won\u2019t travel more than 200 miles per day, so won\u2019t need megachargers. They\u2019ll simply recharge overnight at normal rates. This means a delivery truck traveling an average of 13,116 miles a year will cost Tesla $1,350.95 to charge ( $0.103\/kWh). Electricity will only cost $433 per truck per year, after accounting for income of 7 cents \/ kWh. In 2020, lithium batteries are expected to cost ~$180\/kWh If Tesla manages to replicate Hyundai Translead\u2019s success and gain 3.1% market share in the first year, it would sell 7,271 semis or 14,055 delivery trucks in 2020. Under the semi truck scenario, it would lose at least $200 million. Under the delivery truck scenario, it could very well turn a positive profit. Tesla could also save millions in capital expenditure: Delivery trucks don\u2019t need megacharger stations. Most of them can charge overnight in operators\u2019 parking lots. So why is Tesla building a tractor truck instead of a delivery truck? Certainly not to deliver above average returns to investors. Maybe to save the planet? Let\u2019s compare the environmental effects of electric semis versus that of delivery trucks. Tesla could have 57,418 semi trucks on the road by 2025 based on our growth model. Or they could have 118,979 delivery trucks. This is assuming every truck produced from 2020 and on stays on the road. The fleet of semis would travel a total of 4,754 million miles that year versus 1,560 million miles for the fleet of delivery trucks. A fleet of Tesla semis could therefore save 11,173 million lbs of CO2 from the atmosphere versus 2,536 million lbs of CO2 for a fleet of delivery truck. More than four times the amount of CO2. Semi trucks in the U.S. simply travel far more miles, use much more fuel, and in turn emit a lot more pollution than delivery trucks. Even though there are less of them. (Reminder: Each Tesla semi emits 2.17 lbs CO2\/mile versus 4.52 lbs CO2\/mile for diesel semis; compared with 1.085 lbs CO2\/mile for electric delivery trucks and 2.71 lbs CO2\/mile for diesel delivery trucks.)  A fleet of Tesla semi trucks could as result significantly help the U.S. meet the Paris Climate Change Accord. That agreement calls for a 26% reduction in carbon emissions in 2025 from 2005 levels. The U.S. emitted 7,313M metric tons of CO2 in 2005. The goal is thus to emit 5,412M metric tons of CO2 in 2025. Americans generated 6,587 metric tons of CO2 in 2015, so we still need to cut 1,175M metric tons or 2,590B lbs of CO2 per year. About 1.45% of that amount or 37.54B lbs CO2 comes from trucking. A fleet of 57,418 Tesla Semi trucks could single-handedly cut that by 30%. A fleet of Tesla Semi trucks could single-handedly hit 30% of carbon reductions called for trucks by the Paris Climate Change Accord. Mr. Musk and Tesla are effectively building a semi truck to save the planet. Money comes second. That leaves the delivery truck market for others to profit. Daimler is already experimenting with their Vision One and eCanter trucks for regional and local deliveries. Workhorse is working with UPS and others to build electric walk-in vans for courier service. We also find startups such as Chanje and Boulder Electric fighting for a piece of the pie. A pie that\u2019s big enough for many players to thrive. Maybe even robots. Tesla\u2019s Semi truck program faces many obstacles as they: Let\u2019s explore these issues in detail. It\u2019s unclear how truly committed Tesla is to the truck project. Tesla\u2019s VP of Trucks, Jerome Guillen, is the only Tesla executive on LinkedIn to have any experience building class 8 trucks. He helped with new product development at Freightliner from 2002 to 2007. The rest of Tesla\u2019s executive team have only helped build passenger vehicles: Tesla also doesn\u2019t appear to have a dedicated team working on the truck. Searching for \u201ctruck\u201d or \u201csemi\u201d on their career page leads to no results. Engineers working on the Model 3, such as Aaron Johnson or Rafath Rahman, also list the semi truck design as part of their list of responsibilities. It appears Tesla is relying on existing talent for the truck project. That spreads their engineering team really thin. They\u2019re also unlikely to prioritize work on the truck when grappling with model 3 production and quality problems. Tesla may be underestimating the differences in producing semi trucks and luxury cars. Only one of the top 6 tractor truck manufacturers also makes passenger cars: Freightliner\u2019s parent company Daimler makes Mercedes. Even then, Freightliner stands as an independent entity. Mr. Musk has also promised a new roadster to be launched alongside the semi truck, and is even considering designing a new light truck. All distractions that could hinder the semi truck launch date and production. Beyond limited expertise and dedication to building the semi truck, Tesla also runs the risk of losing critical talent. A string of top leaders have already left Tesla this year, including: Many other leaders have also left the organization in 2016 and 2017: Greg Reichow may be the most missed alumni. Before joining Tesla, Greg had the unique experience of jumpstarting and ramping up manufacturing at SunPower from 2003 to 2011. Designing and building factories that produced solar panels from the ground up. It\u2019s possible Tesla\u2019s Model 3 production ramp up could have been spared some pain should Greg still be around. That leads to our next risk: production. Tesla will likely face problems mass producing semi trucks. Just as it\u2019s struggling to do with the Model 3. Truck operators will not be as patient as consumers. The inability to acquire new revenue generating equipment (trucks and trailers) is cited by operators as a clear risk in their financial reports. It\u2019s very likely they\u2019d cancel orders and buy diesel trucks instead when faced with delays. No truck = no shipments = loss of business. Operators are thus much more likely to choose to make less money than making no money at all. Mr. Musk needs to prove it can consistently meet production targets in the B2B realm. Tesla\u2019s production philosophy may however hinder progress. Its approach to manufacturing is the complete opposite of Toyota\u2019s lean manufacturing system. A system that tops both quality and quantity. Toyota developed its \u201cjust in time\u201d production system to tackle three problems [5]: Tesla faces the same problems, but deals with them very differently. Let\u2019s compare the two schools of thought. One priority is to minimize dead time spent resetting and configuring machines necessary for short production runs of different car models. So it seldomly uses sophisticated machines. Allowing workers to quickly tweak and improve processes. Toyota can in turn quickly change production schedule based on the latest sales trends. One key characteristic of the process is Toyota\u2019s emphasis on putting humans at center of manufacturing. Not robots. As Wil James, president of manufacturing in Kentucky, shared: \u201cMachines are good for repetitive things\u2026 but they can\u2019t improve their own efficiency or the quality of their work.\u201d Toyota is effectively relying on continuous improvement to lower costs. Betting that building things right the first time, and constantly getting better at it, is cheaper than an automated production line that cranks out poor quality products they need to fix later. Mr. Musk explains: \u201cYou really can\u2019t have people in the production line itself. Otherwise you\u2019ll automatically drop to people speed\u2026 There\u2019s still a lot of people at the factory, but what they\u2019re doing is maintaining the machines, upgrading them, dealing with anomalies. But in the production process itself there essentially would be no people.\u201d Mr. Musk\u2019s words have been backed by heavy investments in manufacturing equipment. Assets listed under \u201cProperty, plant and equipment\u201d grew from $3.4B in 2015 to $10B in 2017 as the Model 3 production line came online. Mr. Musk is effectively relying on machines to achieve economies of scale and lower production cost. Betting that robots can crank things out faster than humans. This requires a production line that never stops. As result, Tesla\u2019s manufacturing process tends to be inflexible and unresponsive to changes in the market. There\u2019s little opportunity to improve. Because Tesla\u2019s automated production line can\u2019t quickly adapt to changes in demand for its cars, pre-orders are a necessity. Not a nice-to-have. Any production line can be stopped by workers at any given moment to prevent making too many parts, control for defects, make improvements, or prevent accidents. Allowing for continuous improvement. One line worker recalls a moment when a robot broke down and the supervisor came screaming: \u201cThat\u2019s $18,000, $20,000, $30,000, $50,000 because you guys can\u2019t get this done.\u201d Another employee shared that safety is not a top priority. Tesla\u2019s reliance on automation to lower costs means it cannot afford to stop the machines. Even when improvements need to be made. A robot offline is a robot losing money. Since we humans tend to hate losing money, as described by prospect theory, Tesla is likely to forgo small improvements to their production line they perceive as money losers, and only stop machines for major works. Such as the recent days long production pause in February and again in April. It\u2019s clear they have difficulty tweaking on the go. This means all production lines need accurate knowledge of timing and quantity needed for their products. Information on priority of orders is gathered from the final assembly line and shared with foremen on subassemblies. These front-line leaders then decide on job dispatching and overtime. Control is effectively decentralized. This tends to be expensive not just in terms of storage costs, but also because parts may become obsolete should designs change. Poor management adds fuel to the fire. One employee complained that the company is disorganized from the top down, with priorities changing daily. Another shared that nobody knows what needs to be done on a daily basis, leading to much more confusion and frustration. These signs point to information bottlenecks and centralized command. Setting up a new production line is a matter of building it up slowly, making piecemeal improvements in the process over years. Toyota believes testing for quality is far more expensive than building it right the first time. Employee comments show that Tesla believes trial production runs are pointless. They jump right into production. Coupled with sky high goals, this leads to all hands on deck situations to meet targets. This is consistent with Silicon Valley\u2019s move fast break things mentality and agile software development philosophies. The difference in manufacturing however is that one can\u2019t patch problems later with a software update. Customers must take time away from their lives and visit the shop. Not to mention increased warranty cost for Tesla. Data Sources: See references [5] and [6] Toyota\u2019s human-centered system is best suited to produce different products that change over time. It makes full use of humans\u2019 ability to improve. To spot problems, identify their root cause, and creatively imagine better solutions. By contrast, Tesla\u2019s robot-centered system is best suited to produce one product that meets the needs of many. Manufacturing researcher Andrew Sayer explains [6]: \u201cWhere advanced automation, such as flexible manufacturing systems, is introduced, its effects on productivity are greatest where it is applied to a production system that has already been rationally organised; otherwise it is likely to perpetuate the inefficiencies of the old technology and working practices, as has been found with many major new technologies, including computer-integrated manufacturing and office automation.\u201d In other words, Toyota\u2019s system thrives in a changing world. Tesla\u2019s system thrives when nothing changes. So far, Toyota\u2019s winning the cost game. Toyota\u2019s cost of products sold represented 80.8% of revenue in 2017. Compared with 83.4% for Tesla. Production costs will stay high as long as Tesla continues to develop new models or to improve designs of existing cars. It\u2019s simply expensive to take robots offline for tuning. This heightens the risk of Tesla running out of cash. Tesla\u2019s \u201cbuild it first, fix it later\u201d mentality also leads to high servicing costs. Service cost as percent of total cost of revenues grew from 7.2% in 2014 to 12.9% in 2017. For every $1 customers paid to get service, Tesla had to pay $1.23 to get the job done last year. A sign of high rates of in-warranty repairs. Mr. Musk\u2019s cash problems will likely to persist as long as he puts quantity over quality.  Interestingly, Tesla and Toyota were partners. They worked on an electric version of the Rav4 together. This ended in 2016, supposedly because of a culture clash. It wouldn\u2019t be surprising to find they broke up because of opposing manufacturing philosophies. The irony is that Mr. Musk instinctively believes in continuous improvement. He shared in an interview how \u201cI always see what\u2019s wrong\u2026 When I see a car or a rocket or spacecraft, I only see what\u2019s wrong\u2026 It\u2019s not a recipe for happiness.\u201d He really should really think twice about investing more in automation. It won\u2019t allow him to quickly fix and improve designs. Likely leading to further displeasure. More recently, Elon Musk appears to be awakening to the fact too many robots can slow down production. He tweeted in April 2018: \u201cYes, excessive automation at Tesla was a mistake\u2026 Humans are underrated.\u201d That\u2019s a good start. Tesla however has a long road ahead in convincing its human workers it has their best interest at heart. That it\u2019ll create a safer work environment. That it won\u2019t again try to replace them with robots. Necessary for workers to trust their employer, and in turn, lead continuous improvement initiatives. Tesla looks up to the Ford Model T for inspiration, a 90-year-old car that competed with horses Unlike other car manufacturers, Tesla tends to both design and manufacture its parts in-house. When battery problems surfaced with a supplier, Mr. Musk cut the supplier and directed his engineers onto the issue, bringing manufacturing in-house. The same happened when problems arose with the Model X\u2019s second row seat supplier. This means Tesla only has one supplier for many of its parts: itself. It greatly increases the risk of production delays should any of these parts face manufacturing or design problems. Most manufacturers hedge that risk by using multiple suppliers for the same part. Some even use suppliers in different geographic regions to hedge political and natural disaster risks [7]. The tendency to build parts in-house may stem from Mr. Musk\u2019s success in reducing cost of rockets at SpaceX through vertically integrated manufacturing. Over 80% of parts used in Falcon rockets are made in-house. There is however a fundamental difference in rocket production and car production: Delays experienced in launching a rocket are expected and unlikely to affect revenue, whereas delays in car production directly leads to revenue loss. Threatening liquidity. Tesla\u2019s production model is reminiscent of Ford\u2019s just-in-case system used to make the Model T. Model T production also focused on lowering cost by using more machines and by making parts in-house at the Highland Park plant. The result was the first car every American could afford. Ford\u2019s Model T only cost $370 in 1921 ($5,060 in 2017 dollars). Less than twice the cost of the cheapest GM vehicle: A Chevrolet costing $795 ($10,900 in 2017 dollars). Low cost was partially achieved by producing only one car model at Ford: The Model T. Ford believed that the Model T was all the car a person would, or could, ever need. He was wrong. The Model T fell out of fashion as GM both reduced cost of its vehicles and introduced different models suiting the needs of different customers. Production of Model Ts ended after 18 years in 1927. Tesla is already producing three vehicle models by contrast. Each with dozens of options. Its robots therefore need to be much smarter and flexible than Ford\u2019s, having to adapt to many different specs. Adding a truck to the production line will only complicate things. The semi is also likely to have many options \u2013 Freightliner\u2019s Cascadia comes in 5 different configurations to meet the needs of different jobs. Unless Tesla\u2019s robots are super easy to tweak, they\u2019re bound to continue experiencing much\u00a0downtime. Both Tesla\u2019s Semi and Model 3 also face a greater challenge than the Model T. They\u2019re competing with existing truck and car manufacturers with far more experience mass producing vehicles. The Model T didn\u2019t even have a competitor capable of mass production in the early days. Heightened competition could lower demand for Tesla vehicles. Especially if competitors have trucks in stock while Tesla struggles to produce them. It\u2019s thus critical Tesla learns to consistently mass produce with the Model 3. Peter Hochholdinger, Tesla\u2019s VP of Production, may not be of much help. The Volkswagen Group was already a manufacturing powerhouse producing over 3 million cars\u00a0when he joined them at Audi in 1994. Mr. Hochholdinger\u2019s current challenge thus lies in boosting production without the support of an experienced manufacturing organization. Something he depended on from the very start of his career. During his time at Audi, production increased from 617,000 in 1998 to 1.9 million cars in 2016. An average growth of 6.6% per year. Tesla\u2019s total car production would grow from 103,184 vehicles in 2017 to around 172,000 vehicles in 2025 at that rate, missing all production targets. Three main ingredients in Tesla\u2019s batteries include Lithium Hydroxide (LiOH), Nickel (Ni) and Graphite. Existing reserves of Lithium can sustain current production rates for the next 372 years. Large producers in Chile and Australia have already begun increasing production in anticipation of rising demand from electric cars. This rise in supply is even expected to surpass demand for EVs, so prices won\u2019t likely rise. Interestingly, Lithium is not traded as a commodity. Producers sell many different grades of the material for different purposes. The price battery manufacturers pay for Lithium will therefore vary. Existing reserves of Nickel can sustain current production rates for the next 40 years. More reserves can also be developed on land and at sea. It\u2019s thus unlikely we run out of Nickel. Price of Nickel is expected to increase. 85% of Nickel produced currently goes toward stainless steel production. Electric batteries consume only about 3% of production. However, that figure grew 44% in 2016. It will accelerate further as electric cars become more mainstream. Price of Nickel is forecast to rise 5% a year according to the world bank. Lastly, reserves of graphite\u00a0(one of two forms of carbon, the other being diamonds) can sustain current production rates for over 225 years. It is also not a rare element. Much of the mining for graphite occurs in China, where, ironically, it\u2019s causing an environmental disaster. There are about 156,000 fueling stations across the United States and 55% of them sell Diesel. That\u2019s 85,800 diesel stations serving about 11.2 million trucks on the road. Each station services 130.5 trucks and covers an average land area of 44,000 square miles. How many stations will Tesla need from the get-go? We estimated that 7,270 Tesla semis will be produced in 2020 to gain a market share of 3.1%. That\u2019s 56 charging stations to build at a ratio of 130.5 trucks per station. Quite feasible considering Tesla added almost 150 supercharger stations in 2017 in North America alone. They could cover an area of 2.46 million square miles at 44,000 square miles per station. More than double the combined area of Northeast and Midwest U.S., which together generates over 40% of the nation\u2019s GDP. It will however be a challenge to identify where to exactly position the megachargers. Tesla needs to identify popular freight routes and appeal to as many freight operators as possible. Each truck route will need a megacharger: This is where certainty effect may hinder adoption. We humans tend to pay a premium for certainty. Freight companies may therefore be ok operating at a lower margin with diesel truck that can for certain refuel anywhere, rather than risk running out of electricity for a slightly higher margin. Freight companies may prefer to make less money for sure than maybe make more money. It\u2019s therefore no surprise Tesla Semi customers are partnering up to build the initial charging infrastructure, guaranteeing megachargers on their routes. Mr. Musk\u2019s ability to disrupt industries is second to none. Paypal revolutionized payments. SpaceX is lowering the cost of launching satellites by 10x. He\u2019s also advancing artificial intelligence at Neuralink and digging cheap tunnels at Boring. Mr. Musk is clearly a man of many goals. Bloomberg dedicated a complete website to track his ventures. Yet he is also very focused and dedicated to each one of these goals. SpaceX was founded in 2002 and Tesla in 2003. Mr. Musk has been consistently leading those efforts for more than 15 years. What fuels Mr. Musk\u2019s strong animal spirit? Mr. Musk is not chasing more money. He called SpaceX and Tesla the \u201cdumbest things\u201d from a business standpoint. This is supported by the fact he\u2019s building a semi truck instead of a delivery truck (saving 4x the amount of carbon emissions but sacrificing billions of dollars in potential revenue). Instead, Mr. Musk is driven by a vision for the future. Doing his bit to advance humanity. Commenting on Asimov\u2019s Foundation series, Mr. Musk shared: \u201cThe lesson I drew from that is you should try to take the set of actions that are likely to prolong civilization, minimize the probability of a dark age and reduce the length of a dark age if there is one.\u201d Mr. Musk\u2019s companies are in effect for-profit social ventures. Working to help us survive longer. And if he can make a dollar while at it, why not? Investors need to take note. One dollar invested in Tesla may not yield above average returns. It\u2019s not a priority. Investing in Tesla is supporting a vision of the future where tailpipe emissions don\u2019t exist. Why is Mr. Musk so focused on his visions? He shared in an interview with Rolling Stone a feeling of discomfort in solitude, saying: \u201cI will never be happy without having someone. Going to sleep alone kills me.\u201d He also shared how he was virtually raised by books as a child, not parents. And that he doesn\u2019t respect his father. It appears things are not all rosy in Mr. Musk\u2019s personal life. Perhaps tackling extremely difficult engineering projects is the only thing bringing him joy. That alone should keep him working at Tesla for a really long time. There was no need for a $2.6 billion dollars compensation plan. Humans prefer avoiding losses to acquiring equivalent gains. Given a choice, we\u2019d rather not lose $5 than make $5. This is called loss aversion and was reported by nobel winning psychologists Amos Tversky and Daniel Kahneman. Loss aversion also explains that in order to choose to acquire additional gains, we must perceive the value of gains to be more than twice that of the value of losses. Not losing $5 becomes less appealing of a choice if potential gains stand at $10 or more. To facilitate adoption of Tesla\u2019s semi truck, the perceived switching costs must therefore be compensated by larger gains. At least twice the amount according to Kahneman et al. How does Tesla fare? Switching costs of going electric from diesel include: Gains of switching to an electric truck mainly stem from savings on fuel and maintenance. As per previous calculations, a diesel truck is expected to cost $0.644\/mile for fuel and maintenance, while a Tesla truck is expected to cost $0.365\/mile. Savings of $0.279\/mile. So when will gains sum up to double the amount of losses?\u00a0After about 290,000 miles or 3.5 years if a driver averages 82,805 miles per year. Considering most fleets keep their trucks for roughly 4 years, loss aversion may play to Tesla\u2019s advantage.  That\u2019s of course assuming prices of diesel trucks don\u2019t decrease. It\u2019s very likely manufacturers of diesel trucks will give upfront discounts to compete with electric trucks\u2019 lower operating cost. A $10,000 reduction in price, which raises Tesla\u2019s price premium to $40k, would push the mental break-even point at which gains equal double the losses to ~390,000 miles or 5-year. Fleet operators replacing trucks every four years could become more reluctant to go electric. Especially considering they\u2019re likely okay paying a premium for certainty on refueling their diesel trucks anywhere. Everyone else will get migraines. Freight companies will see new price wars, while diesel related industries face existential threats. Tesla may help truckers lower fuel costs, but not increase profit margins. Trucking is a highly competitive and segmented industry as we explored in the beginning of this investigation. Lower operating costs will likely bring about a price war that pushes profits down. Making low operating cost electric trucks necessities. Freight companies who can\u2019t compete on new low prices will go out of business. Freight customers will be the ultimate winners as truck shipping becomes cheaper. And the environment of course. The fact shipping is perceived as a commodity is the main strategic problem facing freight transportation. Different shipping companies offer relatively undifferentiated service. All are simply taking stuff from point A to B. Customers don\u2019t lose anything by shopping around for better prices, and are unlikely to pay a premium. Freight operators need to offer unique value to customers to gain a competitive edge and charge premium prices. They could also offer added value to their customers\u2019 customers. UPS Access Points that make it convenient for shoppers to pickup and return retail purchases is a good example. Freight companies need to consider playing more active roles in helping customers increase sales, manage logistics (some large freight companies already do), or become an extension of their service teams. Customers should feel pain at the thought of using a different shipping company. Marketing also has a major role to play. Companies turning commodity products into premium brands and premium profits is not uncommon. Intel\u2019s \u201cIntel Inside\u201d campaign and Corning\u2019s \u201cGorilla Glass\u201d serve as examples. It\u2019s no longer a matter of if electric trucks will hit the road, but a matter of when they\u2019ll take over the road. Tesla and the electric truck revolution won\u2019t happen overnight. It will however spook investor confidence in a number of industries as they face the prospect of slower or declining growth. That includes: [1] Yan Ji, Chao Yang Wang, Heating strategies for Li-ion batteries operated from subzero temperatures,\u00a0Electrochimica Acta, Volume 107, 2013, Pages 664-674, ISSN 0013-4686 [2] Bugga R, Smart M, Whitacre J, West W. Lithium Ion batteries for space applications. In: 2007 IEEE aerosp conf; 2007. p. 1\u20137. [3] Linda Gaines, The future of automotive lithium-ion battery recycling: Charting a sustainable course,\u00a0Sustainable Materials and Technologies, Volumes 1\u20132, 2014, Pages 2-7, ISSN 2214-9937, [4] John T. Bowen, A spatial analysis of FedEx and UPS: hubs, spokes, and network structure, Journal of Transport Geography, Volume 24, 2012, Pages 419-431, ISSN 0966-6923, [5] Sugimori, Y., Kusunoki, K., Cho, F., Uchikawa, S. Toyota production system and kanban system materialization of just-in-time and respect-for-human system (1977) International Journal of Production Research, 15 (6), pp. 553-564. [6] Sayer, A. New developments in manufacturing: The just-in-time system (1986) Capital & Class, 10 (3), pp. 43-72. [7] Tang, C.S. Perspectives in supply chain risk management (2006) International Journal of Production Economics, 103 (2), pp. 451-488. \r\n                        Be the First to Comment!                     \nEmail *\n\n\n\n\n Copyright Chord Strategy, All Rights Reserved","time":1525793358,"title":"Tesla forsakes $7.7B to build semis instead of delivery trucks, but saves Earth","type":"story","url":"https:\/\/thebattleofgiants.com\/2018\/04\/26\/special-report-tesla-forsakes-7-7-billion-to-build-semi-trucks-instead-of-delivery-trucks-but-saves-planet-earth\/","label":7,"label_name":"random"},{"by":"aaronbrethorst","descendants":0,"id":17021607,"kids":"None","score":1,"text":"By FRANNIE HANNAN, JOSH HOELTZEL AND PETER RENTZ The New York Times publishes more than 150 articles a day to our iOS and Android apps, and our desktop and mobile websites. All of these platforms were originally developed by separate teams, so each had its own implementation and user experience. Though it may not have been obvious to the users of our products, we were maintaining a fragmented experience spanning multiple applications. Every time we built a new feature or even implemented something as simple as a stylistic change, we needed to build it separately in each place. That meant replicating work, coordinating roadmaps and releasing code across multiple teams\u200a\u2014\u200amobile web, desktop and in each of our native news apps, which required a full app release to roll out even the most minor changes. At the same time, editors producing articles saw a rendering that didn\u2019t represent what the content would ultimately look like on our platforms. This did little to help them envision their work as users would see it. In short, we were blocked by inefficiencies from delivering the best product to our users. May 8, 2018, marks the culmination of a years-long project to create an article ecosystem that promotes internal efficiency and delivers an enhanced reading experience for our users. We now have a single responsive article for both mobile and desktop on the web, and we use a subset of the same code to render stories in our native apps, Google\u2019s Accelerated Mobile Pages (AMP) and our content management system. Here are the biggest benefits of the work we\u2019ve done: In order to create a consistent reading experience on all of our platforms that we could iterate on in tandem, we had to make major changes to how our native apps render articles. Prior to the \u201chybrid\u201d project (i.e. a hybrid between native and web), each native app parsed article data and rendered the headlines, bylines, paragraphs and media natively to display the article. Now, articles are rendered on the server and delivered to the apps as an HTML string that the native apps display in a web view. This means that as we add new features or change the way we treat existing ones, we can do it with a single web application release without updating the apps in the store. Another efficiency of the new article system is the introduction of shared components. As we add elements to our article experience, we are building them in an abstract way outside the context of any specific application. This allows them to be shared across applications, pulled in anywhere that the component needs to be rendered in user-facing applications, as well as content creation tools like our CMS. As an example, consider our bylines: We recently made changes to include headshots and a short blurb about the author, so we built a shared component that would render the byline with these enhancements. We could then utilize that component in the hybrid article, the web article and the CMS, as well as any future application that might need it. Implementing this approach required a lot of planning, experimentation and coordination across teams. Engineers throughout the tech organization and in the newsroom worked together to make it happen. Today we are sharing more than 40 components across our web, native, CMS and off-platform apps. The desktop story page is where we made the most transformational user-facing changes. We\u2019ve moved away from a model that is standard for many news organization on the internet\u200a\u2014\u200aan article template that has a right rail crowded with ads and other content that draws a reader\u2019s attention away from what they came for: the story. With this new launch, articles are presented in a focused, single-column layout that intentionally strips away the clutter and puts our journalism front and center. We now have a clean slate on which to build elements that truly add value for readers. In conjunction with Oak\u200a\u2014\u200aour new visual article editor\u200a\u2014\u200awe will create and enhance story forms that take advantage of the space and flexibility on the new story page. The single-column story page was designed to create an integrated reader and advertising experience, built around our proprietary FlexFrame display units. We\u2019ve removed the cluttered right rail of small, standard banner ads in favor of premium, full-bleed, in-stream units that are responsive to the page\u2019s width. Ads on the new page are achieving twice the click-through rate of our old design, and initial studies show higher brand recall and four-times the reader attention to ads. \u201cWith the new Story page, we\u2019ve successfully integrated our reader and advertising experiences with a pristine, user-focused design,\u201d says Allison Murphy, vice president of ad innovation. \u201cA more engaging page is better from every angle: it means more connection with our journalism, and more connection with the messages of our marketers.\u201d This marathon effort to reimagine The Times article has given us the infrastructure to build features faster and better meet the needs of users and advertisers on all of our platforms. In January 2017, an innovation group within The Times released a report titled Journalism That Stands Apart. It lays out how the company must change in order to meet aggressive goals and thrive in the media landscape of the future. Key recommendations included a more visual report and more diverse forms of storytelling. We believe the new article page is a critical step toward allowing us to evolve how we tell digitally native stories that make The Times stand apart. Frannie Hannan is a senior product manager at The Times overseeing the story page. Josh Hoeltzel is the senior development manager for the story page. Peter Rentz is the design director for the story page. By clapping more or less, you can signal to us which stories really stand out. We\u2019re New York Times employees writing about building digital products and workplace culture. Sharing our stories of making great digital products at The New York Times.","time":1525793326,"title":"Reimagining The New York Times Digital Story Experience","type":"story","url":"https:\/\/medium.com\/@timesopen\/reimagining-the-new-york-times-digital-story-experience-ff698541ac09","label":9,"label_name":"tech"},{"by":"cuchoi","descendants":0,"id":17021602,"kids":"None","score":1,"text":"Sometimes, it's kind of interesting to know the origins of the products\/projects we love. Here's a collection of such products\/projects and the stories of how they got their name. From an interview made to its creator Brendan Eich: InfoWorld: As I understand it, JavaScript started out as Mocha, then became LiveScript and then became JavaScript when Netscape and Sun got together. But it actually has nothing to do with Java or not much to do with it, correct? Eich: That\u2019s right. It was all within six months from May till December (1995) that it was Mocha and then LiveScript. And then in early December, Netscape and Sun did a license agreement and it became JavaScript. And the idea was to make it a complementary scripting language to go with Java, with the compiled language. Michael \"Monty\" Widenius is one of the founder of MySQL and one of his daughter's name is My(after whom MySQL was named). Linus Torvalds (Developer of Git) has quipped about the name git, which is British English slang for a stupid or unpleasant person. Torvalds said: \"I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'.The man page describes git as \"the stupid content tracker\". The OS was named after the Southern African philosophy of ubuntu (literally, 'human-ness'), which Canonical Ltd. suggests can be loosely translated as \"humanity to others\" or \"I am what I am because of who we all are\". The language was initially called Oak after an oak tree that stood outside James Gosling(Developer of Java)'s office. Later the project went by the name Green and was finally renamed Java, from Java coffee. In Van Rossum(Developer of Python)'s own words: \"In December 1989, I was looking for a \"hobby\" programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix\/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).\" The name \"Ruby\" originated during an online chat session between Yukihiro Matsumoto (Developer of Ruby) and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language. Initially two names were proposed: \"Coral\" and \"Ruby\". Matsumoto chose the latter in a later e-mail to Ishitsuka. Matsumoto later noted a factor in choosing the name \"Ruby\" \u2013 it was the birthstone of one of his colleagues. Translated chat when name was decided and email they've shared. The original conversation was in Japanese. The name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users. The framework was named after guitarist Django Reinhardt. An answer from Glen Murphy, Design Lead, Google Chrome on asking \"How Chrome gets its name?\". In his words, \"We had a \u2018pick a codename\u2019 vote early in the development cycle \u2013 the names that came of that competition were so terrible that we were all pretty happy when one of the leads overrode it and declared that the codename would be \u2018Chrome\u2019, presumably because he likes fast cars.\" The history of the name Mozilla goes all the way back to the internal codename for the original 1994 Netscape Navigator browser, with the name meaning \"Mosaic killer\" and aiming to some similarity with the building-crushing Godzilla, as the company's goal was to displace NCSA Mosaic as the world's number one web browser. The name Mozilla was revived as the 1998 open sourcing spinoff organization from Netscape. The name \"Firefox\" (a reference to the red panda) was chosen by Mozilla for its similarity to \"Firebird\" (which was the former name of Firefox), but also for its uniqueness in the computing industry. \"When trying to think of names, I thought about the geography of Tolkien's Middle Earth and C.S. Lewis' Narnia. In Narnia, Cair Paravel is the name of the castle where the kings and queens of Narnia live. Laravel rhymes with Paravel. I thought the name had a classy and sophisticated ring to it.\" \u2014 Taylor Otwell (Creator of Laravel) The name Hadoop is not an acronym; it\u2019s a made-up name. The project\u2019s creator, Doug Cutting, explains how the name came about: The name my kid gave a stuffed yellow elephant. Short, relatively easy to spell and pronounce, meaningless, and not used elsewhere: those are my naming criteria. Kids are good at generating such. Googol is a kid\u2019s term. The name for the software is derived from \"Sky peer-to-peer\", which was then abbreviated to \"Skyper\". However, some of the domain names associated with \"Skyper\" were already taken. Dropping the final \"r\" left the current title \"Skype\", for which domain names were available. The name Adobe was derived from Adobe Creek, a river or creek that ran behind the house of John Warnock, one of the founders. But where is Adobe Creek? It's located in Los Altos, California. According to the FAQ in the Apache project website, the name Apache was chosen out of respect to the Native American tribe Apache and their superior skills in warfare and strategy. The name was widely believed to be a pun on 'A Patchy Server' (since it was a set of software patches). Coined by Bill Gates to represent the company that was devoted to microcomputer software. The word \"Zend\" has been derived from the names of Zeev Suraski and Andi Gutmans, the two founders of the organisation. It has been derived from xerography, a word derived from the Greek xeros (dry) and graphos (writing).  Leave such stories in the comment section if you know about one. Coder, thinker and an aspiring entrepreneur. \n\n amit_merchant\n        \n\n www.amitmerchant.com\n        \n This is absolutely awesome. The Ruby programming language is inspired by perl and that's why it was narrowed down to other stones. And Crystal takes on the same legacy, though it was unclear from their AMA how much of a role that played. I'd also like to know how dev.to got its name. Can you shed some light on it? The C programming language was meant to replace the B programming language, so they just choose the next letter in the alphabet. (There was also a later language called D, though that hasn't caught on nearly as much as C.) less, a unix tool for reading text files, replaced a similar (though less powerful tool) called more. It's a play on the term \"less is more\". The language Ada is named after Ada Lovelace, the first person to publish a computer algorithm and possibly the first to realize, that computers could be used for much more than just calculations. WINE is a recursive acronym, standing for \"WINE Is Not an Emulator\". It's also a backronym, since of course the word \"wine\" existed before the tool did. The programming languages Haskell and Curry are both named after the American mathematician and logician Haskell Curry. The programming technique currying is also named after him. V8, the JavaScript engine used in both Google Chrome and Node.JS, is named after the V8 engine. That was supposed to symbolize, how fast it was. To quote from the MongoDB glossary: \"\u201cMongoDB\u201d derives from the word \u201chumongous\u201d because of the database\u2019s ability to scale up with ease and hold very large amounts of data.\" The LucasArts adventure game engine SCUMM (short for Script Creation Utility for Maniac Mansion) is a play on the word scum. That wasn't enough though, it also played host to the INteractive Streaming ANimation Engine (INSANE) and some other tools such as FLEM, CYST, BYLE and MMUCUS. Later they switched to GrimE for 3D stuff, the name being inspired by both the first game they used it for (Grim Fandango) and the term grime. ScummVM, a later project for playing those old adventure games on newer systems, was named after the SCUMM engine. That and GrimE then inspired ResidualVM, with ResidualVM being for those games that ScummVM doesn't cover and grime being a type of residue. Wonderful article! I can't proof this, but I believe Composer was named Composer because.. well.. you need a composer to compose a Symfony. ;) It does make sense, however, since the motivation behind early Composer was to make it easier for phpBB and Symfony to install bundles\/plugins\/extensions. \n          Hey there, we see you aren't signed in. (Yes you, the reader. This is a fake comment.)\n         \n          Please consider creating an account on dev.to. It literally takes a few seconds and we'd appreciate the support so much. \u2764\ufe0f\n         Plus, no fake comments when you're signed in. \ud83d\ude43 And Debian, from Wikipedia:\n\"Debian was first announced on August 16, 1993, by Ian Murdock, who initially called the system 'the Debian Linux Release'. The word 'Debian' was formed as a portmanteau of the first name of his then-girlfriend Debra Lynn and his own first name.\"\nen.wikipedia.org\/wiki\/Debian Wow! That's really interesting! Why the name \"Delphi\"?\nAs explained in the Delphi Museum article, project codenamed Delphi hatched in mid 1993. Why Delphi? It was simple: \"If you want to talk to [the] Oracle, go to Delphi\". (because of the temple of Delphos) Firefox had to be renamed from Firebird, because of the already database server with this name (en.wikipedia.org\/wiki\/Firebird_(da...). It came to be with the split of the Mozilla suite into two independant products, the other one is the mail software Thunderbird. Great article!  Love this list. I remember learning about \"Ubuntu\" (the philosophy) at a school talk and assumed it was complete coincidence that it was also the name of an operating system. Glad you liked it! Great article! It's really cool to see the origins behind programming languages, especially if they have unique stories. LISP is so named because it is a LISt Processor. Interesting!  That was very interesting to read. Thank you. The first name of Symfony was Sensio Framework, from which became the initials sf. Sensio (Labs) is me company behind the framework. Awesome Job, Amit-\nPassing this on as a quiz for Devs who are job hunting.  This should be a real kick to their ego when they fail the test. ( I get to negotiate a lower pay scale) haha What a beautiful article! Thank you! web.archive.org\/web\/19970106233141... Interesting \n\n          \n\nIn trying to understand a Google snippet I had the thought that it might be an ...\n        \n \n        We\u2019re a place where coders share, stay up-to-date and grow their careers.\n       \nWe strive for transparency and don't collect excess data.\n","time":1525793305,"title":"How Python, JavaScript, Git, and others got their name","type":"story","url":"https:\/\/dev.to\/amit_merchant\/how-products-got-their-name-c6d","label":7,"label_name":"random"},{"by":"prakhar236","descendants":0,"id":17021594,"kids":"None","score":1,"text":"Are you an introvert or an extrovert? You probably already know by taking several personality quizzes online. But have you ever wondered if you can change your personality? Study shows that if you\u2019re like most people, you have. So, can you change your personality? Bringing balance to your personality is one of the best things you can do for your personal development. You can shape your personality to let it help you succeed. I have always been an introvert. I love alone time to recharge myself. My life goals demanded more from my personality. So here\u2019s what I did\u2026 I changed myself. Google says an ambivert is a person who has a balance of extrovert and introvert features in their personality. Being an ambivert brings the best from both worlds and makes you a better version of yourself. Whether you label yourself as an \u201cintrovert\u201d or \u201cextrovert\u201d, you can always work towards becoming an ambivert since it\u2019s not completely opposite of your nature. Identify where you lie on the spectrum of extroversion. The more you are towards either of the sides, the more work it will take to balance your personality. At first, it wasn\u2019t easy to change myself. I had to take bold actions and force myself to grow. I\u2019d fake my personality to act like an extrovert in social situations. Then, I\u2019d come home and enjoy the bliss of solitude. After repeating it enough times, I became who I wanted to be\u200a\u2014\u200anot who I was bound to be. I can call my extrovert side whenever I want and enjoy being an introvert for the rest of the time when I\u2019m reading, journaling, writing, thinking, meditating, learning, training, etc. I\u2019m not denying that your genetics and upbringing determine your personality. So first, embrace who you are and then look forward to who you can be. Shape your life experiences to redirect your personality because anyone can become an ambivert. If you\u2019re an extrovert, force yourself to do activities in isolation which will accelerate your personal growth. If you\u2019re an introvert, force yourself to get into social situations and small talks so you make the most of your life and your potential. At first, it would be awful. But once you force yourself to practice it enough times, you will thank yourself because of the growth you will see in yourself. And that\u2019s not all. Research has confirmed that acting extroverted when you need to, will make you happy. And activities done alone like journaling and meditating are already well known to make you happy. So, take full advantage of your human existence and experience the undiscovered side of you. You can be who you WANT to be: You are limitless. Design your daily checklist for high performance and success. Click here to download your free copy. By clapping more or less, you can signal to us which stories really stand out. Writer, Lifestyle Entrepreneur, Lifelong Learner | Design your day for success -> http:\/\/bit.ly\/daily-success-list We publish stories, videos, and podcasts to make smart people smarter. Subscribe to our newsletter to get them! www.TheMission.co","time":1525793257,"title":"Warning: Is Your Personality Killing Your Chances of Success?","type":"story","url":"https:\/\/medium.com\/the-mission\/warning-is-your-personality-killing-your-chances-of-success-942975bcca83","label":10,"label_name":"thought"},{"by":"devy","descendants":0,"id":17021580,"kids":"None","score":2,"text":"Microsoft\u2019s Windows Store has been struggling to attract developers for years now, and Microsoft is now radically overhauling how it takes a revenue cut from app developers. At Microsoft\u2019s Build developers conference this week, the software maker is changing its Microsoft Store policies to allow developers to keep 95 percent of the revenue from their apps. This is a big change from the 70 percent that developers currently get to keep, and it\u2019s clearly designed to encourage developers to create apps for Windows 10. There are some catches for developers, though. The new 95 percent cap will only be available on consumer apps and not games, and Microsoft switches it to 85 percent if the company helped a developer obtain a customer through marketing in the Microsoft Store. Still, the change from 70 percent to 95 percent is significantly better than Google\u2019s Play Store and Apple\u2019s App Store. Google and Apple both offer 70 percent to developers, and they increase that to 85 percent if a consumer subscribes to an app for a year or more. The new Microsoft Store policy will apply to apps for Windows 10, Windows Mixed Reality, Windows Phone, and Surface Hub, but will exclude the Xbox One. Microsoft says it will make the 95 percent revenue share structure available later this year, and it will apply to all apps currently available on the store. Command Line delivers daily updates from the near-future.","time":1525793177,"title":"Microsoft undercuts Apple and Google to offer Win 10 app developers more money","type":"story","url":"https:\/\/www.theverge.com\/2018\/5\/8\/17330600\/microsoft-windows-store-apps-revenue-cut-build-2018","label":9,"label_name":"tech"},{"by":"doener","descendants":0,"id":17021578,"kids":"None","score":2,"text":"","time":1525793171,"title":"Using Evolutionary AutoML to Discover Neural Network Architectures","type":"story","url":"https:\/\/ai.googleblog.com\/2018\/03\/using-evolutionary-automl-to-discover.html","label":7,"label_name":"random"},{"by":"dfirment","descendants":1,"id":17021577,"kids":"[17021600]","score":2,"text":"At the beginning of 2018, I sent out a survey about cloud usage via a variety of social media channels\u200a\u2014\u200amost notably my sarcastic AWS newsletter. After collecting and carefully analyzing the data from over 900 respondents, the results and insights are ready to be shared. Before we review the results, I\u2019d like to point out a few things: Let\u2019s get to it! The survey started by trying to get an idea for who my respondents were\u200a\u2014\u200aso I asked them about their job function. Over 70% of the people survey identified as a developer of engineer that writes code. Now that we\u2019ve understand their mind, let\u2019s go for the heart. The survey asked individuals to select which solution they are using for their data center. As expected, almost everyone is using AWS\u200a\u2014\u200aalthough almost a third of all respondents also have on-premise environments. Regarding the \u201cother\u201d category, Digital Ocean accounted for the majority of entries which I neglected to include as a response option. I told you I\u2019m terrible at this! Now let\u2019s figure out why people are so angry all of the time. AWS spans 18 geographic regions around the world\u200a\u2014\u200aeach providing multiple, physically separated and isolated availability zones. US East (Northern Virginia) is the oldest of the regions\u200a\u2014\u200alaunched in 2006. The us-east-1 region also contains the largest number (6) of availability zones. It\u2019s no surprise that almost 42% of respondents use that region as their primary\u200a\u2014\u200aand why people get so upset when us-east-1 has a hiccup. When asked free-form \u201cWhat could AWS announce tomorrow that would make you so angry that you ragequit your job and go raise goats in the hills instead?\u201d people had many, many thoughts. Let\u2019s get a word cloud! Asked the other side of the question, \u201cWhat could AWS announce tomorrow that would make you so happy that you rename your child after the company?\u201d people had somewhat different responses. I was curious about the level of technical sophistication of people\u2019s companies. As is congruent with my brand, I asked the survey question in a slightly insulting way: Sure enough, the response mirrors the results of other surveys\u200a\u2014\u200aa plurality of respondents are using the basic \u201cbuilding block\u201d services for most of their work, but serverless and its ilk aren\u2019t far behind. To learn a bit more about the level of automation, I asked about the automation of deployments. I was taken a bit by surprise when I saw the results for how people provision\u200a\u2014\u200alet\u2019s take a look: For most of the time this survey was open, CloudFormation and Terraform were neck and neck. This feels like the cloud version of vim vs emacs. A \u201cgolden image\u201d that instances are provisioned from are called an Amazon Machine Image, or AMI. In most right thinking places, people pronounce that as an abbreviation\u200a\u2014\u200awith all three letters being spoken. And then there\u2019s Amazon itself. At AWS, and nowhere else, they pronounce it \u201cah-mee.\u201d They are wrong. The survey offered some other interesting data points: I also asked people to tell me what my problem is. Oof. I was accused of being an AWS shill. I was accused of being entirely too harsh on AWS itself. People wanted me to email more and less. People wanted a podcast (I started one!), and people wanted me to do videos instead. Thank you all, for the feedback. Follow me on Twitter if you\u2019d like to hear more of my nonsense\u200a\u2014\u200aor subscribe to my weekly newsletter. By clapping more or less, you can signal to us which stories really stand out. Fixes your horrifying AWS bills. Public Speaker. Cloud Economist. Snappy dresser. Father. The #1 community-sourced collection of cloud computing and serverless articles curated and published by A Cloud Guru. We are on a mission to teach the world to cloud.","time":1525793166,"title":"AWS survey results from 979 respondents\u200aincluding correct pronunciation of AMI","type":"story","url":"https:\/\/read.acloud.guru\/last-year-in-aws-841a960ca60d","label":3,"label_name":"dev"},{"by":"edward","descendants":0,"id":17021573,"kids":"None","score":2,"text":"There's a good reason why companies often test self-driving cars in big cities: they'd be lost most anywhere else.  They typically need well-labeled 3D maps to identify curbs, lanes and signs, which isn't much use on a backwoods road where those features might not even exist.  MIT CSAIL may have a solution, though.  Its researchers (with some help from Toyota) have developed a new framework, MapLite, that can find its way without any 3D maps. The system gets a basic sense of the vehicle's location using GPS, and uses that for both the final destination and a \"local\" objective within view of the car.  The machine then uses its onboard sensors to generate a path to those local points, using LiDAR to estimate the edges of the road (which tends to be much flatter than the surrounding landscape).  Generic, parameter-based models give the car a sense of what to do at intersections or specific roads. MapLite still isn't ready to handle everything.  It doesn't know how to cope with mountain roads and other sharp changes in elevation, for instance.  However, the ultimate goal is clear: CSAIL wants autonomous cars that can safely navigate any road without hand-holding.  While 3D maps may still be useful for dealing with the complexity of cities, this could be vital for rural trips, snowy landscapes and other situations where the car needs to improvise.  The show stars Timothy Olyphant and Drew Barrymore. They cut through the winter confusion to see what's actually present. Surprise, surprise: The Pixelbook will get the feature first.  They could make life a lot easier for millions of workers. It's coming alongside support for starting orders inside the app.","time":1525793145,"title":"MIT's self-driving car can navigate unmapped country roads","type":"story","url":"https:\/\/www.engadget.com\/2018\/05\/07\/mit-maplite-self-driving-car\/","label":5,"label_name":"ml"},{"by":"allenlsy","descendants":0,"id":17021571,"kids":"None","score":1,"text":"\n\nallenlsy\n        released this\n          May 8, 2018\n Wrap Google tasks into WebView in Mac","time":1525793133,"title":"Google Tasks Mac application","type":"story","url":"https:\/\/github.com\/allenlsy\/GoogleTasksMac\/releases","label":9,"label_name":"tech"},{"by":"hapnin","descendants":0,"id":17021570,"kids":"None","score":1,"text":"The New York Stock Exchange\u00a0is reflected in a street vendor's mirror in New York. Intercontinental Exchange Inc., the owner of New York Stock Exchange, has been working on a trading platform that would let investors bet on Bitcoin, according to two people with knowledge of the project. ICE has held talks with other financial firms about the project, which is still in development, the people said, asking not to be identified because the conversations are confidential. The project could ultimately be aborted. A spokesman for the company declined to comment. Wall Street firms began offering clients futures from Cboe Global Markets Inc. and CME Group Inc. in December, but the financial industry has broadly shied away from selling Bitcoin itself. One issue is that virtual currencies can be stolen by hackers, a problem that has arisen before at some major crypto venues. That means traditional Wall Street firms looking to facilitate investments have to figure out how to hold such assets securely. ICE may offer swaps to big investors that would end with the buyer owning Bitcoin the next day, the New York Times reported earlier Monday, citing four unidentified people briefed on its efforts. Such contracts would put the trading under the purview of the Commodity Futures Trading Commission. In an interview last month, ICE Chief Executive Officer Jeffrey Sprecher declined to rule out offering contracts based on digital currencies. \u201cThere is a trend here we can\u2019t ignore in my mind, so I don\u2019t discount it,\u201d he told Bloomberg Television. Goldman Sachs Group Inc. also has been working on expanding the types of contracts available to investors as it prepares to open a Bitcoin-trading business. The bank will probably trade Bitcoin futures in a principal, market-making capacity and will also create non-deliverable forward products, a person briefed on the decisions said last week. https:\/\/megaphone.link\/BLM3146792961","time":1525793126,"title":"NYSE Owner ICE Is Said to Be Working on Bitcoin Trading Platform","type":"story","url":"https:\/\/www.bloomberg.com\/amp\/news\/articles\/2018-05-08\/nyse-owner-ice-is-said-to-be-working-on-bitcoin-trading-platform","label":7,"label_name":"random"},{"by":"jgrahamc","descendants":0,"id":17021560,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back  Prometheus exporter for custom eBPF metrics. Motivation of this exporter is to allow you to write eBPF code and export\nmetrics that are not otherwise accessible from the Linux kernel. eBPF was described by Ingo Moln\u00e1r as: One of the more interesting features in this cycle is the ability to attach\neBPF programs (user-defined, sandboxed bytecode executed by the kernel)\nto kprobes. This allows user-defined instrumentation on a live kernel image\nthat can never crash, hang or interfere with the kernel negatively. An easy way of thinking about this exporter is bcc tools as prometheus metrics: To build, you need to have libbcc installed: To run with bio config (you need root privileges): If you pass --debug, you can see raw tables at \/tables endpoint. Currently the only supported way of getting data out of the kernel\nis via maps (we call them tables in configuration). See: See examples section for real world examples. If you have examples you want to share, please feel free to open a PR. Skip to format to see the full specification. You can find additional examples in examples directory. Unless otherwise specified, all examples are expected to work on Linux 4.14,\nwhich is the latest LTS release at the time of writing. In general, exported to work from Linux 4.1. See BCC docs for more details: This program attaches to kernel functions responsible for managing\npage cache and counts pages going through them. This is an adapted version of cachestat from bcc tools: Resulting metrics: You can check out cachestat source code to see how these translate: This program attaches to block io subsystem and reports metrics on disk\nlatency and request sizes for separate disks. The following tools are working with similar concepts: This program was the initial reason for the exporter and was heavily\ninfluenced by the experimental exporter from Daniel Swarbrick: Resulting metrics: To nicely plot these in Grafana, you'll need v5.1:  Programs combine a piece of eBPF code running in the kernel with configuration\ndescribing how to export collected data as prometheus metrics. There may\nbe multiple programs running from one exporter instance. Metrics define what values we get from eBPF program running in the kernel. Counters from maps are straightforward: you pull data out of kernel,\ntransform map keys into sets of labels and export them as prometheus counters. Histograms from maps are a bit more complex than counters. Maps in the kernel\ncannot be nested, so we need to pack keys in the kerne and unpack in user space. We get from this: To this: Prometheus histograms expect to have all buckets when we report a metric,\nbut the kernel creates keys as events occur, which means we need to backfill\nthe missing data. That's why for histogram configuration we have the following keys: For exp2 histograms we expect kernel to provide a map with linear keys that\nare log2 of actual values. We then go from bucket_min to bucket_max in\nuser space and remap keys by exponentiating them: Here map is the map from the kernel and result is what goes to prometheus. We take cumulative count, because this is what prometheus expects. For linear histograms we expect kernel to provide a map with linear keys\nthat are results of integer division of original value by bucket_multiplier.\nTo reconstruct the histogram in user space we do the following: For both exp2 and linear histograms it is important that kernel does\nnot count events into buckets outside of [bucket_min, bucket_max] range.\nIf you encounter a value above your range, truncate it to be in it. You're\nlosing +Inf bucket, but usually it's not that big of a deal. Each kernel map key must count values under that key's value to match\nthe behavior of prometheus. For example, exp2 histogram key 3 should\ncount values for (exp2(2), exp2(3)] interval: (4, 8]. To put it simply:\nuse bpf_log2l or integer division and you'll be good. The side effect of implementing histograms this way is that some granularity\nis lost due to either taking log2 or division. We explicitly set _sum key\nof prometheus histogram to zero to avoid confusion around this. Labels transform kernel map keys into prometheus labels. Maps coming from the kernel are encoded in a special way. For example,\nhere's how [sda, 1] is encoded as a string: We're transforming this to [\"sda\", \"0x1\"] and call it a set of labels. Each label can be transformed with decoders (see below) according to metric\nconfiguration. Generally number of labels matches number of elements\nin the kernel map key. Decoders take a string input of a label value and transform it to a string\noutput that can either be chained to another decoder or used as the final\nlabel value. Below are decoders we have built in. KSym decoder takes kernel address and converts that to the function name. In your eBPF program you can use PT_REGS_IP(ctx) to get the address\nof the kprobe you attached to as a u64 variable. Note that sometimes\nyou can observe PT_REGS_IP being off by one. You can subtract 1 in your code\nto make it point to the right instruction that can be found \/proc\/kallsyms. Regexp decoder takes list of strings from regexp configuration key\nof the decoder and ties to use each as a pattern in golang.org\/pkg\/regexp: If decoder input matches any of the patterns, it is permitted.\nOtherwise, the whole metric label set is dropped. An example to report metrics only for systemd-journal and syslog-ng: Static map decoder takes input and maps it to another value via static_map\nconfiguration key of the decoder. An example to match 0x1 to read and 0x2 to write: String decoder transforms quoted strings coming from the kernel into unquoted\nstring usable for prometheus metrics. For example: \"sda\" -> sda. UInt64 decoder transforms hex encoded uint64 values from the kernel\ninto regular numbers. For example: 0xe -> 14. Configuration file is defined like this: See Programs section for more details. See Metrics section for more details. See Counters section for more details. See Histograms section for more details. See Labels section for more details. See Decoders section for more details. MIT","time":1525793070,"title":"Prometheus exporter for custom eBPF metrics","type":"story","url":"https:\/\/github.com\/cloudflare\/ebpf_exporter","label":4,"label_name":"github"},{"by":"doener","descendants":0,"id":17021554,"kids":"None","score":1,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 The solar photovoltaic sector was the largest employer in the renewable energy industry last year, accounting for 3.4 million jobs, up from 3.1 million in 2016, according to International Renewable Energy Agency data. Bioenergy was the second biggest employer at about 3.1 million jobs, more than double the size of hydropower, which came in third at 1.5 million. Total jobs for the renewable energy industry topped 10 million for the first time, with China alone being responsible for 43 percent of the positions.","time":1525793032,"title":"Renewable Energy Jobs Top Record 10M Led by Solar: Chart","type":"story","url":"https:\/\/www.bloomberg.com\/news\/articles\/2018-05-08\/renewable-energy-job-record-powered-by-solar-and-china-chart","label":0,"label_name":"biz-news"},{"by":"mpweiher","descendants":0,"id":17021549,"kids":"None","score":2,"text":"Some\u00a0Leavers like Jacob-Rees Mogg think that Britain should assert itself more. Photo: Stefan Rousseau\/PA Wire\/PA Images Jacob Rees-Mogg has a rather unpleasant knack for delivering threats as if they are supportive platitudes. He was\u00a0at it again\u00a0in the\u00a0Telegraph\u00a0today, telling readers about why the European Union would prefer Theresa May to Boris Johnson. \u201cThe EU knows if they don\u2019t support and help Theresa May to get a deal, there is the risk of having somebody much, much more aggressive, which they don\u2019t want,\u201d he said. \u201cI think that\u2019s helping her. By being a Remainer, by being moderate, by being courteous, she is doing a highly competent job in negotiations. I don\u2019t think they would like to have Boris Johnson, do you?\u201d The supportive comments are of course a thin veil for an ice-cold threat to dethrone May if she refuses to do his bidding\u00a0on post-Brexit customs.\u00a0The idea that Britain should assert itself more strongly\u00a0is fairly common among Brexiters on the Conservative benches. They are infuriated by the government\u2019s continued capitulations. They think that if only someone really tough, someone with the proper bulldog spirit of Brexit in them, went into those negotiations, the grey bureaucrats of Brussels would blink first. Faced with the true grit of an Englishman, cowardly Europeans would run a mile. You have now reached your limit of 3 free articles in the last 30 days. But don't worry! You can get another 7 articles absolutely free, simply by entering your email address in the box below. When you register we'll also send you a free e-book\u2014Writing with punch\u2014which includes some of the finest writing from our archive of 22 years. And we'll also send you a weekly newsletter with the best new ideas in politics and philosophy of culture, which you can of course unsubscribe from at any time Prospect takes your privacy seriously. We promise never to rent or sell your e-mail address to any third party.\nYou can unsubscribe from the Prospect e-mail newsletter at any time. DEBUG messsage: regular No comments yet You can log in to post a comment under your subscriber name. Name * Mail (will not be published) * \n\n Human verification - please type the words\/numbers from the image: \n\n The big ideas that are shaping our world\u2014straight to your inbox. PLUS a free e-book and 7 articles of your choosing on the Prospect  website. Prospect takes your privacy seriously. We promise never to rent or sell your e-mail address to any third party. Time to rip up the economics textbook and start again? Howard Reed says the discipline needs rebuilding from first principles. Also: Sonia Purnell on Jacob Rees-Mogg's chance of cracking No 10; Will Self on his first acid trip  The true reason Britain cannot call Europe\u2019s bluff Think Shakespeare wasn't Shakespeare? You might just be a snob A landmark legal challenge shows the cruel reality of Universal Credit for disabled people Local elections: the key numbers crunched In defence of the economists Rip it up and start again: the case for a new economics Can Jacob Rees-Mogg become Britain\u2019s 55th prime minister\u2014and Eton\u2019s 20th? Government fecklessness is reaching new heights over the Irish border Assad is the problem\u2014intelligent intervention could help remove him Prospect Book Club\u2014Jesse Norman London, 2018-07-16 Prospect Book Club\u2014Henry Marsh London, 2018-06-18 HowTheLightGetsIn 2018 Hay-on-Wye, 2018-05-25 Supporting UK businesses trading overseas The Commonwealth has put vision for everyone on the world\u2019s agenda\u2014 now we must act Brexit and the future of industry Seeing Clearly Meet the brains behind Exo Prospect was originally founded by Editor David Goodhart and Publisher Derek Coombs, as a home for intelligent debate. The magazine is owned and supported by the Resolution Group, as part of its not-for-profit, public interest activities. The aim is to tackle the big challenges confronting society, through rigorous thinking and fine writing. Editor: Tom ClarkDeputy Editor: Steve Bloomfield  Executive Editor: Jay Elwes Managing Editor (Arts & Books): Sameer RahimHead of Digital: Stephanie Boland  Deputy Digital Editor (Political Correspondent): Alex DeanDesign: Mike Turner Production Editor: Chris Tilbury US Writer-at-Large: Sam Tanenhaus Commercial Director: Alex StevensonFinance Manager: Pauline JoyHead of Marketing: Paul MortimerMarketing and Circulations Executive: James Hawkins Head of Research and Engagement: Saskia Perriard-Abdoh Events Coordinator: Oliver Ward  Head of Advertising Sales: Adam Kinlan 020 3372 2934Senior Account Manager: Sophie Ryan 020 3372 2927Senior Account Manager: Dominic Slonecki 0203 372 2972Account Manager: Scott Smith 020 3372 2931 Forgotten password? Register today and access any 7 articles on the Prospect\u2019s website for FREE in the next 30 days..  PLUS find out about the big ideas that will shape our world\u2014with Prospect\u2019s FREE newsletter sent to your inbox. We'll even send you our e-book\u2014Writing with punch\u2014with some of the finest writing from the Prospect archive, at no extra cost! Prospect takes your privacy seriously. We promise never to rent or sell your e-mail address to any third party.\n            You can unsubscribe from the Prospect e-mail newsletter at any time. It looks like you are a Prospect subscriber.  Prospect subscribers have full access to all the great content on our website, including our entire archive. If you do not know your login details, simply close this pop-up and click 'Login' on the black bar at the top of the screen, then click 'Forgotten password?', enter your email address and press 'Submit'. Your password will then be emailed to you. Thank you for your support of Prospect and we hope that you enjoy everything the site has to offer.  \n\n","time":1525793017,"title":"The true reason Britain cannot call Europe\u2019s bluff","type":"story","url":"https:\/\/www.prospectmagazine.co.uk\/politics\/the-true-reason-britain-cannot-call-europes-bluff","label":7,"label_name":"random"},{"by":"uptown","descendants":23,"id":17021536,"kids":"[17021932, 17021950, 17021763, 17021772, 17021918, 17021899, 17021849, 17021935]","score":78,"text":"","time":1525792922,"title":"Introducing Google AI","type":"story","url":"https:\/\/ai.googleblog.com\/2018\/05\/introducing-google-ai.html","label":7,"label_name":"random"},{"by":"dwighttk","descendants":0,"id":17021533,"kids":"None","score":1,"text":"Bradley Jacobs, CEO of XPO Logistics  ","time":1525792899,"title":"You Won't Like the Consequences of Making Pluto a Planet Again","type":"story","url":"https:\/\/www.forbes.com\/sites\/startswithabang\/2018\/05\/08\/you-wont-like-the-consequences-of-making-pluto-a-planet-again\/","label":7,"label_name":"random"},{"by":"dsego","descendants":0,"id":17021523,"kids":"None","score":1,"text":"The 49-year-old woman struck and killed by a self-driving Uber vehicle in Arizona earlier this year was detected by the autonomous vehicle's sensors, but the car didn't avoid her due to an improper software setting, according to a Monday report from the Information.\u00a0 Elaine Herzberg was hit on a March night while walking a bicycle across a main thoroughfare. The Uber vehicle with an operator in the front seat was in autonomous mode at the time of the crash. \"Two people briefed about the matter\" told the Information that the software in the car detected the woman, but there was a problem with how the software decides how to react to objects. SEE ALSO: Car crash becomes national news because a Waymo minivan was involved The Information says the software is able to decide if an object is a \"false positive,\" like debris or trash that it doesn't need to react to. It seems the software detected a false positive instead of a pedestrian. It mistakenly ignored Herzberg and didn't react appropriately or fast enough.\u00a0 Uber didn't comment on the report about the software problem, but told Mashable they are reviewing the self-driving program and cooperating with a National Transportation Safety Bureau investigation. The company also announced Monday that it hired former NTSB chair Christopher Hart to oversee safety.\u00a0 \"Our review is looking at everything from the safety of our system to our training processes for vehicle operators, and we hope to have more to say soon,\" an Uber spokesperson said in an email. The report is the first tidbit about the crash since police released terrifying footage of the operator behind the wheel the moment before impact. Speculation of what contributed to the fatal crash has ranged from fewer sensors to cutting corners with training.\u00a0 Uber's self-driving program remains suspended at all its testing sites.\u00a0 ","time":1525792823,"title":"Self-driving Uber that killed pedestrian reportedly didn\u2019t realize she was human","type":"story","url":"https:\/\/mashable.com\/2018\/05\/07\/self-driving-uber-fatal-crash-pedestrian-detected.amp","label":7,"label_name":"random"},{"by":"Artemis2","descendants":250,"id":17021518,"kids":"[17021665, 17021690, 17021656, 17023243, 17024831, 17021886, 17021661, 17021651, 17022200, 17022704, 17023609, 17021660, 17022177, 17021632, 17021645, 17022170, 17021966, 17022440, 17022253, 17022172, 17023503, 17023826, 17022894, 17021641, 17022689, 17022744, 17021860, 17021959, 17021854, 17021896, 17021878, 17022676, 17021764, 17021639, 17022457, 17021791, 17022342]","score":355,"text":"A new iOS update is about to roll out in the next few weeks or even days. Reading Apple documentation and researching developer betas, we discovered a major new security feature that is about to be released with iOS 11.4. The update will disable the Lightning port after 7 days since the device has been last unlocked. What is the meaning of this security measure, what reasons are behind, and what can be done about it? Let\u2019s have a closer look. In the iOS 11.4 Beta, Apple introduced a new called USB Restricted Mode. In fact, the feature made its first appearance in the iOS 11.3 Beta, but was later removed from the final release. This is how it works: \u201cTo improve security, for a locked iOS device to communicate with USB accessories you must connect an accessory via lightning connector to the device while unlocked \u2013 or enter your device passcode while connected \u2013 at least once a week.\u201d The functionality of USB Restricted Mode is actually very simple. Once the iPhone or iPad is updated to the latest version of iOS supporting the feature, the device will disable the USB data connection over the Lightning port one week after the device has been last unlocked. At this point, it is still unclear whether the USB port is blocked if the device has not been unlocked with a passcode for 7 consecutive days; if the device has not been unlocked at all (password or biometrics); or if the device has not been unlocked or connected to a trusted USB device or computer. In our test, we were able to confirm the USB lock after the device has been left idle for 7 days. During this period, we have not tried to unlock the device with Touch ID or connect it to a paired USB device. What we do know, however, is that after the 7 days the Lightning port is only good for charging. Restricted USB Mode requires an iPhone running 11.3 to be unlocked at least once every 7 days. Otherwise, the Lightning port will lock down to charge only mode. The iPhone or iPad will still charge, but it will no longer attempt to establish a data connection. Even the \u201cTrust this computer?\u201d prompt will not be displayed once the device is connected to the computer, and any existing lockdown records (iTunes pairing records) will not be honoured until the user unlocks the device with a passcode. In other words, law enforcement will have at most 7 days from the time the device was last unlocked to perform the extraction using any known forensic techniques, be it logical acquisition or passcode recovery via GreyKey or other services . Even the 7 days are not a given, since the exact date and time the device was last unlocked may not be known. Before iOS 11, one could use an existing lockdown record to access the iPhone or iPad device for the purpose of creating a new local backup (logical acquisition). Essentially, this is exactly how experts perform logical acquisition in the vast majority of cases of iPhone and iPad devices that are locked with an unknown passcode. The lockdown record (a small file extracted from the suspect\u2019s computer) allows accessing essential information about the device and initiating the backup sequence without the passcode. In addition to iTunes-style backups, the lockdown record could be used for pulling media files (pictures and videos), list installed apps, and access general information about the device. Once created, the lockdown records would not expire; however, if you power-cycle or reboot the iPhone, even a valid lockdown record will be of little use until you unlock the device with a passcode because of full-disk encryption. iOS 11 brought limitations to the use of lockdown records. In iOS 11.0 through 11.2.1, lockdown records would expire after a certain unspecified time. Once a lockdown record expired, it could no longer be used to establish communication with the iOS device; the user would need to enter their passcode on the device to establish a new pairing relationship. iOS 11.3 further limited the lifespan of iTunes pairing records, making the records expire after 7 days. Apparently, iOS stores information about the date and time the device was last unlocked or had a data connection to a USB port. After the seven days elapse, the Lightning port will be disabled. Once this happens, you will no longer be able to pair the device to a computer or USB accessory, or use an existing lockdown record, without unlocking the device with a passcode. The only thing you\u2019ll be able to do is charging. Whether or not iPhone unlocking solutions developed by GreyShift and Cellerbrite will work is still an open question. USB Restricted Mode is aimed squarely at law enforcement, preventing device acquisition after the device has been stored for 7 consecutive days without being unlocked or connected to a (paired) computer or USB accessory. At this time, we suggest two possible mitigations. In addition, managed devices may disable USB Restricted Mode for good. The success rate will receive on the condition in which the phone is delivered to the lab. If the phone was seized while it was still powered on, and kept powered on in the meanwhile, than the chance of successfully connecting the phone to a computer for the purpose of making a local backup will depend on whether or not the expert has access to a non-expired lockdown file (pairing record). If, however, the phone is delivered in a powered-off state, and the passcode is not known, the chance of successful extraction is slim at best. Actually, it was only a matter of time. Companies such as Cellerbrite and the recent newcomer GreyShift make their business by unlocking protected iPhones. While Cellerbrite offers this exclusively as an in-house service, and the service is only available to select law enforcement agencies with proper court orders, GreyKey supplies the actual unlocking hardware to North American law enforcement. Both companies keep their lips shut as to the details of their techniques, so the exact method they use to gain access to the devices is not known to Apple. However, their ability to unlock even the latest hardware running the latest version of iOS is worrisome, so Apple is taking action with the USB Restricted Mode. In responce to the latest developments, Apple made an attempt to prevent device exploitation. USB Restricted Mode effectively disables the iPhone or iPad Lightning ports after 7 days without an unlock. While this undoubtedly strengthens overall security of iOS devices, effectively disabling logical acquisition through lockdown records after 7 days the device has been in storage, its effect on passcode unlocking techniques developed by Cellerbrite and GreyShift is yet to be seen. What else is Apple baking in iOS 11.4? Without going through the lengthy list of bug fixes and improvements, there is the possibility of iMessage sync with iCloud being finally released in iOS 11.4. iMessage sync showed up in early betas of iOS 11.0, but didn\u2019t make it into the final release. The feature appeared again in iOS 11.3 betas, but was stripped from the final release at the last minute. There are signs the feature might be ready for prime time in iOS 11.4. How will iMessage sync work? How will it be different from Continuity, and what challenges and benefits will it present to the mobile forensic crowd? Stay tuned to find out! Tags: ios 11.4, iPad, iPhone, lightning, logical acquisition, passcode recovery, physical acquisition \n\n\t\t\t\t\t\tThis entry was posted\n\t\t\t\t\t\t\t\t\t\t\t\ton Tuesday, May 8th, 2018 at 1:25 pm\t\t\t\t\t\tand is filed under Did you know that...?, Industry News, Security.\n\t\t\t\t\t\tYou can follow any responses to this entry through the RSS 2.0 feed.\n\n\t\t\t\t\t\t\t\t\t\t\t\t\tBoth comments and pings are currently closed.\n\n\t\t\t\t\t\t\n\t\t\t\t\t\n Sign up for free ElcomSoft Password Recovery Software newsletter Comments are closed. \n\t\tCopyright &copy 2009-2018 ElcomSoft Co. Ltd.\nPowered by WordPress.\n\t\t\n","time":1525792787,"title":"iOS 11.4 to Disable USB Port After 7 Days: What It Means for Mobile Forensics","type":"story","url":"https:\/\/blog.elcomsoft.com\/2018\/05\/ios-11-4-to-disable-usb-port-after-7-days-what-it-means-for-mobile-forensics\/","label":7,"label_name":"random"},{"by":"jonballant","descendants":0,"id":17021514,"kids":"None","score":1,"text":"This story was delivered to Business Insider Intelligence IoT Briefing subscribers hours before it appeared on Business Insider. To be the first to know, please click here. Microsoft made a slew of announcements about its IoT business segment at its annual Build Developer Conference\u00a0on Monday. Below are some of the highlights: Microsoft\u00a0announced\u00a0it's open-sourcing Azure IoT Edge, its product suite that allows companies to run AI and Azure services without connecting to the cloud.\u00a0Microsoft first unveiled IoT Edge at last year's Build Conference to help it keep pace in the rapidly growing edge computing market for IoT devices. The latest update allows any customer to use the service, helps them modify it with little hassle, and makes it easier to detect and weed out software bugs. asdfThe German Microsoft headquarters in Munich.Hannah Schwa\u0308r  In addition, Microsoft\u00a0teamed\u00a0up with semiconductor giant Qualcomm to release a custom AI-powered vision platform.\u00a0The platform allows developers to use both Azure IoT Edge and Azure Machine Learning to build IoT solutions. The companies said it is well-suited for building IoT applications that require advanced image processing, including enterprise drones, industrial robotics, and smart security cameras. The platform builds on Qualcomm's expertise in computer vision technologies, showcased by the\u00a0release\u00a0of its own Intelligent Vision platform earlier this year. Lastly, Microsoft plans to bring Azure Cognitive Services to Azure IoT Edge sometime in the next few months.Azure Cognitive Services, already offered as part of Microsoft's core cloud service, enables developers to train and program their own image, voice, and speech identification and classification tools for specific tasks without building their own models. These initiatives could help Microsoft in its ongoing battle with Amazon Web Services (AWS) for supremacy in the IoT platforms market.\u00a0Microsoft and the e-commerce titan were two of the earliest high-profile technology vendors to enter the IoT when they launched cloud-based IoT services back in 2015. These latest announcements are part of Microsoft's larger effort to gain a leg up on its primary competitor, and they come on the heels of nearly a half dozen\u00a0other moves\u00a0the firm has made in the IoT. Although Microsoft likely now offers more advanced features than Amazon in the IoT space, the latter company might still have a price advantage, as it reportedly\u00a0slashed\u00a0pricing 20-40% for many customers late last year. Amazon may well release its own set of features and expand its partner network to stay competitive, which could give it a key edge in the market if it retains a price advantage on Microsoft. The Internet of Things (IoT) is transforming how companies and consumers go about their days around the world. The technology that underlies this whole segment is evolving quickly, whether it\u2019s the rapid rise of the Amazon Echo and voice assistants upending the consumer space, or growth of AI-powered analytics platforms for the enterprise market.  And Business Insider Intelligence is keeping its finger on the pulse of this ongoing revolution by conducting our second annual Global IoT Executive Survey, which provides us with critical insights on new developments within the IoT and explains how top-level perspectives are changing year-to-year. Our survey includes more than 400 responses from key executives around the world, including C-suite and director-level respondents. Through this exclusive study and in-depth research into the field, Business Insider Intelligence (Business Insider's premium research service) details the components that make up the IoT ecosystem. We size the IoT market and use exclusive data to identify key trends in device installations and investment. And we profile the enterprise and consumer IoT segments individually, drilling down into the drivers and characteristics that are shaping each market. Here are some key takeaways from the report: In full, the report: To get your copy of this invaluable guide to the IoT, choose one of these options: \u00a0  \n This story was delivered to Business Insider... Sparen f\u00fcr Ihr Kind \u00a0 Die besten Storys von Business Insider per WhatsApp ","time":1525792772,"title":"Microsoft made a slew of IoT announcements at Build","type":"story","url":"https:\/\/www.businessinsider.de\/microsoft-build-iot-announcements-2018-5?r=UK&IR=T","label":5,"label_name":"ml"},{"by":"vimarshk","descendants":1,"id":17021511,"kids":"[17021522]","score":3,"text":"Every professional in the world knows about LinkedIn and has a profile on it. Also, its data prowess comes from the fact that it is a professional network, the quality of data it has is better and more accurate than other networks. Google, Apple, Amazon and Microsoft are all older than LinkedIn but the fact that LinkedIn has amassed so much data for AI in just 15 years is very commendable. My AI Interview Questions articles for Microsoft, Google, Amazon, Apple, Facebook, Salesforce and Uber have been very helpful to the readers. As a followup, next couple of articles were on how to prepare for these interviews split into two parts, Part 1and Part 2. If you want to find suggestions on how to showcase your AI work please visit Acing AI Portfolios. Don\u2019t forget to provide feedback. Even though Microsoft acquired LinkedIn for 26B$ LinkedIn, per its CEO LinkedIn still has freedom to grow the company in their own way. But for fiscal year 2017, the service brought in just $2.3 billion, or 2.5% of Microsoft\u2019s total annual revenue. Microsoft\u2019s new Bing Ad service leverages LinkedIn, Graph API and AI foundations. The LinkedIn AI team sits within its LinkedIn data team within the company. LinkedIn has a typical interview process like most other companies who hire Engineers. The Data Science roles usually have a process tweaked a little which reflects the importance of different aspects under the umbrella of Data Science. There are usually phone interviews(involve coding) followed by onsite interviews. Onsite there are about 4\u20135 interviews. There might be 2\u20133 of them really going deep on Data Science related questions, research and models. The remaining ones are aimed to test the coding skills. LinkedIn interviews many coding and product related questions. Product questions require deep product based thinking on the fly. This is different from the other companies we have looked at previously. The questions are also focused on different data applications and recommendation systems which are deployed in their product. Consumable List: 20 LinkedIn AI Interview Questions If you find this article useful, please share your thoughts in the comments below. Please clap on the article to signal me how much you like this article and if you would like me to write more of such articles. By clapping more or less, you can signal to us which stories really stand out. Engineering Manager | Udacity Deep Learning & AI(part1) Alumnus | Creator of Acing AI Acing AI provides analysis of AI companies and ways to venture into them.","time":1525792768,"title":"LinkedIn AI Interview Questions \u2013 Acing the AI Interview","type":"story","url":"https:\/\/medium.com\/acing-ai\/linkedin-ai-interview-questions-acing-the-ai-interview-41028c4b0704","label":5,"label_name":"ml"},{"by":"dmytrish","descendants":0,"id":17021507,"kids":"None","score":2,"text":"This is my blog. There are many like it, but this is mine. So I started this blog after being a long time \u201cGoogle+ only\u201d publisher, and now GDPR is coming. I have looked into making this wordpress instance GDPR compliance, but it\u2019s no fun. The webfonts are easy, but \u201cno more Youtube embeds without a consent orgy\u201d is no fun, and losing the Google\/Facebook\/Twitter SSO integration will basically lose all mobile users (80% or so of all readers). The easiest way to get GDPR compliance is to move back to Google+ only, or to move this blog to wordpress.com or to medium.com. What migration target do you prefer (running a self-hosted instance is not an option for me after May, 25)? Migration to Medium instructions Migration to WordPress.com instructions J\u00fcrgen Geuter nails it (german article on Facebook): Self-Hosted anything after GDPR is only for people with a really good legal expenses insurance. Sorry for writing this reply in german, my english is humble. Ist das eine L\u00f6sung? WordPress wird sicher keine Datenschutzerkl\u00e4rung f\u00fcr ihre Nutzer verfassen.  Und so wie ich es verstanden habe gelten nicht die Gesetze des Landes, in dem der Server\/Cloud\/wasauchimmer steht, sondern die Gesetze Deines Wohnsitzes. Yes, at the moment a completely silo\u2019ed solution such as medium.com or Google plus looks most attractive. That way I can publish stuff without offering a site, I am just a stream on a larger site. I wouldn\u2019t have to deal with any of this stuff that way, and could focus on content. I wouldn\u2019t count on that, remember all the Facebook\/Linkedin\/Xing impress rules \u2026 Some people say that for private pages you do not have to follow GDPR. Some people have another opinion. Why do you need Google\/Facebook\/Twitter SSO ? For the comments? Disabling comment function would be the easiest way but your blog without comments is only 75% :) The big question is: what is a \u201cprivate page\u201d? IANAL, but I tend to the opinion that a private site is only accessible to family and friends. This is, it is password protected. On my blog, I have removed the comments, disabled all cookies, turned off the access log, made sure nothing (like webfonts) is loaded from CDNs. Still I need to think about GDPR because I can receive e-mails (I have to because of German laws), which is again processing of personal data. I\u2019m still indecisive. Are those GDPR alarmists just scaremongers, and my blog is okay now and everything is going to be fine? Or should I better shut down my webserver before May 25th and wait for some months until the dust has settled? The GDPR doesn\u2019t differentiate between private and public. It differentiates between businesses\/organisations and personal activities. See recital 18: \u201cThis Regulation does not apply to the processing of personal data by a natural person in the course of a purely personal or household activity and thus with no connection to a professional or commercial activity.\u201d There is no mention that the activity has to be private. That\u2019s true, but  publishing on the web for others is neither \u201cpersonal\u201d nor \u201chousehold\u201d. So you\u2019re saying as soon as you publish something on the web it becomes a professional or commercial activity? It still leaves so much room for interpretation. What is a \u201cpurely personal\u201d activity? Is it a personal blog, or is it just (for example) an own NextCloud instance that I only use for myself? What is \u201cno connection to a professional activity\u201d? If I blog about my profession, is it still personal, or already professional? I prefer to assume that my blog is commercial, even if I don\u2019t earn a single cent with it. I think it is only \u201cpersonal or household\u201d if it is not reachable for the public. I have found this today, while I looked for a solution for a friend of mine. She wants to continue to use wordpress, but I think I can\u2019t help her with this. https:\/\/www.blogmojo.de\/wordpress-plugins-dsgvo\/ I would suggest to wait for the 15th. WordPress will release a new version and if I understand it correctly, it will contain something for GDPR.  I would be very sad to see you go back to G+ or to medium. The former does not offer RSS and the latter one wants an automatic  license to your content AFAIK. WordPress \/ blogs are one thing. What if you run your own root\/virtual server which handles some nextcloud or email \u2013 even if you\u2019re the only person using it. If you send or receive mail, it will for sure contain personal data. What now? I\u2019m not running any business, so is this \u201chousehold use\u201d?  But running a mailserver is certainly not \u201ccommon household use\u201d in terms of the average user. BTW, WordPress announced that WP 4.9.6 will bring a GDPR-ready solution: https:\/\/make.wordpress.org\/core\/2018\/05\/01\/4-9-6-beta-delayed-two-days\/\n\u201cThe primary focus of 4.9.6 is delivering a set of tools for controlling and managing private information in a GDPR compliant manner. With GDPR going into effect on May 25th, 2018, this will be the last minor release before the new laws go into effect.\u201d I hope the best. I would vote for Medium. Am planning to do it for my site, too. Your email address will not be published. Required fields are marked * \nComment\n\n \nName*\n\n \nEmail*\n\n \nWebsite\n\n  \n\n  ","time":1525792747,"title":"Moving from self-hosted Wordpress due to GDPR","type":"story","url":"http:\/\/blog.koehntopp.info\/index.php\/3156-self-hosted-wordpress-and-gdpr-compliance\/","label":7,"label_name":"random"},{"by":"Shanerostad","descendants":1,"id":17021499,"kids":"[17021538]","score":1,"text":" By Carlo Thomas |  May 8th, 2018 The mobile app market is ever-growing. By 2020, the mobile app market is expected to generate $189 billion. Reaching mobile users across operating systems like Android, iOS, or another operating system like Windows is critical. Enter in cross-platform development, defined as using the same codebase across multiple OSs. Cross platform mobile development allows companies to leverage their resources by creating a single app that only requires minimal adjustments for every device. For those new to cross-platform development, you may be thinking the following: In this guide, we\u2019ll cover everything you need to know about cross-platform mobile development. To start, we\u2019ll cover the basics of mobile app development by: The rest of this guide will cover the resources you need to get started with cross-platform development, including: Let\u2019s get started. Note: We build hybrid web and mobile apps for companies ranging from startups to Fortune 500 brands. If you\u2019re looking for help building a hybrid app \u2013 Contact Us Today In a nutshell, cross-platform development is the creation of an app that\u2019s compatible across all mobile devices (i.e. Android and iOS) using a single codebase.  Cross-platform development is possible for a couple of reasons: The two types of cross-platform mobile development, native and hybrid HTML5, take advantage of these reasons when developing apps. Each mobile OS runs on its own software development kit (DSK) and tech stack. Android apps, for example, are programmed using Java. iOS apps, on the other hand, are programmed using Objective-C and Swift. Developers, however, can work around these differences by creating an API that acts as a middleman between the code base and the OS. Native cross-platform development is accomplished by using a third-party vendor which provides an integrated development environment (IDE) specific to this type of programming. Since application code execution is handled on backend servers, developers can get around OS differences by developing aspects of an app\u2019s graphic user interface (GUI) in languages like CSS, HTML5, and JavaScript. Developers use HTML5, in particular, because today\u2019s mobile OSs have advanced browser capabilities (HTML5 is the markup language used on the internet).  By coding in HTML5, developers create a \u201chybrid\u201d app that consists of the OS\u2019s native frame and the code that\u2019s executed in a WebView. Cross-platform mobile development offers several advantages to developers and companies alike. These all start with the fact that, by choosing the right cross-platform tools and having a strong development plan, developers can use up to 80 percent of the same codebase. This cut in time alone can: From a development standpoint, cross-platform development can: From a business standpoint, cross-platform development can: Considering these benefits, it may seem ideal that you move forward with cross-platform development for your app. In fact, we\u2019d like to argue that much of today\u2019s debate over native versus cross-platform development may be overstated Nevertheless, it\u2019s important you consider the potential drawbacks of cross-platform development. And we\u2019re happy to discuss them below. Among the native app development evangelists, there is an argument that native apps have higher performance. Their argument is justified by the fact that the rendering of components made with HTML5 and CSS (a common protocol for cross-developed apps) may affect response time. This is because HTML5 and CSS components require a lot of a mobile device\u2019s resources. Mobile devices simply don\u2019t have the computing power of desktops. Another argument has to do with design, as today\u2019s apps are expected to deliver smooth interfaces and superior user experiences. While a strong cross-platform developer can create apps with great design across OSs, there may still be design aspects that are less than ideal. Consider, for instance, that Android is used across many device brands such as Samsung and Motorola, each with their own lineup of devices. These various screen sizes mean that apps may not appear the same across all of them. The design game gets more complicated when you consider that iOS has a completely different design style from Android\u2019s material design, not to mention that Apple devices lack a back button. These subtleties can add up. Users who aren\u2019t satisfied with the app\u2019s design may want to mention it in a review. Poor design prevents your app from being approved for the app stores to begin with. In fact, 20 percent of app store rejections are related to poor UI design. To avoid UI issues, Developers have to adjust their code for each OS. While the bulk of the codebase applies across the board, companies will have to consider (and budget for) these extra development costs. That\u2019s a great question. While cross-platform development comes with many advantages, they do have some drawbacks.  The decision comes down to what\u2019s right for your product. Every app has a different purpose and will be used by a different user base. To help you answer this question, consider the following: Before moving forward with any development, it\u2019s important to understand your target market. Is your app a game or useful tool, intended to be used by millions of people worldwide? Or is your app intended to be used by a set group of people, such as company employees, to streamline internal communication? Understanding this difference is crucial because we\u2019re essentially discussing internal use versus external use apps. External use apps are intended to be used by a large audience typically for commercial purposes. Apps from Candy Crush to Spotify to Gmail are external use apps. Internal apps, on the other hand, are restricted to a specific user base. Enterprise-based apps often fall into this category because only company employees can download these apps to their devices. So\u2026. what does target audience have to do with cross-platform development? Sometimes, the design isn\u2019t the most important feature in an app. Some apps are content-heavy and are mainly used to disperse information. In these cases, a sleek design probably isn\u2019t top priority. People just want their information. Another instance where design may not be top-of-mind is with B2B or enterprise apps. These apps aren\u2019t built around generating a profit, so business will be more concerned about reeling in budgets and getting a return on their investment (e.g. increased productivity). As long as the app is functional and gets the job done, a flashy design isn\u2019t too important. The bottom line is this: cross-development development is better suited for content-heavy and enterprise apps. Of course, this isn\u2019t to say you can\u2019t move forward with the cross-platform approach. Every app is different, and the right app development team (like us at TriFin!) can advise on the right way to move forward. But\u2026 Let\u2019s say you decide to go ahead with cross-platform development. Great! There are many tools and resources available to your team for creating your app. In the remainder of this article, we\u2019ll cover these resources. We\u2019ve already discussed one advantage of cross-platform development: the ability to hire developers that only need an expertise in one language. Even better, cross-platform development can be accomplished with several programming languages, including: The gold standard of programming languages for cross-platform development, not to mention it\u2019s the core language used for Android. Further reading: An introduction to Java Though not as sophisticated as Java, C++ will still get the job done for any cross-platform development project. The only drawback is that C++ may add some strain to mobile device resources. Further reading: What is C++ JavaScript is the programming language used for HTML5, the markup language designed to make mobile apps compatible with desktops. While each isn\u2019t particularly useful on their own, together, these two make for an excellent choice for cross-platform development. Further reading: What is HTML5? Created by Microsoft, C# originally started off as an equivalent to the Objective-C language for Mac. However, C# has become a popular choice for cross-platform developers. Further Reading:\u00a0Why C# is among the most popular programming languages in the world. A mobile-specific programming language that\u2019s straightforward and meant to use as few resources as possible on mobile devices. Further Reading: Beginner\u2019s Guide to Ruby Fortunately, developers today have a variety of tools to choose from. Each tool offers something different, such as the use of certain programming languages. When researching tools for your development team, consider the following: One of the most popular cross-platform development frameworks is the open source Apache Cordova. The framework uses JavaScript, CSS, and HTML5 for app creation and gives developers several advantages, including direct access to a smartphone\u2019s assets (e.g. contact data, file storage, notifications). Cordova also includes a straightforward API and the ability to employ most JS frameworks. Developers may also know the popular Apache Cordova cross-platform development product PhoneGap, which was purchased from creator Nitobi in 2011 by Adobe. PhoneGap is Cordova\u2019s cloud-based development tool, which eliminates the need for compilers, hardware, and SDKs altogether. Another popular cross-platform tool is Xamarin, founded by the creators of the cross-platform implementations MonoTouch and MonoDroid. Using C#, developers can easily write and reuse their code across various platforms. Xamarin also simplifies cross-platform processes, such as the creation of dynamic layouts for iOS.  Xamarin integrates with Microsoft Visual Studio, which includes add-ins that allow for Android, iOS, and Windows development. Xamarin passed the 1 million developer milestone back in 2015. Unity is a cross-platform game engine created by Unity Technologies. Unity is used to create both 2 and 3-dimensional games for consoles (Xbox, PlayStation, etc.), desktops (Windows, Linux, macOS), and mobile (Android and iOS). The engine primarily uses C# for coding. Among the advantages of Unity are the availability of free plugins and the availability of detailed documentation to help developers with almost every aspect of the engine. However, Unity does have a steep learning curve and requires licensing fees for superior graphics and deployment, both of which can drive up development costs. NativeScript is an open-source development platform that allows developers to build Android and iOS apps using their native UIs and development libraries. Programming is done mainly through JavaScript, though NativeScript also supports Angular and TypeScript. Furthermore, NativeScript allows developers to use most JavaScript libraries that don\u2019t rely on the internet. For teams that need to create feature-rich apps, NativeScript is an excellent choice. However, keep in mind that NativeScript doesn\u2019t have as much documentation available as other platforms. This may affect development time if developers need to troubleshoot unfamiliar issues. Sencha is a web development framework specifically designed for mobile application development. Among Sencha\u2019s popular tools is Sencha Ext JS, which allows users to create HTML5 apps. Ext JS also includes over one hundred UI components and native-looking themes for Android, iOS, Windows, and even Blackberry. Finally, Sencha integrates with other cross-platform platforms like Apache Cordova. Despite these benefits, Sencha is far from free. The platform is currently priced at over three thousand per year for up to five developers. Pricing jumps up to over twelve thousand for twenty developers. Appcelerator is a development platform for building enterprise apps. The platform uses the common JavaScript language for creating native and cloud-connected mobile apps for Android, iOS, HTML5, and more. Appcelerator comes with a few advantages for companies looking to build an internal enterprise app. The use of JavaScript makes it easier for companies to find skilled developers. Appcelerator also comes with an optional virtual private cloud option for companies that handle sensitive and private data. Another enterprise-focused platform is Kony AppPlatform. This low-code development framework is great for companies that want to push their app out to workers with minimal development hassle. Developers use the drag-and-drop interface to build their apps using component from the Kony Marketplace or their own libraries. RhoMobile\u2019s development platform is great for developers looking to build data-centric enterprise apps. RhoStudio includes the Eclipse plug-in so developers can test applications without the need for emulators or hardware. From a security standpoint, RhoMobile Suite also provides automatic data encryption. Applications can be built for mobile and non-mobile OSs. We hope this guide will serve as a valuable resource for cross-platform development. We\u2019ve covered the basics of cross-platform development, including the essential languages and platforms to get started. Of course, there\u2019s nothing wrong with seeking out additional advice. Try reaching out to companies that have used cross-platform development and ask them about their experience. Also, consider reaching out to web development agencies for a consultation to hear their expert opinions. Cross-platform development, like any other development method, requires careful planning. Yet with the right resources and advice, you\u2019ll be able to move forward and create an app using the method that\u2019s best for you. If you\u2019re looking for a partner to help you build a cross-platform web and mobile apps \u2013 contact us today.  \u00a9 2018 TriFin Labs Inc., All right reserved. All other trademarks cited herein are the property of their respective owners. Privacy policy","time":1525792697,"title":"A Beginner's Guide to Cross Platform Mobile Development in 2018","type":"story","url":"https:\/\/trifinlabs.com\/cross-platform-mobile-development\/","label":3,"label_name":"dev"},{"by":"xiaoxian_z","descendants":0,"id":17021489,"kids":"None","score":3,"text":"[This article is an excerpt from our special report on Tesla\u2019s Semi Strategy: in which we compare operating cost of the electric Semi against diesel trucks, evaluate whether Tesla would make more revenue selling delivery trucks, and analyze risks facing the truck program.] Federal regulations dictate that a commercial truck driver can only drive 8 hours straight before having to take a 30 minutes break. That\u2019s 480 miles at 60mph on the highway. Less than Tesla Semi\u2019s 500 mile range. The driver can then drive for another 3 hours to complete his 11-hour daily driving limit. Adding 180 miles to his daily trip at 60mph. This totals 660 miles a day under ideal conditions (i.e. no traffic and no time wasted loading). Tesla Semi\u2019s max daily travel range is thus only compromised if its battery fails to allow the truck to cover 660 miles in a day, or when x + 80%*x < 660 miles (where x represents the truck\u2019s max range). This assumes the truck starts the day fully charged and uses the megacharger once to recharge to 80% of its max capacity. Solving for x, we get 367 miles as max range. At that point, the truck\u2019s battery would only have a max capacity of 73.6% relative to its original state. Way past the battery\u2019s end-of-life. Engineers define end-of-life when a battery\u2019s max capacity drops to 80% of its original state. We expect the Tesla Semi to hit that point after roughly 675,000 miles or ~8 years. Considering most large trucking fleets replace their trucks after 3 to 5 years to keep maintenance costs low (the average truck operated one of the largest trucking companies, Knight Swift, is only 1.9 years old), most Tesla Semi operators won\u2019t have their max daily range limited by the battery. Max daily range will however be limited by location of megachargers. 367 miles is also the minimum distance a Tesla Semi has to travel before needing to use a megacharger to achieve its optimal daily travel distance of 660 miles. The max distance a megacharger can be located from the start is 480 miles, limited by regulations on how long a driver can drive. This means truck operators absolutely need a megacharger located between 367 miles and 480 miles from departure. This strict requirement in megacharger location may limit the routes on which the truck can drive. Limiting its adoption rate. Tesla\u2019s megacharger construction plans are still unclear. It appears the first megachargers will be found at regional distribution centers of Tesla partners (likely close to metropolitan regions). Aside from Tesla partners, only trucks operating routes that pass by these stations will be able to complete their journey. Looking at North American supercharger stations as proxy, their numbers grew 100% annually from 2014 and on. From 50 in 2014 to 361 stations in 2017. Even if megacharger stations grow at this rate, it\u2019ll still be a tiny number compared to the existing 156,000+ diesel stations. Finding a spot for Tesla Semis to recharge overnight is another problem. There\u2019s already a shortage of parking spaces for tractor-trailers, with 90% of drivers reporting difficulties finding safe places to park their trucks overnight. Needing an electrical outlet won\u2019t help. Cold weather will also limit the routes Tesla trucks can operate on. According to recent research from Penn State, lithium-ion batteries have reduced capacity in freezing temperatures due to bad charge transfer, low electrolyte conductivity, and reduced solid-state diffusivity. That\u2019s bad news as half of America experiences freezing winters between December and February. Research by NASA found that the capacity of a Li-ion cell at -20\u2103 is only 60% of its room-temperature value. To combat this issue, electric vehicles use heaters to actively manage the batteries\u2019 temperature. Two different sources (Teslarati, Consumer Reports) report a reduced range of ~180 miles in real-life winter conditions for Tesla\u2019s 265 mile range Model S P85. That\u2019s a 30% drop in range. Pushing the battery\u2019s max capacity below 70%. Just enough to compromise the max daily driving range of the truck below 660 miles. One solution may be for operators maintain a mixed fleet of electric and diesel trucks. Sending the electric trucks south and diesel trucks north in winter months. And the opposite in summer months. [This article was an excerpt from our special report on Tesla\u2019s Semi Strategy: in which we compare operating cost of the electric Semi against diesel trucks, evaluate whether Tesla would make more revenue selling delivery trucks, and analyze risks facing the truck program.] By clapping more or less, you can signal to us which stories really stand out. Analyzing strategy at theBattleOfGiants.com, researching why innovation fails We exist to answer one important question: Is the company\u2019s latest strategy the most effective way to grow?","time":1525792649,"title":"Tesla Semi can go as far as diesel trucks, except when Winter's Coming","type":"story","url":"https:\/\/medium.com\/the-battle-of-giants\/teslas-semi-can-go-as-far-as-diesel-trucks-except-when-winter-s-coming-5147468bb8c9","label":7,"label_name":"random"},{"by":"reddotX","descendants":0,"id":17021482,"kids":"None","score":1,"text":"With our castor Castor now out for all to enjoy, and the Twitterverse delighted with the new minimal desktop and smooth snap integration, it\u2019s time to turn our attention to the road ahead to 20.04 LTS, and I\u2019m delighted to say that we\u2019ll kick off that journey with the Cosmic Cuttlefish, soon to be known as Ubuntu 18.10. Each of us has our own ideas of how the free stack will evolve in the next two years. And the great thing about Ubuntu is that it doesn\u2019t reflect just one set of priorities, it\u2019s an aggregation of all the things our community cares about. Nevertheless I thought I\u2019d take the opportunity early in this LTS cycle to talk a little about the thing I\u2019m starting to care more about than any one feature, and that\u2019s security. If I had one big thing that I could feel great about doing, systematically, for everyone who uses Ubuntu, it would be improving their confidence in the security of their systems and their data. It\u2019s one of the very few truly unifying themes that crosses every use case. It\u2019s extraordinary how diverse the uses are to which the world puts Ubuntu these days, from the heart of the mainframe operation in a major financial firm, to the raspberry pi duck-taped to the back of a prototype something in the middle of nowhere, from desktops to clouds to connected things, we are the platform for ambitions great and small. We are stewards of a shared platform, and one of the ways we respond to that diversity is by opening up to let people push forward their ideas, making sure only that they are excellent to each other in the pushing. But security is the one thing that every community wants \u2013 and it\u2019s something that, on reflection, we can raise the bar even higher on. So without further ado: thank you to everyone who helped bring about Bionic, and may you all enjoy working towards your own goals both in and out of Ubuntu in the next two years. \n          This entry was posted\n          on Tuesday, May 8th, 2018 at 2:45 pm          and is filed under free software, ubuntu.\n          You can follow any responses to this entry through the\n          RSS 2.0 feed.\n\n                      You can leave a response, or trackback from your own site.\n\n                   \nName (required) \nMail (will not be published) (required) \nWebsite  \n\n   Entries (RSS) \u00b7\n\t\tComments (RSS) \u00b7 \n        Log in  Copyright \u00a9 2006 - 2007 Mark Shuttleworth","time":1525792590,"title":"Cue the Cosmic Cuttlefish","type":"story","url":"http:\/\/www.markshuttleworth.com\/archives\/1521","label":7,"label_name":"random"},{"by":"yasintoy","descendants":0,"id":17021464,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Visualize your GitHub profile and use in your CV. I'll add pdf export feature as soon as\nCurrently working demo is on\nalize.me Before start, you should install redis. Install Redis : If you have't redis Follow this steps For Mac\/Linux : When the installation completes, add your GitHub Token to export TOKEN = '<enter your github token here>. Before start, you have to start redis server in a new tab redis-server \nand then run the project with python manage.py runserver MIT License Copyright (c) 2018 Yasin Toy Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","time":1525792484,"title":"Visualize Your GitHub Profile","type":"story","url":"https:\/\/github.com\/yasintoy\/alize","label":4,"label_name":"github"},{"by":"raleighm","descendants":0,"id":17021458,"kids":"None","score":1,"text":"\r\n        We are sorry but we cannot find the file you have requested. Please try again.\r\n     \r\n        If the problem persists, please contact admin@lexology.com\r\n        .\r\n     \u00a9 Copyright - Globe Business Media Group","time":1525792459,"title":"Antitrust Concerns About Big Data May Be Overblown","type":"story","url":"https:\/\/www.lexology.com\/library\/detail.aspx?g","label":7,"label_name":"random"},{"by":"mgdo","descendants":0,"id":17021456,"kids":"None","score":1,"text":"\r\n            Receive news, sky-event information, observing tips, and \r\n            more from Astronomy's weekly email newsletter.\r\n         Click here to receive a FREE e-Guide exclusively from Astronomy magazine.\r\n","time":1525792457,"title":"The case against dark matter","type":"story","url":"http:\/\/www.astronomy.com\/news\/2018\/05\/the-case-against-dark-matter","label":7,"label_name":"random"},{"by":"raleighm","descendants":0,"id":17021454,"kids":"None","score":1,"text":"\n8th May 2018\nartificiallawyer\nLegal Tech Incubators\n0\n Today UK law firm, Mishcon de Reya, has announced the five companies that will be part of its MDR LAB legal tech and prop tech incubator. They were selected following a pitch day where 16 companies from around the world presented. The move follows a successful first cohort last year which saw the innovative law firm make investments in\u00a0Everchron, who are developers of collaborative litigation management software;\u00a0and Ping, whose product automates timekeeping for lawyers and provides financial and productivity data analysis for law firms. It has also maintained relationships with the other startups that formed part of the first group. The firm said in a statement that the new cohort \u2018will now have the opportunity to work within their target market to pilot and improve their products and to gain a better understanding of how legal services are provided,\u2019 over a 10-week period. All will have access to Mishcon de Reya lawyers and other business experts from inside and outside the firm for advice, mentorship and education. Two of the selected companies are based in the UK, two in the US and one in Israel. The five companies are: \u2013 ThirdFort Founded in September 2017 and based in London, ThirdFort is at product development stage with its product: a web-hosted software platform facilitating exchange of money in property transactions. \u2013 dealWIP Based in Brooklyn, NY, dealWIP was founded in June 2017. It has a user-stage product: an integrated legal workspace platform for transactional lawyers.(See profile of dealWIP in Artificial Lawyer here.)\u00a0 \u2013 LitiGate LitiGate was founded by experienced litigator Nimrod Aharon and AI expert Guy Uziel and is based in Tel Aviv. LitiGate offers an arguments analysis solution for dispute resolution using advanced AI algorithms. The company aims to revolutionise the conduct of litigation by delivering contentious legal services faster, at less cost and with improved accuracy. \u2013 Digitory Legal Founded in 2016 and based in San Francisco, US, Digitory Legal has a user-stage product: a pricing prediction and management tool for litigators. Using historical data and industry trends, it helps customers understand what legal matters cost and why. \u2013 LawPanel London-based LawPanel was founded in January 2016. It is at user-stage with its online trademark management platform, designed to allow firms to deliver more of their services online. Mishcon de Reya\u2019s Chief Technology Officer and Director of MDR LAB, Nick West, who has been a driving force behind the project, said: \u2018The first year of MDR LAB really exceeded our expectations, as our ongoing relationships and investments in the 2017 cohort illustrate. We genuinely enjoy working with these early stage legal tech start-ups to help them bring their propositions to life.\u2019 \u2018We\u2019ve found this collaborative approach has accelerated the development of some incredible products and we\u2019re proud of what all of the 2017 cohort have achieved.\u00a0We are now really excited about this year\u2019s cohort and can\u2019t wait to get started,\u2019 he added. If you would like to read a review of what happened last year and what lawyers at the firm thought of the experience, please see this feature piece in Artificial Lawyer. The news from MDR LAB comes soon after Allen & Overy also announced the second cohort for their incubator, FUSE, which is profiled here. iDefendo \u2013 the New Blockchain Platform for Secure Data Sharing Enter your email address to subscribe to this blog and receive notifications of new posts by email. \n\n\t\t\t\t\t\t\tEmail Address\t\t\t\t\t\t\n\n \n\n\n\n\n\n FEATURED JOB Yerra Solutions \u2013 Legal Engineer \u2026 \n\t\t\tCopyright \u00a9 2018 | MH Magazine WordPress Theme by MH Themes ","time":1525792447,"title":"Legal Tech Incubator MDR LAB Launches 2nd Cohort","type":"story","url":"https:\/\/www.artificiallawyer.com\/2018\/05\/08\/legal-tech-incubator-mdr-lab-launches-2nd-cohort\/","label":5,"label_name":"ml"},{"by":"raleighm","descendants":0,"id":17021451,"kids":"None","score":1,"text":"Home > M&A > The fallacy of the fully paid share Often I will review agreements where clients are making representations that shares are fully paid upon their issuance. This practice has developed from US law and migrated north of the border over time, but the basis for it as a concern here is rather limited, even putting it charitably. Under Canadian law, shares cannot be issued at all unless\u00a0fully paid. You may want a representation that the shares have been validly issued, but once you have that, asking for one saying they are fully paid does not actually get you any further. They can\u2019t be the former without the latter. This does, however, raise a question of what a fully paid share should look like, not in terms of the appearance of the certificate, but when you can count a share as fully paid so that it can be issued and you can close your transaction. Let\u2019s take a look at the three most common ways to pay for shares: For this reason, we are increasingly encouraging clients to rely solely on wires for the payment of shares. We are always available to act as an intermediary for funds and it is a service we are happy to provide, so we or opposing counsel can receive funds and hold them until we are given your express approval to release them. Of course, all of this could end up looking very different in a few years with the arrival of blockchain. Blockchain technologies such as smart contracts may facilitate and hasten payment for shares by eliminating many of the intermediaries in the payment processing system that may account for delays in receiving and issuing funds. An in-depth discussion goes a little beyond what I usually write about in these posts, but we do have a whole section on our website dedicated to it, so don\u2019t let that stop you from exploring if you\u2019re interested. The author would like to thank Samantha Sarkozi, Articling Student, for her assistance in preparing this legal update. Stay informed on M&A developments and\u00a0subscribe\u00a0to our blog today.","time":1525792440,"title":"The fallacy of the fully paid share","type":"story","url":"https:\/\/www.deallawwire.com\/2018\/05\/08\/the-fallacy-of-the-fully-paid-share\/","label":7,"label_name":"random"},{"by":"raleighm","descendants":1,"id":17021444,"kids":"[17021696]","score":3,"text":"Opinion An alliance of heretics is making an end run around the mainstream conversation. Should we be listening? Eric WeinsteinCreditDamon Winter\/The New York Times Supported by By Bari Weiss Photographs by Damon Winter Here are some things that you will hear when you sit down to dinner with the vanguard of the Intellectual Dark Web: There are fundamental biological differences between men and women. Free speech is under siege. Identity politics is a toxic ideology that is tearing American society apart. And we\u2019re in a dangerous place if these ideas are considered \u201cdark.\u201d I was meeting with Sam Harris, a neuroscientist; Eric Weinstein, a mathematician and managing director of Thiel Capital; the commentator and comedian Dave Rubin; and their spouses in a Los Angeles restaurant to talk about how they were turned into heretics. A decade ago, they argued, when Donald Trump was still hosting \u201cThe Apprentice,\u201d none of these observations would have been considered taboo. Today, people like them who dare venture into this \u201cThere Be Dragons\u201d territory on the intellectual map have met with outrage and derision \u2014 even, or perhaps especially, from people who pride themselves on openness. It\u2019s a pattern that has become common in our new era of That Which Cannot Be Said. And it is the reason the Intellectual Dark Web, a term coined half-jokingly by Mr. Weinstein, came to exist. What is the I.D.W. and who is a member of it? It\u2019s hard to explain, which is both its beauty and its danger. Most simply, it is a collection of iconoclastic thinkers, academic renegades and media personalities who are having a rolling conversation \u2014 on podcasts, YouTube and Twitter, and in sold-out auditoriums \u2014 that sound unlike anything else happening, at least publicly, in the culture right now. Feeling largely locked out of legacy outlets, they are rapidly building their own mass media channels. The closest thing to a phone book for the I.D.W. is a sleek website that lists the dramatis personae of the network, including Mr. Harris; Mr. Weinstein and his brother and sister-in-law, the evolutionary biologists Bret Weinstein and Heather Heying; Jordan Peterson, the psychologist and best-selling author; the conservative commentators Ben Shapiro and Douglas Murray; Maajid Nawaz, the former Islamist turned anti-extremist activist; and the feminists Ayaan Hirsi Ali and Christina Hoff Sommers. But in typical dark web fashion, no one knows who put the website up. The core members have little in common politically. Bret and Eric Weinstein and Ms. Heying were Bernie Sanders supporters. Mr. Harris was an outspoken Hillary voter. Ben Shapiro is an anti-Trump conservative. But they all share three distinct qualities. First, they are willing to disagree ferociously, but talk civilly, about nearly every meaningful subject: religion, abortion, immigration, the nature of consciousness. Second, in an age in which popular feelings about the way things ought to be often override facts about the way things actually are, each is determined to resist parroting what\u2019s politically convenient. And third, some have paid for this commitment by being purged from institutions that have become increasingly hostile to unorthodox thought \u2014 and have found receptive audiences elsewhere. \u201cPeople are starved for controversial opinions,\u201d said Joe Rogan, an MMA color commentator and comedian who hosts one of the most popular podcasts in the country. \u201cAnd they are starved for an actual conversation.\u201d [Receive the day\u2019s most urgent debates right in your inbox by subscribing to the Opinion Today newsletter.]  That hunger has translated into a booming and, in many cases, profitable market. Episodes of \u201cThe Joe Rogan Experience,\u201d which have featured many members of the I.D.W., can draw nearly as big an audience as Rachel Maddow. A recent episode featuring Bret Weinstein and Ms. Heying talking about gender, hotness, beauty and #MeToo was viewed on YouTube over a million times, even though the conversation lasted for nearly three hours. Ben Shapiro\u2019s podcast, which airs five days a week, gets 15 million downloads a month. Sam Harris estimates that his \u201cWaking Up\u201d podcast gets one million listeners an episode. Dave Rubin\u2019s YouTube show has more than 700,000 subscribers. Offline and in the real world, members of the I.D.W. are often found speaking to one another in packed venues around the globe. In July, for example, Jordan Peterson, Douglas Murray and Mr. Harris will appear together at the O2 Arena in London. But as the members of the Intellectual Dark Web become genuinely popular, they are also coming under more scrutiny. On April 21, Kanye West crystallized this problem when he tweeted seven words that set Twitter on fire: \u201cI love the way Candace Owens thinks.\u201d Candace Owens, the communications director for Turning Point USA, is a sharp, young, black conservative \u2014 a telegenic speaker with killer instincts who makes videos with titles like \u201cHow to Escape the Democrat Plantation\u201d and \u201cThe Left Thinks Black People Are Stupid.\u201d Mr. West\u2019s praise for her was sandwiched inside a longer thread that referenced many of the markers of the Intellectual Dark Web, like the tyranny of thought policing and the importance of independent thinking. He was photographed watching a Jordan Peterson video. All of a sudden, it seemed, the I.D.W. had broken through to the culture-making class, and a few in the group flirted with embracing Ms. Owens as their own. Yet Ms. Owens is a passionate Trump supporter who has dismissed racism as a threat to black people while arguing, despite evidence to the contrary, that immigrants steal their jobs. She has also compared Jay-Z and Beyonc\u00e9 to slaves for supporting the Democratic Party. Many others in the I.D.W. were made nervous by her sudden ascendance to the limelight, seeing Ms. Owens not as a sincere intellectual but as a provocateur in the mold of Milo Yiannopoulos. For the I.D.W. to succeed, they argue, it needs to eschew those interested in violating taboo for its own sake. \u201cI\u2019m really only interested in building this intellectual movement,\u201d Eric Weinstein said. \u201cThe I.D.W. has bigger goals than anyone\u2019s buzz or celebrity.\u201d And yet, when Ms. Owens and Charlie Kirk, the executive director of Turning Point USA, met last week with Mr. West at the Southern California Institute of Architecture, just outside of the frame \u2014 in fact, avoiding the photographers \u2014 was Mr. Weinstein. He attended both that meeting and a one-on-one the next day for several hours at the mogul\u2019s request. Mr. Weinstein, who can\u2019t name two of Mr. West\u2019s songs, said he found the Kardashian spouse \u201ckind and surprisingly humble despite his unpredictable public provocations.\u201d He has also tweeted that he\u2019s interested to see what Ms. Owens says next. This episode was the clearest example yet of the challenge this group faces: In their eagerness to gain popular traction, are the members of the I.D.W. aligning themselves with people whose views and methods are poisonous? Could the intellectual wildness that made this alliance of heretics worth paying attention to become its undoing? There is no direct route into the Intellectual Dark Web. But the quickest path is to demonstrate that you aren\u2019t afraid to confront your own tribe. The metaphors for this experience vary: going through the phantom tollbooth; deviating from the narrative; falling into the rabbit hole. But almost everyone can point to a particular episode where they came in as one thing and emerged as something quite different. A year ago, Bret Weinstein and Heather Heying were respected tenured professors at Evergreen State College, where their Occupy Wall Street-sympathetic politics were well in tune with the school\u2019s progressive ethos. Today they have left their jobs, lost many of their friends and endangered their reputations. All this because they opposed a \u201cDay of Absence,\u201d in which white students were asked to leave campus for the day. For questioning a day of racial segregation cloaked in progressivism, the pair was smeared as racist. Following threats, they left town for a time with their children and ultimately resigned their jobs. \u201cNobody else reacted. That\u2019s what shocked me,\u201d Mr. Weinstein said. \u201cIt told me that a culture that told itself it was radically open-minded was actually a culture cowed by fear.\u201d Sam Harris says his moment came in 2006, at a conference at the Salk Institute with Richard Dawkins, Neil deGrasse Tyson and other prominent scientists. Mr. Harris said something that he thought was obvious on its face: Not all cultures are equally conducive to human flourishing. Some are superior to others. \u201cUntil that time I had been criticizing religion, so the people who hated what I had to say were mostly on the right,\u201d Mr. Harris said. \u201cThis was the first time I fully understood that I had an equivalent problem with the secular left.\u201d After his talk, in which he disparaged the Taliban, a biologist who would go on to serve on President Barack Obama\u2019s Commission for the Study of Bioethical Issues approached him. \u201cI remember she said: \u2018That\u2019s just your opinion. How can you say that forcing women to wear burqas is wrong?\u2019 But to me it\u2019s just obvious that forcing women to live their lives inside bags is wrong. I gave her another example: What if we found a culture that was ritually blinding every third child? And she actually said, \u2018It would depend on why they were doing it.\u2019\u201d His jaw, he said, \u201cactually fell open.\u201d \u201cThe moral confusion that operates under the banner of \u2018multiculturalism\u2019 can blind even well-educated people to the problems of intolerance and cruelty in other communities,\u201d Mr. Harris said. \u201cThis had never fully crystallized for me until that moment.\u201d Before September 2016, Jordan Peterson was an obscure psychology professor at the University of Toronto. Then he spoke out against Canada\u2019s Bill C-16, which proposed amending the country\u2019s human-rights act to outlaw discrimination based on gender identity and expression. He resisted on the grounds that the bill risked curtailing free speech by compelling people to use alternative gender pronouns. He made YouTube videos about it. He went on news shows to protest it. He confronted protesters calling him a bigot. When the university asked him to stop talking about it, including sending two warning letters, he refused. While most people in the group faced down comrades on the political left, Ben Shapiro confronted the right. He left his job as editor at large of Breitbart News two years ago because he believed it had become, under Steve Bannon\u2019s leadership, \u201cTrump\u2019s personal Pravda.\u201d In short order, he became a primary target of the alt-right and, according to the Anti-Defamation League, the No. 1 target of anti-Semitic tweets during the presidential election. Other figures in the I.D.W., like Claire Lehmann, the founder and editor of the online magazine Quillette, and Debra Soh, who has a Ph.D. in neuroscience, self-deported from the academic track, sensing that the spectrum of acceptable perspectives and even areas of research was narrowing. Dr. Soh said that she started \u201cwaking up\u201d in the last two years of her doctorate program. \u201cIt was clear that the environment was inhospitable to conducting research,\u201d she said. \u201cIf you produce findings that the public doesn\u2019t like, you can lose your job.\u201d When she wrote an op-ed in 2015 titled \u201cWhy Transgender Kids Should Wait to Transition,\u201d citing research that found that a majority of gender dysphoric children outgrow their dysphoria, she said her colleagues warned her, \u201cEven if you stay in academia and express this view, tenure won\u2019t protect you.\u201d Nowadays Ms. Soh has a column for Playboy and picks up work as a freelance writer. But that hardly pays the bills. She\u2019s planning to start a podcast soon and, like many members of the I.D.W., has a Patreon account where \u201cpatrons\u201d can support her work. These donations can add up. Mr. Rubin said his show makes at least $30,000 a month on Patreon. And Mr. Peterson says he pulls in some $80,000 in fan donations each month. Mr. Peterson has endured no small amount of online hatred and some real-life physical threats: In March, during a lecture at Queen\u2019s University in Ontario, a woman showed up with a garrote. But like many in the I.D.W., he also seems to relish the outrage he inspires. \u201cI\u2019ve figured out how to monetize social justice warriors,\u201d Mr. Peterson said in January on Joe Rogan\u2019s podcast. On his Twitter feed, he called the writer Pankaj Mishra, who\u2019d written an essay in The New York Review of Books attacking him, a \u201csanctimonious prick\u201d and said he\u2019d happily slap him. And the upside to his notoriety is obvious: Mr. Peterson is now arguably the most famous public intellectual in Canada, and his book \u201c12 Rules for Life\u201d is a best-seller. The exile of Bret Weinstein and Ms. Heying from Evergreen State brought them to the attention of a national audience that might have come for the controversy but has stayed for their fascinating insights about subjects including evolution and gender. \u201cOur friends still at Evergreen tell us that the protesters think they destroyed us,\u201d Ms. Heying said. \u201cBut the truth is we\u2019re now getting the chance to do something on a much larger scale than we could ever do in the classroom.\u201d \u201cI\u2019ve been at this for 25 years now, having done all the MSM shows, including Oprah, Charlie Rose, \u2018The Colbert Report,\u2019 Larry King \u2014 you name it,\u201d Michael Shermer, the publisher of Skeptic magazine, told me. \u201cThe last couple of years I\u2019ve shifted to doing shows hosted by Joe Rogan, Dave Rubin, Sam Harris and others. The I.D.W. is as powerful a media as any I\u2019ve encountered.\u201d Mr. Shermer, a middle-aged science writer, now gets recognized on the street. On a recent bike ride in Santa Barbara, Calif., he passed a work crew and \u201cthe flag man stopped me and says: \u2018Hey, you\u2019re that skeptic guy, Shermer! I saw you on Dave Rubin and Joe Rogan!\u2019\u201d When he can\u2019t watch the shows on YouTube, he listens to them as podcasts on the job. On breaks, he told Mr. Shermer, he takes notes. \u201cI\u2019ve had to update Quillette\u2019s servers three times now because it\u2019s caved under the weight of the traffic,\u201d Ms. Lehmann said about the publication most associated with this movement. Yet there are pitfalls to this audience-supported model. One risk is what Eric Weinstein has called \u201caudience capture.\u201d Since stories about left-wing-outrage culture \u2014 the fact that the University of California, Berkeley, had to spend $600,000 on security for Mr. Shapiro\u2019s speech there, say \u2014 take off with their fans, members of the Intellectual Dark Web may have a hard time resisting the urge to deliver that type of story. This probably helps explain why some people in this group talk constantly about the regressive left but far less about the threat from the right. \u201cThere are a few people in this network who have gone without saying anything critical about Trump, a person who has assaulted truth more than anyone in human history,\u201d Mr. Harris said. \u201cIf you care about the truth, that is quite strange.\u201d Emphasis is one problem. Associating with genuinely bad people is another. Go a click in one direction and the group is enhanced by intellectuals with tony affiliations like Steven Pinker at Harvard. But go a click in another and you\u2019ll find alt-right figures like Stefan Molyneux and Milo Yiannopoulos and conspiracy theorists like Mike Cernovich (the #PizzaGate huckster) and Alex Jones (the Sandy Hook shooting denier). It\u2019s hard to draw boundaries around an amorphous network, especially when each person in it has a different idea of who is beyond the pale. \u201cI don\u2019t know that we are in the position to police it,\u201d Mr. Rubin said. \u201cIf this thing becomes something massive \u2014 a political or social movement \u2014 then maybe we\u2019d need to have some statement of principles. For now, we\u2019re just a crew of people trying to have the kind of important conversations that the mainstream won\u2019t.\u201d But is a statement of principles necessary to make a judgment call about people like Mr. Cernovich, Mr. Molyneux and Mr. Yiannopoulos? Mr. Rubin has hosted all three on his show. And he appeared on a typically unhinged episode of Mr. Jones\u2019s radio show, \u201cInfowars.\u201d Mr. Rogan regularly lets Abby Martin \u2014 a former 9\/11 Truther who is strangely sympathetic to the regimes in Syria and Venezuela \u2014 rant on his podcast. He also encouraged Mr. Jones to spout off about the moon landing being fake during Mr. Jones\u2019s nearly four-hour appearance on his show. When asked why he hosts people like Mr. Jones, Mr. Rogan has insisted that he\u2019s not an interviewer or a journalist. \u201cI talk to people. And I record it. That\u2019s it,\u201d he has said. Mr. Rubin doesn\u2019t see this is a problem. \u201cThe fact is that Jones reaches millions of people,\u201d he said. \u201cGoing on that show means I get to reach them, and I don\u2019t think anyone is a lost cause. I\u2019ve gotten a slew of email from folks saying that they first heard me on Jones, but then watched a bunch of my interviews and changed some of their views.\u201d The subject came up at that dinner in Los Angeles. Mr. Rubin, whose mentor is Larry King, insisted his job is just to let the person sitting across from him talk and let the audience decide. But with a figure like Mr. Cernovich, who can occasionally sound reasonable, how is a viewer supposed to know better? Of course, the whole notion of drawing lines to keep people out is exactly what inspired the Intellectual Dark Web folks in the first place. They\u2019re committed to the belief that setting up no-go zones and no-go people is inherently corrupting to free thought. \u201cYou have to understand that the I.D.W. emerged as a response to a world where perfectly reasonable intellectuals were being regularly mislabeled by activists, institutions and mainstream journalists with every career-ending epithet from \u2018Islamophobe\u2019 to \u2018Nazi,\u2019\u201d Eric Weinstein said. \u201cOnce I.D.W. folks saw that people like Ben Shapiro were generally smart, highly informed and often princely in difficult conversations, it\u2019s more understandable that occasionally a few frogs got kissed here and there as some I.D.W. members went in search of other maligned princes.\u201d But people who pride themselves on pursuing the truth and telling it plainly should be capable of applying these labels when they\u2019re deserved. It seems to me that if you are willing to sit across from an Alex Jones or Mike Cernovich and take him seriously, there\u2019s a high probability that you\u2019re either cynical or stupid. If there\u2019s a reason for shorting the I.D.W., it\u2019s the inability of certain members to see this as a fatal error. What\u2019s more, this frog-kissing plays perfectly into the hands of those who want to discredit the individuals in this network. In recent days, for example, Mr. Harris has been labeled by the Southern Poverty Law Center as a bridge to the alt-right: \u201cUnder the guise of scientific objectivity, Harris has presented deeply flawed data to perpetuate fear of Muslims and to argue that black people are genetically inferior to whites.\u201d That isn\u2019t true. The group excoriated Mr. Harris, a fierce critic of the treatment of women and gays under radical Islam, for saying that \u201csome percentage, however small\u201d of Muslim immigrants are radicalized. He has also estimated that some 20 percent of Muslims worldwide are Islamists or jihadis. But he has never said that this should make people fear all Muslims. He has defended the work of the social scientist Charles Murray, who argues that genetic differences may explain differences in average IQ across racial groups \u2014 while insisting that this does not make one group inferior to another. But this kind of falsehood is much easier to spread when other figures in the I.D.W. are promiscuous about whom they\u2019ll associate with. When Mr. West tweeted his praise for Ms. Owens, the responses of the people in the network reflected each person\u2019s attitude toward this problem. Dave Rubin took to Twitter to defend Ms. Owens and called Mr. West\u2019s tweet a \u201cgame changer.\u201d Jordan Peterson went on \u201cFox and Friends\u201d to discuss it. Bret Weinstein subtweeted his criticism of these choices: \u201cSmart, skeptical people are often surprisingly susceptible to being conned if a ruse is tailored to their prejudices.\u201d His brother was convinced that Mr. West was playing an elaborate game of chess. Ms. Heying and Mr. Harris ignored the whole thing. Ben Shapiro mostly laughed it off. Mr. West is a self-obsessed rabble-rouser who brags about not reading books. But whether or not one approves of the superstar\u2019s newest intellectual bauble, it is hard to deny that he has consistently been three steps ahead of the zeitgeist. So when he tweets \u201conly freethinkers\u201d and \u201cIt\u2019s no more barring people because they have different ideas,\u201d he is picking up on a real phenomenon: that the boundaries of public discourse have become so proscribed as to make impossible frank discussions of anything remotely controversial. \u201cSo many of our institutions have been overtaken by schools of thought, which are inherently a dead end,\u201d Bret Weinstein said. \u201cThe I.D.W. is the unschooling movement.\u201d Am I a member of this movement? A few months ago, someone suggested on Twitter that I should join this club I\u2019d never heard of. I looked into it. Like many in this group, I am a classical liberal who has run afoul of the left, often for voicing my convictions and sometimes simply by accident. This has won me praise from libertarians and conservatives. And having been attacked by the left, I know I run the risk of focusing inordinately on its excesses \u2014 and providing succor to some people whom I deeply oppose. I get the appeal of the I.D.W. I share the belief that our institutional gatekeepers need to crack the gates open much more. I don\u2019t, however, want to live in a culture where there are no gatekeepers at all. Given how influential this group is becoming, I can\u2019t be alone in hoping the I.D.W. finds a way to eschew the cranks, grifters and bigots and sticks to the truth-seeking. \u201cSome say the I.D.W. is dangerous,\u201d Ms. Heying said. \u201cBut the only way you can construe a group of intellectuals talking to each other as dangerous is if you are scared of what they might discover.\u201d Bari Weiss is a staff editor and writer for the Opinion section.\u00a0@bariweiss Damon Winter joined The Times as a photographer in 2007. He won the 2009 Pulitzer Prize for Feature Photography for his coverage of Barack Obama\u2019s presidential campaign.\u00a0 Advertisement    Collapse SEE MY OPTIONS","time":1525792387,"title":"Meet the Renegades of the Intellectual Dark Web","type":"story","url":"https:\/\/mobile.nytimes.com\/2018\/05\/08\/opinion\/intellectual-dark-web.html","label":7,"label_name":"random"},{"by":"raleighm","descendants":0,"id":17021430,"kids":"None","score":1,"text":"Who maintains the security and stability of the internet\u2014and how do they do it? It\u2019s a simple question, but a difficult one to answer. Internet security, writ large, comprises a diverse set of social and technical tools and an equally diverse set of industry norms around mitigating and remediating abusive behavior. Those tools are developed and used by what I term operational security communities\u2014groups of individuals, largely unaffiliated with governments, that do the day-to-day work of maintaining the security and stability of the internet. What these communities actually do, and the scope and nature of the challenges that they face, is often poorly understood, even among sophisticated state actors. But one of the key mechanisms on which operational security communities rely is a surprisingly familiar one: reputation. Let\u2019s begin with a simple statistic:\u00a0At least 90 percent of the email that transits the internet is unsolicited and unwanted\u2014what the operational security communities refer to as \u201cabuse.\u201d For the most part, though, these messages do not appear in user mailboxes. Mailbox providers\u2014internet service providers such Comcast and Verizon and webmail providers such as Yahoo! Mail and Gmail\u2014take the necessary actions to stop abusive messages at their borders, protecting both end users and their own infrastructure. In protecting users from these illicit messages, these providers are part of the anti-abuse community\u2014one subset of the broader Internet security industry. Email may be the internet\u2019s original \u201ckiller application,\u201d but it is notoriously difficult to secure. What\u2019s more, it\u2019s one of the easiest ways for a bad actor to infect a computer with malware\u2014it\u2019s what\u2019s called an \u201cinfection vector.\u201d (Cryptolocker, a strain of ransomware circulated by email attachment, is a textbook instance of this.) Because of email\u2019s prominence and its vulnerabilities, efforts to secure email characterize the canonical challenges facing security professionals every day. The first challenge is how to install safeguards that will not get in the way of everyday use. The second (often an unintended consequence of the first) is how to avoid creating bigger problems when users discard or circumvent \u201csafeguards\u201d that do get in the way of everyday workflows\u2014which further obscure the root causes of the security problem. The anti-abuse community leverages reputation-based mechanisms to mitigate both of these problems. By reputation, I mean just that: When it becomes clear to the anti-abuse community that abusive actors are using a particular network to send abusive messages, that network garners a bad reputation. Rather than placing security burdens on the end user\u2014like requiring users to maintain a spam filter themselves\u2014reputation mechanisms block messages at the border to avoid exposing end users to abusive messages (and their malicious payloads) and denying malicious actors their prize. Reputation is not an exotic or esoteric tool, but a common, powerful enforcement mechanism that is often overlooked. It plays out tacitly, and often informally, in the behavioral norms of rural communities, amongst firms vying for market position, and in transnational politics. In each case, reputation is a form of soft power that operates at the interstices of community best practices and the black letter of the law to enforce norms that fill the gaps or avoid the transaction costs associated with conventional legal processes. A classic instance from rural communities comes from Robert Ellickson\u2019s \u201cOrder Without Law.\u201d Cattle ranchers in Shasta County California have well-defined informal norms and processes for managing common resources. When cattle stray or destroy private property or commonly accepted rights-of-way, ranchers choose not to invoke formal legal processes to mediate conflicts, but instead turn to informal norms and reputation for compliance with those norms. At its core, reputation in messaging works much like in any other community. When an actor repeatedly violates an accepted norm, members of the community proximate to the violation notice. And while the internet is vast, at the level of email and routing infrastructure operations, the security operations community is a surprisingly small, tight-knit community\u2014much like rural communities that rely on informal norms. In particular, the gatekeepers to messaging are much more like a rural community\u2014they all know each other. They all share information about where they see abusive messages coming from. And they all have common incentives to block abusive messages: protecting end users and reducing the operational costs of malware infections. The cornerstone of reputation is trusted professional relationships. To operate, as a transnational enforcement mechanism, though, reputation must also scale. The anti-abuse community has substantive access to information about networks originating abuse: tools facilitating user reports of abusive messages, malware detection tools, and honeypots. In fora like the Messaging, Malware, and Mobile Anti-Abuse Working Group and the Anti-Phishing Working Group, these professionals have developed best practices and supporting tools for collecting information about not only where abuse is coming from, but also how to share that information quickly and effectively in support of ongoing mitigation efforts. There are also what I will call \u201creputation aggregators\u201d (often referred to by the community as blocking lists), which offer neutral, third-party scaling mechanisms. Mailbox operators provide these actors with real-time data feeds about which networks are sending abusive messages. When these reputation aggregators see the same kind of abuse, originating from the same network, reported by mailbox operators all over the world\u2014and also see similar abuse from their own honeypots\u2014they have sufficient evidence to issue a public reputation advisory. In effect, the reputation aggregator publicly announces, \u201cWe have seen sufficient evidence, from diverse sources, that network X is consistently sending abusive messages. We recommend you block messages from network X until we remove this advisory.\u201d In turn, mailbox operators around the world use these recommendations to block messages from abusive networks, protecting end users while staying out of the way of everyday workflows. Reputation is not only an enforcement mechanism\u2014it is also a signaling mechanism. In some cases, an actor new to the messaging industr, who is not yet part of the anti-abuse community, simply may not know the rules. A reputation \u201cding\u201d is one way to signal that there are rules, and that there are also consequences to violating those rules. The anti-abuse community publishes its best practices and is quite willing to help newcomers find their way; once the newcomer eliminates the source of abusive messaging, reputation aggregators will recognize the change in their data feeds and remove the blocking advisory. In the successful case, these new entrants embrace anti-abuse norms as good business practices. As they develop their own monitoring tools, the newcomers soon become good remediators: networks that quickly recognize when they have an abusive actor on their network, often before reputation aggregators, and quickly resolve the problem. Reputation aggregators recognize this credible commitment and often opt to send these actors private signals rather than publishing public advisories. Such a private signal reduces impact on the good remediator, improves industry relations, and ultimately improves the good remediator\u2019s bottom line. Not all actors that know the rules are credibly committed, though. Rather than embrace anti-abuse norms, some actors are satisficers: They see best practices as exogenously imposed costs and view the cost of remediating as greater than the cost of losing business, even if that line of business creates costs for others in the messaging industry and risks for end users. Reputation aggregators deal with satisficers with graduated sanctions. As the satisficer persists, the reputation aggregator recommends blocking more and more of the satisficer\u2019s network infrastructure, imposing costs not only on abusive actors but also on that satisficer\u2019s legitimate customers. Satisficers only respond when these customers complain about service interruptions. Satisficers react to resolve the immediate problem, repairing their immediate reputation, but do little more. It is only a matter of time before they take on another abusive client, and the cycle starts again. Ultimately, this is a losing strategy. Legitimate customers have a low tolerance for service interruptions and take their business elsewhere. If the satisficer does not recognize this trend in time, they can be left with nothing but abusive clients. In the worst case, the satisficer gives up on legitimate messaging and becomes a haven for abusive actors, shifting their efforts from satisficing to the rules to concerted efforts to circumvent these rules in order to maintain a revenue stream. They promulgate messages whose value is exclusively derived from malicious payloads such as those supporting phishing, ransomware, and botnet campaigns. T In the case of the well-intentioned newcomer, we see the success and benefits of reputation. In the case of the satisficer, that ultimately becomes a haven for abuse, and we see the limits. Reputation can guide a marketing firm that doesn\u2019t yet know the rules, and it can deny abusive actors access to legitimate markets for messaging infrastructure. But it does not remediate the root cause of that abusive behavior, the dedicated criminals that use these infrastructures for illicit economic and political gain. When reputation mechanisms reach the limits of their efficacy for disciplining such recidivists, other mechanisms come into play. Reputation aggregators, often in concert with others in anti-abuse community, reach out to law enforcement (LE) to supplement their efforts, offering intelligence on criminal activity that state actors can leverage in evidence-collection efforts supporting prosecutions and takedowns necessary to ultimately remediate abuse. This cooperation does not always come to fruition, of course. But there are a number of well-known instances\u2014Zeus Gameover and Cryptolocker, DNSChanger, and Avalanche\u2014in which the members of the anti-abuse community, the broader cybersecurity community, and law enforcement have acted cooperatively, working together towards a common goal. Reputation mechanisms have a variety of benefits. First and most obviously, they are a powerful mechanism for monitoring and enforcing norms in a seemingly chaotic internet. But there are other benefits as well. Reputation is not only an enforcement mechanism, it is also a nuanced mechanism for initial signaling: it is just as much about opening a dialog about best practices as it is about enforcing norms. Reputation mechanisms also demonstrate that industry actors can collaborate to create a collateral public good\u2014and as implied in the last section of this analysis, these actors also know when to reach out to state authorities to supplement their private enforcement capabilities. Such lessons suggest that rather than continuously looking to high politics for solutions that promote internet stability and security, perhaps policymakers should shift our gaze to the parts of the web that society takes for granted, and take lessons from those institutions, processes, and norm entrepreneurs underneath the hood that continuously work to ensure we all have the safe and secure online experiences we expect. \u00a9 2018 The Lawfare Institute","time":1525792249,"title":"The Role of Norms in Internet Security: Reputation and Its Limits","type":"story","url":"https:\/\/www.lawfareblog.com\/role-norms-internet-security-reputation-and-its-limits","label":7,"label_name":"random"},{"by":"macco","descendants":0,"id":17021426,"kids":"None","score":2,"text":"A collection of tools to easily generate assets such as launcher icons for your Android app.","time":1525792205,"title":"A collection of tools to easily generate assets for your Android app","type":"story","url":"https:\/\/romannurik.github.io\/AndroidAssetStudio\/","label":7,"label_name":"random"},{"by":"bamblehorse","descendants":0,"id":17021401,"kids":"None","score":1,"text":"","time":1525792059,"title":"Show HN: Visualizing a Cube in CSS","type":"story","url":"https:\/\/codepen.io\/Bamblehorse\/full\/odoJNq\/","label":7,"label_name":"random"},{"by":"akulkarni","descendants":0,"id":17021393,"kids":"None","score":2,"text":"As a member of the Star Wars generation (or Xennial), I more or less missed the Harry Potter craze when it happened. Sure, I saw all of the movies in the theatre and witnessed the minor miracle of long lines of children outside bookstores every time a new installment came out. How could fast paced action scenes on flying vehicles, an orphan with hidden powers and a Dark Lord reviving an evil empire as a nemesis compare to Star W\u200a\u2014\u200aok, they were similar, but I liked my Space Opera and didn\u2019t internalize any of the excitement about a magic British boarding school. That is, I didn\u2019t until my son decided he loved Harry Potter. Having a rough idea of what was in the books, I was a little nervous about it being too scary for a 5 year old, but a distinct plus in my eyes for reading them as a bedtime story was the character of Hermione. What would be better for a little boy than a story with a tough, smart female character, written by a woman who takes on misogynists on Twitter like a boss? Seemed like a good way to help him see women as full people before the world taught him differently. But the reality didn\u2019t match my hopes. In the books, Hermione frequently cried or whined or timidly took action\u200a\u2014\u200athe way the narrator told it, she was not the smart, capable hero I pictured her as. She is introduced as someone with \u201ca bossy sort of voice\u201d. Bossy. This blog post outlines my process for a project where I set out to programmatically identify gender bias in the Harry Potter series. I did it because I was disappointed in the reality the female characters are frequently described with biased language. And I wanted to prove it. Channeling my inner Hermione My first step was to make sure I wasn\u2019t getting it wrong. Was I imaging this? Nope. There are some good blog posts and magazine articles on this topic, and academics have written articles about the gender stereotypes in the Harry Potter novels too. They point out how badly Ron treats Hermione (ostensibly her friend then love interest), portrayals of Aunt Petunia and Mrs. Weasley as nagging mothers, the lack of female representation in positions of power at Hogwarts and the Ministry of Magic, everything about the Veelas, and so on. While some of these articles mentioned the language used to describe female behavior (with \u201cgiggling\u201d and \u201csqueaking\u201d most frequently cited), I couldn\u2019t find a quantitative analysis of the language anywhere. Probably because there are more than 1.1 million words in the seven books. Not trivial. So, I decided to do that analysis myself. With code. Approaching the problem This could be a massive project, but I needed to start with a reasonable scope, so I settled on this as a research question: compared to Harry and Ron, are Hermione\u2019s actions described exclusively in the narration with some negative words usually directed at women\u00a0? To get the data I needed for this, I used the following tools: There were three steps I needed to take to answer my question: This post will take us quickly through high level explanations of the steps as I explained them in a talk at Codeland 2018. For more details on how I did it, you can check out the Github repo, or stay tuned for future posts where I\u2019ll dive into each step in a lot of detail. Finding the answer Step 1: Get the text into a format that Python can read. We need to go from a text file to a format that Python can understand and we can use our tools with. That means reading a file into our program and splitting the text into a list of individual words, as shown below. You might wonder why I didn\u2019t take out the punctuation. Good catch. You\u2019ll see why in a moment. 2. Isolate the parts of the text to analyze. For this project, I\u2019m looking just at narration, only for the three principle characters, and only how actions are described. That means that I need to: separate the narration from the dialog, and find references to the three protagonists. Separating the narration from the dialog is what I need the quotation marks for\u200a\u2014\u200aI wrote an algorithm to find text between quotation marks and categorize it as dialog, the categorize the rest as narrative. This will be the topic of a future post. The next task was to chop up the narrative into sentences by splitting on the periods, and group them based on mentions of the protagonists by name. While this had the downside that I dropped some sentences where the characters were referred to in other ways (e.g. \u2018she\u2019, \u2018he\u2019, \u2018her\u2019, etc.) this approach made the exercise much more precise. I used a dictionary\u200a\u2014\u200awith \u2018Harry\u2019, \u2018Ron\u2019 and \u2018Hermione\u2019 as keys\u200a\u2014\u200ato organize the snippets of text into groups. The final part of this step was to identify the parts of speech I wanted to analyze: verbs and adverbs. To do this, I used NLTK\u2019s pos_tag function, which identifies what part of speech each word represents and assigns it the corresponding code (e.g. NNP is a proper noun, JJ is an adverb, etc). How this works will be the subject of a future post too! Here\u2019s how this looks using the same text snippet from The Philosopher\u2019s Stone. Here comes the really fun part\u200a\u2014\u200aanalysis! 3. Find and summarize occurrences of the the relevant words. Now that we have each word labeled as a part of speech, we need to grab all of the verbs and adverbs for each character and find the ones that are unique to each. To do this, I needed to write some code to find the verbs and adverbs associated with a specific noun. Even though I had split the narrative by character name, I couldn\u2019t simply take all verbs and adverbs from those sentences and associate them with the character. The text in our example is a good demonstration of why: Notice that: English is a complex language, and while parsing this sentence in this way isn\u2019t necessarily challenging for a person who speaks English, teaching a computer this isn\u2019t trivial. In my algorithm, I look for the following language patterns around each character\u2019s name: How I did this will be the subject of a future blog post, too! This enabled me to make a new dictionary. Again, the character names are the keys, but the values are only the verbs and adverbs used to describe that character. For example, in our sample text: The final step is to find which of these words are only used to describe each character in the book. To do this for each of the characters, I made a set of all of the words used to describe the other two, then looped through the characters\u2019 words to check if they appeared in the set. If they didn\u2019t, they went into a new dictionary of unique descriptors. With our text sample, that dictionary of unique words looks like this: Effectively, because both Harry and Hermione \u2018said\u2019 something, that word was not unique, and didn\u2019t make the cut. Here\u2019s a summary of the actions in this step and the changes in the data with each iteration. That\u2019s it! But what did I find? The analysis It really isn\u2019t just me\u200a\u2014\u200aHermione is described at times by Rowling with words that are applied almost exclusively to women and girls, words that are not used when she writes about Harry and Ron. The image below shows word clouds for each of the 7 books, representing frequency of action words that are used in that book to describe Hermione, but not Harry and Ron. Notice that Hermione is frequently described as squealing, crying shrieking, squeaking, doing things breathlessly, cooly and timidly. Sure, some of the words are innocuous, but many are not, especially when they are only being used to describe the female character, and when that female character is smart and resourceful. Also, teenaged boys are squeaky too. Yet they are not described that way. Sure, you may say, but what about the words used exclusively to describe Harry and Ron? I looked at those too, and it doesn\u2019t make me feel much better, to be honest. In the table below, rows represent Harry, Ron and Hermione, columns are for each of the 7 books. In each cell, I\u2019ve listed the top 10 \u201cexclusive\u201d verbs and adverbs for that character as word, frequency\u00a0. The numbers may look low, but keep in mind I gave up some volume for accuracy. So, what about Harry and Ron\u2019s unique descriptors, what can those tell us? I think it tells us what\u2019s special about each of these characters relative to one another. Here\u2019s what I take from this data: Conclusions I don\u2019t hate Harry Potter, and I didn\u2019t stop reading the books to my son. I\u2019m still partial to the galaxy far, far away, but I\u2019ve become a Potter fan. On Pottermore I found out my house is Ravenclaw (obvs), and I went out and purchased a t-shirt to let the world know this. My patronus is a mole, which is unfortunate when I was hoping for wolf or lynx or lioness or something more clearly badass, but I have learned to accept it. When my son has screen time, I suggest a Harry Potter movie. Totally for him, of course. Talking to someone about this project, they told me that they had always found Hermione \u201clow key annoying\u201d and hadn\u2019t really stopped to think why. I think this is why. Even the best writer can\u2019t undo all of their biases when creating female characters. Hermione might be the greatest witch (or wizard, let\u2019s be real) of her age, but our inherent biases let this coexist with narrative that paints her in a way that reinforces stereotypes about women. These biases so engrained, we barely notice they are being tripped. So, what would I like you to take from this? Always watch for bias, even in the literature and media that you love, and when you see it, name it, acknowledge it, and point it out. Especially to the kids in your life. By clapping more or less, you can signal to us which stories really stand out. Product manager, pythonista, lover of NLP and AI. Answering human questions with artificial intelligence","time":1525791981,"title":"A Bossy Sort of Voice: Quantifying Gender Bias in \u201cHarry Potter\u201d with Python","type":"story","url":"https:\/\/medium.com\/agatha-codes\/a-bossy-sort-of-voice-3c3a18de3093","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17021388,"kids":"None","score":3,"text":"\n When Zach Aarons and two colleagues launched the real estate tech accelerator Metaprop in 2015, few major landlords were thinking seriously about disruption in their industry, let alone investing money in exploring the possibilities. Three years later the city's proptech sector has exploded, attracting billions of dollars in funding in 2017 alone. \"This is no longer fringe stuff that's only talked about by enthusiastic wackos like me,\" Aarons said. \"You walk into every corporate boardroom of every global real estate company and this is one of their major initiatives.\" Of all the factors that helped create the primordial soup of the local proptech scene\u2014including the meteoric rise of game-changing startups WeWork and Compass\u2014Aarons believes that the most important ingredient was fear. The commercial and residential markets have softened in the past two years after establishing new post-recession heights. Profit margins have narrowed, and sales prices and rents have plateaued. \"All those things scare real estate professionals,\" Aarons said. \"And when you're scared, you become more aggressive in protecting your flank.\" One way to do that is to gain an operational edge over competitors, and a crop of startups is eager to offer just that, with some of the oldest real estate families now picking up the tab. Because New York City is the real estate capital of the country, many expect proptech to thrive here rather than in the low-rise, sprawling communities of Silicon Valley. But it remains to be seen how big an impact new companies can have on an industry that has been staunchly impervious to change. The current collection of angel investors, venture capitalists and private-equity backers has yet to spawn any startups aside from WeWork and Compass that threaten to replace or transform a significant sector of the business. Until that happens, it's difficult to predict what will come next. Despite the size and complexity of the local real estate market, much of the behind-the-scenes work has remained steadfastly analog. It is not uncommon for property managers of older residential buildings to keep track of inventory with pen and paper and solicit rent payments by slipping a pre-addressed envelope under a tenant's door. \"Real estate is late to this party,\" said John Gilbert of Rudin Management, a dynastic real estate firm that has become an early adopter and investor in proptech. \"We saw the disruption with Airbnb, Uber and Lyft, but it is just now starting within the built environment.\" Gilbert is also co-chair of a subgroup within the Real Estate Board of New York, a trade and lobbying organization that is trying to foster innovation and has even adopted the tech world's penchant for spaceless monikers. The REBNYTech Committee hosts networking events and explores big questions about how data and artificial intelligence might help owners save money by better managing and populating their properties. Rudin itself developed the building operating system Nantum, which it sees as a step in that direction. \"We like to say buildings have always had a heart\u2014the boiler or engine room,\" Gilbert said. \"Now they have a brain with Nantum.\" A laundry list of startups are trying to carve out a niche in other corners of the real estate world. Cadre, for example, has raised $133 million, hoping to change how investors engage with real estate projects. VTS, a commercial leasing program; Knotel, a WeWork rival; and Updater, which hopes to eliminate the hassle of moving for tenants and homeowners, have racked up around $100 million each. Co-living company Common has attracted $63 million to improve the loathsome process of finding reliable roommates, and Honest Buildings has raised $43 million to streamline bidding out construction jobs to subcontractors. They have plenty of company. As of last week online data repository Crunchbase was tracking around 575 proptech startups, investors, venture funds and accelerators headquartered in the five boroughs. In 2013 Floored raised more than $5 million to develop its concept of creating three-dimensional digital models of commercial buildings. The company was later acquired by commercial brokerage CBRE for an undisclosed price, a potential prototype of the city's proptech life cycle: raise millions, develop a coherent business plan, grow like hell, and then exit through acquisition. But following the Darwinian laws that govern startups, most will fail. In 2014 CityFunders raised $1 million for a real estate investment platform. It no longer exists. Nationwide, less than half of all startups make it to a second round of funding, which begins at angel or seed level, and then progresses by series through the alphabet. That high failure rate is at least partially tied to the fact that several competing firms are often developing similar products to tackle similar problems\u2014and there's typically room for only one winner. Just 1% of all new companies attain the vaunted unicorn status\u2014which signals a valuation of $1 billion of more\u2014according to a 2017 study from CB Insights. And while it is easy to determine which companies fall into either extreme, measuring the success of those in between is much harder. \"It doesn't exactly make sense,\" said Luke Schoenfelder, chief executive of Latch, a company he co-founded in 2013. \"When I first got into this I thought, Wait, I'm not supposed to make money?\" Latch supplies landlords with smart locks that can be installed on all types of doors to more efficiently manage security, deliveries and services such as dog walking. So far the firm has raised $26 million, installed its product in 1,000 buildings and struck a deal to outfit a 5,500-unit complex rising on the Greenpoint waterfront. If Latch were solely focused on profit, Schoenfelder said, it would not be able to pump resources into growth and achieve the scale that could make it a serious player. Even the large company Zillow, the Seattle-based listings website that acquired New York's StreetEasy for $50 million in 2013, was not profitable as of last year despite going public in 2011. WeWork recently disclosed a nearly $1 billion operating loss. But if making money doesn't signal success, attracting it apparently does. In the past few years, WeWork and Compass have led the local pack, collecting $7.6 billion and $808 million, respectively, according to Crunchbase. But the majority of proptech startups are netting investments in the millions and below, and most of that money is coming from traditional real estate companies that are not necessarily looking for a direct return. Instead, the Rudins, Milsteins and Dursts of the world often want to invest in companies that they can incorporate into their portfolio to reduce operating costs and boost profits. \"If I can put $5 million into a company that enables me to save $10 million in long-term operating income,\" Aarons said, \"that's a win,\" Other established firms such as CBRE and Cushman & Wakefield also want to keep tabs on startups that could shake up their business model or give competitors an advantage. Doling out funding is the price they pay for staying current. \"If we're seeing this innovation on the front lines, it helps us to be better at our core competency, whether we make money or not,\" said Ben Brown, head of investments for the Northeast arm of Brookfield Properties. \"By getting exposure to different companies across the space, we can be more thoughtful about what we should be doing as landlord and investor, and futureproof our business.\" Brookfield, which uses Latch and Honest Buildings in its portfolio, is one of the few companies that invest directly in startups. This year it announced a $250 million fund to pursue that initiative. But most firms remain wary of straying too far from their comfort zone and have instead opted to farm out the task of picking proptech investments to venture capital funds or accelerators. The California-based venture fund Fifth Wall has become one of the go-to specialists in this area. The company now manages $265 million on behalf of its backers, which include CBRE and Rudin. The pitch from those funds is simple: For an annual fee that is typically 2%, give us your money and our experts will spread it over a number of different companies that could prove to be game changers. If one is acquired or goes public, the returns will be enough to compensate for other investments that fizzled, even with the fund taking a 20% cut of the profits. Accelerators such as Metaprop operate in a similar fashion but typically dole out smaller investments and try to foster successful companies by providing guidance and access to major landlords. Despite all the players, the city has yet to see a proptech startup exit with a major IPO. Thus far, the biggest funding rounds have gone to just a handful of companies, including Cadre, Common, Compass, VTS, WeWork and Convene, which raised $68 million last year to more efficiently manage conference rooms in commercial office buildings. The most lavish spenders, who can truly transform an upstart into an industry killer, have not been traditional real estate companies but private-equity investors such as Japanese conglomerate Softbank, which dropped $4.4 billion on WeWork and $450 million on Compass last year. \"The exits have been virtually nonexistent,\" said Common founder Brad Hargreaves. \"What I'm hoping to see next is real estate companies that come to the table and don't just write checks but start acquiring companies.\" Metaprop and other accelerators have staked their reputation on this happening. Aarons is optimistic that some city startups are nearing the threshold. \"I think you're going to see big exits this year,\" he said. \"I can't tell you who or when, but it would be silly for more traditional companies not to buy up as many proptech companies as they can.\" A version of this article appears in the May 7, 2018, print issue of Crain's New York Business. Sign up for our FREE daily email newsletter. A summary of the day's top business and political headlines from the newsroom of Crain's New York Business. \nMore Newsletters \u203a\n","time":1525791946,"title":"Is real estate ripe for disruption?","type":"story","url":"http:\/\/www.crainsnewyork.com\/article\/20180507\/FEATURES\/180509935","label":7,"label_name":"random"},{"by":"thisisit","descendants":0,"id":17021387,"kids":"None","score":2,"text":"Home Research - Our Economists Publications Indicators and Data Center for Pacific Basin Studies Past Conferences About Us Banking + Supervision and Regulation Discount Window Data Publications Banking Programs Asia Program Fintech About Education + Teacher Resources Activities Events About Community Development + Publications Data Events Initiatives Blog About Cash + Cash and How We Use It The Cash Lifecycle Publications About Our District + About Leadership Careers Press #unreserved Subscriptions The Fed System + Board of Governors Atlanta Boston Chicago Cleveland Dallas Kansas City Minneapolis New York Philadelphia Richmond St. Louis Home Research - Our Economists Publications Indicators and Data Center for Pacific Basin Studies Past Conferences About Us Banking + Supervision and Regulation Discount Window Data Publications Banking Programs Asia Program Fintech About Education + Teacher Resources Activities Events About Community Development + Publications Data Events Initiatives Blog About Cash + Cash and How We Use It The Cash Lifecycle Publications About Our District + About Leadership Careers Press #unreserved Subscriptions The Fed System + Board of Governors Atlanta Boston Chicago Cleveland Dallas Kansas City Minneapolis New York Philadelphia Richmond St. Louis FRBSF Economic Letter \n\nSubscribe\nRSS Feed\nShare\n \n\n\n\n\n 2018-12 | May 7, 2018 \n\n                Galina Hale, Arvind Krishnamurthy, Marianna Kudlyak, and Patrick Shultz\n             From Bitcoin\u2019s inception in 2009 through mid-2017, its price remained under $4,000. In the second half of 2017, it climbed dramatically to nearly $20,000, but descended rapidly starting in mid-December. The peak price coincided with the introduction of bitcoin futures trading on the Chicago Mercantile Exchange. The rapid run-up and subsequent fall in the price after the introduction of futures does not appear to be a coincidence. Rather, it is consistent with trading behavior that typically accompanies the introduction of futures markets for an asset.  Bitcoin is a \u201ccryptocurrency\u201d\u2014a digital currency that is not backed by any tangible or intangible assets of intrinsic value. After its launch in January 2009, the dollar price of a bitcoin remained under $1,150 until February 22, 2017, when it increased exponentially for about 10 months, as shown in Figure 1. This explosive growth ended on December 17, 2017, when bitcoin reached its peak price of $19,511. Notably these dynamics aren\u2019t driven by overall market fluctuations, as shown by comparison with the Standard & Poor\u2019s 500 stock index. Figure 1Bitcoin prices and S&P 500 stock index Source: Bloomberg. The peak bitcoin price coincided with the day bitcoin futures started trading on the Chicago Mercantile Exchange (CME). In this Economic Letter, we argue that these price dynamics are consistent with the rise and collapse of the home financing market in the 2000s, as explained in Fostel and Geanakoplos (2012). They suggested that the mortgage boom was driven by financial innovations in securitization and groupings of bonds that attracted optimistic investors; the subsequent bust was driven by the creation of instruments that allowed pessimistic investors to bet against the housing market. Similarly, the advent of blockchain introduced a new financial instrument, bitcoin, which optimistic investors bid up, until the launch of bitcoin futures allowed pessimists to enter the market, which contributed to the reversal of the bitcoin price dynamics. Bitcoin with a capital B is a decentralized network that relies on a peer-to-peer system, rather than banks or credit card companies, to verify transactions using the digital currency known as bitcoin with a lowercase b. The first bitcoin was \u201cmined\u201d in 2009 after the anonymous person or group named Satoshi Nakamoto published a proof of concept for a currency that uses cryptography, rather than reliable third parties (Nakamoto 2008). Blockchain, the underlying infrastructure and ledger of bitcoin, provides a secure platform for two parties to do business with one another (Chiu and Koeppl 2017 and Berentsen and Schar 2018). Bitcoin miners contribute computing resources to verify bitcoin transactions and hence maintain blockchain. They are compensated for sharing their computing resources with new bitcoins. The total numbers of bitcoins to be mined has been arbitrarily set at 21 million. When this volume is reached\u2014estimates suggest in 2140\u2014miners will be compensated by transaction fees rather than new bitcoins (Nian and Chuen 2016). When discussing the price of a currency or an asset like bitcoin, it is useful to separate transactional demand, which arises from using bitcoins in transactions such as purchases of goods and services, from speculative demand, which arises when people are buying bitcoins in the hope that their value will increase. Speculative demand is basically a bet on the price of the underlying asset or currency increasing, because the investor does not need the asset itself. For most currencies and assets, investors have ways to bet on the increase or decline in their value using a variety of financial instruments based on the asset or a currency, so-called financial derivatives. Before December 2017, there was no market for bitcoin derivatives. This meant that it was extremely difficult, if not impossible, to bet on the decline in bitcoin price. Such bets usually take the form of short selling, that is selling an asset before buying it, forward or future contracts, swaps, or a combination. Betting on the increase in bitcoin price was easy\u2014one just had to buy it. Speculative demand for bitcoin came only from optimists, investors who were willing to bet money that the price was going to go up. And until December 17, those investors were right: As with a self-fulfilling prophecy, optimists\u2019 demand pushed the price of bitcoin up, energizing more people to join in and keep pushing up the price. The pessimists, however, had no mechanism available to put money behind their belief that the bitcoin price would collapse. So they were left to wait for their \u201cI told you so\u201d moment. This one-sided speculative demand came to an end when the futures for bitcoin started trading on the CME on December 17. Although the Chicago Board Options Exchange (CBOE) had opened a futures market a week earlier on December 10, trading was thin until the CME joined the market. Indeed, the average daily trading volume the month after the CME issued futures was approximately six times larger than when only the CBOE offered these derivatives. With the introduction of bitcoin futures, pessimists could bet on a bitcoin price decline, buying and selling contracts with a lower delivery price in the future than the spot price. For example, they could sell a promise to deliver a bitcoin in a month\u2019s time at a lower price than the current spot price and hope to buy a bitcoin during the month at an even lower price to make a profit. With offers of future bitcoin deliveries at a lower price coming through, the order flow necessarily put downward pressure on the spot price as well. For all investors who were in the market to buy bitcoins for either transactional or speculative reasons and were willing to wait a month, this was a good deal. The new investment opportunity led to a fall in demand in the spot bitcoin market and therefore a drop in price. With falling prices, pessimists started to make money on their bets, fueling further short selling and further downward pressure on prices. Figure 2 shows the three largest bitcoin price declines in 2017. We scale the three series so that the peak values are equal to 100 on the peak event days. Hence, each point on the figure can be interpreted as a percent of the peak value. The horizontal axis represents the number of days before and after the peak dates. The price decline following the issuance of bitcoin futures on the CME (red line) is clearly larger than in the previous two reversals. Additionally, the two earlier decreases in prices returned to pre-crash levels in about a month. As of late April, the bitcoin price had not returned to its pre-futures peak. Figure 2Comparison of three largest bitcoin price declines in 2017 Source: Bloomberg, authors\u2019 calculations. This is not the first time that markets observed a turning point following the introduction of a new instrument, as Fostel and Geanakoplos (2012) show for the more complex mortgage-backed securities market. The mechanism they describe hinges on the same driving force of optimistic and pessimistic traders. Why, then, did the price of bitcoin fall somewhat gradually rather than collapse overnight? The answer to this is difficult. It could be that pessimistic investors lack the attention, willingness, or ability to enter the market on the first day or week of trading. Consistent with this assertion, the total volume of transactions in the CME futures market started very low, with an average trading volume of contracts promising to deliver approximately 12,000 bitcoins during the first week of trading, relative to the estimated spot market turnover of 200,000 bitcoins. So where is the price of bitcoin going? This is a very difficult question, and we do not pretend to be able to forecast bitcoin prices, nor will we offer any guesses. Instead, we outline a few factors that may affect the fundamental price of bitcoin, which is where we would expect the price to go in the long run, once speculative demand by optimists and pessimists balances out. The supply of bitcoins is determined by the volume of bitcoin currently in circulation and the additional volume to be mined. The decision to mine a bitcoin depends on the cost and benefit from mining. Hayes (2015) estimated a bitcoin mining cost in 2015 of around $250, which was close to the bitcoin price at the time. More generally, however, the mining cost of bitcoin should not affect its value any more than the cost of printing regular currency affects its value\u2014basically not at all. Given that there is no actual asset that backs the value of bitcoin and it doesn\u2019t provide a natural hedge as insurance against sharp moves in any other asset\u2019s value, what will eventually determine the \u201cfundamental\u201d price of bitcoin is transactional demand relative to supply. We know that bitcoin is used as a means of exchange in a number of markets. The amount of bitcoins needed for these markets to function constitutes transactional demand. The supply growth of bitcoin is becoming more limited as the mining price increases. If transactional demand grows faster than supply, we would expect the price to grow. Transactional demand in turn depends on a number of factors. One is the availability of substitutes. If a different cryptocurrency becomes more widely used as a means of exchange in the markets currently dominated by bitcoin, demand for bitcoin may drop precipitously because these tend to be winner-takes-all markets. Second, if traditional financial institutions become more willing to accept bitcoin as collateral, a means of payment, or a direct investment, demand may increase substantially. Finally, official recognition and regulatory acceptance of bitcoin as a means of payments would increase its circulation, while regulatory constraints or introduction of transaction fees may reduce it. We suggest that the rapid rise of the price of bitcoin and its decline following issuance of futures on the CME is consistent with pricing dynamics suggested elsewhere in financial theory and with previously observed trading behavior. Namely, optimists bid up the price before financial instruments are available to short the market (Fostel and Geanakoplos 2012). Once derivatives markets become sufficiently deep, short-selling pressure from pessimists leads to a sharp decline in value. While we understand some of the factors that play a role in determining the long-run price of bitcoin, our understanding of the transactional benefits of bitcoin is too imprecise to quantify this long-run price. But as speculative dynamics disappear from the bitcoin market, the transactional benefits are likely to be the factor that will drive valuation. Galina B. Hale is a research advisor in the Economic Research Department of the Federal Reserve Bank of San Francisco. Arvind Krishnamurthy is John S. Osterweis Professor of Finance at the Stanford Graduate School of Business. Marianna Kudlyak is a research advisor in the Economic Research Department of the Federal Reserve Bank of San Francisco. Patrick Shultz is a research associate in the Economic Research Department of the Federal Reserve Bank of San Francisco. Berentsen, Aleksander, and Fabian Schar. 2018. \u201cA Short Introduction to the World of Cryptocurrencies.\u201d Federal Reserve Bank of St. Louis Review 100(1), pp. 1\u201316. Chiu, Jonathan, and Thorsten Koeppl. 2017. \u201cThe Economics of Cryptocurrencies\u2014Bitcoin and Beyond.\u201d Queen\u2019s Economics Department Working Paper 1389. Fostel, Ana, and John Geanakoplos. 2012. \u201cTranching, CDS, and Asset Prices: How Financial Innovation Can Cause Bubbles and Crashes.\u201d American Economic Journal: Macroeconomics 4(1), pp. 190\u2013225. Hayes, Adam. 2015. \u201cA Cost of Production Model for Bitcoin.\u201d Working Paper 05\/2015, Department of Economics, The New School for Social Research. Nakamoto, Satoshi. 2008. \u201cBitcoin: A Peer-To-Peer Electronic Cash System.\u201d Unpublished paper. Nian, Lam Pak, and David Lee Kuo Chuen. 2016. \u201cIntroduction to Bitcoin.\u201d Chapter 1, Handbook of Digital Currency: Bitcoin, Innovation, Financial Instruments, and Big Data, 1st edition, ed. David Lee Kuo Chuen. London: Academic Press. pp. 19\u201321. Opinions expressed in FRBSF Economic Letter do not necessarily reflect the views of the management of the Federal Reserve Bank of San Francisco or of the Board of Governors of the Federal Reserve System. This publication is edited by Anita Todd. Permission to reprint must be obtained in writing. \n\nSubscribe\nRSS Feed\nShare\n \n\n\n\n\n Please send editorial comments and requests for reprint permission to\nResearch Library\n\n                  Attn: Research publications, MS 1140\n                  Federal Reserve Bank of San Francisco\n                  P.O. Box 7702\n                  San Francisco, CA 94120\n             \u00a9 2018 Federal Reserve Bank of San Francisco","time":1525791944,"title":"How Futures Trading Changed Bitcoin Prices","type":"story","url":"https:\/\/www.frbsf.org\/economic-research\/publications\/economic-letter\/2018\/may\/how-futures-trading-changed-bitcoin-prices\/","label":7,"label_name":"random"},{"by":"SharpSightLabs","descendants":0,"id":17021376,"kids":"None","score":1,"text":" by Sharp Sight | May 8, 2018 For many data scientists and data analytics professionals, as much as 80% of their work is data wrangling and exploratory data analysis. Of course, everyone wants to focus on machine learning and advanced techniques, but the reality is that a lot of the work of many data scientists is a little more mundane. That isn\u2019t to discourage you from entering the field (data science is great).  But you need to realize how important it is to know and master \u201cfoundational\u201d techniques. One of the techniques you will need to know is the density plot.  The density plot is a basic tool in your data science toolkit. If you\u2019re not familiar with the density plot, it\u2019s actually a relative of the histogram.  Like the histogram, it generally shows the \u201cshape\u201d of a particular variable. But there are differences.  In a histogram, the height of bar corresponds to the number of observations in that particular \u201cbin.\u201d  However, in the density plot, the height of the plot at a given x-value corresponds to the \u201cdensity\u201d of the data.  Ultimately, the shape of a density plot is very similar to a histogram of the same data, but the interpretation will be a little different. Either way, much like the histogram, the density plot is a tool that you will need when you visualize and explore your data.  It\u2019s a technique that you should know and master. Let\u2019s take a look at how to make a density plot in R. For better or for worse, there\u2019s typically more than one way to do things in R.  For just about any task, there is more than one function or method that can get it done. That\u2019s the case with the density plot too.  There\u2019s more than one way to create a density plot in R. I\u2019ll show you two ways.  In this post, I\u2019ll show you how to create a density plot using \u201cbase R,\u201d and I\u2019ll also show you how to create a density plot using the ggplot2 system. I want to tell you up front: I strongly prefer the ggplot2 method.  I\u2019ll explain a little more about why later, but I want to tell you my preference so you don\u2019t just stop with the \u201cbase R\u201d method. Before we get started, let\u2019s load a few packages: We\u2019ll use ggplot2 to create some of our density plots later in this post, and we\u2019ll be using a dataframe from dplyr. Now, let\u2019s just create a simple density plot in R, using \u201cbase R\u201d. First, here\u2019s the code: And here\u2019s what it looks like:  I\u2019m going to be honest.  I don\u2019t like the base R version of the density plot.  In fact, I\u2019m not really a fan of any of the base R visualizations. Part of the reason is that they look a little unrefined.  They get the job done, but right out of the box, base R versions of most charts look unprofessional.  Base R charts and visualizations look a little \u201cbasic.\u201d For this reason, I almost never use base R charts.  My go-to toolkit for creating charts, graphs, and visualizations is ggplot2. Readers here at the Sharp Sight blog know that I love ggplot2. There are at least two reasons for this. First, ggplot makes it easy to create simple charts and graphs. ggplot2 makes it easy to create things like bar charts, line charts, histograms, and density plots. Second, ggplot also makes it easy to create more advanced visualizations.  I won\u2019t go into that much here, but a variety of past blog posts have shown just how powerful ggplot2 is. Finally, the default versions of ggplot plots look more \u201cpolished.\u201d ggplot2 charts just look better than the base R counterparts. Having said that, let\u2019s take a look. Let\u2019s take a look at how to create a density plot in R using ggplot2:  Personally, I think this looks a lot better than the base R density plot.  With the default formatting of ggplot2 for things like the gridlines, fonts, and background color, this just looks more presentable right out of the box. Ok.  Now that we have the basic ggplot2 density plot, let\u2019s take a look at a few variations of the density plot. There are a few things we can do with the density plot.  We can add some color.  We can \u201cbreak out\u201d a density plot on a categorical variable.  We can create a 2-dimensional density plot. Let\u2019s take a look. First, let\u2019s add some color to the plot.  We will \u201cfill in\u201d the area under the density plot with a particular color. To do this, we can use the fill parameter.  There are a few things that we could possibly change about this, but this looks pretty good. Before moving on, let me briefly explain what we\u2019ve done here.  The fill parameter specifies the interior \u201cfill\u201d color of a density plot.  In fact, in the ggplot2 system, fill almost always specifies the interior color of a geometric object (i.e., a geom). So in the above density plot, we just changed the fill aesthetic to \u201ccyan.\u201d  A more technical way of saying this is that we \u201cset\u201d the fill aesthetic to \u201ccyan.\u201d One final note: I won\u2019t discuss \u201cmapping\u201d verses \u201csetting\u201d in this post.  But if you really want to master ggplot2, you need to understand aesthetic attributes, how to map variables to them, and how to set aesthetics to constant values. Now let\u2019s create a chart with multiple density plots. Here, we\u2019re going to be visualizing a single quantitative variable, but we will \u201cbreak out\u201d the density plot into three separate plots. We\u2019ll plot a separate density plot for different values of a categorical variable. The code to do this is very similar to a basic density plot.  We\u2019ll use ggplot() to initiate plotting, map our quantitative variable to the x axis, and use geom_density() to plot a density plot. But, to \u201cbreak out\u201d the density plot into multiple density plots, we need to map a categorical variable to the \u201ccolor\u201d aesthetic:  Here, Sepal.Length is the quantitative variable that we\u2019re plotting; we are plotting the density of the Sepal.Length variable. Species is a categorical variable in the iris dataset.  We are \u201cbreaking out\u201d the density plot into multiple density plots based on Species.  By mapping Species to the color aesthetic, we essentially \u201cbreak out\u201d the basic density plot into three density plots: one density plot curve for each value of the categorical variable, Species. Another way that we can \u201cbreak out\u201d a simple density plot based on a categorical variable is by using the small multiple design. I am a big fan of the small multiple.  The small multiple chart (AKA, the trellis chart or the grid chart) is extremely useful for a variety of analytical use cases.  This chart type is also wildly under-used.  Because of it\u2019s usefulness, you should definitely have this in your toolkit. When you\u2019re using ggplot2, the first few lines of code for a small multiple density plot are identical to a basic density plot.  We\u2019ll use ggplot() the same way, and our variable mappings will be the same. However, we will use facet_wrap() to \u201cbreak out\u201d the base-plot into multiple \u201cfacets.\u201d We are using a categorical variable to break the chart out into several small versions of the original chart, one small version for each value of the categorical variable. In the following case, we will \u201cfacet\u201d on the Species variable.  Remember, Species is a categorical variable.  So, the code facet_wrap(~Species) will essentially create a small, separate version of the density plot for each value of the Species variable.  Notice that this is very similar to the \u201cdensity plot with multiple categories\u201d that we created above.  But instead of having the various density plots in the same plot area, they are \u201cfaceted\u201d into three separate plot areas. I won\u2019t give you too much detail here, but I want to reiterate how powerful this technique is.  \u201cBreaking out\u201d your data and visualizing your data from multiple \u201cangles\u201d is very common in exploratory data analysis.  You\u2019ll need to be able to do things like this when you are analyzing data.  It can also be useful for some machine learning problems. Ultimately, you should know how to do this. Beyond just making a 1-dimensional density plot in R, we can make a 2-dimensional density plot in R. Be forewarned: this is one piece of ggplot2 syntax that is a little \u201cun-intuitive.\u201d  Syntactically, this is a little more complicated than a typical ggplot2 chart, so let\u2019s quickly walk through it. In the first line, we\u2019re just creating the dataframe.  It contains two variables, that consist of 5,000 random normal values: In the next line, we\u2019re just initiating ggplot() and mapping variables to the x-axis and the y-axis: Finally, there\u2019s the last line of the code: Essentially, this line of code does the \u201cheavy lifting\u201d to create our 2-d density plot. stat_density2d() indicates that we\u2019ll be making a 2-dimensional density plot. geom = 'tile' indicates that we will be constructing this 2-d density plot out of many small \u201ctiles\u201d that will fill up the entire plot area.  When you look at the visualization, do you see how it looks \u201cpixelated?\u201d Do you see that the plot area is made up of hundreds of little squares that are colored differently?  Those little squares in the plot are the \u201ctiles.\u201d As you\u2019ve probably guessed, the tiles are colored according to the density of the data. Syntactically, aes(fill = ..density..) indicates that the fill-color of those small tiles should correspond to the density of data in that region. So essentially, here\u2019s how the code works: the plot area is being divided up into small regions (the \u201ctiles\u201d).  These regions act like bins.  There\u2019s a statistical process that counts up the number of observations and computes the density in each bin.  The color of each \u201ctile\u201d (i.e., the color of each bin) will correspond to the density of the data. Finally, the code contour = F just indicates that we won\u2019t be creating a \u201ccontour plot.\u201d stat_density2d() can be used create contour plots, and we have to turn that behavior off if we want to create the type of density plot seen here. Just for the hell of it, I want to show you how to add a little color to your 2-d density plot. Using colors in R can be a little complicated, so I won\u2019t describe it in detail here.  But I still want to give you a small taste.  Using color in data visualizations is one of the secrets to creating compelling data visualizations.  If you want to be a great data scientist, it\u2019s probably something you need to learn. Here, we\u2019ll use a specialized R package to change the color of our plot: the viridis package. viridis contains a few well-designed color palettes that you can apply to your data.  Wow. I love the viridis package. So what exactly did we do to make this look so damn good? We used scale_fill_viridis() to adjust the color scale.  A little more specifically, we changed the color scale that corresponds to the \u201cfill\u201d aesthetic of the plot.  Remember, the little bins (or \u201ctiles\u201d) of the density plot are filled in with a color that corresponds to the density of the data.  But what color is used?  The default is the simple dark-blue\/light-blue color scale.  But when we use scale_fill_viridis(), we are specifying a new color scale to apply to the fill aesthetic. scale_fill_viridis() tells ggplot() to use the viridis color scale for the fill-color of the plot. In the last several examples, we\u2019ve created plots of varying degrees of complexity and sophistication.  Having said that, one thing we haven\u2019t done yet is modify the formatting of the titles, background colors, axis ticks, etc. If you\u2019re just doing some exploratory data analysis for personal consumption, you typically don\u2019t need to do much plot formatting.  But if you intend to show your results to other people, you will need to be able to \u201cpolish\u201d your charts and graphs by modifying the formatting of many little plot elements.  If you want to publish your charts (in a blog, online webpage, etc), you\u2019ll also need to format your charts.  And ultimately, if you want to be a top-tier expert in data visualization, you will need to be able to format your visualizations. That being said, let\u2019s create a \u201cpolished\u201d version of one of our density plots.  Here, we\u2019re going to take the simple 1-d R density plot that we created with ggplot, and we will format it.  We\u2019ll change the plot background, the gridline colors, the font types, etc. To do this, we\u2019ll need to use the ggplot2 formatting system. We\u2019ll basically take our simple ggplot2 density plot and add some additional lines of code.  Here, we\u2019ve essentially used the theme() function from ggplot2 to modify the plot background color, the gridline colors, the text font and text color, and a few other elements of the plot. Full details of how to use the ggplot2 formatting system is beyond the scope of this post, so it\u2019s not possible to describe it completely here.  I just want to quickly show you what it can do and give you a starting point for potentially creating your own \u201cpolished\u201d charts and graphs. If you really want to learn how to make professional looking visualizations, I suggest that you check out some of our other blog posts (or consider enrolling in our premium data science course). Ultimately, the density plot is used for data exploration and analysis.  Let\u2019s briefly talk about some specific use cases. One of the critical things that data scientists need to do is explore data. Do you need to \u201cfind insights\u201d for your clients?  You need to explore your data. Do you need to create a report or analysis to help your clients optimize part of their business?  You need to explore your data. Do you need to build a machine learning model?  You need to explore your data. Data exploration is critical.  In fact, I think that data exploration and analysis are the true \u201cfoundation\u201d of data science (not math). Having said that, the density plot is a critical tool in your data exploration toolkit. You\u2019ll typically use the density plot as a tool to identify: This is sort of a special case of exploratory data analysis, but it\u2019s important enough to discuss on it\u2019s own. The density plot is an important tool that you will need when you build machine learning models. Essentially, before building a machine learning model, it is extremely common to examine the predictor distributions (i.e., the distributions of the variables in the data).  In order to make ML algorithms work properly, you need to be able to visualize your data.  You need to see what\u2019s in your data.  You need to find out if there is anything unusual about your data. These basic data inspection tasks are a perfect use case for the density plot. You can use the density plot to look for: There are some machine learning methods that don\u2019t require such \u201cclean\u201d data, but in many cases, you will need to make sure your data looks good.  To do this, you can use the density plot. That\u2019s just about everything you need to know about how to create a density plot in R. To be a great data scientist though, you need to know more than the density plot.  Moreover, when you\u2019re creating things like a density plot in r, you can\u2019t just copy and paste code \u2026 if you want to be a professional data scientist, you need to know how to write this code from memory. If you\u2019re thinking about becoming a data scientist, sign up for our email list.  We\u2019ll show you essential skills like how to create a density plot in R \u2026 but we\u2019ll also show you how to master these essential skills. To get a great data science job, you need to be one of the best.\u00a0 Sign up for our email list and discover how to rapidly master data science and become a top performer.   Your email address will not be published. Required fields are marked * Comment Name *  Email *  Website        ","time":1525791875,"title":"The ultimate guide to density plots in R","type":"story","url":"http:\/\/www.sharpsightlabs.com\/blog\/density-plot-in-r\/","label":5,"label_name":"ml"},{"by":"logotype","descendants":0,"id":17021368,"kids":"None","score":1,"text":"","time":1525791840,"title":"JavaScript FIX Parser (Financial Information EXchange)","type":"story","url":"http:\/\/fixparser.sendercompid.com\/","label":7,"label_name":"random"},{"by":"bakery2k","descendants":0,"id":17021358,"kids":"None","score":1,"text":"Coming from statically typed languages, C and Java, I felt a little insecure writing Python code. Suddenly, silly type mismatch errors which I was used to catch during compilation were only caught (if at all, in the best case scenario) at runtime. This became especially annoying while learning new APIs or diving into a new large codebase, and made me completely reliant on documentation. While reading the docs is important on its own, I truly missed the comfortable and time-saving code completion on typing \u2018.\u2019 using IDEs such as IntelliJ. Luckily, since Python 3.5, type-annotations have been officially added to Python (PEP 484), and now we can write something like this: While having a different syntax than in C and Java, I find it rather intuitive (at least the simple cases). Importantly, the type annotations are completely optional and are ignored at runtime\u200a\u2014\u200ameaning two things: With type annotations out there for quite some time, several articles and guides were written about their benefits, of which the most common are: One important benefit which, as I see it, should tilt your favor towards adopting this new feature if you have resisted it so far, is code completion. Early error-catching and clean code are super important, but I have seen people writing that it is more \u201cpythonic\u201d to write parameter assertions (which include type checking) in the beginning of a function, or that docstrings should be enough. While we can argue about that, the advantages of code completion (of course while using a good IDE, such as PyCharm) are undisputed. First, let\u2019s see an example of such code completion. Let\u2019s say we are writing a library modeling music objects. We have an Album class and a Track class, and in the Album class we have a method returning all tracks shorter than a specified duration: Now let\u2019s say I am a user of this library, and I want to print the names of all tracks shorter than 10 minutes in Wish You Were Here (which I instantiated earlier): Wait a minute, was that variable called \u2018name\u2019 or \u2018title\u2019? I don\u2019t know that! And the code completion is of no help at all\u200a\u2014\u200aI must go to the declaration of the Track class or read its documentation\u200a\u2014\u200athat takes time! This is the best case scenario\u200a\u2014\u200awhat if the programmer does not acknowledge she does not remember the variable name and guesses \u2018name\u2019 instead of \u2018title\u2019? It will cost even more time because this error will be found only in runtime (hopefully shes use uses TDD and catch it quickly with a pre-written test). Now what happens if we use type annotations? Let\u2019s add only the minimal type annotation directly needed at the moment: The return value of this method is a list of Tracks, so we add \u2018-> List[Track]\u2019 to its signature (more about the syntax of type annotations). Let\u2019s see what happens now on typing \u2018.\u2019: Great! The IDE now knows what type this variable is, and can recommend the correct attributes\u200a\u2014\u200aerror averted! And the users of this package will have a friendlier time coding with it. Here is how the entire Album class looks like annotated (with my style of annotating): Wrapping up, we\u2019ve seen that type annotations are a great addition to Python\u200a\u2014\u200athey improve the integrity of the code, they are self-documenting code, and they may improve the productivity of the clients of your code. Further reading: By clapping more or less, you can signal to us which stories really stand out.","time":1525791744,"title":"The other (great) benefit of Python type annotations","type":"story","url":"https:\/\/medium.com\/@shamir.stav_83310\/the-other-great-benefit-of-python-type-annotations-896c7d077c6b","label":3,"label_name":"dev"},{"by":"mgdo","descendants":1,"id":17021355,"kids":"[17023877]","score":3,"text":"Opinion An alliance of heretics is making an end run around the mainstream conversation. Should we be listening? Eric WeinsteinCreditDamon Winter\/The New York Times Supported by By Bari Weiss Photographs by Damon Winter Here are some things that you will hear when you sit down to dinner with the vanguard of the Intellectual Dark Web: There are fundamental biological differences between men and women. Free speech is under siege. Identity politics is a toxic ideology that is tearing American society apart. And we\u2019re in a dangerous place if these ideas are considered \u201cdark.\u201d I was meeting with Sam Harris, a neuroscientist; Eric Weinstein, a mathematician and managing director of Thiel Capital; the commentator and comedian Dave Rubin; and their spouses in a Los Angeles restaurant to talk about how they were turned into heretics. A decade ago, they argued, when Donald Trump was still hosting \u201cThe Apprentice,\u201d none of these observations would have been considered taboo. Today, people like them who dare venture into this \u201cThere Be Dragons\u201d territory on the intellectual map have met with outrage and derision \u2014 even, or perhaps especially, from people who pride themselves on openness. It\u2019s a pattern that has become common in our new era of That Which Cannot Be Said. And it is the reason the Intellectual Dark Web, a term coined half-jokingly by Mr. Weinstein, came to exist. What is the I.D.W. and who is a member of it? It\u2019s hard to explain, which is both its beauty and its danger. Most simply, it is a collection of iconoclastic thinkers, academic renegades and media personalities who are having a rolling conversation \u2014 on podcasts, YouTube and Twitter, and in sold-out auditoriums \u2014 that sound unlike anything else happening, at least publicly, in the culture right now. Feeling largely locked out of legacy outlets, they are rapidly building their own mass media channels. The closest thing to a phone book for the I.D.W. is a sleek website that lists the dramatis personae of the network, including Mr. Harris; Mr. Weinstein and his brother and sister-in-law, the evolutionary biologists Bret Weinstein and Heather Heying; Jordan Peterson, the psychologist and best-selling author; the conservative commentators Ben Shapiro and Douglas Murray; Maajid Nawaz, the former Islamist turned anti-extremist activist; and the feminists Ayaan Hirsi Ali and Christina Hoff Sommers. But in typical dark web fashion, no one knows who put the website up. The core members have little in common politically. Bret and Eric Weinstein and Ms. Heying were Bernie Sanders supporters. Mr. Harris was an outspoken Hillary voter. Ben Shapiro is an anti-Trump conservative. But they all share three distinct qualities. First, they are willing to disagree ferociously, but talk civilly, about nearly every meaningful subject: religion, abortion, immigration, the nature of consciousness. Second, in an age in which popular feelings about the way things ought to be often override facts about the way things actually are, each is determined to resist parroting what\u2019s politically convenient. And third, some have paid for this commitment by being purged from institutions that have become increasingly hostile to unorthodox thought \u2014 and have found receptive audiences elsewhere. \u201cPeople are starved for controversial opinions,\u201d said Joe Rogan, an MMA color commentator and comedian who hosts one of the most popular podcasts in the country. \u201cAnd they are starved for an actual conversation.\u201d [Receive the day\u2019s most urgent debates right in your inbox by subscribing to the Opinion Today newsletter.]  That hunger has translated into a booming and, in many cases, profitable market. Episodes of \u201cThe Joe Rogan Experience,\u201d which have featured many members of the I.D.W., can draw nearly as big an audience as Rachel Maddow. A recent episode featuring Bret Weinstein and Ms. Heying talking about gender, hotness, beauty and #MeToo was viewed on YouTube over a million times, even though the conversation lasted for nearly three hours. Ben Shapiro\u2019s podcast, which airs five days a week, gets 15 million downloads a month. Sam Harris estimates that his \u201cWaking Up\u201d podcast gets one million listeners an episode. Dave Rubin\u2019s YouTube show has more than 700,000 subscribers. Offline and in the real world, members of the I.D.W. are often found speaking to one another in packed venues around the globe. In July, for example, Jordan Peterson, Douglas Murray and Mr. Harris will appear together at the O2 Arena in London. But as the members of the Intellectual Dark Web become genuinely popular, they are also coming under more scrutiny. On April 21, Kanye West crystallized this problem when he tweeted seven words that set Twitter on fire: \u201cI love the way Candace Owens thinks.\u201d Candace Owens, the communications director for Turning Point USA, is a sharp, young, black conservative \u2014 a telegenic speaker with killer instincts who makes videos with titles like \u201cHow to Escape the Democrat Plantation\u201d and \u201cThe Left Thinks Black People Are Stupid.\u201d Mr. West\u2019s praise for her was sandwiched inside a longer thread that referenced many of the markers of the Intellectual Dark Web, like the tyranny of thought policing and the importance of independent thinking. He was photographed watching a Jordan Peterson video. All of a sudden, it seemed, the I.D.W. had broken through to the culture-making class, and a few in the group flirted with embracing Ms. Owens as their own. Yet Ms. Owens is a passionate Trump supporter who has dismissed racism as a threat to black people while arguing, despite evidence to the contrary, that immigrants steal their jobs. She has also compared Jay-Z and Beyonc\u00e9 to slaves for supporting the Democratic Party. Many others in the I.D.W. were made nervous by her sudden ascendance to the limelight, seeing Ms. Owens not as a sincere intellectual but as a provocateur in the mold of Milo Yiannopoulos. For the I.D.W. to succeed, they argue, it needs to eschew those interested in violating taboo for its own sake. \u201cI\u2019m really only interested in building this intellectual movement,\u201d Eric Weinstein said. \u201cThe I.D.W. has bigger goals than anyone\u2019s buzz or celebrity.\u201d And yet, when Ms. Owens and Charlie Kirk, the executive director of Turning Point USA, met last week with Mr. West at the Southern California Institute of Architecture, just outside of the frame \u2014 in fact, avoiding the photographers \u2014 was Mr. Weinstein. He attended both that meeting and a one-on-one the next day for several hours at the mogul\u2019s request. Mr. Weinstein, who can\u2019t name two of Mr. West\u2019s songs, said he found the Kardashian spouse \u201ckind and surprisingly humble despite his unpredictable public provocations.\u201d He has also tweeted that he\u2019s interested to see what Ms. Owens says next. This episode was the clearest example yet of the challenge this group faces: In their eagerness to gain popular traction, are the members of the I.D.W. aligning themselves with people whose views and methods are poisonous? Could the intellectual wildness that made this alliance of heretics worth paying attention to become its undoing? There is no direct route into the Intellectual Dark Web. But the quickest path is to demonstrate that you aren\u2019t afraid to confront your own tribe. The metaphors for this experience vary: going through the phantom tollbooth; deviating from the narrative; falling into the rabbit hole. But almost everyone can point to a particular episode where they came in as one thing and emerged as something quite different. A year ago, Bret Weinstein and Heather Heying were respected tenured professors at Evergreen State College, where their Occupy Wall Street-sympathetic politics were well in tune with the school\u2019s progressive ethos. Today they have left their jobs, lost many of their friends and endangered their reputations. All this because they opposed a \u201cDay of Absence,\u201d in which white students were asked to leave campus for the day. For questioning a day of racial segregation cloaked in progressivism, the pair was smeared as racist. Following threats, they left town for a time with their children and ultimately resigned their jobs. \u201cNobody else reacted. That\u2019s what shocked me,\u201d Mr. Weinstein said. \u201cIt told me that a culture that told itself it was radically open-minded was actually a culture cowed by fear.\u201d Sam Harris says his moment came in 2006, at a conference at the Salk Institute with Richard Dawkins, Neil deGrasse Tyson and other prominent scientists. Mr. Harris said something that he thought was obvious on its face: Not all cultures are equally conducive to human flourishing. Some are superior to others. \u201cUntil that time I had been criticizing religion, so the people who hated what I had to say were mostly on the right,\u201d Mr. Harris said. \u201cThis was the first time I fully understood that I had an equivalent problem with the secular left.\u201d After his talk, in which he disparaged the Taliban, a biologist who would go on to serve on President Barack Obama\u2019s Commission for the Study of Bioethical Issues approached him. \u201cI remember she said: \u2018That\u2019s just your opinion. How can you say that forcing women to wear burqas is wrong?\u2019 But to me it\u2019s just obvious that forcing women to live their lives inside bags is wrong. I gave her another example: What if we found a culture that was ritually blinding every third child? And she actually said, \u2018It would depend on why they were doing it.\u2019\u201d His jaw, he said, \u201cactually fell open.\u201d \u201cThe moral confusion that operates under the banner of \u2018multiculturalism\u2019 can blind even well-educated people to the problems of intolerance and cruelty in other communities,\u201d Mr. Harris said. \u201cThis had never fully crystallized for me until that moment.\u201d Before September 2016, Jordan Peterson was an obscure psychology professor at the University of Toronto. Then he spoke out against Canada\u2019s Bill C-16, which proposed amending the country\u2019s human-rights act to outlaw discrimination based on gender identity and expression. He resisted on the grounds that the bill risked curtailing free speech by compelling people to use alternative gender pronouns. He made YouTube videos about it. He went on news shows to protest it. He confronted protesters calling him a bigot. When the university asked him to stop talking about it, including sending two warning letters, he refused. While most people in the group faced down comrades on the political left, Ben Shapiro confronted the right. He left his job as editor at large of Breitbart News two years ago because he believed it had become, under Steve Bannon\u2019s leadership, \u201cTrump\u2019s personal Pravda.\u201d In short order, he became a primary target of the alt-right and, according to the Anti-Defamation League, the No. 1 target of anti-Semitic tweets during the presidential election. Other figures in the I.D.W., like Claire Lehmann, the founder and editor of the online magazine Quillette, and Debra Soh, who has a Ph.D. in neuroscience, self-deported from the academic track, sensing that the spectrum of acceptable perspectives and even areas of research was narrowing. Dr. Soh said that she started \u201cwaking up\u201d in the last two years of her doctorate program. \u201cIt was clear that the environment was inhospitable to conducting research,\u201d she said. \u201cIf you produce findings that the public doesn\u2019t like, you can lose your job.\u201d When she wrote an op-ed in 2015 titled \u201cWhy Transgender Kids Should Wait to Transition,\u201d citing research that found that a majority of gender dysphoric children outgrow their dysphoria, she said her colleagues warned her, \u201cEven if you stay in academia and express this view, tenure won\u2019t protect you.\u201d Nowadays Ms. Soh has a column for Playboy and picks up work as a freelance writer. But that hardly pays the bills. She\u2019s planning to start a podcast soon and, like many members of the I.D.W., has a Patreon account where \u201cpatrons\u201d can support her work. These donations can add up. Mr. Rubin said his show makes at least $30,000 a month on Patreon. And Mr. Peterson says he pulls in some $80,000 in fan donations each month. Mr. Peterson has endured no small amount of online hatred and some real-life physical threats: In March, during a lecture at Queen\u2019s University in Ontario, a woman showed up with a garrote. But like many in the I.D.W., he also seems to relish the outrage he inspires. \u201cI\u2019ve figured out how to monetize social justice warriors,\u201d Mr. Peterson said in January on Joe Rogan\u2019s podcast. On his Twitter feed, he called the writer Pankaj Mishra, who\u2019d written an essay in The New York Review of Books attacking him, a \u201csanctimonious prick\u201d and said he\u2019d happily slap him. And the upside to his notoriety is obvious: Mr. Peterson is now arguably the most famous public intellectual in Canada, and his book \u201c12 Rules for Life\u201d is a best-seller. The exile of Bret Weinstein and Ms. Heying from Evergreen State brought them to the attention of a national audience that might have come for the controversy but has stayed for their fascinating insights about subjects including evolution and gender. \u201cOur friends still at Evergreen tell us that the protesters think they destroyed us,\u201d Ms. Heying said. \u201cBut the truth is we\u2019re now getting the chance to do something on a much larger scale than we could ever do in the classroom.\u201d \u201cI\u2019ve been at this for 25 years now, having done all the MSM shows, including Oprah, Charlie Rose, \u2018The Colbert Report,\u2019 Larry King \u2014 you name it,\u201d Michael Shermer, the publisher of Skeptic magazine, told me. \u201cThe last couple of years I\u2019ve shifted to doing shows hosted by Joe Rogan, Dave Rubin, Sam Harris and others. The I.D.W. is as powerful a media as any I\u2019ve encountered.\u201d Mr. Shermer, a middle-aged science writer, now gets recognized on the street. On a recent bike ride in Santa Barbara, Calif., he passed a work crew and \u201cthe flag man stopped me and says: \u2018Hey, you\u2019re that skeptic guy, Shermer! I saw you on Dave Rubin and Joe Rogan!\u2019\u201d When he can\u2019t watch the shows on YouTube, he listens to them as podcasts on the job. On breaks, he told Mr. Shermer, he takes notes. \u201cI\u2019ve had to update Quillette\u2019s servers three times now because it\u2019s caved under the weight of the traffic,\u201d Ms. Lehmann said about the publication most associated with this movement. Yet there are pitfalls to this audience-supported model. One risk is what Eric Weinstein has called \u201caudience capture.\u201d Since stories about left-wing-outrage culture \u2014 the fact that the University of California, Berkeley, had to spend $600,000 on security for Mr. Shapiro\u2019s speech there, say \u2014 take off with their fans, members of the Intellectual Dark Web may have a hard time resisting the urge to deliver that type of story. This probably helps explain why some people in this group talk constantly about the regressive left but far less about the threat from the right. \u201cThere are a few people in this network who have gone without saying anything critical about Trump, a person who has assaulted truth more than anyone in human history,\u201d Mr. Harris said. \u201cIf you care about the truth, that is quite strange.\u201d Emphasis is one problem. Associating with genuinely bad people is another. Go a click in one direction and the group is enhanced by intellectuals with tony affiliations like Steven Pinker at Harvard. But go a click in another and you\u2019ll find alt-right figures like Stefan Molyneux and Milo Yiannopoulos and conspiracy theorists like Mike Cernovich (the #PizzaGate huckster) and Alex Jones (the Sandy Hook shooting denier). It\u2019s hard to draw boundaries around an amorphous network, especially when each person in it has a different idea of who is beyond the pale. \u201cI don\u2019t know that we are in the position to police it,\u201d Mr. Rubin said. \u201cIf this thing becomes something massive \u2014 a political or social movement \u2014 then maybe we\u2019d need to have some statement of principles. For now, we\u2019re just a crew of people trying to have the kind of important conversations that the mainstream won\u2019t.\u201d But is a statement of principles necessary to make a judgment call about people like Mr. Cernovich, Mr. Molyneux and Mr. Yiannopoulos? Mr. Rubin has hosted all three on his show. And he appeared on a typically unhinged episode of Mr. Jones\u2019s radio show, \u201cInfowars.\u201d Mr. Rogan regularly lets Abby Martin \u2014 a former 9\/11 Truther who is strangely sympathetic to the regimes in Syria and Venezuela \u2014 rant on his podcast. He also encouraged Mr. Jones to spout off about the moon landing being fake during Mr. Jones\u2019s nearly four-hour appearance on his show. When asked why he hosts people like Mr. Jones, Mr. Rogan has insisted that he\u2019s not an interviewer or a journalist. \u201cI talk to people. And I record it. That\u2019s it,\u201d he has said. Mr. Rubin doesn\u2019t see this is a problem. \u201cThe fact is that Jones reaches millions of people,\u201d he said. \u201cGoing on that show means I get to reach them, and I don\u2019t think anyone is a lost cause. I\u2019ve gotten a slew of email from folks saying that they first heard me on Jones, but then watched a bunch of my interviews and changed some of their views.\u201d The subject came up at that dinner in Los Angeles. Mr. Rubin, whose mentor is Larry King, insisted his job is just to let the person sitting across from him talk and let the audience decide. But with a figure like Mr. Cernovich, who can occasionally sound reasonable, how is a viewer supposed to know better? Of course, the whole notion of drawing lines to keep people out is exactly what inspired the Intellectual Dark Web folks in the first place. They\u2019re committed to the belief that setting up no-go zones and no-go people is inherently corrupting to free thought. \u201cYou have to understand that the I.D.W. emerged as a response to a world where perfectly reasonable intellectuals were being regularly mislabeled by activists, institutions and mainstream journalists with every career-ending epithet from \u2018Islamophobe\u2019 to \u2018Nazi,\u2019\u201d Eric Weinstein said. \u201cOnce I.D.W. folks saw that people like Ben Shapiro were generally smart, highly informed and often princely in difficult conversations, it\u2019s more understandable that occasionally a few frogs got kissed here and there as some I.D.W. members went in search of other maligned princes.\u201d But people who pride themselves on pursuing the truth and telling it plainly should be capable of applying these labels when they\u2019re deserved. It seems to me that if you are willing to sit across from an Alex Jones or Mike Cernovich and take him seriously, there\u2019s a high probability that you\u2019re either cynical or stupid. If there\u2019s a reason for shorting the I.D.W., it\u2019s the inability of certain members to see this as a fatal error. What\u2019s more, this frog-kissing plays perfectly into the hands of those who want to discredit the individuals in this network. In recent days, for example, Mr. Harris has been labeled by the Southern Poverty Law Center as a bridge to the alt-right: \u201cUnder the guise of scientific objectivity, Harris has presented deeply flawed data to perpetuate fear of Muslims and to argue that black people are genetically inferior to whites.\u201d That isn\u2019t true. The group excoriated Mr. Harris, a fierce critic of the treatment of women and gays under radical Islam, for saying that \u201csome percentage, however small\u201d of Muslim immigrants are radicalized. He has also estimated that some 20 percent of Muslims worldwide are Islamists or jihadis. But he has never said that this should make people fear all Muslims. He has defended the work of the social scientist Charles Murray, who argues that genetic differences may explain differences in average IQ across racial groups \u2014 while insisting that this does not make one group inferior to another. But this kind of falsehood is much easier to spread when other figures in the I.D.W. are promiscuous about whom they\u2019ll associate with. When Mr. West tweeted his praise for Ms. Owens, the responses of the people in the network reflected each person\u2019s attitude toward this problem. Dave Rubin took to Twitter to defend Ms. Owens and called Mr. West\u2019s tweet a \u201cgame changer.\u201d Jordan Peterson went on \u201cFox and Friends\u201d to discuss it. Bret Weinstein subtweeted his criticism of these choices: \u201cSmart, skeptical people are often surprisingly susceptible to being conned if a ruse is tailored to their prejudices.\u201d His brother was convinced that Mr. West was playing an elaborate game of chess. Ms. Heying and Mr. Harris ignored the whole thing. Ben Shapiro mostly laughed it off. Mr. West is a self-obsessed rabble-rouser who brags about not reading books. But whether or not one approves of the superstar\u2019s newest intellectual bauble, it is hard to deny that he has consistently been three steps ahead of the zeitgeist. So when he tweets \u201conly freethinkers\u201d and \u201cIt\u2019s no more barring people because they have different ideas,\u201d he is picking up on a real phenomenon: that the boundaries of public discourse have become so proscribed as to make impossible frank discussions of anything remotely controversial. \u201cSo many of our institutions have been overtaken by schools of thought, which are inherently a dead end,\u201d Bret Weinstein said. \u201cThe I.D.W. is the unschooling movement.\u201d Am I a member of this movement? A few months ago, someone suggested on Twitter that I should join this club I\u2019d never heard of. I looked into it. Like many in this group, I am a classical liberal who has run afoul of the left, often for voicing my convictions and sometimes simply by accident. This has won me praise from libertarians and conservatives. And having been attacked by the left, I know I run the risk of focusing inordinately on its excesses \u2014 and providing succor to some people whom I deeply oppose. I get the appeal of the I.D.W. I share the belief that our institutional gatekeepers need to crack the gates open much more. I don\u2019t, however, want to live in a culture where there are no gatekeepers at all. Given how influential this group is becoming, I can\u2019t be alone in hoping the I.D.W. finds a way to eschew the cranks, grifters and bigots and sticks to the truth-seeking. \u201cSome say the I.D.W. is dangerous,\u201d Ms. Heying said. \u201cBut the only way you can construe a group of intellectuals talking to each other as dangerous is if you are scared of what they might discover.\u201d Bari Weiss is a staff editor and writer for the Opinion section.\u00a0@bariweiss Damon Winter joined The Times as a photographer in 2007. He won the 2009 Pulitzer Prize for Feature Photography for his coverage of Barack Obama\u2019s presidential campaign.\u00a0 Advertisement    Collapse SEE MY OPTIONS","time":1525791677,"title":"Meet the Renegades of the Intellectual Dark Web","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/08\/opinion\/intellectual-dark-web.html#news","label":7,"label_name":"random"},{"by":"ShadowFaxSam","descendants":0,"id":17021353,"kids":"None","score":1,"text":"You don\u2019t need me to tell you that the Sun is pretty important to all of us. Without it, we wouldn\u2019t be here, but it won\u2019t be around forever. Scientists predict that the Sun will begin dying in around 10 billion years from now, but since mankind has never been around for such an event it\u2019s impossible to know exactly how it will all go down. Researchers have observed far-off stars in different stages of their lives for decades, and with the data they\u2019ve gathered we\u2019re now closer than ever to being able to predict how our own star will enter the final phases of its life. In a new research paper published in Nature Astronomy a team of astronomers from around the world crunch the numbers and attempt to paint a picture of what will happen to our star billions of years from now.   Using advanced data models that take into account the mass of a star and the type of star it is, the scientists have built a formula to predict whether or not a star create a massive, dusty nebula when it finally dies. The model can forecast the luminosity of the resulting nebula, and when the scientists added the data of our own Sun to the model it spit out the answer. The formula suggests that the Sun is large enough to produce a planetary nebula in its wake, leaving a dusty, glowing cloud of material where much of the Solar System used to be. However, the cloud will likely be quite faint, since our star isn\u2019t actually all that large compared to those which create more stunning nebulae. \u201cWe found that stars with mass less than 1.1 times the mass of the sun produce fainter nebula, and stars more massive than 3 solar masses brighter nebulae, but for the rest the predicted brightness is very close to what had been observed,\u201d Professor Albert Zijlstra of Jodrell Bank explains. \u201cThis is a nice result. Not only do we now have a way to measure the presence of stars of ages a few billion years in distant galaxies, which is a range that is remarkably difficult to measure, we even have found out what the sun will do when it dies!\u201d Obviously mankind will need to be long gone by the time this all happens, and any living creatures roaming on whatever is left of Earth by that point (10 billion years is a long, long time, after all) won\u2019t have anything left to stand on. So pack your bags, and praise the Sun. Copyright 2018 BGR Media, LLC Powered by WordPress.com VIP | Privacy Policy | Your Privacy Rights | Terms Of Use","time":1525791660,"title":"Here\u2019s what might happen when the Sun finally bites the cosmic dust","type":"story","url":"http:\/\/bgr.com\/2018\/05\/08\/what-will-happen-when-the-sun-dies-study\/","label":5,"label_name":"ml"},{"by":"prostoalex","descendants":0,"id":17021342,"kids":"None","score":1,"text":"Our mission is to help leaders in multiple sectors develop a deeper understanding of the global economy. Our flagship business publication has been defining and informing the senior-management agenda since 1964. Our team of 2000+ innovators bring new ideas, solutions, and services to our clients. Our learning programs help organizations accelerate growth by unlocking their people's potential. Our mission is to help leaders in multiple sectors develop a deeper understanding of the global economy. Our flagship business publication has been defining and informing the senior-management agenda since 1964. Our team of 2000+ innovators bring new ideas, solutions, and services to our clients. Our learning programs help organizations accelerate growth by unlocking their people's potential. \r\n                Artificial intelligence has the potential to create trillions of dollars of value across the economy\u2014if business leaders work to understand what AI can and cannot do.\n\r\n             \nIn this episode of the McKinsey Podcast, McKinsey Global Institute partner Michael Chui and MGI chairman and director James Manyika speak with McKinsey Publishing\u2019s David Schwartz about the cutting edge of artificial intelligence. \nDavid Schwartz: Hello, and welcome to the McKinsey Podcast. I\u2019m David Schwartz with McKinsey Publishing. Today, we\u2019re going to be journeying to the frontiers of artificial intelligence. We\u2019ll touch on what AI\u2019s impact could be across multiple industries and functions. We\u2019ll also explore limitations that, at least for now, stand in the way. \nI\u2019m joined by two McKinsey leaders who are at the point of the spear, Michael Chui, based in San Francisco and a partner with the McKinsey Global Institute, and James Manyika, the chairman of the McKinsey Global Institute and a senior partner in our San Francisco office. Michael and James, welcome. \nJames Manyika: Thanks for having us. \nMichael Chui: Great to be here. \nDavid Schwartz: Michael, where do we see the most potential from AI? \nMichael Chui: The number-one thing that we know is just the widespread potential applicability. That said, we\u2019re quite early in terms of the adoption of these technologies, so there\u2019s a lot of runway to go. One of the other things that we\u2019ve discovered is that one way to think about where the potential for AI is, is just follow the money. \nIf you\u2019re a company where marketing and sales is what drives the value, that\u2019s actually where AI can create the most value. If you\u2019re a company where operational excellence matters the most to you, that\u2019s where you can create the most value with AI. If you\u2019re an insurance company, or if you\u2019re a bank, then risk is really important to you, and that\u2019s another place where AI can add value. It goes through everything from managing human capital and analyzing your people\u2019s performance and recruitment, et cetera, all through the entire business system. We see the potential for trillions of dollars of value to be created annually across the entire economy [Exhibit 1]. \nDavid Schwartz: Well, it certainly sounds like there\u2019s a lot of potential and a lot of value yet to be unleashed. James, can you come at it from the other direction? What are the big limitations of AI today? And what do these mean in practical terms for business leaders? \nJames Manyika: When we think about the limitations of AI, we have to keep in mind that this is still a very rapidly evolving set of techniques and technologies, so the science itself and the techniques themselves are still going through development. \nWhen you think about the limitations, I would think of them in several ways. There are limitations that are purely technical. Questions like, can we actually explain what the algorithm is doing? Can we interpret why it\u2019s making the choices and the outcomes and predictions that it\u2019s making? Then you\u2019ve also got a set of practical limitations. Questions like, is the data actually available? Is it labeled? We\u2019ll get into that in a little bit. \nBut I\u2019d also add a third limitation. These are limitations that you might call limitations in use. These are what lead you to questions around, how transparent are the algorithms? Is there any bias in the data? Is there any bias in the way the data was collected? \nDavid Schwartz: Michael, let\u2019s drill down on a first key limitation, data labeling. Can you describe the challenge and some possible ways forward? \nMichael Chui: One of the things that\u2019s a little bit new about the current generations of AI is what we call machine learning\u2014in the sense that we\u2019re not just programming computers, but we\u2019re training them; we\u2019re teaching them. \nThe way we train them is to give them this labeled data. If you\u2019re trying to teach a computer to recognize an object within an image, or if you\u2019re trying to teach your computer to recognize an anomaly within a data stream that says a piece of machinery is about to break down, the way you do that is to have a bunch of labeled data and say, \u201cLook, in these types of images, the object is present. In these types of images, the object\u2019s not present. In these types of data streams, the machine\u2019s about to break, and in these types of data streams, the machine\u2019s not about to break.\u201d \nWe have this idea that machines will train themselves. Actually, we\u2019ve generated a huge amount of work for people to do. Take, for example, self-driving cars. These self-driving cars have cameras on them, and one of the things that they\u2019re trying to do is collect a bunch of data by driving around. \nIt turns out, there is an army of people who are taking the video inputs from this data and then just tracing out where the other cars are\u2014where the lane markers are as well. So, the funny thing is, we talk about these AI systems automating what people do. In fact, it\u2019s generating a whole bunch of manual labor for people to do. \nJames Manyika: I know this large public museum where they get students to literally label pieces of art\u2014that\u2019s a cat, that\u2019s a dog, that\u2019s a tree, that\u2019s a shadow. They just label these different pieces of art so that algorithms can then better understand them and be able to make predictions. \nIn older versions of this, people were identifying cats and dogs. There have been teams, for example, in the UK that were going to identify different breeds of dogs for the purposes of labeling data images for dogs so that when algorithms use that data, they know what it is. The same thing is happening in a lot of medical applications, where people have been labeling different kinds of tumors, for example, so that when machines read those images, they can better understand what\u2019s a tumor and what kind of tumor is it. But it has taken people to label those different tumors for that to then be useful for the machines. \nMichael Chui: A medical diagnosis is the perfect example. So, for this idea of having a system that looks at X-rays and decides whether or not people have pneumonia, you need the data to tell whether or not this X-ray was associated with somebody who had pneumonia or didn\u2019t have pneumonia. Collecting that data is an incredibly important thing, but labeling it is absolutely necessary. \nDavid Schwartz: Let\u2019s talk about ways to possibly solve it. I know that there are two techniques in supervised learning that we\u2019re hearing a lot about. One is reinforcement learning, and the other is GANs [generative adversarial networks]. Could you speak about those? Companies and organizations that are taking AI seriously are playing these multiyear games to acquire the data that they need. \nMichael Chui: A number of these techniques are meant to basically create more examples that allow you to teach the machine, or have it learn. \nReinforcement learning has been used to train robots, in the sense that if the robot does the behavior that you want it to, you reward the robot for doing it. If it does a behavior you don\u2019t want it to do, you give it negative reinforcement. In that case, what you have is a function that says whether you did something good or bad. Rather than having a huge set of labeled data, you just have a function that says you did good or you did the wrong thing. That\u2019s one way to get around label data\u2014by having a function that tells you whether you did the right thing. \nWith GANs, which stands for generative adversarial networks, you basically have two networks, one that\u2019s trying to generate the right thing; the other one is trying to discriminate whether you\u2019re generating the right thing. Again, it\u2019s another way to get around one potential limitation of having huge amounts of label data in the sense that you have two systems that are competing against each other in an adversarial way. It\u2019s been used for doing all kinds of things. The generative\u2014the \u201cG\u201d part of it\u2014is what\u2019s remarkable. You can generate art in the style of another artist. You can generate architecture in the style of other things that you\u2019ve observed. You can generate designs that look like other things that you might have observed before. \nJames Manyika: The one thing I would add about GANs is that, in many respects, they\u2019re a form of semisupervised learning techniques in the sense that they typically start with some initial labeling but then, in a generative way, build on it\u2014in this adversarial, kind of a contest way. \nThere\u2019s also a whole host of other techniques that people are experimenting with. One of the things, for example, is researchers at Microsoft Research Lab have been working on instream labeling, where you\u2019ll actually label the data through use. You\u2019re trying to interpret based on how the data\u2019s being used, what it actually means. This idea of instream labeling has been around for quite a while, but in recent years, it has started to demonstrate some quite remarkable results. This problem of labeling is one we\u2019re going to be with for quite a while. \nDavid Schwartz: What about limitations when there is not enough data? \nMichael Chui: One of the things that we\u2019ve heard from Andrew Ng, who\u2019s one of the leaders in machine learning and AI, is that companies and organizations that are taking AI seriously are playing these multiyear games to acquire the data that they need. \nIn the physical world, whether you\u2019re doing self-driving cars or drones, it takes time to go out and drive a whole bunch of streets or fly a whole bunch of things. To try to improve the speed at which you can learn some of those things, one of the things you can do is simulate environments. By creating these virtual environments\u2014basically within a data center, basically within a computer\u2014you can run a whole bunch more trials and learn a whole bunch more things through simulation. So, when you actually end up in the physical world, you\u2019ve come to the physical world with your AI already having learned a bunch of things in simulation. That\u2019s the holy-grail question: How do you build generalizable systems that can learn anything? \nJames Manyika: A good example of that is some of the demonstrations, for example, that the team at DeepMind Technologies has done. They\u2019ve done a lot of simulated training for robotic arms, where much of the manipulation techniques that these robotic arms have been able to develop and learn was from having actually been done in simulation\u2014way before the robot arm was even applied to the real world. When it shows up in the real world, it comes with these prelearned data sets that have come out of simulation as a way to get around the limitations of data. \nDavid Schwartz: It sounds like we may be considering a deeper issue\u2014what machine intelligence actually means. How can we move from a process of rote inputs and set outputs to something more along the lines of the ways that humans learn? \nJames Manyika: That\u2019s, in some ways, the holy-grail question, which is: How do you build generalizable systems that can learn anything? Humans are remarkable in the sense that we can take things we\u2019ve learned over here and apply them to totally different problems that we may be seeing for the first time. This has led to one big area of research that\u2019s typically referred to as transfer learning, the idea of, how do you take models or learnings or insights from one arena and apply them to another? While we\u2019re making progress in transfer learning, it\u2019s actually one of the harder problems to solve. And there, you\u2019re finding new techniques. \nThis idea of simulating learning where you generate data sets and simulations is one way to do that. AlphaGo Zero, which is a more interesting version, if you like, of AlphaGo, has learned to play three different games but has just a generalized structure of games. Through that, it\u2019s been able to learn chess and Go\u2014by having a generalized structure. But even that is limited in the sense that it\u2019s still limited to games that take a certain form. \nMichael Chui: In the AI field, what we\u2019re relearning, which neurologists have known for a long time, is that as people, we don\u2019t come as tabula rasa. We actually have a number of structures in our brain that are optimized for certain things, whether it\u2019s understanding language or behavior, physical behavior, et cetera. People like Geoff Hinton are using capsules and other types of concepts. This idea of embedding some learning in the structure of the systems that we\u2019re using is something that we\u2019ve seen as well. And so, you wonder whether for transfer learning, part of the solution is understanding that we don\u2019t start from nothing. We start from systems that have some configuration already, and that helps us be able to take certain learnings from one place to another because, actually, we\u2019re set up to do that. \nJames Manyika: In fact, Steve Wozniak has come out with certain suggestions, and this has led to all kinds of questions about what\u2019s the right Turing test or the kind of test you can come up with generalized learning. One version that he has is the so-called \u201ccoffee test,\u201d which is, the day we can get a system that could walk into an unknown American household and make a cup of coffee. That\u2019s pretty remarkable, because that requires being able to interpret a totally unknown environment, being able to discover things in a totally unknown place, and being able to make something with unknown equipment in a particular household. \nThere are a lot of general problems that need to be solved along the way of making a cup of coffee in an unknown household, which may sound trivial compared to solving very narrow, highly technical, specific problems which we think of as remarkable. The more we can then look to solving what are generalized often as, quite frankly, garden-variety, real-world problems, those might actually be the true tests of whether we have generalized systems or not. \nAnd it is important to remember, by the way, as we think about all the exciting stuff that\u2019s going on in AI and machine learning, that the vast majority\u2014whether it\u2019s the techniques or even the applications\u2014are mostly solving very specific things. They\u2019re solving natural-language processing; they\u2019re solving image recognition; they\u2019re doing very, very specific things. There\u2019s a huge flourishing of that, whereas the work going toward solving the more generalized problems, while it\u2019s making progress, is proceeding much, much more slowly. We shouldn\u2019t confuse the progress we\u2019re making on these more narrow, specific problem sets to mean, therefore, we have created a generalized system. \nThere\u2019s another limitation, which we should probably discuss, David\u2014and it\u2019s an important one for lots of reasons. This is the question of \u201cexplainability.\u201d Essentially, neural networks, by their structure, are such that it\u2019s very hard to pinpoint why a particular outcome is what it is and where exactly in the structure of it something led to a particular outcome. \nDavid Schwartz: Right. I\u2019m hearing that we\u2019re dealing with very complicated problems, very complex issues. How would someone, outside in, ever understand what may appear to be\u2014may in fact be\u2014almost a black box? \nJames Manyika: This is the question of explainability, which is: How do we even know that? You think about where we start applying these systems in the financial world\u2014for example, to lending. If we deny you for a mortgage application, you may want to know why. What is the data point or feature set that led to that decision? If you apply the system set to the criminal-justice system, if somebody\u2019s been let out on bail and somebody else wasn\u2019t, you may want to understand why it is that we came to that conclusion. It may also be an important question for purely research purposes, where you\u2019re trying to self-discover particular behaviors, and so you\u2019re trying to understand what particular part of the data leads to a particular set of behaviors. \nThis is a very hard problem structurally. The good news, though, is that we\u2019re starting to make progress on some of these things. One of the ways in which we\u2019re making progress is with so-called GANs. These are more generalized, additive models where, as opposed to taking massive amounts of models at the same time, you almost take one feature model set at a time, and you build on it. \nFor example, when you apply the neural network, you\u2019re exploring one particular feature, and then you layer on another feature; so, you can see how the results are changing based on this kind of layering, if you like, of different feature models. You can see, when the results shift, which model feature set seemed to have made the biggest difference. This is a way to start to get some insight into what exactly is driving the behaviors and outcomes you\u2019re getting. \nMichael Chui: One of the other big drivers for explainability is regulation and regulators. If a car decides to make a left turn versus a right turn, and there\u2019s some liability associated with that, the legal system will want to ask the question, \u201cWhy did the car make the left turn or the right turn?\u201d In the European Union, there\u2019s the General Data Protection Regulation that will require explainability for certain types of decisions that these machines might make. The machines are completely deterministic. You could say, \u201cHere are a million weights that are associated with our simulated neurons. Here\u2019s why.\u201d But that\u2019s not engaging to a human being. \nAnother technique is an acronym, LIME, which is locally interpretable model-agnostic explanations. The idea there is from the outside in\u2014rather than look at the structure of the model, just be able to perturb certain parts of the model and the inputs and see whether that makes a difference on the outputs. If you\u2019re taking a look at an image and trying to recognize whether an object is a pickup truck or an ordinary sedan, you might say, \u201cIf I change the wind screen on the inputs, does that cause me to have a different output? On the other hand, if I change the back end of the vehicle, it looks like that makes a difference.\u201d That says, that what this model is paying attention to as it\u2019s determining whether it\u2019s a sedan or a pickup truck is the back part of the vehicle. It\u2019s basically doing experiments on the model in order to figure out what makes a difference. Those are some of the techniques that people are trying to use in order to explain how these systems work. \nDavid Schwartz: At some level, I\u2019m hearing from the questions and from what the rejoinder might be that there\u2019s a very human element. A question would be: Why is the answer such and such? And the answer could be, it\u2019s the algorithm. But somebody built that algorithm, or somebody\u2014or a team of somebodies\u2014and machines built that algorithm. That brings us to a limitation that is not quite like the others: bias\u2014human predilections. Could you speak a little bit more about what we\u2019re up against, James? It becomes very, very important to think through what might be the inherent biases in the data, in any direction. \nJames Manyika: The question of bias is a very important one. And I\u2019d put it into two parts. \nClearly, these algorithms are, in some ways, a big improvement on human biases. This is the positive side of the bias conversation. We know that, for example, sometimes, when humans are interpreting data on CVs [curriculum vitae], they might gravitate to one set of attributes and ignore some other attributes because of whatever predilections that they bring. There\u2019s a big part of this in which the application of these algorithms is, in fact, a significant improvement compared to human biases. In that sense, this is a good thing. We want those kinds of benefits. \nBut I think it\u2019s worth having the second part of the conversation, which is, even when we are applying these algorithms, we do know that they are creatures of the data and the inputs you put in. If those inputs you put in have some inherent biases themselves, you may be introducing different kinds of biases at much larger scale. \nThe work of people like Julia Angwin and others has actually shown this if the data collected is already biased. If you take policing as an example, we know that there are some communities that are more heavily policed. There\u2019s a much larger police presence. Therefore, the data we\u2019ve got and that\u2019s collected about those environments is much, much, much higher. If we then start to compare, say, two neighborhoods, one where it\u2019s oversampled\u2014meaning there\u2019s lots and lots of data available for it because there\u2019s a larger police presence\u2014versus another one where there isn\u2019t much policing so, therefore, there isn\u2019t much data available, we may draw the wrong conclusions about the heavily policed observed environment, just simply because there\u2019s more data available for it versus the other one. \nThe biases can go another way. For example, in the case of lending, the implications might go the other way. For populations or segments where we have lots and lots of financial data about them, we may actually make good decisions because the data is largely available, versus in another environment where we\u2019re talking about a segment of the population we don\u2019t know much about, and the little bit that we know sends the decision off in one way. And so, that\u2019s another example where the undersampling creates a bias. \nThe point about this second part is that I think it becomes very, very important to make sure that we think through what might be the inherent biases in the data, in any direction, that might be in the data set itself\u2014either in the actual way it\u2019s constructed, or even the way it\u2019s collected, or the degree of sampling of the data and the granularity of it. Can we debias that in some fundamental way? \nThis is why the question of bias, for leaders, is particularly important, because it runs a risk of opening companies up to all kinds of potential litigation and social concern, particularly when you get to using these algorithms in ways that have social implications. Again, lending is a good example. Criminal justice is another example. Provision of healthcare is another example. These become very, very important arenas to think about these questions of bias. \nMichael Chui: Some of the difficult cases where there\u2019s bias in the data, at least in the first instance, isn\u2019t around, as a primary factor, people\u2019s inherent biases about choosing either one or the other. It is around, in many cases, these ideas about sampling\u2014sampling bias, data-collection bias, et cetera\u2014which, again, is not necessarily about unconscious human bias but an artifact of where the data came from. \nThere\u2019s a very famous case, less AI related, where an American city used an app in the early days of smartphones that determined where potholes were based on the accelerometer shaking when you drove over a pothole. Strangely, it discovered that if you looked at the data, it seemed that there were more potholes in affluent parts of the city. That had nothing to do with the fact there were actually more potholes in that part of the city, but you had more signals from that part of the city because more affluent people had more smartphones at the time. That\u2019s one of those cases where it wasn\u2019t because of any intention to not pay attention to certain parts of the city. Understanding the providence of data\u2014understanding what\u2019s being sampled\u2014is incredibly important. \nThere\u2019s another researcher who has a famous TED Talk, Joy Buolamwini at MIT Media Lab. She does a lot of work on facial recognition, and she\u2019s a black woman. And she says, \u201cLook, a lot of the other researchers are more male and more pale than I am. And as a result, the accuracy for certain populations in facial recognition is far higher than it is for me.\u201d So again, it\u2019s not necessarily because people are trying to exclude populations, although sometimes that happens, it really has to do with understanding the representativeness of the sample that you\u2019re using in order to train your systems. \nSo, as a business leader, you need to understand, if you\u2019re going to train machine-learning systems: How representative are the training sets there that you\u2019re using? People forget that one of the things in the AI machine-deep-learning world is that many researchers are using largely the same data sets that are shared\u2014that are public. \nJames Manyika: It actually creates an interesting tension. That\u2019s why I described the part one and the part two. Because in the first instance, when you look at the part-one problem, which is the inherent human biases in normal day-to-day hiring and similar decisions, you get very excited about using AI techniques. You say, \u201cWow, for the first time, we have a way to get past these human biases in everyday decisions.\u201d But at the same time, we should be thoughtful about where that takes us to when you get to these part-two problems, where you now are using large data sets that have inherent biases. \nI think people forget that one of the things in the AI machine-deep-learning world is that many researchers are using largely the same data sets that are shared\u2014that are public. Unless you happen to be a company that has these large, proprietary data sets, people are using this famous CIFAR data set, which is often used for object recognition. It\u2019s publicly available. Most people benchmark their performance on image recognition based on these publicly available data sets. So, if everybody\u2019s using common data sets that may have these inherent biases in them, we\u2019re kind of replicating large-scale biases. This tension between part one and part two and this bias question are very important ones to think through. The good news, though, is that in the last couple years, there\u2019s been a growing recognition of the issues we just described. And I think there are now many places that are putting real research effort into these questions about how you think about bias. \nDavid Schwartz: What are best practices for AI, given what we\u2019ve discussed today about the wide range of applications, the wide range of limitations, and the wide range of challenges before us? \nMichael Chui: It is early, so to talk about best practices might be a little bit preliminary. I\u2019ll steal a phrase that I once heard from Gary Hamel: we might be talking about next practices, in a certain sense. That said, there a few things that we\u2019ve observed from leaders who are pioneers and vanguards. \nThe first thing is one we\u2019ve described as \u201cget calibrated,\u201d but it\u2019s really just to start to understand the technology and what\u2019s possible. For some of the things that we\u2019ve talked about today, business leaders over the past few years have had to understand technology more. This is really on the tip of the spear, on the cutting edge. So, really try to understand what\u2019s possible in the technology. \nThen, try to understand what the potential implications are across your entire business. As we said, these technologies are widely applicable. So, understand where in your business you\u2019re deriving value and how these technologies can help you derive value, whether it\u2019s marketing and sales, whether it\u2019s supply chain, whether it\u2019s manufacturing, whether it\u2019s in human capital or risk [Exhibit 2]. \nAnd then, don\u2019t be afraid to be bold. At least experiment. This is a type of technology where it\u2019s a learning curve, and the earlier you to start to learn, the faster you\u2019ll go up the curve and the quicker you\u2019ll learn where you can add value, where you can find data, and how you can have a data strategy in order to unlock the data you need to do machine learning. Getting started early\u2014there\u2019s really no substitute for that. \nJames Manyika: The only other thing I would add is something you\u2019ve been working a lot on, Michael. One of the things that leaders are going to have to understand, or make sure that their teams understand, is this question of which techniques map to which kinds of problems, and also which techniques lead to what kind of value. \nWe know that the vast majority of the techniques, in the end, are largely classifiers. Knowing that is helpful. Then knowing if the kind of problem sets in your business system are ones that look like classification problems; if so, you have an enormous opportunity. This leads to where you then think about where economic value is and if you have the data available. \nThere\u2019s a much more granular understanding that leaders are going to have to have, unfortunately. The reason why this matters, back to Michael\u2019s next-practice point, is that we are already seeing, if you like, a differentiation between those leaders and company who are at the frontier of understanding this and applying these techniques, versus others who are, quite frankly, dabbling\u2014or, at least, paying lip service. \nIt\u2019s worth occasionally as a leader, I would think, visiting or spending time with researchers at the frontier, or at least talking to them, just to understand what\u2019s going on and what\u2019s not possible. Because this field is moving so quickly. Things that may have been seen as limitations two years ago may not be anymore. And if you\u2019re still relying on a conversation you had with an AI scientist two years ago, you may be behind already. \nDavid Schwartz: James and Michael, absolutely fascinating. Thank you for joining us. \nJames Manyika: Thank you. \nMichael Chui: Thank you. Please sign in to print or download this article. Please create a profile to print or download this article. \r\n                                    Create a profile to get full access to our articles and reports, including those by McKinsey Quarterly and the McKinsey Global Institute, and to subscribe to our newsletters and email alerts.\r\n                                   McKinsey uses cookies to improve site functionality, provide you with a better browsing experience, and to enable our partners to advertise to you. Detailed information on the use of cookies on this Site, and how you can decline them, is provided in our cookie policy. By using this Site or clicking on \"OK\", you consent to the use of cookies. Select topics and stay current with our latest insights","time":1525791611,"title":"The real-world potential and limitations of artificial intelligence","type":"story","url":"https:\/\/www.mckinsey.com\/featured-insights\/artificial-intelligence\/the-real-world-potential-and-limitations-of-artificial-intelligence?cid=podcast-eml-alt-mkq-mck-oth-1805&hlkid=5484d298eec3407fa2e0bb8aee2c561c&hctky=1886072&hdpid=8b6826bd-a3a2-403c-85bf-4bae5416f343","label":5,"label_name":"ml"},{"by":"petethomas","descendants":1,"id":17021336,"kids":"[17021663]","score":2,"text":"Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world. Americas+1 212 318 2000 EMEA+44 20 7330 7500 Asia Pacific+65 6212 1000 Also Bitcoin futures, BCFP vs. CFPB, adviser pay, Jay-Z and probably not insider trading. T+1 settlement. If I sell you a Bitcoin, what have I sold you? Is it a currency? A commodity? A financial asset? A security, even? Something new and different? There are tedious debates. I suppose you might find them interesting if you\u2019re a philosopher of money, or of Bitcoin. But you might find them important\u2014yet still tedious!\u2014if you are a regulated financial institution thinking about trading Bitcoins. The regulations that apply to you contain lots of lists of things and what you can do to them, and you need to figure out where Bitcoin fits on those lists. You can do some things to securities, and other things to commodities, and still other things to currencies. What things can you do to Bitcoin? On the other hand, if I agree to give you a Bitcoin in a month, and you agree to give me the money for\u00a0it in a month, now we are getting somewhere. Now we know what I\u2019ve sold you. The name for it\u00a0is a bit squishy\u2014\u201cforward\u201d is conventional, \u201cfuture\u201d is a variant (normally applied to exchange-traded contracts), and \u201cswap\u201d is a good general term\u2014but nonetheless the applicable U.S. regulatory regime is much better understood. It\u2019s a swap, and it\u2019s regulated by the Commodity Futures Trading Commission, and if the relevant list of things says you can do stuff with swaps, then you can do exactly that stuff with Bitcoin swaps.\u00a0(Unless Bitcoin is a security; securities-based swaps have their own regulatory regime.) Anyway the Intercontinental Exchange might start offering Bitcoin trading, or at least, one-day physically-settled Bitcoin swap trading: ICE has had conversations with other financial institutions about setting up a new operation through which banks can buy a contract, known as a swap, that will end with the customer owning Bitcoin the next day \u2014 with the backing and security of the exchange, according to the people familiar with the project. The swap contract is more complicated than an immediate trade of dollars for Bitcoin, even if the end result is still ownership of a certain amount of Bitcoin. But a swap contract allows the trading to come under the regulation of the Commodity Futures Trading Commission and to operate clearly under existing laws \u2014 something today\u2019s Bitcoin exchanges have struggled to do. One-day physically-settled swaps! It sounds like a joke, a trick. We can sell you Bitcoins, but for regulatory purposes we have to wrap them in a plain brown paper bag called a \u201cswap,\u201d and you can\u2019t open the bag until you leave the store.\u00a0 This does not solve all of the world\u2019s categorization problems. If you\u2019re a regulated institution that is allowed to own swaps, and you buy a Bitcoin swap, then you own a Bitcoin swap and that\u2019s fine, but if it\u2019s a one-day physically-settled swap, then the next day you own a Bitcoin, and you\u2019re right back where you started from. It does, however, crucially, solve the\u00a0exchange\u2019s\u00a0categorization problems: It\u2019s offering swaps trading, so it can go to the CFTC\u00a0for approval of its offering, its processes, etc. Not only that, but there is a long history of commodity exchanges\u00a0offering\u00a0trading in futures\u00a0and also helping with physical settlement processes. Metals exchanges let customers trade metals futures contracts, and also regulate systems of warehouses to store the actual metal so that futures customers can take delivery. Once you\u2019re a regulated exchange, you can start building a regulated system for holding and delivering and keeping track of the underlying stuff.\u00a0 Of course, when the underlying stuff is Bitcoin, there are some ironies there. For one thing, Bitcoin already\u00a0has\u00a0a system for holding and delivering and keeping track of itself. The\u00a0point\u00a0of Bitcoin is that it is a currency (asset, whatever)\u00a0that doesn\u2019t need any sort of third-party mechanism for storage or delivery: The Bitcoin blockchain is a decentralized ledger that keeps track of Bitcoins, and I can give you Bitcoins using that blockchain directly; a bank or exchange doesn\u2019t need to be involved. But exchanges keep getting involved. (Separately, conventional financial\u00a0exchanges keep talking about using blockchain technology to settle trades in stocks or bonds or whatever. I hope ICE will use a\u00a0blockchain\u00a0to keep track of its Bitcoin-swap trades, but not the Bitcoin blockchain.) Another irony is that advocates of blockchain technology in finance often talk about the magic of instant settlement. One advantage of being able to transact directly over the blockchain is that you can\u00a0do\u00a0the transaction at the same time you\u00a0agree\u00a0to do the transaction. You don\u2019t need to do a trade on the exchange and then spend days tracking down a custodian to get it to send your stocks to your counterparty; you just do a trade on the blockchain and, by virtue of that trade, your counterparty has your stuff. But for Bitcoins, the exchanges will actually\u00a0create a settlement delay. I and others often talk about the idea that cryptocurrency advocates are rebuilding the financial system from scratch, and keep discovering why the old financial system had features\u2014reversibility of transactions, centralized custody, securities-registration rules, flexible\u00a0monetary policy\u2014that cryptocurrency\u00a0enthusiasts initially rejected. Perhaps delayed settlement will be one of those features that gets reintroduced to cryptocurrency because it is so useful. But that's not what this looks like. This looks like reintroducing a feature of old-fashioned finance solely for dumb mechanical compliance with old rules. You have the one-day delay not because it serves any useful purpose, but because it lets you check a regulatory box to call your thing a swap. Also \u2026\u00a0will\u00a0ICE build a warehouse to hold\u00a0Bitcoins for its customers? Lots of exchanges have done that; a basic function of Bitcoin exchanges is to hold Bitcoins on behalf of their customers, rather than giving the customers the Bitcoins directly through the Blockchain. The problem is that there is a long and extremely consistent history of all those exchanges being hacked\u00a0and having their Bitcoins stolen. If a giant regulated exchange like ICE\u2014the parent company of the New York Stock Exchange\u2014starts holding Bitcoins, it will be an irresistible target. If it does get hacked successfully, then that will be funny. (And a\u00a0setback for Bitcoin trading among big institutions.) If it\u00a0doesn\u2019t, that will be funny in its way too. Bitcoin was designed by cryptographers to be secure and to supplant the regulated financial system; it\u2019ll be funny if the way to securely hold Bitcoins turns out to be through the regulated financial system. Incidentally: When I sell you a share of stock, what have I sold you? Well, conventionally, we would say that I have sold you\u00a0a share of stock. But\u00a0actually, if I sell you a share of stock on the stock exchange, what happens is that I agree to give you that share of stock in two days, and you agree to give me the money for it in two days. (It was three days until recently.) No one calls this a \u201cstock swap\u201d (or forward or future). They just call it a stock trade; a common term is \u201cregular-way settlement.\u201d You need a longer delay to transform a stock sale into a stock swap\/forward\/future. Perhaps a one-day settlement delay is enough to turn a Bitcoin trade into a Bitcoin swap, but it does seem odd. Meanwhile in\u00a0 Bitcoin futures. Have I apologized yet for how wrong I was about Bitcoin futures? In the weeks before Bitcoin futures started trading at Cboe Global Markets Inc. and CME Group Inc., a lot of people argued that Bitcoin futures would finally provide a convenient way to short Bitcoin, and that the shorts would rush in and deflate the price. Meanwhile\u00a0I was galaxy-braining around being like \u201cno no no you see this is a convenient way to\u00a0buy\u00a0Bitcoin, which was previously just as hard for careful conservative financial institutions as shorting it was, and so the introduction of Bitcoin futures will make the price go\u00a0up.\u201d And then the futures started trading and I took a victory lap, like, five minutes into the trading session, when Bitcoin hit a new record high. And then it fell and has never recovered; Bitcoin is worth about half what it was\u00a0when futures were introduced.\u00a0If you read my posts about how futures would be good for Bitcoin prices and put all your money in Bitcoin \u2026\u00a0sorry about that? I don\u2019t know, if you took anything here as investing advice then I think that is more your fault than mine, but nonetheless I confess that my call was pretty bad. Here\u2019s a Federal Reserve Bank of San Francisco Economic Letter about \u201cHow Futures Trading Changed Bitcoin Prices.\u201d I suppose it is easier to be right about that question six months after the futures were introduced than it was a week\u00a0before\u00a0they were introduced, but still, the San Francisco Fed\u2019s analysis\u00a0is a little too textbook for my tastes: Before December 2017, there was no market for bitcoin derivatives. This meant that it was extremely difficult, if not impossible, to bet on the decline in bitcoin price. Such bets usually take the form of short selling, that is selling an asset before buying it, forward or future contracts, swaps, or a combination. Betting on the increase in bitcoin price was easy\u2014one just had to buy it. Speculative demand for bitcoin came only from optimists, investors who were willing to bet money that the price was going to go up. And until December 17, those investors were right: As with a self-fulfilling prophecy, optimists\u2019 demand pushed the price of bitcoin up, energizing more people to join in and keep pushing up the price. The pessimists, however, had no mechanism available to put money behind their belief that the bitcoin price would collapse. So they were left to wait for their \u201cI told you so\u201d moment. This one-sided speculative demand came to an end when the futures for bitcoin started trading on the CME on December 17. Umm look I guess. On the other hand let\u2019s say you had found a convenient and inexpensive way to short Bitcoin in, say, December 2016. No borrow costs, no creepy exchanges, just a perfect seamless way to sell Bitcoins now and buy them back in the future. If you did that on December 17, 2016\u2014a year before futures were actually introduced\u2014you\u2019d have had a 100-percent loss by May. If you did it in May, you\u2019d have had a 100-percent loss by August. If you did it in August, you\u2019d have had a 100-percent loss by October. If you did it in October, you\u2019d have had a 100-percent loss by November. If you shorted Bitcoins in November 2017, hoo boy.\u00a0 I don\u2019t entirely believe\u00a0that the Bitcoin futures market is full\u00a0of people taking naked short Bitcoin bets, is I guess my point here? The difficulty of shorting Bitcoin is not\u00a0primarily\u00a0about the mechanics of finding a way to short. It\u2019s primarily about Bitcoin\u2019s huge volatility and rapid rise and general ability to blow shorts up in like a day. You can find a lot of people pontificating that they\u2019d love to short Bitcoin (here\u2019s Bill Gates!), but they all \u2026\u00a0can \u2026\u00a0and \u2026\u00a0don\u2019t? (Here\u2019s Tyler Winklevoss telling Gates, go ahead, short Bitcoin, be my guest.) Even with the introduction of Bitcoin futures, there is no convenient way to express the view that \u201cBitcoin will eventually go to zero but I have no idea what these crazy kids will get up to for the next few years.\u201d And that seems to be the\u00a0actual\u00a0short-Bitcoin thesis. If your thesis is \u201cBitcoin will go to zero in a month\u201d then, sure, go ahead, short the futures, but I have trouble believing that there\u2019s\u00a0much money staked on that thesis. It seems a little nerve-wracking, you know? How\u2019s Mick Mulvaney doing? Mick Mulvaney, who was installed as the head of the Consumer Financial Protection Bureau as a grim joke, has played a series of\u00a0pranks there: Since taking over in November, he has halted all new investigations, frozen hiring, stopped data collection and proposed cutting off public access to a database of consumer complaints. He dropped most cases against payday lenders \u2014 a primary focus of the consumer bureau \u2014 and also proposed scrapping a new rule that would have heightened scrutiny of an industry accused of trapping vulnerable customers in a cycle of debt. And he has tried hard to persuade Congress to take away funding authority for the bureau from the Federal Reserve \u2014 so that Congress can cut it. Good ones, good ones. And here\u2019s kind of a meta joke: Mr. Mulvaney seems happiest when describing new ways to undermine the consumer bureau by, say, removing its online complaint system from public view \u2014 or using the agency\u2019s obscure statutory name, the \u201cBureau of Consumer Financial Protection,\u201d to undo years of branding. \u201cThe reading of the statute actually revealed some very fun things,\u201d an excited Mr. Mulvaney told his friendly audience of bankers last week. \u201cC.F.P.B. doesn\u2019t exist! C.F.P.B. has never existed!\u201d We\u2019ve talked before about the dumb fight between Mulvaney and Leandra English, the former deputy director of the CFPB, who tried to mount a coup and declare herself director. That fizzled out, unsurprisingly, but Mulvaney\u2019s comments suggest the compromise that should have happened. Mulvaney should be head of the BCFP, which will have no funding, do no investigations, hire no one and do nothing. He\u2019d love it!\u00a0English can be head of the CFPB, which will employ all the people who used to work at the CFPB to\u00a0do all the investigations it used to do. And then they can argue about whether it exists. Conflicts of interest. We have talked before about the dumbest and most obvious conflict of interest in financial advising, which is that if you are a financial adviser it is in your financial interests to sell more financial advice, while that is not always in your customer\u2019s interests. It is a conflict that is not limited to financial advising\u2014it is just as applicable in car sales or clothing sales or anything else\u2014and one that is difficult to resolve, since it is not some weird arcane scam but is inherent in the nature of capitalism. Still you could do this: One of Australia\u2019s biggest banks has scrapped sales-based bonuses for its financial planners and vowed to drop planners who provide inappropriate advice. In a first among the country\u2019s largest lenders, Australia & New Zealand Banking Group Ltd. said Monday that it is implementing initiatives to improve the quality of financial planning and of remediation for customers when things go wrong. \u2026 ANZ said it would end sales incentives for bonuses and assess planners solely on customer satisfaction, the bank\u2019s values and risk and compliance standards. It only gets you part of the way there. I assume that an ANZ financial adviser who never sells\u00a0anything\u00a0will eventually be fired. You gotta do some business, to stay in business. I look forward to the first case of a financial-advice conscientious objector who goes to work as a financial adviser, never sells any financial products because she doesn\u2019t believe in them, gets fired, and sues.\u00a0 Jay-Z. We talked last week about Shawn Carter, a\/k\/a Jay-Z, who was subpoenaed by the Securities and Exchange Commission to testify in a securities investigation. I joked, \u201cHe\u2019ll probably show up at the SEC\u2019s offices and they\u2019ll begin \u2018Mr. Carter, would it be accurate to say that Iconix\u00a0had 99 problems and an accounting write-down\u00a0of Rocawear intangible assets\u00a0was one?\u2019\u201d But Jay-Z\u2019s lawyers made more or less the same point, seriously, in responding to the subpoena: Jay-Z, whose given name is Shawn Carter, was willing to testify for a full day, but not \u201cday-to-day until completed\u201d as the SEC is seeking, his attorneys wrote in a court filing dated Monday. The lawyers cited Jay-Z\u2019s concern that the agency\u2019s request \u201cis driven more by governmental fascination with celebrity and headlines than by any proper investigative purpose.\u201d I do not want to impugn the motives of the SEC here, but \u2026 right? Like, I mean, they are conducting an investigation into accounting irregularities. That is fine as far as it goes, but it\u2019s a little dry. It\u2019s not gonna thrill your kids when you talk about it at the dinner table. Breaking up the forensic accounting analysis with a little time spent\u00a0hanging out with Jay-Z\u00a0is obviously pleasant. Though not so much for him.\u00a0\u201cSuffice to say, the professional and publicity demands on his time are enormous,\u201d add his lawyers. Nor is it obvious that his testimony is that relevant. As Bloomberg points out, it\u2019s a \u201cprobe of a firm he did business with more than a decade ago.\u201d It feels like one day would be enough?\u00a0 Legal secretaries. Here is a heartwarming story about a recently deceased secretary at a big New York law firm who lived frugally, saved her money, invested wisely, and left a $9 million fortune when she died, almost all of which will go to charity.\u00a0Because my readers are miserable cold-hearted cynics, like five people have sent me this story to be like \u201csurely this was insider trading?\u201d No? I feel like I read enough of these stories that focus on the magic of frugality and compound interest that I am not inclined to suspect the secretary. On the other hand: \u201cShe was a secretary in an era when they ran their boss\u2019s lives, including their personal investments,\u201d recalled her niece Jane Lockshin. \u201cSo when the boss would buy a stock, she would make the purchase for him, and then buy the same stock for herself, but in a smaller amount because she was on a secretary\u2019s salary.\u201d What if\u00a0they\u00a0were all insider trading? (She apparently started there in the 1940s, when\u00a0that might have been a bit more socially and legally acceptable than it is today.) Still I don\u2019t buy it. Also, I don\u2019t spend a ton of time arguing that insider trading is a victimless crime, but, you know. So what if she was insider trading? What if she spent 67 years accumulating $9 million from the stock market, a little bit at a time, and then gave it all to charity? Is that actually worse than not doing it? Things happen. Retooling the leverage ratio. Goldman, Wells Fargo Look to Credit Cards for Bigger Returns. Activist Sets Sights on Citigroup. Shire board recommends \u00a346bn takeover by Takeda. \u201cWild swings in aluminum prices have jolted buyers and sellers of the metal, threatening profits of companies that make everything from jets to beer cans.\u201d Samsung Securities Seeks Charges Against Some Employees Over \u2018Fat Finger\u2019 Mistake. (Earlier.) The dangers of private placements.\u00a0NY Attorney General Schneiderman Resigns. \u201cIn fact, even at its most commercial, sexual interaction is not a transferable commodity. It is labour, which was classified as a distinct input of production as far back as Ricardo and Smith.\u201d Elon Musk is dating Grimes. If you'd like to get\u00a0Money\u00a0Stuff\u00a0in handy email form, right in your inbox, please\u00a0subscribe at this link. Thanks!\u00a0 This column does not necessarily reflect the opinion of the editorial board or Bloomberg LP and its owners. To contact the author of this story:Matt Levine at mlevine51@bloomberg.net To contact the editor responsible for this story:James Greiff  at jgreiff@bloomberg.net","time":1525791530,"title":"Putting Bitcoin in the Swaps Box","type":"story","url":"https:\/\/www.bloomberg.com\/view\/articles\/2018-05-08\/putting-bitcoin-in-the-swaps-box","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17021331,"kids":"None","score":1,"text":"Twitter is reportedly testing a secret, encrypted messaging option that would enable its platform to go up against Signal, Telegram or WhatsApp. TechCrunch is citing developer Jane Manchun Wong, who found the feature after diving through Twitter's Android app. Twitter wouldn't give the site an official response, but if parts of the system have made their way into the app, clearly testing is at an advanced stage. The feature, which would form part of Twitter's Direct Messages, is currently titled \"Secret Conversation,\" with users able to view your, and their, encryption keys. That's pretty much all that's known about the service right now, but it's likely that Twitter is thinking about how better protect its users. Not to mention, as the report points out, such a feature was initially requested by Edward Snowden at the end of 2016. reasonable and something we'll think about Twitter is coming under the same scrutiny as many other social platforms, all of which need to demonstrate that they can be trusted with your personal data (oops!). The service, famously, played a part in fomenting the uprising that would come to be known as the Arab Spring. But has since found itself as a vessel for nation state propaganda -- a problem that it's working hard to resolve as it finally reaches profitability. The show stars Timothy Olyphant and Drew Barrymore. They cut through the winter confusion to see what's actually present. Surprise, surprise: The Pixelbook will get the feature first.  They could make life a lot easier for millions of workers. It's coming alongside support for starting orders inside the app.","time":1525791505,"title":"Twitter is testing an encrypted messaging feature","type":"story","url":"https:\/\/www.engadget.com\/2018\/05\/08\/twitter-encrypted-messaging\/","label":7,"label_name":"random"},{"by":"helmsdeep","descendants":0,"id":17021329,"kids":"None","score":1,"text":"Love cars? Climb in the driver's seat for the latest in reviews, advice and picks by our editors. By Style By Make & Model We cover it all, click your favorite The change comes soon after Uber said it's eliminating its on-demand delivery service called UberRush. Walmart won't be catching rides with Uber anymore. Walmart said Tuesday it's ending its two-year partnership with Uber to deliver groceries to people's homes, following the demise of Uber's on-demand delivery service called UberRush. Walmart will continue to use UberRush in four US markets through June, until the service shuts down. After that, Walmart will transition to another delivery service in those markets, Walmart spokeswoman Molly Blakeman said. The change, first reported by Reuters on Tuesday, comes just a few months after Walmart unveiled a big expansion of its grocery delivery service across the US in a bid to challenge Amazon's push into groceries. Losing the ride-hailing company could be seen as a setback to Walmart's efforts, though the retailer does have a handful of other partners to lean on, including Deliv, Postmates and DoorDash. Reuters also reported that Walmart had ended its partnership with Lyft to deliver groceries too. However, Blakeman said the partnership was a short-lived tie-up in Denver in 2016 and that Walmart hasn't worked with Lyft since then. Amazon has been working to expand in the roughly $800 billion US grocery market, with its $13.2 billion deal to buy Whole Foods in 2017 and growth of its online grocery business. Walmart remains the No. 1 grocery seller in the country, with Amazon currently a distant competitor. Looking to maintain that lead, Walmart in March said it would expand its online grocery delivery business to 100 US metro areas, up from six test markets, by year's end. Walmart is now delivering groceries in nine markets, Blakeman said, with Las Vegas added Tuesday. Walmart plans to operate the service from about 800 stores, using its team of personal shoppers to pick up items and then handing off deliveries to partners like Deliv and DoorDash. Be respectful, keep it clean and stay on topic. We delete comments that violate our policy, which we encourage you to read. Discussion threads can be closed at any time at our discretion.","time":1525791488,"title":"Walmart ends grocery partnership with Uber, Lyft","type":"story","url":"https:\/\/www.cnet.com\/roadshow\/news\/walmart-ends-grocery-partnership-with-uber-lyft\/","label":7,"label_name":"random"},{"by":"brabbit","descendants":0,"id":17021322,"kids":"None","score":4,"text":"Buttons are everywhere. Elevator buttons, machinery buttons, and even \"Nuclear Button\" that sits on the President's office desk. But are you always sure the button you push really performs what you want it to do? <iframe> is a HTML element that lets a web page embed another web page. Here I'm embedding a fake banking website (shamelessly stolen from @lcamtuf with slight modification). The scenario here is after clicking the \"Transfer everything\" button, all your funds will be transferred to me. Of course, clicking it won't actually do anything. But will you click it? Now, I'm presenting you another button. It doesn't have much to say except \"harmless\", and I challenge you to click it. Uh-oh looks like all your moneyz are belong to me. Don't worry, it's just virtual money worth nothing like Bitcoin. Anyway what's happened? CSS has a property called position that allows an element to sit on top of another element. In addition, the property pointer-events allows click events to passthrough an element, so that the click is actually registered on the element underneath. When two combined, I can stack a fake button on top of the actual button and trick you to click it. You can use the \"Behind the scene\" button to have a visual understanding of how it works. This is an often overlooked and misunderstood web application vulnerability called Clickjacking. So <iframe> is dangerous, pointer-events is lethal and we should remove them? Yes and no. Things wouldn't exist if there were no use cases, and same as <iframe>. Online advertising heavily relies on <iframe>. More importantly, web widgets use <iframe> too. Facebook like button and Facebook comments plugin are prime examples of such kind. Here's another harmless button and I challenge you to click it. If you have logged in Facebook, then what you just did was to like my blog post (which I appreciate) without realizing it. This time, opacity is used instead of pointer-events. opacity is a CSS property that controls the opacity of an element. Therefore, I can simply stack the transparent Facebook like button on top of another button visible to you to achieve the same Clickjacking effect, making the remediation of removing pointer-events useless. And by the way, this is a Blackhat SEO technique called Likejacking which exploits Clickjacking to gain organic likes. People have been exploiting Clickjacking with web widgets. Many social media sites were actually vulnerable to this. For example, you could use Clickjacking to gain followers with Twitter follow button, and a more recent issue on LinkedIn AutoFill button that leaks visitor's infomation to third-party websites. Sites have started fixing it by requiring addition user interactions, for example, open a new window for users to confirm an action. In fact, Likejacking probably no longer works nowadays. If you try to click the above Facebook like button, it may turn into a \"Confirm\" button which requires you to click again after you click it. Essentially, they now use an algorithm to determine if an embedding site is trustworthy and hence the number of additional user interactions. Ultimately this is a trade-off between usability and security. Apparently killing <iframe> and CSS properties is not a good idea. What we really need is the ability for websites to forbid other websites from embedding them. Back in the day, people developed a technique called Framebusting which uses JavaScript to check if a website is being embedded by another site. Since window.top always points to the outermost frame, by comparing it to window.self it is possible to determine if the embedding site is not itself. Here was the most commonly used snippet: However, JavaScript based framebusters have many flaws. For example, onBeforeUnload can abort navigation, XSS Filter can disable partial JavaScript execution as mentioned in my previous blog post, and the sandbox attribute of <iframe> can even completely disable JavaScript of the embedded site. The research Busting Frame Busting:\na Study of Clickjacking Vulnerabilities on Popular Sites further shows that JavaScript based framebusters are fragile. Fortunately, browsers started supporting the HTTP response header X-Frame-Options which allows websites to control framing behavior. X-Frame-Options: DENY indicates the site doesn't want anyone to frame it, X-Frame-Options: SAMEORIGIN indicates that only web pages on the same origin can frame it, and X-Frame-Options: ALLOW-FROM indicates only the specified site can frame it. The frame-ancestors directive of Content Security Policy Level 2 later attempts to deprecate X-Frame-Options. However, X-Frame-Options: SAMEORIGIN has a serious flaw. It only compares the embedded site against the top frame.  You may wonder, yeah as long as the top frame is same origin why does it matter? Keep in mind some websites explicitly and implicitly allow custom iframe. In fact, Twitter Player Card was vulnerable to it. When you tweet a link, Twitter will fetch the link and read its meta. If the Open Graph attributes specify a player URL, Twitter will embed the URL in an iframe directly in your timeline. I could embed a sensitive page on Twitter on my \"player\", say OAuth authorization, and mask a fake button on top of the authorize button.  In the actual exploit I used onblur on the main window to detect any click events since clicking on the iframe will trigger a focus event on the iframe, then I delivered a feedback (play video) to make the Clickjacking less obvious. You can learn more vulnerable examples from my presentations. At the time of writing, only Chrome and Firefox have fixed the issue by making the check against all frame ancestors, which is compliant to CSP's frame-ancestors 'self'. I know what you're thinking. What does it have to do with \"Google YOLO\"? Before I tell you, I recommend you to login Google and refresh this page :)I have a surprise if you browse this page with a modern browser in PC :)  YOLO (You Only Login Once) is a web widget for \"One-tap sign-up and auto sign-in on websites\" by Google. You embed Google provided iframe on your websites and your users can authenticate with their Google account in one simple click. Ironically, \"web widgets\" and \"one click\" is indeed YOLO (You Only Live Once). Remember the cookie consent button you clicked at the very beginning? That's right, it was a Clickjacking attempt :). When you try to login with Google YOLO, the following data will be transmitted: Exploiting Clickjacking on Google YOLO allows visitors' name, profile picture and email address to be leaked. That's right, I can even know your email address. :). Click here if you want to see behind the sense (make sure you have logged in Google with a modern browser, PC preferably). I've reported this privacy issue to Google and they refused to fix it as they consider it an \"intended behiavor\". Thanks for your bug report and research to keep our users secure! We've investigated your submission and made the decision not to track it as a security bug. The login widget has to be frameable for it to work.  I'm not sure how we could fix this to prevent this problem, but thanks for the report! This report will unfortunately not be accepted for our VRP. Only first reports of technical security vulnerabilities that substantially affect the confidentiality or integrity of our users' data are in scope, and we feel the issue you mentioned does not meet that bar :( Don't push random buttons Actually don't even click anything. Malicious websites can simply track your cursor's position and change the invisible button\/iframe's position accordingly. So even if you make a click by mistake you will be forced to click on something else. There's no reliable way to prevent Clickjacking, though mitigation can be done on both ends: Last but not least, dare you click this button :)? ???","time":1525791456,"title":"Google YOLO","type":"story","url":"https:\/\/blog.innerht.ml\/google-yolo\/","label":7,"label_name":"random"},{"by":"uptown","descendants":0,"id":17021311,"kids":"None","score":2,"text":"Bradley Jacobs, CEO of XPO Logistics  ","time":1525791420,"title":"How Mapbox Is Winning Over Developers to Challenge Google's Mapping Dominance","type":"story","url":"https:\/\/www.forbes.com\/sites\/bizcarson\/2018\/05\/08\/mapbox-maps-developers\/","label":7,"label_name":"random"},{"by":"ZeljkoS","descendants":0,"id":17021310,"kids":"None","score":1,"text":"                              \r\r     2. Some memeticists have used `infection' as a synonym for `belief' \r(i.e. only believers are infected, non-believers are not). However, this \rusage ignores the fact that people often transmit memes they do not \r\"believe in.\"  Songs, jokes, and fantasies are memes which do not rely on \r\"belief\" as an infection strategy.             \r\r[meme] that their own survival becomes inconsequential in their own minds.\" \r(Henson) (Such as: Kamikazes, Shiite terrorists, Jim Jones followers, any \rmilitary personnel). hosts and membots are not  necessarily memeoids. (See \rauto-toxic; exo-toxic.)               \r\r    2. A class of similar memes. (GMG)                       \r\r     Every scheme includes a vaccime to protect against rival memes. For \rinstance:       \r\r+++** \r\rShare-Right (S), 1990, by Glenn Grant,  PO Box 36 Station H, Montreal, \rQuebec, H3C 2K5. (You may reproduce this material, only if your recipients \rmay also reproduce it, you do not change it, and you include this notice \r[see: threat]. If you publish it, send me a copy, okay?)        Date  1990    \r  \r   Up\r   \r    Prev.  Next \r   Down\r           Memetic Lexicon Expansion,   Comment by Richard PocklingtonMeme Definition Far Too Restrictive,   Comment by Lee BorkmanMemes could be considered capable of reproducing,   Comment by Adewale Oshineye The Good Times Email Virus Meme,   Illustration by Joseph Vaughn-PerlingIs WWW only a vector?,   comment by Carol SteinDiversity as a virtue,  Comment by Don WillardPessimistic view of mimetics,  Comment by Kevin LaPierreFalse Attribution of Meme Definition,  Comment by Aaron LynchMemes As Masters:  An Incendiary Intensification,  Illustration by Atholth OneironautUnderstanding of Memes as a Destructive force,  Comment by David van den BergMemetics is ignoring Issues?,  Comment by Dave UlmerLearning through Memes,  Comment by Shane GreenupThe Biotech Century,  Comment by Josh KaneSuperMem ,  Comment by ToXxa(Sumy)  Add comment...   ","time":1525791412,"title":"Memetic Lexicon","type":"story","url":"http:\/\/pespmc1.vub.ac.be\/MEMLEX.html","label":7,"label_name":"random"},{"by":"deegles","descendants":0,"id":17021303,"kids":"None","score":1,"text":"Member Feature Story Or, \u201cWho Needs an AR-15 Anyway?\u201d Photo by Kyle Glenn on Unsplash Photo by Kyle Glenn on Unsplash As gun policy discussions unfold in the wake of mass shooter incidents, they routinely end in three buckets. There\u2019s the \u201ctyranny can never happen here\u201d bucket, which the left has mostly abdicated in the wake of Trump winning after they called (and still call) him a tyrant. There\u2019s the \u201cyou can\u2019t fight the army with small arms\u201d bucket, which is increasingly unsound given our ongoing decade-and-a-half war with Afghani tribal goat herders. And there\u2019s the \u201cwhat the hell do you need an AR-15 for anyway?\u201d bucket, which, by its very language, eschews a fundamental lack of understanding of what those people are thinking. I am not a prepper. But I know a few. Some of the ones I do know are smart. They may not be doing as deep an analysis as I present here, on a mathematical level, but the smart ones are definitely doing it at a subconscious level. If you want to understand the perspectives of others, as everyone in my opinion should strive to do, then you would do well to read to the end of this article. To get where we\u2019re going, we will need to discuss the general framework of disaster mathematics. I\u2019m not a writer by trade. I\u2019m a stormwater hydrologist, and in my opinion, a pretty good one. Hydrology is the science of tracking water as it moves through the water cycle, from ocean evaporation through cloud formation, precipitation, groundwater infiltration, runoff, evapotranspiration, riverine hydraulics, and the time series behavior of reservoirs. It is a deep and fascinating field, but one of its most relevant applications to our lives is delineating floodplain boundaries. To determine a floodplain boundary, we first identify a \u201cstorm event\u201d that concerns us. We use historical rainfall data and some statistical magic to calculate the worst storm event a place is likely to experience in a 100-year time span, probabilistically speaking, and we call that the \u201c100-year storm.\u201d There\u2019s a push in the field to quit calling it that, because it confuses the muggles, so now we often say something like \u201cthe storm which has a 1% chance of happening in any given year.\u201d Then we take that rainfall data, judiciously apply more math, and turn it into a flow rate in a river. Then we do hydraulics (more math) to determine how deep the river will have to be to carry that much water, and we draw a line on a map. You should have seen this line, if you\u2019ve ever bought a house near a floodplain. If you bought a house near a floodplain and were not shown this line, contact me professionally to ensure you didn\u2019t make a terrible mistake. We don\u2019t buy houses in the floodplain if we can help it, because we are risk averse, even though the chance of it flooding in any given year is only 1%. Why? We will live in the house longer than one year. Over the 30-year life of a mortgage, the chance of the house flooding at least once vastly exceeds 1%, because every year is another roll of the dice. It\u2019s not cumulative, though. The mathematics for back-calculating the odds is called a Bernoulli Process. Here\u2019s what it looks like: Let\u2019s quickly walk through this. The chance of flooding, P(F), is 1%, or 0.01. The chance of not flooding, which we notate P(F\u2019), is 100%-1%, or 99%, or 0.99. To see the chance you don\u2019t flood two years in a row, you would have to \u201cnot-flood\u201d the first year, and then \u201cnot-flood\u201d the second year, so you multiply the two probabilities together, and get 0.9801. The chance of \u201cnot-flooding\u201d 30 years in a row is calculated by multiplying the chance of not flooding with itself, over and over, 30 times, which is a power relationship. P(F\u2019)\u00b3\u2070. That\u2019s 0.7397 chance of 30 consecutive years of no flood, which means a 26% chance of at least one flood. And then your mortgage broker doesn\u2019t give you your thirty-year fixed rate loan, because a 26% chance of a disaster is a big chance, when we\u2019re talking about disasters. Now let\u2019s talk about a bigger, nastier disaster than a flood. There\u2019s a common misconception in the media about the eventuality for which the preppers are exactly prepping. That\u2019s because they\u2019re a diverse group, and prep for many different things. No, they aren\u2019t planning for a revolution to overthrow the government. (Most of them, anyway.) Mostly they\u2019re planning to keep themselves and their families safe while someone else tries to overthrow the government. That, or zombies. (More on zombies below.) While we don\u2019t have any good sources of data on how often zombies take over the world, we definitely have good sources of data on when the group of people on the piece of dirt we currently call the USA attempt to overthrow the ruling government. It\u2019s happened twice since colonization. The first one, the American Revolution, succeeded. The second one, the Civil War, failed. But they are both qualifying events. Now we can do math. (post publication author\u2019s note: This is the \u201cfive minute\u201d version of how to do the math. There are certainly deeply more complicated analyses someone could use to establish the P(R) number, and someone with the resources to do so should absolutely do that. But I don\u2019t find this result unreasonable. 5\/5\/2018) Stepping through this, the average year for colony establishment is 1678, which is 340 years ago. Two qualifying events in 340 years is a 0.5882% annual chance of nationwide violent revolution against the ruling government. Do the same math as we did above with the floodplains, in precisely the same way, and we see a 37% chance that any American of average life expectancy will experience at least one nationwide violent revolution. This is a bigger chance than your floodplain-bound home flooding during your mortgage. It\u2019s noticeably bigger. Following the same procedure, we can see that even over an 18-year span we have a 10% chance of violent revolution, which is an interesting thought experiment to entertain before you have kids. It\u2019s also important to note that a violent nation-state transition doesn\u2019t just affect people who live in a floodplain. It affects everyone stuck in the middle. Especially the poor and defenseless. Am I? Two instances in 340 years is not a great data pool to work with, I will grant, but if you take a grab sample of other countries around the world you\u2019ll see this could be much worse. Since our 1678 benchmark, Russia has had a two world wars, a civil war, a revolution, and at least half a dozen uprisings, depending on how you want to count them. Depending on when you start the clock, France had a 30-year war, a seven-year war, a particularly nasty revolution, a counter-revolution, that Napoleon thing, and a couple of world wars tacked on the end. China, North Korea, Vietnam, and basically most of the Pacific Rim has had some flavor of violent revolution in the last 100 years, sometimes more than one. With Africa, it\u2019s hard to even conceive where to start and end the data points. Most Central and South American countries have had significant qualifying events in the time span. And honestly, if we were to widen our analysis to not only include nationwide violent civil wars, but also instances of slavery, internment, and taking of native lands, our own numbers go way up. Or we could look at a modern snapshot. Counting places like the Vatican, we have 195 countries on the planet today. Somalia is basically in perpetual war, Syria is a hot mess with no signs of mitigation any time soon, Iraq is sketchy, Afghanistan has been in some flavor of civil war or occupation my entire life outside the salad days of the Taliban, and Libya is in such deep throes of anarchy that they\u2019ve reinvented the African slave trade. Venezuela. The Israeli-Palestinian conflict may be a qualifying event depending on how you define it. And again, Africa is\u00a0\u2026 hard to even conceive of where to start. Spitballing, perhaps 3% of the nations in the modern world are in some version of violent revolt against the ruling government, some worse than others. There\u2019s at least some case to be made that our 0.5% annual chance estimate may be low, if we\u2019re looking at comps. Or we could look at a broader historical brush. Since the fall of Constantinople in 1453, there have been 465 sovereign nations which no longer exist, and that doesn\u2019t even count colonies, secessionist states, or annexed countries. Even if we presume that half of these nation-state transitions were peaceful, which is probably a vast over-estimation, that\u2019s still an average of one violent state transition every 2.43 years. If we look at raw dialectic alone, we reach dismal conclusions. \u201cDo you think the United States will exist forever and until the end of time?\u201d Clearly any reasonable answer must be \u201cno.\u201d So at that point, we\u2019re not talking \u201cif,\u201d but \u201cwhen.\u201d If you don\u2019t believe my presumed probability, cook up your own, based on whatever givens and data pool you\u2019d like, and plug it in. The equations are right up there. Steelman my argument in whatever way you like, and the answer will still probably scare you. In 2010, 8.5 million tourists visited Syria, accounting for 14% of their entire GDP. Eight years later, they have almost half a million dead citizens, and ten million more displaced into Europe. They didn\u2019t see this coming, because if they did, they would have fled sooner. Nobody notices the signs of impending doom unless they\u2019re looking carefully. Further, the elites of a nation rarely take it on the chin. They can hop on a plane. The poor, disenfranchised, and defenseless experience the preponderance of the suffering, violence, and death. They\u2019re the ones that should be worried. Pretend you\u2019re someone with your eyes on the horizon. What would you be looking for, exactly? Increasing partisanship. Civil disorder. Coup rhetoric. A widening wealth gap. A further entrenching oligarchy. Dysfunctional governance. The rise of violent extremist ideologies such as Nazism and Communism. Violent street protests. People marching with masks and dressing like the Italian Blackshirts. Attempts at large scale political assassination. Any one of those might not necessarily be the canary in the coal mine, but all of them in aggregate might be alarming to someone with their eyes on the horizon. Someone with disproportionate faith in the state is naturally inclined to disregard these sorts of events as a cognitive bias, while someone with little faith in the state might take these signs to mean they should buy a few more boxes of ammunition. \u201cBut if one of these things happens, you\u2019re screwed anyway!\u201d Well, sure. The point of disaster planning for a hurricane, tornado, earthquake, or wildfire, is not to be \u201cnot-screwed.\u201d It\u2019s to be notably less screwed. Ready.gov is the central point for information about family disaster preparedness planning here in the US. They list a wide range of things they think you might want to prepare for. Chief among these is flooding, which is my field, but they also list many other things an alarmist might include in their family disaster preparedness plan, from volcanoes and tsunamis to space debris, nukes, and terrorist dirty bombs. Violent nation-state transition doesn\u2019t make the list, though, because the list was compiled by the government. But the best one to prepare for, in my opinion, is zombies. The zombie apocalypse is obviously pure fiction, but it has an allure to a few tongue-in-cheek preppers because of its functional completeness. If you are prepared for zombies, you are literally prepared for anything. The key fixture of zombie preparedness is a fundamental understanding of what happens when our systems of economics, governance, and civil infrastructure fail. There\u2019s a great one going on right now in Venezuela, with people eating rats and dogs, incapable of trading in the local currency, and a general humanitarian disaster associated with descent into anarchy. No class of person is more capable of riding out a situation like that than a well-provisioned zombie prepper. Various fixtures of zombie prepping include: For the ethical zombie prepper, firearms are a relatively small piece of this overall disaster plan, but a necessary one. For an unethical zombie prepper, firearms may be all they need, if they can find someone else from whom to steal. The Bosnian War is a great test case for this, and many firsthand experiences have been chronicled since, about how prepper-minded people were the likeliest to survive. And it\u2019s not just tin foil hat equipped right wingers thinking about this stuff. There\u2019s a widely reported trend of Silicon Valley billionaires building apocalypse bunkers, as many as 50% according to Steve Huffman, the guy who founded Reddit. Yishan Wong, another former Reddit CEO, goes through a conceptual ROI analysis with The New Yorker. And it\u2019s not just the techies. A lot of folks in Hollywood are thinking the same thing. It\u2019s big business out there. So that\u2019s another canary in the coal mine for the tin foil hat right winger\u200a\u2014\u200ayou have a class of people who are vehemently demanding confiscation of rifles in the public sphere, while some of them are secretly building underground fortresses in the private sphere. Buy another box of ammo. Clearly. When our semiannual mass shooter culture war erupts after the latest round of media Handwaving Freakoutery, it always seems to focus on rifles. Most recently, we had some guy cut his own AR-15 in half on YouTube, to thunderous applause. Don\u2019t mischaracterize my position. If Mr. Pappalardo thought that he might be prone to murdering someone with his rifle, or more statistically likely\u200a\u2014\u200apurposely killing himself with it, then he should absolutely sell or destroy it. But if he isn\u2019t going to do either of those things, all he must do to ensure it doesn\u2019t hurt anyone is not shoot anyone with it. He could leave it in his attic with a couple of cans of ammunition, just in case something horrible does transpire where he might actually need it. There are certain things in the world you\u2019d rather have and not need, than need and not have. And paramount among those things, given the state of the modern human condition, is a rifle. So if you ask someone else on the opposite side of a culture war argument, \u201cWhy would you want to own one of those things, anyway?\u201d please don\u2019t be surprised if they simply respond, \u201cWhy wouldn\u2019t you?\u201d Conscientious objector to the culture war. I think a lot. Welcome to a place where words matter. On Medium, smart voices and original ideas take center stage \u2014 with no ads in sight. Watch Follow all the topics you care about, and we\u2019ll deliver the best stories for you to your homepage and inbox. Explore Get unlimited access to the best stories on Medium \u2014 and support writers while you\u2019re at it. Just $5\/month. Upgrade","time":1525791341,"title":"The Surprisingly Solid Mathematical Case of the Tin Foil Hat Gun Prepper","type":"story","url":"https:\/\/medium.com\/s\/story\/the-surprisingly-solid-mathematical-case-of-the-tin-foil-hat-gun-prepper-15fce7d10437","label":10,"label_name":"thought"},{"by":"ssorallen","descendants":0,"id":17021299,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back An in-browser stock portfolio manager that you can deploy anywhere. You can use the version\ndeployed at https:\/\/ssorallen.github.io\/finance\/ or deploy your own. If you have a spreadsheet (.csv) file containting transactions in your portfolio, you can import the\nentire file at once in Finance!. Your portfolio CSV file is expected to have the following columns: the values should be of the following types: Other formats are currently not supported, and Finance! might fall apart if you try to use another\nformat. If you exported your Google Finance portfolio to a spreadsheet (.csv) file before the site shut\ndown, you can easily import that file into Finance! The data format is fully supported, so jump up\nto Importing Your Portfolio and follow the steps. Finance! doesn't collect or store any of your data. Actually, Finance! has no server of any kind,\nthe entire app runs inside your browser. Transaction data and which symbols you're watching is stored entirely in your browser (using\nlocal storage). Transaction data is never sent anywhere; it never leaves your computer. In order to see the value of your portfolio, a list of the symbols in your portfolio is sent to\nIEX to get the latest quotes for each symbol. None of your transaction data is sent to IEX or\nto anyone else, only a list of the symbols leaves your computer. For example, if your portfolio looked like the following: the following data would be sent to IEX: That's it. There's no tracking, no advertisements, and no other API requests. All stock data is provided for free by IEX. Use is subject to IEX Exhibit A. The IEX API currently does not provide market index data. For more information, check out\nIEX-API Issue #36. The best replacement is ETFs that track associated indices like SPY for the S&P500 and DIA for the\nDow Jones Industrial Average (DJIA). The IEX API currently does not provide mutual fund data. Please watch IEX-API Issue #16 for\nupdates on when that data might become available. Clone this repository Use Yarn to install dependencies (Optional) To develop and make changes, use start Build the app for deployment All the code necessary to run Finance! is in this repository. There's no database required, no\nother application server. Once you build the app, you can copy the static files to\nany webserver you like. If you want to do this all on GitHub and deploy with GitHub Pages, follow these steps: Fork this repo with the \"Fork\" button in the upper right Clone your new repo to your computer Change the \"homepage\" in package.json to your site's new home,\nhttps:\/\/YOURNAME.github.io\/finance\/ Build the app for release using the How to Build instructions Using the gh-pages package that's already installed, deploy to your very own page \u2728\u2728 Your version of Finance! will be available at https:\/\/YOURNAME.github.io\/finance\/ \u2728\u2728 As an unsophisticated investor, I liked Google Finance and missed the simple interface for managing\na stock portfolio. Alternatives offer more than I need, so I built only the functionality I wanted. The idea to run this all in a browser was sparked by seeing Todd Schneider's\ntoddwschneider\/stocks project.","time":1525791318,"title":"Show HN: A free in-browser stock tracker, use your old Google Finance CSV","type":"story","url":"https:\/\/github.com\/ssorallen\/finance","label":4,"label_name":"github"},{"by":"Dangeranger","descendants":0,"id":17021283,"kids":"None","score":2,"text":"It\u2019s no secret Tesla is struggling, thanks to a prominent death that potentially implicated the company\u2019s Autopilot technology and problems ramping up Model 3 production. But Elon Musk\u2019s behavior of late has been downright bizarre. During a conference call last week, Musk insulted analysts and investors before spending ~25 minutes talking to one young retail investor on YouTube. Now, he\u2019s apparently declared that all Tesla contractors are fired, effective immediately \u2014 unless employees are willing to put their jobs on the line to defend specific individuals and why those individuals should be retained by Tesla. Musk has made his feelings about contractors extremely clear. During last week\u2019s conference call, he stated: \u201c[T]he number of sort of third-party contracting companies that we\u2019re using has really gotten out of control, so we\u2019re going to scrub the barnacles on that front. It\u2019s pretty crazy. We\u2019ve got barnacles on barnacles. So there\u2019s going to be a lot of barnacle removal.\u201d An email leaked earlier, on April 17, offered some concrete detail about Musk\u2019s thinking on contractors. In that email, Musk wrote: I have been disappointed to discover how many contractor companies are interwoven throughout Tesla. Often, it is like a Russian nesting doll of contractor, subcontractor, sub-subcontractor, etc. before you finally find someone doing actual work. This means a lot of middle-managers adding cost but not doing anything obviously useful. Also, many contracts are essentially open time & materials, not fixed price and duration, which creates an incentive to turn molehills into mountains, as they never want to end the money train. There is a very wide range of contractor performance, from excellent to worse than a drunken sloth. All contracting companies should consider the coming week to be a final opportunity to demonstrate excellence. Any that fail to meet the Tesla standard of excellence will have their contracts ended on Monday. Fair enough. Maybe Tesla really does have a problem with contractors and their work output isn\u2019t what it needs to be. But in a follow-up email, released yesterday, Musk pulled a move that can only be described as bizarre. In this latest missive, Musk states: I extended the performance evaluation deadline to provide more opportunity to demonstrate excellence, but now time is up. Please send a note to HR before Monday justifying the excellence, necessity and trustworthiness by individual (not just the contractor company as a whole) of every non-Tesla person who has badge access to our buildings or network access to our systems. By default, anyone who does not have a Tesla employee putting their reputation on the line for them will be denied access to our facilities and networks on Monday morning. This applies worldwide. (Emphasis added) Consider the implications of this for a moment. First, summarily locking the doors on contractors provides no wind-down period for any you don\u2019t want to keep to finish projects or assignments. Sudden staff departures create communication snarls and slow production as existing employees pick up the work. Second, it\u2019s not the job of most employees to keep track of the productivity or added value of other employees. Yes, in any given office there will be people who obviously are dropping the ball and people who are truly excellent, high-value employees \u2014 but statistically, most of us fall into a middle ground. Tesla\u2019s stock price dropped to its 52-week low last week thanks to Musk\u2019s behavior, but has mostly recovered. It remains well off the 52-week high it set last fall and slightly below the same price one year ago. Asking employees to put their reputations on the line over whether to keep a contractor is equivalent to asking them to put their jobs on the line to keep a specific individual employed, especially when Tesla has launched layoffs in the past year and when Musk has declared himself extremely unhappy with the contractor situation. Would you want to take a position that contradicted that of the CEO, even to save the job of someone who is important to the company? Maybe. But that\u2019s an extremely high bar to clear, particularly given that ordinary employees are explicitly not the people who are supposed to be making that kind of decision. Ask yourself if you\u2019d trust your boss to risk his own career to save yours if the CEO of your company had proposed eliminating your entire department, and you\u2019ve got a pretty good idea why this isn\u2019t the best way to decide which contractors to keep on staff. Most people wouldn\u2019t take that chance, and the handful of people who would might not be in a position to evaluate the value of specific individuals, either, outside the handful they personally know. Musk is assuming that every employee who knows of a critically important contractor has the temperament to stick their neck out for said contractor in a way that could be read as contrary to the CEO\u2019s direct desires. That\u2019s not a common characteristic or a reasonable ask and it could cause future delays to the Model 3\u2019s already-delayed production. Again, the question here isn\u2019t whether Tesla had too many contractors, or if those contractors contributed useful work. Elon Musk is in a far better position to evaluate those questions than I am. The question is whether asking employees to personally vouch for the value of contractors represents an appropriate or acceptable method of ensuring that the right people are kept at the company, given that Musk has publicly stated that he wants contract workers out of Tesla. There\u2019s a right way and a wrong way to deal with many problems, and it\u2019s hard to see how this represents the right one. Musk could sabotage morale and cause himself further problems in the name of resolving them. Subscribe Today to get the latest ExtremeTech news delivered right to your inbox. \r\n            \u00c2\u00a9 1996-2018            Ziff Davis, LLC. PCMag Digital GroupExtremeTech is among the federally registered trademarks of Ziff Davis, LLC and may not be used by third parties without explicit permission. We have updated our PRIVACY POLICY and encourage you to read it by clicking here.","time":1525791186,"title":"Elon Musk Fires All Tesla Contractors, Unless Employees Vouch for Them","type":"story","url":"https:\/\/www.extremetech.com\/extreme\/268948-elon-musk-fires-all-tesla-contractors-unless-employees-personally-vouch-for-them","label":7,"label_name":"random"},{"by":"timvdalen","descendants":0,"id":17021280,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Easy guided tours for Angular apps.  Import TelemachyModule in your root module (make sure the RouterModule is loaded, Telemachy uses it to automatically start the tour for any compontent that a user navigates to): Finally, add the <telemachy-tour> component to your root app component (or one that is always loaded when you want to display a tour): In its easiest form, you can use Telemachy just by implementing the HasGuidedTour interface: By implementing the optional tourAutoStart() method (and having it return true), Telemachy will automatically start this component's tour whenever it becomes active through the angular router.\nYou can also explicitly say it should be started by calling TelemachyService.startTour(this).\nIn both cases, Telemachy will only show the tour if the persistency layer says the user should see it. By default, Telemachy will always show a user the tour on each visit.\nIf you want to save if a user has already seen a tour, you should provide an alternate implementation for the TourPersistency service. Telemachy comes with an alternate implementation that uses localstorage to save this state.\nThis implementation depends on phenomnomnominal\/angular-2-local-storage.\nYou can use this as follows: We recommend you replace the TourPersistency service with an implementation that saves the user's state in a remote system, for instance through a REST API. You can restart the tour for a currently visible component by calling TelemachyService.restartTour().\nTo check if there is a currently visible component that has a tour, use TelemachyService.canRestart(). If async is true, the ElementTourStep will attempt to find the element on the page after it has been initialized.\nIf the element isn't available by the time the step should be rendered, the step will be skipped.","time":1525791141,"title":"Show HN: Telemachy \u2013 Plug and play guided tours for Angular","type":"story","url":"https:\/\/github.com\/code-orange\/telemachy","label":4,"label_name":"github"},{"by":"sjroot","descendants":1,"id":17021277,"kids":"[17021291]","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back This README is being written.\n 2 May 2018 18 April 2018 18 March 2018 17 February 2018 27 November 2016 2 November 2016 31 October 2016 24 October 2016 22 October 2016 21 October 2016 18 June 2016 17 June 2016 5 June 2016 Old announcements can be found in the ANNOUNCE.md file. Note that today's entry (Eastern Time) may be updated later today. 18 April 2018 17 June 2016 16 June 2016 15 June 2016 14 June 2016 13 June 2016 12 June 2016 8 June 2016 6 June 2016 5 June 2016 Old updates can be found in the Changelog.md file. Out-of-tree builds typical of cmake are preferred: Pass -DBUILD_SHARED_LIBS=OFF to cmake to build a static library. The standard cmake build configurations are provided; if none is specified, Debug is used. If you use a makefile generator with cmake, then and pass VERBOSE=1 to see build commands. Build targets will be in the build\/out folder. Project file generators should work, but are untested by me. On Windows, I use the Unix Makefiles generator and GNU make (built using the build_w32.bat script included in the source and run in the Visual Studio command line). In this state, if MinGW-w64 (either 32-bit or 64-bit) is not in your %PATH%, cmake will use MSVC by default; otherwise, cmake will use with whatever MinGW-w64 is in your path. set PATH=%PATH%;c:\\msys2\\mingw(32\/64)\\bin should be enough to temporarily change to a MinGW-w64 build for the current command line session only if you installed MinGW-w64 through MSYS2; no need to change global environment variables constantly. Can be built from AUR: https:\/\/aur.archlinux.org\/packages\/libui-git\/ Needs to be written. Consult ui.h and the examples for details for now. libui was originally written as part of my package ui for Go. Now that libui is separate, package ui has become a binding to libui. As such, package ui is the only official binding. Other people have made bindings to other languages: OS X normally does not start program executables directly; instead, it uses Launch Services to coordinate the launching of the program between the various parts of the system and the loading of info from an .app bundle. One of these coordination tasks is responsible for bringing a newly launched app into the foreground. This is called \"activation\". When you run a binary directly from the Terminal, however, you are running it directly, not through Launch Services. Therefore, the program starts in the background, because no one told it to activate! Now, it turns out there is an API that we can use to force our app to be activated. But if we use it, then we'd be trampling over Launch Services, which already knows whether it should activate or not. Therefore, libui does not step over Launch Services, at the cost of requiring an extra user step if running directly from the command line. See also this and this. See CONTRIBUTING.md. From examples\/controlgallery:   ","time":1525791138,"title":"Libui: portable GUI library in C with many other language bindings","type":"story","url":"https:\/\/github.com\/andlabs\/libui","label":4,"label_name":"github"},{"by":"tigranhakobian","descendants":1,"id":17021271,"kids":"[17021399]","score":1,"text":"Lots of people still seem to confuse the terms UX and UI, although there is too much information on the internet and everybody is talking about those terms. UX stands for user experience and UI meaning user interface. It is important to know that UX and UI are crucial for a product and always work together. In this article, we will talk more about these two terms and highlight the difference between UI and UX design. User experience design is the process of building a relationship between the company, its products, and their customers. UX design encloses user\u2019s entire journey with the product. It aims at creating a product that will not only be useful and valuable but also easily accessible, pleasant and most importantly, desirable. UX design creates a product that will provide a valid and significant user experience. It has different disciplines, including visual design, interaction design, usability and more. While doing UX design, designers concentrate on the WHY, WHAT, HOW of the product use. Designers start with the WHY, to understand the motivations people would have, to use the product. Then they work on the WHAT, as if the functionality and features, what they need to provide to make the product useful. After this comes the HOW, which is the most significant part. This part is the design of the functionality which needs to be accessible and satisfying, in order for users to form meaningful experiences. User interface design, mostly used as UI design, is basically the design of the human-computer interface. For example, it is the interface in a mobile game that allows you to interact with players, characters and game\u2019s other objects. UI design refers to the visual part of the product or experience. It is the look of your product and the way it is presented. For example, UI is the design of the buttons, icons you use to interact with a certain product, to create a pleasing user experience. Users need efficiency in achieving their goals while using your product. But it is not only about being simple. You need to combine the beauty and easy navigation. Users will be amazed at adapting to the product easily, but if the path itself is beautiful and enjoyable, it will double the happiness, for sure. It can be considered that UX design is done first, then the UI design happens. Designers work on the features, that will be preferred to be used by users and then they do the design of the \u2018look\u2019 which will be eye-catching and enjoyable. UX makes the interface functional, while UI makes it beautiful. UX design focuses on providing a product that will help users achieve their goals, and UI helps the experience be joyful. For example, let\u2019s take an online bookstore. If you go on the website, see everything in a clear way, calm colors, nice interface design and you can find the book you want easily, this means that UI design has been done well. But what if the store only sells books of English authors or based on some other factors, such as editions, origin and more? In this case, the UX will be poor for a book lover of Chinese literature. UX is the overall experience user has with the product, and UI is the elements user sees and interacts with. If something is very beautiful but is hard to use, this means you have good UI and poor UX design, and vice versa. If something is very usable but looks horrible indicates the presence of good UX but poor UI. However, there are people who believe there is no difference between UI and UX design at all. Craig Morrison, the Head of Product at RecordSetter and the Founder of Usability Hour, has been asked the question on the difference too many times, and he has come to a conclusion and sets the definition the following way: UX design is the validity and significance of your product, whereas, UI design focuses on the easy use and pleasurability. UI and UX are inseparable. A well designed UI can create an ideal user experience that designer has foreseen and the user is happy about. At last, ending this article with one of the greatest quotes on design: Curious to know what users do in your app? Try Inapptics for free and visually see every action users do in your app, e.g. where they tap, how they navigate, what they did that caused the crash you can\u2019t reproduce and more. If you enjoyed this article, feel free to hit that clap button \ud83d\udc4f to help others find it. Follow Inapptics: Medium | Twitter | Facebook Originally published at blog.inapptics.com on May 8, 2018. Read Next: By clapping more or less, you can signal to us which stories really stand out. Inapptics helps app makers analyze user behavior in their mobile apps. Create your free account at http:\/\/inapptics.com\/. Daily design news, inspiration and deals: Everything you need to supercharge your design skills. (https:\/\/prototypr.io)","time":1525791097,"title":"The difference between UI and UX design","type":"story","url":"https:\/\/blog.prototypr.io\/the-difference-between-ui-and-ux-design-5fa7e23e3c83","label":10,"label_name":"thought"},{"by":"micrum","descendants":4,"id":17021264,"kids":"[17022677, 17021397, 17022449, 17021671]","score":17,"text":"We are pleased to announce the launch of Fetch  - a Natural Language Search Interface for enterprise data. Fetch enables non-technical team members to access data quickly and easily, using their own language, without the need for expensive developers or database professionals. Fetch allows you to ask questions about organization\u2019s data in plain English and returns result in a suitable format back to you (charts, tables or direct answer). FriendlyData has addressed a huge need in the world of enterprise data by helping non-technical people access critical data in a secure, intuitive way,\u201d said Alex Nenadavets, Lead Strategic Intelligence Analyst at Wargaming.net. \u201cIndividuals and teams that rely on timely, trustworthy data now have an amazing new tool at their disposal.\" The following features of Fetch ensure intuitive and efficient access to enterprise data: At FriendlyData we have reimagined the world of enterprise data, and we are working hard to fundamentally change the way people interact with their internal data. So FETCH makes data accessible to all, saving organizations time and money and making their teams smarter by bringing data to them in a secure, easy to use way. Decisions need data - provide your company\u2019s decision makers with the data they need to drive the business forward! Be the first to try Fetch!","time":1525790998,"title":"FriendlyData Announces FETCH, a Natural Language Interface for Enterprise Data","type":"story","url":"https:\/\/friendlydata.io\/blog\/fetch-announcement","label":7,"label_name":"random"},{"by":"dsr12","descendants":0,"id":17021263,"kids":"None","score":2,"text":"It\u2019s the biggest health crisis you\u2019ve never heard of. Doctors, philanthropists and companies are trying to solve it.  While not seen as urgent as other world health problems, untreated vision problems cost the global economy $200 billion annually to lost productivity, according to the W.H.O.CreditAtul Loke for The New York Times Supported by By Andrew Jacobs PANIPAT, India \u2014 Shivam Kumar\u2019s failing eyesight was manageable at first. To better see the chalkboard, the 12-year-old moved to the front of the classroom, but in time, the indignities piled up. Increasingly blurry vision forced him to give up flying kites and then cricket, after he was repeatedly whacked by balls he could no longer see. The constant squinting gave him headaches, and he came to dread walking home from school. \u201cSometimes I don\u2019t see a motorbike until it\u2019s almost in my face,\u201d he said. As his grades flagged, so did his dreams of becoming a pilot. \u201cYou can\u2019t fly a plane if you\u2019re blind,\u201d he noted glumly. The fix for Shivam\u2019s declining vision, it turns out, was remarkably simple. He needed glasses. More than a billion people around the world need eyeglasses but don\u2019t have them, researchers say, an affliction long overlooked on lists of public health priorities. Some estimates put that figure closer to 2.5 billion people. They include thousands of nearsighted Nigerian truck drivers who strain to see pedestrians darting across the road and middle-aged coffee farmers in Bolivia whose inability to see objects up close makes it hard to spot ripe beans for harvest. Then there are the tens of millions of children like Shivam across the world whose families cannot afford an eye exam or the prescription eyeglasses that would help them excel in school. \u201cMany of these kids are classified as poor learners or just dumb and therefore don\u2019t progress at school,\u201d said Kovin Naidoo, global director of Our Children\u2019s Vision, an organization that provides free or inexpensive eyeglasses across Africa. \u201cThat just adds another hurdle to countries struggling to break the cycle of poverty.\u201d In an era when millions of people still perish from preventable or treatable illness, many major donors devote their largess to combating killers like AIDS, malaria and tuberculosis. In 2015, only $37 million was spent on delivering eyeglasses to people in the developing world, less than one percent of resources devoted to global health issues, according to EYElliance, a nonprofit group trying to raise money and bring attention to the problem of uncorrected vision. So far, the group\u2019s own fund-raising has yielded only a few million dollars, according to its organizers. It has enlisted Ellen Johnson Sirleaf, the former Liberian president, Elaine L. Chao, the transportation secretary for the United States and Paul Polman, the chief executive of Unilever, among others, in an attempt to catapult the issue onto global development wish lists. They contend that an investment in improving sight would pay off. The World Health Organization has estimated the problem costs the global economy more than $200 billion annually in lost productivity. \u201cLack of access to eye care prevents billions of people around the world from achieving their potential, and is a major barrier to economic and human progress,\u201d said Madeleine K. Albright, the former secretary of state who is also involved in the group. Hubert Sagnieres, the chief executive of Essilor, a French eyeglass company and a partner in the fund-raising campaign, said he often confronts ambivalence when pitching the cause to big-name philanthropists. In an interview, he recalled a recent conversation with Bill Gates, whose foundation has spent tens of billions of dollars battling infectious diseases in the developing world. He said he reminded Mr. Gates of his own childhood nearsightedness, noting that without glasses, he might have faltered in school and perhaps never gone on to start Microsoft. Mr. Gates, he said, politely demurred, saying he had other priorities. A spokeswoman for the Gates Foundation declined to comment. The initiative\u2019s backers point out that responding to the world\u2019s vision crisis does not require the invention of new drugs or solving nettlesome issues like distributing refrigerated vaccines in countries with poor infrastructure. Factories in Thailand, China and the Philippines can manufacture so-called readers for less than 50 cents a pair; prescription glasses that correct nearsightedness can be produced for $1.50.  But money alone won\u2019t easily solve systemic challenges faced by countries like Uganda, which has just 45 eye doctors for a nation of 41 million. In rural India, glasses are seen as a sign of infirmity, and in many places, a hindrance for young women seeking to get married. Until last year, Liberia did not have a single eye clinic. \u201cPeople in rural areas have never even seen a child wearing glasses,\u201d said Ms. Sirleaf, who was president of Liberia from 2006 to this year. \u201cDrivers don\u2019t even know they have a deficiency. They just drive the best they can.\u201d On a recent afternoon, hundreds of children in powder-blue uniforms giddily jostled one another in the dusty courtyard of a high school in Panipat, two hours north of New Delhi. The students, all from poor families, were having their eyesight checked by VisionSpring, a nonprofit group started by Jordan Kassalow, a New York optometrist who helped set up EYElliance, that works with local governments to distribute subsidized eyeglasses in Asia and Africa. For most, it was the first time anyone had checked their eyesight. The students were both excited and terrified. Roughly 12 percent were flagged as having weak vision and sent to an adjacent classroom where workers using refractor lenses conducted more tests. Shivam, the boy who dreamed of being a pilot, walked away with a pair of purple-framed spectacles donated by Warby Parker, the American eyewear company, which also paid for the screenings. \u201cEverything is so clear,\u201d Shivam exclaimed as he looked with wonder around the classroom. Anshu Taneja, VisonSpring\u2019s India director, said that providing that first pair of glasses is pivotal; people who have experienced the benefits of corrected vision will often buy a second pair if their prescription changes or they lose the glasses they have come to depend on. Ratan Singh, 45, a sharecropper who recently got his first pair of reading glasses, said he could not imagine living without them now. Standing in a field of ripening wheat, he said his inability to see tiny pests on the stalks of his crop had led to decreasing yields. He sheepishly recalled the time he sprayed the wrong insecticide because he couldn\u2019t read the label. \u201cI was always asking other people to help me read but I was becoming a burden,\u201d he said. Last month, after he accidentally broke his glasses, Mr. Singh, who supports his wife and six daughters, did not hesitate to fork out the 60 rupees, roughly 90 cents, for a new pair. Most adults over 50 need reading glasses \u2014 more than a billion people in the developing world, according to the International Agency for the Prevention of Blindness \u2014 though the vast majority simply accept their creeping disability. That\u2019s what happened to D. Periyanayakam, 56, a power company employee whose job requires him to read electrical meters. His failing eyesight also made it hard to drive or respond to text messages from customers and co-workers. \u201cI figured it was a only matter of time before they suspended me,\u201d he said during a visit to a mobile eye clinic run by Aravind Eye Hospital, a nonprofit institution that screened his vision and told him he would soon need cataract surgery. Mr. Periyanayakam returned to work that day with a $2 pair of glasses. He was among 400 people who showed up at a daylong clinic in a high school run by ophthalmologists, lens grinders and vision screeners. Aravind dispenses 600,000 pairs of glasses each year in India and has expanded its efforts to Nepal, Bangladesh and countries in Africa through local partners. The hospital trains its own vision screeners, most of them young women; a separate program trains primary schoolteachers to test their students\u2019 sight using eye charts. Then there is the matter of road safety. Surveys show that a worrisome number of drivers on the road in developing countries have uncorrected vision. Traffic fatality rates are far higher in low-income countries; in Africa, for example, the rate is nearly triple that of Europe, according to the W.H.O. Experts say a significant number of India\u2019s roughly 200,000 traffic deaths each year are tied to poor vision. In a country with a huge number of drivers, among them nine million truckers, the government agencies that administer licenses are ill-equipped to deal with the problem of declining vision, critics say. Sightsavers, a British nonprofit that has been treating cataract-related blindness in India since the 1960s, has spent the past two years trying to get glasses to commercial drivers. It operates mobile eye-screening camps at truck stops and tollbooths in 16 cities. A driver who has his eyes examined at a clinic in north India can pick up his glasses 10 days later at a clinic in the far south. \u201cThese men are always on the move and they are pressed for time, so we try to make it as easy as possible for them,\u201d said Ameen, a Sightsaver employee who uses a single name. On a recent morning, dozens of drivers, many wearing flip-flops and oil-stained trousers, lined up in front of an eye chart taped to the wall of a trucking company in the town of Chapraula. Asked why they had waited so long to have their vision checked, some shrugged. Others said they were too busy. A few cited fears they would be fired if an employer discovered that their vision was flawed. About half the men, it turned out, needed glasses. They included Jagdish Prasad, 55, a father of nine with a deeply lined face who had never had his eyes tested. \u201cI haven\u2019t had an accident in 35 years,\u201d Mr. Prasad exclaimed \u2014 but then reluctantly admitted that he has lately been squinting to see whether a traffic light had changed. Then he gestured to the cavalcade of honking vehicles behind him and told a story. Four days earlier, he said, a mentally ill man had been lying on the edge of the road, forcing drivers to swerve to avoid him. One of those vehicles, a truck not unlike his own, tried to avoid the man but ended up killing two students who were crossing the road on their way to school. The next day, the mentally ill man was also struck and killed, Mr. Prasad said. He paused and then considered the piece of paper in his hand. It contained the prescription for his first pair of glasses. Mr. Prasad hesitated and then gently placed it in his pocket. Andrew Jacobs is a reporter with the Health and Science Desk, based in New York. He previously reported from Beijing and Brazil and had stints as a Metro reporter, Styles writer and National correspondent, covering the American South.@AndrewJacobsNYT Advertisement    Collapse SEE MY OPTIONS","time":1525790997,"title":"A Simple Way to Improve a Billion Lives: Eyeglasses","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/05\/health\/glasses-developing-world-global-health.html","label":7,"label_name":"random"},{"by":"Mougatine","descendants":0,"id":17021260,"kids":"None","score":1,"text":"This article contains note of the research paper: This paper was awarded the CVPR 2017 Best Paper Award. DenseNet is a new CNN architecture that reached State-Of-The-Art (SOTA) results\non classification datasets (CIFAR, SVHN, ImageNet) using few parameters. Thanks to its new use of residual it can be very deep and still be easy to\noptimize. DenseNet is composed of several Dense blocks. In those blocks, the layers are\ndensely connected together: Each layer receive in input all previous layers\noutput feature maps.  This extreme use of residual creates a deep supervision because each layer\nreceive more supervision from the loss function thanks to the shorter connections. A dense block is serie of layers connected to all their previous layers. A single\nlayer looks like this: The authors found that the pre-activation mode (BN and ReLU before the Conv) was\nmore efficient than the usual post-activation mode. Note that the authors recommend a zero padding before the convolution in order\nto have a fixed size. Instead of summing the residual like in ResNet,\nDenseNet concatenates all the feature maps. It would be impracticable to concatenate feature maps of different sizes (although\nsome resizing may work). Thus in each dense block, the feature maps of\neach layer has the same size. However down-sampling is essential to CNN. Transition layers between two\ndense blocks assure this role. A transition layer is made of: Concatenating residuals instead of summing them has a downside when the model\nis very deep: It generates a lot of input channels! You may now wonder how could I say in the introduction that DenseNet has few\nparameters. There are two reasons: First of all a DenseNet\u2019s convolution generates a low number of feature maps.\nThe authors recommend 32 for optimal performance but shows SOTA results with as\nfew as 12 output channels! The number of output feature maps of a layer is defined as the growth\nrate. DenseNet has lower need of of wide layers because as layers are densely connected\nthere is few redundancy in the learned features. All layers of a same dense block\nshare a collective knowledge. The growth rate regulates how much new information each layer contributes\nto the global state. The second reason DenseNet has few parameters despite concatenating many residuals\ntogether is that each 3x3 convolution can be upgraded with a bottleneck. A layer of a dense block with a bottleneck will be: 1x1 Convolution bottleneck producing: \\text{grow rate} * 4 feature maps. With a growth rate of 32, the tenth layer would have in input 288 feature maps!\nThanks to the bottleneck at most 128 feature maps would be fed to a layer. This\nhelps the network have hundred, if not thousand, layers. The authors further improves the compactness of the model with a compression.\nThis compression happens in the transition layer. Normally the transition layer\u2019s convolution does not change the number of feature\nmaps. In the case of the compression, its number of output feature maps is\n\\theta * m. With m the number of input feature maps and \\theta a\ncompression factor between 0 and 1. Note that the compression factor \\theta has the same role as the parameter\n\\alpha in MobileNet. The final architecture of DenseNet is the following:  To summarize, the DenseNet architecture uses the residual mecanism to its maximum\nby making every layer (of a same dense block) connect to their subsequent\nlayers. This model\u2019s compactness makes the learned features non-redundant as they are all\nshared throught a common knowledge. It is also far more easy to train deep network with the dense connections because\nof an implicit deep supervision where the gradient is flowing back more easily\nthanks to the short connections. CS student @ EPITA, Teacher assistant in C\/C++, Java, SQL, Algo, etc.,\nDeep Learning Intern @ Dataiku, Machine Learning enthusiast,\ncurious by nature. A blog about Machine Learning, Computer Science, and everything else.","time":1525790967,"title":"An Explanation of Densely Connected Convolutional Networks","type":"story","url":"https:\/\/arthurdouillard.com\/2018\/05\/07\/densenet\/","label":5,"label_name":"ml"},{"by":"wickedmanok","descendants":0,"id":17021257,"kids":"None","score":2,"text":"By Ardy Dedase Software engineers at Skyscanner have access to internally developed tools, services and integrated Open Source platforms that help accelerate our speed of iteration on a day-to-day basis. These tools enable our engineering teams to avoid churning hours away on boiler-plate code and instead focus their efforts on building product and features that improve the lives of our users and travelers. Skyscanner has evolved towards this position as a result of the company\u2019s organic growth, i.e. we are currently a 900+ people organisation spread across 10 global locations that has quadrupled in size over the past 4 years (numbers are correct at the time of writing). It\u2019s very easy to imagine that aligning an entire company\u2019s approach to engineering is exceptionally difficult given such a continuously changing landscape. Small decisions\u200a\u2014\u200alike choosing a language or framework for a service, or how we test and deploy our services, or even just how we log and monitor\u200a\u2014\u200aall become significant organisational challenges when 60+ engineering teams start making decisions in isolation of one another. From the people perspective alone, our portability as engineers in such an eco-system starts to also depend heavily on the projects and technologies we may have used in previous teams, projects or services\u200a\u2014\u200aand this is not a desirable situation to be in as an engineer looking to grow and evolve with the organisation. So how have we avoided those issues? We are lucky to have a dedicated tribe of engineers collectively referred to as our \u2018Developer Enablement\u2019 tribe (organised into multiple squads) who are responsible for building and maintaning the tools and services that help accelerate the speed of iteration of every Skyscanner engineer on a day-to-day basis. This article will highlight just three of the essential Developer Enablement tools that our engineers use on a regular basis, these are the Microservices Shell (MShell), Drone and Slingshot. The Microservices Shell and Slingshot are both in-house developments while Drone is Open Source. I\u2019m going to walk you through these three tools so I can explain why they are essential to Skyscanner engineers. MShell is Skyscanner\u2019s internal collection of standardised templates, components, libraries, language frameworks, and Docker images that helps facilitate the creation, deployment and operation of our microservices. This collection of standardised components are packaged into cookiecutter templates that are pre-configured to work with our infrastructure. Which means all the basic configurations are already taken care of, no need to worry about the \u201chow\u201d of deploying my service as a Docker container on production or \u201chow\u201d do I make sure that my service is properly sending logs to our log streaming service. You can imagine how this tool enables me to create a new microservice that adheres to the Twelve-Factor methodology out-of-the-box and run it in my local development environment in less than 10~ minutes. Standardisation has improved our velocity, this was made possible with MShell. Drone is an Open Source Continuous Delivery Platorm. It has built-in integration with GitLab which makes it a suitable continuous delivery platform for us. Code as configuration also is a big plus. Drone has a configuration file which is already pre-configured from MShell. The configuration determines the steps that needs to be done during the build process like building the Docker image, running the tests, and generating the service\u2019s environment variable configuration. This gives us a very flexible continuous delivery pipeline. It also comes with free integration to other useful tools like SonarQube, Snyk, and Danger. After successfully running all the build steps specified in the Drone configuration file, there needs to be a way to orchestrate the deployment of those images built by Drone as Docker containers on production. Slingshot manages the deployment of these Docker containers into their target platform which could be ECS, Kubernetes or Lambda. It also provides orchestration for Blue\/Green deployments to reduce service downtime and enable fast rollback if service KPIs aren\u2019t looking healthy after deploying the new version. Just like Drone, configuration is also declarative in Slingshot. It comes with its own configuration file with default values set by MShell when the service details like service name and owner are set. You can imagine the flexibility it provides with the configuration file, especially when you deploy those Docker containers into different platforms. Because once you have these three tools available, you can build other tools on top of it or around it. One real example would be a log streaming service that can be deployed using the deployment pipeline made up of Drone and Slingshot. This log streaming service will then have a client that will become part of the standard MShell template. This is a great example of how this set of tools enables engineers to build more tools to further increase their productivity. One mantra that stuck with me after reading \u201cThe Effective Engineer\u201d by Edmond Lau is \u201cInvest in tooling\u201d. I also like this quote from the book: I\u2019m fortunate to be working in a company that shares the same principles in \u201cThe Effective Engineer\u201d and executes on them. Sign up for our Skyscanner Engineering newsletter for more case-studies, news and job vacancies or take a look at our current job roles available across our 10 global offices. My name is Ardy Dedase, I\u2019m currently leading a team in the Partner Engine tribe in Singapore. We are building a platform that enables our online travel agents, airlines and affiliate partners serve our travelers better. Outside work, I enjoy travelling, playing guitar, reading and tinkering with new tech that I come across. Remember! Sign up for our Skyscanner Engineering newsletter to hear more about what we\u2019re working on, interesting problems we\u2019re trying to solve and our latest job vacancies. By clapping more or less, you can signal to us which stories really stand out. We are the engineers at Skyscanner, and we are transforming the travel industry. Sign up for our newsletter http:\/\/bit.ly\/SignupforOurNewsletter","time":1525790937,"title":"\u201cDeveloper Enablement\u201d Tools for Software Engineers","type":"story","url":"https:\/\/medium.com\/@SkyscannerEng\/our-essential-developer-enablement-tools-for-software-engineers-1e61d3751e46","label":3,"label_name":"dev"},{"by":"severine","descendants":0,"id":17021244,"kids":"None","score":2,"text":"Last February, leaked messages from a private WikiLeaks chat group offered an unfiltered glimpse of Julian Assange, complete with misogyny, anti-Semitism, and a clear, early preference for a GOP election victory. Now the U.K. man who leaked those messages is stepping forward to explain why he turned on the secret-spilling group, which he argues has abandoned its truth-telling mission altogether. Iain (The Daily Beast has agreed not to use his last name) is a 41-year-old writer, artist, and musician based in Edinburgh who joined a small circle of key WikiLeaks supporters after meeting with Assange in 2013, and remained connected to the group until October 2016. \u201cI never understood at the time that these people are really quite fanatical, and not in a good way,\u201d said Iain. \u201cThey will lie to your face, they will lie on Twitter\u2026 The pressure is being piled on, the lies are being piled on.\u201d Iain knows he faces risks by publicly criticizing WikiLeaks now. Previously unpublished portions of the message logs show that Assange harbors lasting animus for those he views as opponents and traitors. While strategizing a response to a critique by former WikiLeaks volunteer James Ball, Assange lays out the need \u201cto create doubt that he\u2019s an accurate narrator.\u201d Of Birgitta J\u00f3nsd\u00f3ttir, an Icelandic politician who worked with Assange on the first big WikiLeaks release, he writes, \u201cThere\u2019s no trust, one cannot trust a back-stabber, but there is an alignment of interests.\u201d  In February 2016 Assange offered this this ominous observation: \u201cWL enemies end badly\u2026 Sometimes it is because we cause it to happen, other times it seems like fate.\u201d Iain entered WikiLeaks\u2019 orbit in 2012. He was a longtime liberal anti-war activist, but until then most of his political engagement had been through his art. He felt the need to become more directly involved in a worthy cause. WikiLeaks faced global pressure during the Chelsea Manning leaks, and Iain was outraged that so many mainstream news outlets that had benefited from those leaks had later turned on Assange. \u00a0 He got Assange\u2019s attention through supportive posts on Twitter and his now-dormant website, titled Martha Mitchell Effect and later Hazelpress. In 2013, his writing style and staunch support of WikiLeaks led Ball, by then a journalist at The Guardian newspaper, to accuse him publicly of being Assange writing under a pseudonym. The real Assange was amused by the mixup and invited Iain to the Ecuadorian Embassy in London for a visit, according to Iain, who told his story to The Daily Beast in a telephone interview and over email. He made the visit that spring, settling into an embassy office that served as Assange\u2019s living space for a chat that stretched to three or four hours. \u201cIt was very relaxed,\u201d said Iain. \u201cHe\u2019s very charismatic, nattering away about all kinds of stuff, and he\u2019s very well informed.\u201d  Assange began granting Iain insider access for some of his articles and promoting them reliably over the WikiLeaks Twitter feed, while Iain occasionally filed Freedom of Information requests on Assange\u2019s behalf. In May 2015, at WikiLeaks\u2019 request, Iain set up a private Twitter direct messaging group called \u201cWikiLeaks Plus 10\u201d for Assange, himself, and other key online supporters. The group was described as a \u201clow-security channel for some very long term and reliable supporters who are on Twitter.\u201d It\u2019s the logs from that group\u2014containing more than 11,000 messages, over 1,200 of them from the WikiLeaks Twitter account\u2014that Iain leaked to The Intercept late last year. It led to a 3,000-word expos\u00e9 accompanied by redacted excerpts from the log. Assange responded to the story in a series of tweets accusing one of the reporters of harboring an anti-WikiLeaks bias. He dismissed the leaked messages as \u201cfreewheeling banter and quips\u201d and not \u201ceditorial policy.\u201d Iain said he felt compelled to leak the messages after the revelation that Assange held friendly, supportive online conversations with Donald Trump Jr. during the election. Before then, Iain had struggled to understand WikiLeaks\u2019 sudden embrace of the alt-right. After the Trump Jr. chats emerged, Iain went back over the logs of the WikiLeaks Plus 10 group chats and also dug into Assange\u2019s pre-WikiLeaks history. \u201cWhat happened in 2016 was just so shocking to me. I was just trying to figure out, What was that? Where did it come from? And so I started looking back.\u201d He finally found what he sees as a kind of Rosetta Stone into Assange\u2019s thinking in a leaked email Assange wrote way back in 2007 while soliciting support for the nascent WikiLeaks concept. One of the goals for WikiLeaks, Assange wrote, \u201cis total annihilation of the current U.S. regime and any other regime that holds its authority through mendacity alone.\u201d Start and finish your day with the top stories from The Daily Beast. A speedy, smart summary of all the news you need to know (and nothing you don't). The \u201ctotal annihilation of the current U.S. regime\u201d line, which got some attention during the Chelsea Manning leaks, explains everything, said Iain, including Assange\u2019s curious embrace of alt-right conspiracy theories and memes during the 2016 election season and beyond.  WikiLeaks baffled some supporters when it started seeding or promoting fake news about Hillary Clinton\u2019s health, Pizzagate, and even Democrats engaging in satanic rituals at the same time it was releasing genuine material stolen from the Democratic National Committee and the Clinton campaign.  This was not, as it might appear, a cynical bid to garner Donald Trump\u2019s favor; nor was it simple pandering to a new funding base, argues Iain. It was part and parcel of Assange\u2019s alignment with Russian President Vladimir Putin against their common adversary, the United States. In retrospect, the shift was apparent way back in 2013, during Assange\u2019s failed bid to win a Senate seat in Australia, Iain said. To that end Assange established the country\u2019s WikiLeaks Party, with a platform built on government transparency and libertarianism. But just weeks before the election the party was roiled by a controversy over its ranking of other political parties in Australia\u2019s ranked-choice voting, preferencing, in two races, the white-nationalist Australia First Party and the right-wing Shooters and Fishers Party above the liberal favorite, the Australian Greens. The WikiLeaks Party blamed the choices on an \u201cadministrative error,\u201d and Assange took to Australian TV to deny any role in the scandal, saying he was too preoccupied helping Edward Snowden to pay attention to what was happening in the party he led. But as soon as the election was over, the WikiLeaks Party transformed. Its website was suddenly overrun with posts that would have been at home on Kremlin outlets like RT and Sputnik. \u201cThe puppet politicians who Washington intended to put in charge of Ukraine have lost control,\u201d read a post on the Ukraine conflict. \u201cThe government of Crimea, a Russian province\u2026 has disavowed the illegitimate government that illegally seized power in Kiev and requested Russian protection.\u201d Posts on Syria described the 2013 Ghouta chemical-weapons attack by forces loyal to Bashar al-Assad as \u201cunsubstantiated\u201d and \u201cstaged.\u201d \u201cThe party\u2019s website content now operated solely to legitimize Russian propaganda by placing it under the umbrella of WikiLeaks\u2019 valuable brand of truth and transparency,\u201d said Iain.  Four months after the election, in December 2013, Assange\u2019s father, John Shipton, led a WikiLeaks Party delegation to Damascus and had tea with Assad. Shipton, the party chair, told a local TV station that the WikiLeaks Party wanted to open an office in Damascus, and after the trip the party tweeted that its members had seen firsthand \u201chow Syrians defeated the conspiracy of more than 86 countries.\u201d Iain was appalled by all of this and wrote articles lambasting the WikiLeaks Party\u2019s new direction. Despite Assange\u2019s smiling photo on the front page of the party website, Iain accepted Assange\u2019s assurances that he had nothing to do with the party\u2019s overnight embrace of Putinism, until WikiLeaks followed precisely the same path to influence America\u2019s 2016 election. In October 2016, Iain withdrew from the WikiLeaks Plus 10 conversation and ended his public support for WikiLeaks. \u00a0 More than a year later, when the Trump Jr. chats surfaced, he returned to the chat group one last time to archive the contents for a wikileak of his own. \u201cInformation and false information have always been weapons, used at all levels of society, from the family structure to the state, and WikiLeaks... uses both, and not just to inform or mislead society, but to literally reshape it according to a singular viewpoint,\u201d said Iain. \u201cIt is a viewpoint that speaks of \u2018annihilation\u2019 and then acts in the taking of political \u2018scalps.\u2019\u201d \u201cIt has succeeded in empowering harmful fascistic structures against those who believe in and uphold progressive values,\u201d he continued. \u201cAs a result, it is and will remain dangerous to all of us, whether we know it or not.\u201d Got a tip? Send it to The Daily Beast here.","time":1525790860,"title":"Defector: WikiLeaks \u2018Will Lie to Your Face\u2019","type":"story","url":"https:\/\/www.thedailybeast.com\/defector-wikileaks-will-lie-to-your-face","label":7,"label_name":"random"},{"by":"leonagano","descendants":1,"id":17021239,"kids":"[17021241]","score":1,"text":"Countries available: Belgium, Canada, France, Germany, Hong Kong, Italy, Japan, Netherlands, Singapore, United States, UK  ** The following US Holidays were considered 1st Jan 18, 28th May 18, 4th Jul 18, 3rd Sep 18, 22nd Nov 18, 23rd Nov 18, 25th Dec 18. 2019 to be included by Leo Nagano","time":1525790833,"title":"Show HN: DaysUntilNextHoliday","type":"story","url":"https:\/\/daysuntilnextholiday.carrd.co\/","label":7,"label_name":"random"},{"by":"mymmaster","descendants":0,"id":17021232,"kids":"None","score":1,"text":"Entire libraries could be filled with tips and tricks about how to ace a coding challenge. This step of the interview process is so feared that few developers think about the other side of the coin: what goes into creating an effective coding challenge? How do you design a problem that will select candidates that your company wants to hire? The goal of a coding challenge, like any other step of an interview, is to determine what kind of employee the candidate would be. A good challenge should reveal a coder\u2019s hard technical skills, their approach to problem solving, and their ability to think clearly under the pressure of an approaching deadline. One common mistake many CTOs make is using a challenge with a widely-known solution, or creating a challenge that does not accurately reflect a professional coding environment. These mistakes do no favors to you or to the candidates you are vetting. Here are some steps to take that will make your challenge more than a formality. You don\u2019t necessarily need to reinvent the wheel. Sometimes holding your own coding challenge is impractical. If you have a large number of candidates to get through, giving each one a coding challenge and then analyzing each one can take too long, particularly for smaller start-ups. If this is the case for you, there are a number of third-party companies that challenge prospective candidates for you: a choice that reduces some of the costs of coding challenges. The most popular of these are TopCoder, Coderbyte, and HackerRank.  The goal of a coding challenge should be to help you determine how well the candidate would fit into your team. In order to make a judgment about that, your coding challenge should be representative of the kind of work that they would be doing if you hire them. Classic coding challenges like FizzBuzz or designing a simple REST interface won\u2019t help you here, because these standardized challenges won\u2019t help you distinguish candidates: any programmer worth interviewing can handle these. This means that there is no one best coding challenge; it has to be customized so that it can simulate the working conditions and needs of your company. Another serious consideration is how to administer the coding challenge; should they take it home or do it in person? Neither option is better than the other, and many companies do both. What type of challenge you use depends on what kind of role you want to fill, and where your priorities are. The primary advantage of an in-person coding challenge is that it gives you the chance to see how a candidate approaches a problem in real time. It is the best way to determine how well they respond to a deadline in a professional environment. If your team pair programs frequently, consider having paired programming be a part of this challenge. When attempting to find somebody who will be a good cultural fit into the company, an in-person challenge is perfect, because if your experience with them during the interview doesn\u2019t go well, they\u2019re not likely to impress you any further if you hire them. Giving candidates a take-home challenge gives you insight into other strengths and weakness. Since take-home challenges tend to be longer, you can examine their approach to larger projects. More importantly, you\u2019ll see how they code when a prospective employer isn\u2019t looking over their shoulder. Do they still adhere to best practices when they aren\u2019t being watched? Whether you administer an in-person challenge, a take-home challenge, or both, consider how long it should take to finish. An overly long challenge will not give you any more insight than a shorter one; all you\u2019ll do is drive away candidates who might not be able to afford to take a lot of time off of work. A general rule of thumb is that an in-person challenge shouldn\u2019t last longer than three hours, and a take-home challenge shouldn\u2019t take more than eight.  Once the coding challenge is done, how should it be judged? The most important thing to keep in mind is not to fixate on whether or not they got the \u201ccorrect\u201d answer. Look at the candidate\u2019s process, and think about what your team values most. If test driven development is a priority, look at their unit test.  Another thing to assess is if they understood the requirements of the challenge. Any developer needs to show an ability to grasp the requirements of a given project, and a willingness to ask for clarification if they are confused. When you look at the mistakes they made, look to see if they were due to a misunderstanding or to lack of attention to detail.  Finally, it\u2019s worth taking the time to talk through the challenge with the candidate afterwards. This will help you understand their thought process, and most importantly, it will reveal how they respond to criticism. If you rely only on comparing the candidate\u2019s solution to the one you had in mind, you can overlook a strong coder\u2019s bad qualities, or a weaker coder\u2019s good qualities. Skills can be developed further, but if the person responds poorly to criticism or is difficult to have a conversation with, they may be more of a detriment than a help to your team.  A good coding challenge can reveal a candidate\u2019s strengths and weaknesses from both a technical and interpersonal view. The ROI on a well-executed and reviewed coding challenge is high, since it\u2019s the closest thing you\u2019ll get to seeing how well a person would work in your team\u2019s environment. Coding challenges are \u00a0too valuable a tool for a CTO to phone in. If you don\u2019t have the time or inclination to create a unique challenge, at least find a way to put some kind of twist on one of the classics. Remember, your coding challenge should be challenging and realistic, and you should set clear expectations. Giving candidates a \u201cgotcha\u201d challenge will not help you determine their value, and will hurt your company\u2019s reputation. After all, an interview is also a chance for prospective employees to understand your values and philosophy, and word of unreasonable interviews tends to spread. If you take the time and effort to craft a strong coding challenge, you will find yourself hiring stronger coders, and better employees in general. Your time is valuable, so here are the things to keep in mind when considering how to approach coding challenges:  About the author: Noah Heinrich is a web developer, content producer, and educator living in Chicago. He is also a producer of the weekly podcast Tabletop Potluck.","time":1525790762,"title":"How to design a coding challenge that is actually effective","type":"story","url":"https:\/\/buttercms.com\/blog\/using-coding-challenges-with-your-interview-process","label":7,"label_name":"random"},{"by":"fibo","descendants":0,"id":17021225,"kids":"None","score":1,"text":"GitLab Documentation Gemnasium has been acquired by GitLab\nin January 2018. Since May 15, 2018, the services provided by Gemnasium are no longer available.\nThe team behind Gemnasium has joined GitLab as the new Security Products team\nand is working on a wider range of tools than just Dependency Scanning:\nSAST,\nDAST,\nContainer Scanning and more.\nIf you want to continue monitoring your dependencies, see the \"Migrating to GitLab\"\nsection below. Your account has been automatically closed on May 15th, 2018. If you had a paid\nsubscription at that time your card will be refunded on a pro rata temporis basis.\nYou may contact us regarding your closed account at gemnasium@gitlab.com. All accounts and data have been deleted on May 15th. GitLab doesn't know anything\nabout your private data, nor your projects, and therefore if they were vulnerable\nor not. GitLab takes personal information very seriously. To avoid broken 404 images, all badges pointing to gemnasium.com will be a\nplaceholder, inviting you to migrate to GitLab (and pointing to this page). Gemnasium has been ported and integrated directly into GitLab CI\/CD.\nYou can still benefit from our dependency monitoring features, and it requires\nsome steps to migrate your projects. There is no automatic import since GitLab\ndoesn't know anything about any projects which existed on Gemnasium.com.\nSecurity features are free for public (open-source) projects hosted on GitLab.com. You almost set! If you are already using\nAuto DevOps, you are already covered.\nOtherwise, you must configure your .gitlab-ci.yml according to the\ndependency scanning page. Since 10.6 coming with GitHub integration,\nGitLab users can now create a CI\/CD project in GitLab connected to an external\nGitHub.com or GitHub Enterprise code repository. This will automatically prompt\nGitLab CI\/CD to run whenever code is pushed to GitHub and post CI\/CD results\nback to both GitLab and GitHub when completed. Create a new project, and select the \"CI\/CD for external repo\" tab:  Use the \"GitHub\" button to connect your repositories.  Select the project(s) to be set up with GitLab CI\/CD:  and chose \"Connect\". Once the configuration is done, you may click on your new\nproject on GitLab:  Your project is now mirrored on GitLab, where the runners will be able to access\nyour source code and run your tests. Optional step: Make sure the project is public (in the project settings) if your\nGitHub project is public unless the security feature will be available only for paid accounts. To set up the dependency scanning job, corresponding to what Gemnasium what doing,\nyou must create a .gitlab-ci.yml file, or update it according to\nhttps:\/\/docs.gitlab.com\/ee\/user\/project\/merge_requests\/dependency_scanning.html.\nThe mirroring is pull-only by default, so you may create or update the file on GitHub:  Once your file has been committed, a new pipeline will be automatically\ntriggered if your file is valid:  The result of the job will be visible directly from the pipeline view:  If you don't commit very often to your project, you may want to use\nScheduled pipelines\nto run the job on a regular basis. \n\n            Was this helpful? Do you think that something is unclear?\n            Use the comments area below and leave your feedback.\n            \n              For support and other enquiries, see\n              getting help.\n            \n\n \n\n            Any comments that aren't related to docs feedback will be deleted.\n          \n","time":1525790695,"title":"Gemnasium will be deleted","type":"story","url":"https:\/\/docs.gitlab.com\/ee\/user\/project\/import\/gemnasium.html","label":3,"label_name":"dev"},{"by":"devy","descendants":1,"id":17021223,"kids":"[17021233]","score":5,"text":"Jonathan Rauch, author of The Happiness Curve, was relieved to find an explanation for his gloom \u2013 academics say adulthood happiness is U-shaped \nLucy Rock \n\nSat 5 May 2018 06.00\u00a0EDT\n\n\nLast modified on Sat 5 May 2018 07.06\u00a0EDT\n\n When Jonathan Rauch fell into the doldrums in his 40s, he had no idea why. Life was good: he had a successful career, a solid relationship, good health and sound finances. Then he learnt about the happiness curve and it all became clear. Academics have found increasing evidence that happiness through adulthood is U-shaped \u2013 life satisfaction falls in our 20s and 30s, then hits a trough in our late 40s before increasing until our 80s.  Forget the saying that life begins at 40 \u2013 it\u2019s 50 we should be looking toward. Rauch, a senior fellow at the US thinktank the Brookings Institution, was so relieved to have found an explanation for the gloom that hit him and, he believed, many others in middle age that he became evangelical about spreading the word. He has written a book, The Happiness Curve: Why Life Gets Better After 50 (out in the US 1 May and UK 14 June), which includes personal stories, the latest data and illuminating interviews with economists, psychologists and neuroscientists. \u201cThe most surprising thing is that age tends to work in favour of happiness, other things being equal,\u201d he tells the Guardian. \u201cThe most strange thing is that midlife slump is often about nothing.\u201d Hold off on splashing out on that flashy sports car or embarking on an affair though. It is not the same as a midlife crisis, which according to the stereotype demands an urgent, rash response. The slump isn\u2019t caused by anything, according to Rauch. It is a natural transition, simply due to the passing of time.  \u201cIt\u2019s a self-eating spiral of discontent,\u201d he says. \u201cIt\u2019s not because there\u2019s something wrong with your life, or your marriage, or your mind, or your mental health.\u201d Not everyone will experience a sunnier outlook in their 50s and beyond, Rauch acknowledges, because factors such as divorce, unemployment or illness can counter this. But, other things being equal, the U-curve holds. Rauch, an author and journalist, adds: \u201cThose most likely to notice the arrow of time are the people without a lot of other change or difficulty in their life. Things seem to be going well for them, they\u2019re achieving their goals, and nothing much has changed. They think, \u2018Why do I feel less satisfied than I expected to? Why is this going on year after year? Why does it seem to be getting worse and not better? There must be something wrong with my life.\u2019 \u201cWell, there\u2019s nothing wrong with your life, you\u2019re just feeling the effects of time which others who may have more turbulent lives may not notice as much.\u201d  Rauch details a raft of research in his book to back up his claims. A 2008 study by economists David Blanchflower and Andrew Oswald found the U-curve \u2013 with the nadir, on average, at age 46 \u2013 in 55 of 80 countries, and they cited more than 20 other papers finding the U. It tends to show up in wealthier countries where people live longer, healthier lives. Life satisfaction statistics for the UK in 2014-15 show happiness declining from youth through middle age, hitting a low at 50 and rising to a peak at 70. Not all economists and psychologists agree. Economists Paul Frijters and Tony Beatton factored in the possibility that those who become happier in the studies are the same people who are more content when they start out. This can help them achieve greater career or relationship success, which leads to more happiness. Correcting for this effect, the U-shape disappears. Rauch, however, believes he is a textbook example of the U-curve.  His mother suffered from depression and his parents broke up when he was 12, leaving his father to bring up three children on his own. Two years later, his father, a stressed and overworked lawyer in his mid-40s, lost his biggest client. Rauch remembers himself at 20, keen to accomplish something worthwhile by middle age and believing that when he did, he\u2019d appreciate it. By his 40s, he had surpassed his dreams. He had published books; he was winning journalism prizes; he was in a relationship with Michael, the man he would later marry; he lived in an area of north Virginia with a strong sense of community. Yet he was preoccupied with what he had not achieved. He explains: \u201cI was someone who was fortunate. I had good health and after my 20s, which were difficult because I came out as gay, I met one goal after another with more success than I\u2019d ever expected.  \u201cYet around the time I turned 40 I noticed this strange feeling of restlessness and discontent. This continued to grow as I got into my 40s to the point where I was 45 and I won the most prestigious award in magazine journalism [a National Magazine award] and that gave me a great feeling of satisfaction with my life for approximately 10 days.  \u201cAll these feelings of discontent and restlessness \u2013 and even sometimes worthlessness and this feeling I\u2019d almost wasted my life \u2013 kept coming back. \u201cNone of this made any rational sense. I began to think there must be something wrong with me. I began to think my personality had begun to turn dark in some way and that of course compounded the problem.\u201d Around 50, the fog began to lift, despite the death of both his parents, the loss of his magazine job and the failure of a startup venture.  Rauch, 58, says: \u201cIn my 50s, first the volume of the demons\u2019 voices went down, and now I rarely hear their voices at all.\u201d  While researching his book, Rauch spoke to many people who\u2019d experienced similar feelings. Karla, 54, is on the upswing of the curve. She says she is savouring her friendships more, feeling more organised and efficient, and doing more volunteering work. \u201cNow I feel grateful for the now,\u201d she tells Rauch. \u201cOn a day-to-day basis I probably do the same things, but I feel different.\u201d Rauch tells the Guardian: \u201cThat\u2019s a very profound insight because what we\u2019re talking about here is not that the conditions of your life change in some huge way, but how you feel about your life changes.\u201d Rauch puts forward various explanations for why we feel happier in our 50s and beyond. Research shows that older people feel less stress and regret, dwell less on negative information and are better able to regulate their emotions. Nor is status competition as important. Rauch says: \u201cWe seem to be wired to seek maximum status when we are young \u2013 the ambition to be on top of the world, to have the big job, to have the extraordinary marriage to the wonderful person or lots of money. Or some form of greatness, which is what I dreamed of in my 20s, to write some book that would outdo Shakespeare.\u201d We are over-optimistic in youth about how much satisfaction we will get out of our future successes, he believes. \u201cAs we get into our 30s and 40s, we\u2019ve achieved most of those things, but we\u2019re not wired to sit back and enjoy our status.  \u201cThe same ambition that made us status hungry makes us hungry for more status. We\u2019re on the hedonic treadmill. We don\u2019t feel the satisfaction we expected, so we think there\u2019s something wrong with our lives.\u201d As we get older, our values change. \u201cYou hear people say, \u2018I don\u2019t feel the need to check those boxes any more\u2019, or \u2018I don\u2019t care that much what other people think\u2019.\u201d Older people feel relieved of a burden that makes it easier to savour other simpler pursuits such as spending time with grandchildren, a hobby or volunteer work.  Rauch would like to see more help for people to relaunch themselves after this midlife transition, including greater opportunities for adult learning and companies creating more part-time positions or allowing gap years. \u201cThere\u2019s a huge amount of untapped wisdom and potential to be unlocked. Because of the happiness curve, they\u2019re often in a position where they want to give back. They want to be mentors, they want to be volunteers and they want to work at not so difficult jobs which allow them to use their skills.\u201d Rauch has a few tips for relieving midlife malaise, such as talking to friends about it and understanding it\u2019s normal. It is also helpful to stop comparing yourself to others, he says. But if all that makes no difference, give it time. As Rauch approaches 60, he feels ever more grateful for his life. He wishes he\u2019d known this when he was in the trough of the curve because, as he says: \u201cIt\u2019s worth the wait.\u201d ","time":1525790686,"title":"Life gets better after 50: why age tends to work in favour of happiness","type":"story","url":"https:\/\/www.theguardian.com\/lifeandstyle\/2018\/may\/05\/happiness-curve-life-gets-better-after-50-jonathan-rauch","label":7,"label_name":"random"},{"by":"CrazedGeek","descendants":0,"id":17021216,"kids":"None","score":3,"text":"This is the story of how corporate raiding, complacency, excess, and incompetence are gutting a media company that matters to tens of millions of people. It\u2019s not a novel story, and perhaps not even scandalous by the standards of corporate opulence: A shark-obsessed boss, millions wasted on consultants, and an executive who insisted on publishing softcore porn are more embarrassing buffoonery than insidious greed. The main problem\u2014the billions in debt the company ran up in the process of its owners buying it and weighing it down\u2014is practically routine in media and beyond; that doesn\u2019t make it any less infuriating. This company is Univision, which until recently obligingly filled the role of absentee stepfather to Gizmodo Media Group, our employer. Now, Univision\u2019s business is struggling, and GMG has suddenly found itself under a very watchful eye. Once upon a time, Univision, an American broadcasting operation aimed primarily at Spanish speakers in the United States, was a tremendous golden goose laying tremendous golden eggs: It made incredible amounts of money and had to do essentially nothing for it other than run programming produced by Televisa, a Mexican broadcasting operation. The fairy tale ended long ago. Univision has been in decline for years, thanks to a disastrous private equity buyout finalized in 2007; an aging audience; a burdensome program-licensing deal with Televisa; competition from Telemundo and Netflix; layers of overpaid and useless middle management; and a general failure to position itself for a digital future. No one at Univision, from working journalists to officers of the company to investors looking for a return, wanted anything other than for the company to thrive and become a behemoth for the ages, a legacy operation that would show the way forward for media in a changing America. That didn\u2019t matter. We  recently spoke to more than two dozen current and former Univision employees from every level of the company, private equity experts, and management consultants, and reviewed a wide range of documents, and the picture that emerged is clear: Despite the debt hanging over Univision\u2019s head, the company indulged a culture of complacency and excess, embodied in many ways by the operations of Fusion Media Group, an ill-fated attempt to stay relevant in the digital age. And now that the ship is off course, the rival interests in the company are at each other\u2019s throats, in some cases diving off the ship and in some cases teaming up to force enemies off the gangplank. The first few months of this year have seen severe and ongoing cuts, the replacement of the chief financial officer, and the announcement that chief executive officer Randy Falco\u2014who in a recent company-wide email announcing layoffs was reduced to describing them as \u201cdisruption ... required to transform this business into a company that will not only exist but continue to thrive\u201d\u2014will retire by December.                 This post was produced by the Special Projects Desk of Gizmodo Media Group. Reach our team by phone, text, Signal, or WhatsApp at (917) 999-6143, email us at tips@gizmodomedia.com, or contact us securely using SecureDrop. Advertisement  From routine human resources fuckups to vastly overselling the prospects of an IPO whose ultimate doom this March precipitated the company\u2019s current cost-cutting spree, Univision has been deeply mismanaged and is in the midst of making huge cuts that have, among other things, already claimed vast swaths of Univision Noticias\u2014the most vital newsgathering operation serving the Spanish-speaking community in the U.S.\u2014and Fusion Media Group. Consultants from Boston Consulting Group, who have reportedly recommended budget cuts of up to 35 percent in some parts of the company, have been combing through the books for months, and more than 150 people have been laid off so far. Plenty more cuts are pending (Univision president of news Daniel Coronell reportedly described them as \u201ccatastrophic\u201d to his newsroom), including at GMG, the staff of which fears the newsroom may be cut by up to a third by the end of June, perhaps as part of a broader pivot toward video and branded content. What is happening to the company is not ultimately a failure of editorial or even executive management, though: If Univision was a mammoth whose failure to adapt slowed it down, it was private equity investors, consumed by the thought of turning their riches into more riches, who brought it down and bled it dry. While there had been plenty of signs that something was amiss, the first hint that something was seriously wrong came buried at the end of a March all-staff email opaquely titled \u201cFinance Update and Appointment | Informaci\u00f3n y nombramiento en Finanzas.\u201d The email informed staffers that chief financial officer Frank Lopez-Balboa was departing the company. News that Univision was scrapping its long-held plans for an IPO was slipped in at the very end. Advertisement  \u201cGiven that we are no longer on a path to pursue a public offering, this is an opportunity for Frank to begin the next chapter of his career,\u201d Randy Falco wrote in the message. The pursuit of a public offering failed due to a combination of antsy investors, new competition, and a private equity deal that the Financial Times would later describe as \u201ca symbol of the excesses of the credit bubble.\u201d That last bit is the most important. In 2007, a consortium including Texas Pacific Group, Thomas H. Lee, Madison Dearborn, Providence Equity, and Saban Capital took Univision private for $13.7 billion. These firms\u2014executives of which still shape Univision\u2019s board\u2014borrowed heavily to finance the deal, saddling their new prize with more than $10 billion of debt. According to an FCC filing, each firm holds between 20.6 and 7.1 percent of Univision\u2019s equity, and between 27.3 and zero percent of the voting interests. Thomas H. Lee, the only firm with no voting rights, has no official members on Univision\u2019s board, but two of THL\u2019s employees, James Carlisle and Laura Grattan, are listed as Univision board observers in their company bios; Univision would not say if the firm had appointed members to the board or who they were. Univision, for its part, declined to answer questions about the board, while all the involved firms either declined to comment or did not respond to questions about their involvement with Univision. Advertisement  Leveraged buyouts such as the ones by which these companies acquired control of Univision were common in the years leading up to the financial crisis: Investors borrow a huge amount of money to purchase a company and then make that company responsible for paying back the debt. The amount of borrowing required is often large relative to a company\u2019s earnings. This relationship\u2014known as leverage\u2014is used to gauge whether a company is likely to be able to pay back its lenders. The financial world commonly measures this through the ratio of \u201cdebt to EBITDA,\u201d or earnings before interest, taxes, and depreciation and amortization of various assets. (The finance industry\u2019s inscrutable jargon is a feature, not a bug. Just think of this ratio as a company\u2019s debt compared to how much money it makes each year.) Univision\u2019s ratio, estimated at 12.5-to-1, made it highly leveraged even by the standards of the pre-crisis boom period. (In 2013, Obama administration regulators would urge banks to limit companies\u2019 leverage to roughly half this level to reduce the risk of default.) Still, in 2007\u2014when the company maintained a tight grip on the then-swelling U.S. market for Spanish-language media, and before media enterprises came to be viewed as dead investments\u2014Univision found itself in a position of relative strength. \u201cThere is a term that investors like to use: \u2018growing into your capital structure,\u2019\u201d said Jack Kranefuss, a senior director at Fitch Ratings who analyzes the media business. \u201cYou start off with too much debt, but you have a company that has good growth prospects. Univision, for the longest time, was thought of as a company that could grow into its capital structure.\u201d Advertisement  Private equity firms typically cash out their investments after a few years, and Univision\u2019s backers have been looking for an exit since at least 2014, when they held preliminary negotiations with CBS and Time Warner to sell the company for a reported asking price of more than $20 billion. Those talks soon collapsed. The following year, Univision positioned itself for an initial public offering just before media company stocks began tumbling amid fears that cord-cutting would threaten the traditional TV business. Top Univision brass continued postponing IPO plans in the hope that market conditions would improve, attempting to reorient the company toward an increasingly bilingual Latinx media market while simultaneously  trying to exit their positions. They couldn\u2019t pull it off. They were reportedly close to a $13 billion merger with Discovery Communications, which would have represented a clean exit for Univision investors, but rejected it. The delayed IPO piqued the interest of Liberty Media\u2019s John Malone, but Univision reportedly could not agree with him on the company\u2019s valuation. Eventually the board abandoned their IPO plans altogether earlier this year. This brings us to that March email to all Univision employees, whom Falco thanked for their \u201ccommitment to our company, our networks and our community.\u201d Advertisement  \u201cAnyone who saw that email knew shit was about to hit the fan,\u201d a current Fusion staffer told GMG. Sure enough, two weeks later the Wall Street Journal reported that Univision\u2019s Fusion Media Group\u2014a unit including the Fusion cable channel, its associated digital assets, Gizmodo Media Group, and the Onion properties\u2014faced budget cuts as high as 35 percent, and that a number of the company\u2019s top executives, including Felipe Holgu\u00edn, chief executive of Fusion Media Group, and Daniel Eilemberg, chief content officer of Fusion TV, had departed, having been forced out in what was essentially a palace coup. (Holgu\u00edn declined to comment; Eilemberg could not be reached.) The significance wasn\u2019t so much in the news itself (Fusion, despite its lofty goals, was never the most important part of the Univision empire) as in the abruptness with which an experiment whose rise and fall indexed its parent company\u2019s broader aspirations was abandoned. Launched in 2013, Fusion was the brainchild of Isaac Lee, currently chief content officer of both Univision and Televisa; Lee was hired in 2010 by Haim Saban, the soon-to-be-former Power Rangers mogul and a Democratic donor, and came to oversee an ever-expanding portfolio. Fusion, targeted at Latinx millennials, was Univision\u2019s first English-language TV channel. Initially a joint venture with ABC, Fusion aimed to solve Univision\u2019s fundamental problem, which was and is that its main business is airing Spanish-language TV shows for an aging audience that has increasing access to other, better programming through streaming platforms. Advertisement   With a cash infusion from Disney, additional financing from Univision, and tens of millions in cable carriage fees scheduled to come in, Fusion was set, even if it reportedly lost $35 million in 2014, its first year in business. Part of its ambitious if incoherent plan to appeal to everyone Univision did not\u2014multicultural millennials and cord-cutters prime among them\u2014involved an expansion into digital publishing. \u201cEssentially, that was the strategy: We\u2019ll take this guaranteed money that\u2019s coming in the door, hire a bunch of really good people, and use them to build something that can compete with the Voxes and the BuzzFeeds of the world,\u201d a former Fusion digital employee said. In a few short years, the well-funded project would become one of the most spectacular digital media failures in recent memory.  Fusion hired big names from digital, print, and television\u2014infamously, it reportedly paid financial journalist Felix Salmon, who came on proclaiming that he would be \u201cpost-text,\u201d more than $400,000 a year to do, as far as anyone could tell, nothing in particular\u2014as well as a thick layer of \u201cfriends of Isaac Lee\u201d known internally as FOILs. Felipe Holgu\u00edn and Daniel Eilemberg were prime examples, but Lee has a long history of keeping it in the family. Back in 2008, according to a FOIL, when Lee was running a local magazine called Poder in Miami, he helped out a friend from the business community (he dubbed her his \u201cfairy godmother\u201d) by hiring her son as an intern after he graduated college. That same intern was then hired at the Univision network a month after Lee was, and eventually came to Fusion TV, too. Advertisement  At the working level, a lack of clear editorial vision left employees adrift. Relentless pivots meant that people hired to do one job were often given different assignments within a matter of months, ending up working on projects that didn\u2019t align with their experience; meanwhile, executives seemed far more focused on creating the narrative of a fast-growing media company than on what that media company was actually producing. \u201cWe didn\u2019t know how our projects were helping the company or how they were being monetized,\u201d one Fusion TV employee said. For years, money flowed into a series of pet projects\u2014this U.S.-Mexico border concert is a typical example\u2014favored by executives and FOILs, while editorial staffers were left confused about the value of the stories they were pushed to pursue and wondering about the long-term viability of a company spending huge amounts of money without doing much the public seemed especially interested in. (As far as anyone could tell, practically no one watched the Fusion channel.) Advertisement  One often-mocked example was Project Earth, Fusion TV\u2019s environmental unit. Headed by Nico Ibarg\u00fcen, an environmentalist and FOIL in top standing, it produced so much content about sharks that it became not just an internal joke but a subject of outside coverage. \u201cThere were teams that had to work on shark stuff,\u201d a former Fusion staffer said. \u201cIt was, \u2018[Nico] is a genius and therefore we must make shark content because he wants it.\u2019 \u201d Fusion\u2019s problems weren\u2019t a matter of mismanagement alone: Isaac Lee may have run it as his own personal fiefdom, directing resources to his pet projects and those of his cronies, but that wasn\u2019t the fundamental problem. Even perfect execution might not have made the concept work. Its digital offerings launched at the height of Facebook\u2019s dominance over the media industry, into a crowded marketplace where even sites with clear voices  would have had difficulty standing out. Some of Fusion\u2019s biggest-budget Facebook pushes struggled to gain the following necessary to organically propel the new publication to the top of the News Feed\u2014something that led to a new focus on viral-friendly video and, according to a source with direct knowledge, seven-figure sums spent on simply buying traffic. \u201cThe first thing I remember feeling is, \u2018I just have to go out and do really good journalism that\u2019s going to impress people and make a name for this place,\u2019\u201d a former Fusion digital staffer said. \u201cLater there was traffic pressure and then I think the mission sort of changed at some point into: We\u2019re going to be the super-woke millennial-focused media outlet.\u201d Writers brought on with a mandate to chase important stories faced pressure from an audience development team that repackaged stories for virality, sometimes in cringeworthy, decidedly un-woke ways: Advertisement  Although Fusion was co-launched with Disney, that company sold its stake to Univision in April 2016\u2014something framed internally as a positive move that would allow reporters to pursue more controversial work, even if it more realistically involved the world\u2019s most successful media company cutting its losses. Univision soon acquired Gawker Media Group and wrapped the former Gawker sites into its portfolio, and just days after the presidential election, which had already set the company\u2019s workforce on edge, Fusion was hit with heavy cuts. In 2017, Fusion TV executives laid claim to Fusion.net, which had been the home of Fusion\u2019s digital content since the site\u2019s inception and had earlier that year become the flagship news site for the newly rebranded Gizmodo Media Group. This site grab came because, staffers were told, TV executives were angry that the site was more closely associated with what was running online than with their TV productions. The Fusion digital team was directed to brainstorm a new name for their publication, and, in July 2017, the site was renamed Splinter. After the recent cuts, not only is Isaac Lee\u2019s play at digital empire-building seemingly derailed\u2014his lieutenants have been dismissed, and his biggest public move this year has been having dinner with Jared Kushner\u2014but Fusion Media Group, as an entity distinct from GMG and the Onion sites, seems barely to exist. Advertisement  \u201cIsaac is the best deal maker I\u2019ve ever seen in media. He\u2019s one of those people on Craigslist who starts with a paperclip and somehow barters his way to a whole house,\u201d said Alexis Madrigal, who joined Fusion in 2014 and worked as the company\u2019s editor-in-chief before departing in 2017. \u201cHe started with almost nothing on the Univision digital side and created this massive set of digital media companies using whatever resources he could. But I think it made it very difficult to create an operational digital media company because it was such an assemblage of deals, not an organically grown entity.\u201d The lukewarm response to Univision\u2019s attempts to sell or go public over the years can largely be attributed to the long-term debt its owners burdened it with in the first place. Univision has shrunk that pot from more than $10 billion in 2007 to just under $8 billion today, according to its most recent year-end report. But that debt is still far more than the amount of money Univision makes each year\u2014roughly six times the size of the company\u2019s EBITDA. That\u2019s about one-third higher than the levels seen at Sinclair Broadcast Group and Netflix in 2017, according to Fitch, and more than double that of AMC Networks. The ratings agency Moody\u2019s said in a 2017 report that Univision\u2019s debt has junk-bond status. Advertisement  What\u2019s more, interest payments on debt still eat up a sizable chunk of Univision\u2019s earnings. Without such payments, the company would have been safely profitable since 2012. Univision financial reports show interest expenses totaled $442 million in 2017 alone; over the past three years, they\u2019ve amounted to $1.46 billion. These payments came in addition to tens of millions in fees paid to Univision\u2019s private equity investors over the years, as required by \u201cmanagement and technical assistance agreements\u201d that the company finally terminated in 2015 with a lump-sum payment of more than $112 million. Strategic missteps have compounded these financial pressures. In 2010, as the financial crisis was cratering the ad market, Univision sold a stake of the company to Televisa, its programming supplier, which had unsuccessfully bid to purchase Univision four years earlier. The $1.2 billion deal not only provided Univision a lifeline to help make its debt payments, but extended an agreement for Univision to continue broadcasting Televisa\u2019s programming\u2014most importantly, its then-popular telenovelas. In return, Univision would fork over a healthy cut of its Spanish-language media revenue. Advertisement  \u201cThe impact of Univision now having more exploitable content than ever before, both through our in-house efforts and from our expanded programming agreement with our most important partner, Televisa, cannot be underestimated,\u201d Univision president and chief executive Joe Uva said at the time. In retrospect, it was a drastic miscalculation. While Univision has paid Televisa about $300 million in licensing fees in each of the past three years, the telenovelas that are the core of its content have flopped among younger viewers, with Univision\u2019s primetime ratings dropping precipitously as a result. Telemundo, its main competitor, has eaten up market share with more Americanized shows, such as series about Mexican drug lords. (Televisa has reportedly been tepid about this change in demand, in part due to its close ties to the Mexican government.) Meanwhile, Netflix and other streaming services have continued to eat away at the cable subscriptions that translate into carriage fees paid to broadcasters. For its part, Univision says it still sees the relationship as fruitful. A spokeswoman said, \u201cTelevisa is an important partner and we\u2019re pleased with their continued efforts to evolve their content pipeline to meet the needs of Hispanic audiences in the U.S. The refreshed content pipeline has resonated with our audiences and has allowed us to regain our significant lead as the top U.S. Spanish-language network.\u201d All of this points to 2018 being a rough year for Univision, which brings in about $3 billion in annual revenue. Its content-licensing agreement with Televisa got even more expensive in December 2017, potentially adding more than $100 million in costs per year; the ad market is already facing industry-wide headwinds; and Telemundo is not only increasingly attractive to advertisers, but it also outbid Univision for the rights to broadcast this year\u2019s World Cup. (Univision financial statements suggest programming around the tournament brought it $120 million in revenue in 2014.) Advertisement  This is what led Univision brass to see the need to frantically cut a reported $200 million\u2014or $300 million, per another report\u2014in costs. (Univision has declined to comment on the size of the cuts.) \u201cAs the media industry continues to evolve, change is no longer optional for our company, it is imperative,\u201d Randy Falco said in his end-of-year earnings call in 2017. \u201c2018 is about continuing to transform our company for the future. As we reshape our business, we will identify efficiencies and savings opportunities in the areas that require change and we will reinvest for growth.\u201d If knifing Fusion Media Group represented the board cutting off a hand, the layoffs that came next were a thrust aimed at the heart: Univision\u2019s news operation, the reporters and programming most directly connected to the American Spanish-speaking audience. Advertisement  \u201cAt the end of the day,\u201d as one Univision insider said of the company, \u201cit\u2019s a network that airs Mexican TV shows. It\u2019s a tube from Mexico to the U.S.\u201d Much of what makes Univision more than a television tunnel is its acclaimed Noticias division. (In 2017, a glowing story by New York Times media critic Jim Rutenberg described Noticias as \u201cone of the most striking examples I\u2019ve seen all year of a news organization that is meeting the moment.\u201d) Noticias addresses an audience of more than 50 million Spanish speakers in the United States, a population vastly underserved by the mainstream media. While Jorge Ramos deservedly serves as the face of the division, and of Univision, there is more to what Noticias does than what appears on TVs across the country: According to a ComScore analysis, Univision\u2019s news traffic beat out that of several other leading Spanish-language digital news sources\u2014Telemundo.com, Yahoo.ES, and CNN Espa\u00f1ol\u2014fairly handily. Univision\u2019s news site drew between 2 and 3 million unique visitors per month between March 2017 and March 2018, according to ComScore data, a vast reach in relative if not absolute terms. Noticias\u2019 work is critical, and hardly going unnoticed. This year, the Univision Noticias digital news team won a Premio Ortega y Gasset de Periodismo for its \u201cMejor Vete, Cristina\u201d podcast, and first place at the NPPA Best of Photojournalism awards for its multimedia report \u201cA Night Bearing Witness to the Violence of El Salvador.\u201d Just last month, Noticias won a Hillman Prize, given to \u201cjournalists who pursue investigative reporting and deep storytelling in service of the common good,\u201d for its work on a multimedia project called \u201cFrom Migrants to Refugees: The New Plight of Central Americans.\u201d In an internal email sent April 2017, Randy Falco boasted about the prize: \u201c[T]he project epitomizes Univision at its best,\u201d he wrote. Advertisement  What he didn\u2019t mention was that in that same month nearly 35 percent of a staff of 85 people was cut from Noticias\u2019 digital branch. (Univision declined to comment of the size of the cuts.) Noticias digital\u2014which since 2010 was part of Isaac Lee\u2019s portfolio\u2014began staffing up in earnest after Borja Echevarr\u00eda was hired in 2014 to lead the digital news operation. Rutenberg\u2019s Times story quoted Univision president of news Daniel Coronell summing up the role of Univision Noticias for the U.S. Latinx population: \u201cWe are the voice of the voiceless people \u2026 probably the most weak members of this important country.\u201d Coronell was more recently quoted as telling his newsroom that the cuts that took place there and the ones slated to come are \u201ccatastrophic.\u201d Advertisement  With these deep cuts, Univision\u2019s voice\u2014the work of journalists who in many cases were forced to flee countries like Venezuela, Mexico, and Guatemala, where they were persecuted for their reporting\u2014has been muted. In the Miami office, where the newsroom is based, morale is dragging. Staffers, some hired less than two years ago, are confused about why these cuts are happening right now; some in the U.S. on work visas are scrambling to find ways to stay in the country legally. Two newsroom sources say that reporters and supervisors cooperated and in some cases even sacrificed their jobs to ensure that reporters facing unstable or unsafe conditions in their home countries, like Venezuela, could keep their jobs and stay in the U.S. The value of Univision Noticias digital\u2014a news site investigating and reporting in practical ways on topics that are literally life or death for millions of people in this country\u2014can\u2019t be measured in money alone. As the Times wrote, \u201cOne of [Univision\u2019s] most shared digital features this year has been an explainer on the papers that documented and undocumented immigrants should always have on them, in case of immigration raids or stops.\u201d As one person at Univision said, \u201cNoticias is the core. It\u2019s the lifestream, it\u2019s Jorge Ramos, it\u2019s what connects you to the community.\u201d With nearly 35 percent of the digital news staff gone and layoffs pending on the TV side, it\u2019s obvious that highers-up don\u2019t value that core as much as they once did. Advertisement  All of the cuts seem to point back to debt. Indebted companies typically have agreements with lenders that establish leverage maximums. (In Univision\u2019s case, its debt cannot be more than 8.5 times the size of its EBITDA.) If a company passes an agreed-upon threshold or misses an interest payment, \u201ccreditors can potentially accelerate the payments on that debt,\u201d said Patrice Cucinello, director of the media and entertainment group at Fitch. A company could then risk default if it doesn\u2019t have the cash on hand to make its lenders whole. This isn\u2019t a danger for Univision right now, because it has managed to increase its revenue: \u201cAt least from our financial model,\u201d Cucinello said, \u201cI\u2019m not seeing a near-term risk of default.\u201d The possibility that Univision could default on its debt, though, explains a great deal about the company\u2019s priorities. Univision has reduced its leverage over the years through billions in quarterly payments, and Cucinello pegs its current ratio of debt to EBITDA at around 6.3-to-1 by Fitch\u2019s model. It\u2019s an improvement, she said, but still as much as double the level seen at some comparable media companies. One bad year could gravely change the company\u2019s outlook. Advertisement  For Thomas H. Lee Partners, one of the five private equity firms in the consortium that bought Univision in 2007, this story of leverage is familiar. Thomas H. Lee\u2019s portfolio also includes iHeartMedia (formerly Clear Channel), the country\u2019s largest radio broadcaster, which has swum in as much as $20 billion in debt since a leveraged buyout in 2008. iHeartMedia had weaker-than-expected results in recent quarters and missed an interest payment earlier this year, declaring bankruptcy in March. While iHeartMedia was forced to lay off dozens of employees across the country in several rounds of layoffs, Thomas H. Lee was reportedly expected to break even on the deal, having bought some of iHeartMedia\u2019s debt at a discounted rate\u2014surely a point of pride for the firm. Univision\u2019s debt load is nowhere near that of iHeartMedia\u2019s before its bankruptcy\u2014about 12 times EBITDA, per Fitch\u2014but Univision\u2019s remains stubbornly high compared to the company\u2019s often volatile earnings. IPOs are typically seen as avenues to deleverage indebted companies, an option now off the table for Univision. Coupled with the loss of market share to Telemundo and the lack of World Cup programming this year, Cucinello added, \u201cthere are definitely some headwinds as the company heads further into 2018.\u201d Advertisement  This is the context within which the board\u2019s decisions should be understood. With the prospect of increasing revenues seemingly off the table as a mechanism for deleveraging, slashing costs was the remaining alternative. For investors to extricate themselves from the company, the board would need to drastically prune Univision to spruce it up for potential buyers. The conditions of Univision\u2019s acquisition created a logic by which the company had to attack itself in order to escape its debt. In the aftermath of the failed IPO, conflict grew on the board over how this attack would be carried out; that conflict\u2019s precise nature remains frustratingly opaque. Reports have put the estimated total cuts the board looks to make at $200 million (or more); the company has disputed this estimate but has not asked the publications reporting it for corrections or clarified what the target is. Univision declined to answer questions about the budget cuts. In one version of the story, management decided to bring in BCG to help trim the budgets of underperforming units. In another version of the story, Isaac Lee and Haim Saban advocated cutting as much of this amount as possible from the corporate side of the company, which one FOIL characterized as armies of managers making seven-figure salaries in do-nothing jobs, rather than from Lee\u2019s own fiefdom. (Lee declined to comment; Saban could not be reached.) Lee\u2019s and Saban\u2019s enemies, meanwhile\u2014among them, supposedly, Thomas H. Lee Partners, rattled by the iHeartMedia bankruptcy\u2014took the cost-cutting exercise as an opportunity to, among other things, destroy Lee\u2019s power base, which, if it contained plenty of waste and bloat, also contained the newsgathering and digital operations that were the most vital and forward-looking parts of the company. In this reading, Univision journalism was essentially collateral damage in a conflict between a set of bloodsucking capitalists and a would-be empire builder protecting small armies of loyalists. However you read it, Lee lost out: Fusion Media Group and Noticias were gutted, FOILs like Eilemberg and Holgu\u00edn were turfed out, and rumors spread in the company that Lee\u2019s power had diminished to the point where he didn\u2019t have so much as an assistant to book his flights. If not actually true (Univision would not answer specific questions about the board\u2019s operations, though it says Lee does have an assistant)\u2014Lee remains chief of content for both Televisa and Univision, after all\u2014the fact that such rumors rippled, and were believed, still said something about the force and speed of Lee\u2019s fall, and the unraveling of Fusion Media Group, which functionally exists in name only. Advertisement  To make their cuts, Univision\u2019s board hired Boston Consulting Group in December. It\u2019s common enough for media companies to hire consultants to evaluate their businesses and workflows, typically in preparation for budget cuts. In 2017, a reported two dozen analysts from McKinsey & Company helped Time Inc. slash hundreds of millions in costs as it transitioned to new ownership. Cond\u00e9 Nast, meanwhile, brought in FTI Consulting in 2015 and McKinsey in 2009. \u201cThere\u2019s like 10 reasons to hire a consultant, and only one of them is that they\u2019ll tell you something you wouldn\u2019t know,\u201d said Duff McDonald, author of the 2013 history of McKinsey, The Firm. The other nine? \u201cTo help you get a decision by your board. To help you get a decision by your employees. If you\u2019re going to make a public announcement, it helps to have the cover \u2026 It\u2019s typically an exercise with a preordained conclusion.\u201d (Sources familiar with Univision\u2019s situation said they heard that cuts of 30 to 40 percent were on the table before BCG was brought on, and that the consultants acted according to instructions management gave them. Univision declined to answer specific questions about the time frame for the budget cut decisions.) Advertisement  McKinsey works under strict nondisclosure agreements with its clients, McDonald added, and consultants at other firms reached by GMG said that they do the same. \u201cThey sell the credit for their ideas,\u201d McDonald said. \u201cThey take no credit publicly. But they also accept no blame. It\u2019s power with no responsibility.\u201d While it\u2019s unclear how much Univision is paying BCG for its services, the cost for such a long-term consulting engagement runs well into the millions. McKinsey charges clients flat fees, McDonald said, adding, \u201cThey don\u2019t really play small ball.\u201d Advertisement  With Lee defenestrated, the board has trained its eye on the parts of the company he oversaw, and on lower-level employees there. While the company spends millions on consultants, Fusion TV is essentially extinct and Univision digital has been slashed despite overriding wisdom that says digital media is the future for the flailing giant. Univision news still faces cuts, as does GMG. Perhaps you\u2019re curious what\u2019s going to happen to us in light of all this; we are as well. Sameer Deen, head of digital for Univision, who has been acting in place of Felipe Holgu\u00edn as Fusion Media Group\u2019s CEO and is thus in charge of GMG, has said in meetings with reporters, editors, and union officials that \u201cchanges\u201d could come to GMG in late June, after the consultants have time to complete a review. He has repeatedly declined to specify what they may look like. There is nothing like Gizmodo Media Group on the internet. While that\u2019s mainly a function of what we do\u2014publish true and interesting things that others can\u2019t or won\u2019t\u2014it\u2019s also a function of what we don\u2019t do. GMG doesn\u2019t find itself forced to deny that it is acting to defraud advertisers. GMG\u2019s bread and butter has never been quirky listicles, nonsensical quizzes, and cynical plays at gaming social algorithms. GMG never embraced a distribution strategy that left it at the mercy of merciless platforms like Facebook. GMG isn\u2019t reliant on sponsored content disguised as news. GMG doesn\u2019t use unpaid and underpaid labor to prop up our entire business model. Advertisement  What GMG does do is draw a large and growing audience for its journalism\u2014according to ComScore, it drew 58 million unique U.S. visitors this March\u2014and make money off it. (Revenues are believed within GMG to have been up by a double-digit percentage last year; Univision declined to comment.) It is lean\u2014GMG\u2019s staff of about 200 pales in comparison to other digital media staffs\u2014and, according to a statement Univision issued in response to questions from GMG, \u201cthriving.\u201d BuzzFeed\u2019s news division alone is about 300 people, and that doesn\u2019t include BuzzFeed entertainment and BuzzFeed media brands, which include the likes of Tasty and Nifty. BuzzFeed\u2019s total staff is roughly 1,700 people. Not only is GMG lean, but it is in an increasingly strong position relative to competitors, in part because of its relative lack of reliance on Facebook. According to ComScore, Buzzfeed\u2019s U.S. unique visitors were at 67 million this March, down nearly 20 percent from March 2017; GMG traffic is up by a third over the same period. Why, then, is the company facing deep cuts? An answer has never quite been given, but according to Univision, while GMG\u2014whose predecessor, Gawker Media, was characterized upon its acquisition in August 2016 as a \u201cconsistent, compelling and profitable digital media business\u201d in a company-wide email signed by Isaac Lee\u2014may be thriving, it did not turn a profit last year. Advertisement  Citing various reasons, Univision has declined to share any internal numbers. It\u2019s therefore unclear how large the losses were or what might have caused them. How much those losses have to do with the intricacies of corporate accounting, or with GMG having been made to integrate with Fusion digital\u2019s struggling and top-heavy video operation, whose leadership has since been cleared out, is unknown. The question of profitability is, in some ways, a red herring. The corporate priority for GMG in 2017 was rapid expansion\u2014though what that scale would look like was never quite defined to the newsroom\u2014clearly tied to hopes of a capital raise. (A Univision spokeswoman declined to comment on whether GMG met the goals Univision set for it in 2017 and whether its revenues were up. Univision chief revenue officer Tonia O\u2019Connor was not made available for an interview.) Univision bought Gawker Media in 2016 with the intent of expanding its reach into English-language digital media ahead of its planned IPO, but didn\u2019t appear to understand the company it bought. It still seems not to. To GMG staffers, the clearest sign of this is probably just that Deen is now in charge of the operation. His conversations with staffers have, they say, showed no apparent familiarity with what GMG does or how it works, or with the distinction between business and editorial. He also has little in the way of a relevant background. Before joining Univision in 2015, he worked in \u201ccorporate development\u201d and \u201cmultiplatform distribution and strategy\u201d for Scripps; since, he has overseen Univision properties which between them do about a tenth the collective traffic of GMG sites, according to a ComScore analysis. Not only does nothing about this suggest readiness to take over a major digital property, but one anecdote relayed by various sources reveals startlingly poor judgment. Advertisement  Univision Deportes\u2019 website has an entire section devoted to what is essentially softcore porn, offering laughably thin connections to sports as pretexts for running it. This very horny section, called \u201cSensaci\u00f3n,\u201d features a wide array of photo galleries, the subjects of which range from male athletes\u2019 girlfriends dressed in skimpy clothing to male athletes\u2019 wives dressed in skimpy clothing to certain sports teams\u2019 fans who just happen to be women dressed in skimpy clothing. This post, which is typical, consists of 26 photos of a Liverpool fan, each with the same vapid caption about how star player Mo Salah owes her a championship. After digital editors decided to stop running these posts for four months in 2017 because they weren\u2019t consistent with the organization\u2019s journalistic standards, Deen decided, we are told, that the bikini-clad women had to return because traffic had dropped too much without them. It should be noted that these posts aren\u2019t even especially popular\u2014according to Chartbeat data, posts tagged Sensaci\u00f3n Deporte accounted for 176,861 page views in April, whereas the overall total for posts tagged Deportes was 7,470,423. (Univision declined to comment on Sensaci\u00f3n; Deen did not respond to a request for comment.) No one at GMG labors under the delusion that Deen is the real power here; any potential \u201cchanges\u201d seem likely to play out as they did at Univision digital, where editors decided who to let go after being told how much of their budget they needed to cut, with the nature of those cuts decided ultimately by some opaque combination of the board and the consultants the board has hired. Still, Deen\u2019s leading role in the decision-making process about cuts and the future of the enterprise\u2014a process in which O\u2019Connor and GMG editors will also take part\u2014does not inspire confidence in the staff. And the fact that Univision is considering cuts at all infuriates GMG staff because the company is healthy and successful. Advertisement  While Univision has invested in GMG, the company\u2019s success is in many ways despite Univision, not because of it. GMG employees have described nightmare scenarios in which they had to fight to get their newborn babies on their healthcare plans or haggle for months to enroll in other basic benefits. One employee became suddenly ill shortly after being hired, and the company failed to provide interim healthcare coverage while the employee waited to receive their health insurance card. Last month, a day before the national tax deadline, employees received an email from human resources director Brian Ford, who had recently been hired to fix chronic HR problems, warning them that there had been \u201can error in the reporting of your Roth contributions on your 2017 W2 Form\u201d and that employees would have to refile their taxes. Shortly after that, the company failed to pay employees promised performance bonuses on time, apparently because of a clerical error. And for many staffers, an under-resourced newsroom is not a looming prospect but a present reality: Due to recent austerity measures, one GMG site had to put two job offers it had already made on hold, and another functioned without an official editor-in-chief for nearly half a year before pressure from the union helped force a promotion. The fate of GMG now rests in the hands of Univision, an indulgently run and suicidally structured company. In turn, Univision\u2019s fate rests in the hands of impatient private equity vampires who are not concerned with the long-term damage to revenue streams that could result from slashed budgets, or with damage to the company\u2019s connection  with tens of millions of Spanish speakers\u2014or with journalism, or with anything, as far as anyone can tell\u2014but with their desire to get out of a bad deal with as little damage as possible. This is their right; the people who work for Univision and its subsidiaries are simply unlucky enough to be employed under an economic system that considers the livelihoods of thousands to be less valuable than the panic and avarice of a few fathomlessly wealthy people. Advertisement  There is no clear way forward for media organizations today, even ones with large audiences and sustainable business models. Google and Facebook have solidified their stranglehold on digital advertising, and even the deep-pocketed likes of Vice and BuzzFeed have missed revenue targets amid this shift. The subscription-based model used by the New York Times is not feasible for most publications, where newsrooms are whittled down to component parts, and audiences expect free content. Startups like the ad-free, subscription-only website The Athletic are using investors\u2019 money, which will eventually run out, to hire at seemingly unsustainable rates. Conglomerates like Sinclair Broadcast Group and Gannett have snapped up local outlets to cut costs through synergies, threatening those outlets\u2019 individual voices with a corporate political project and standardized content, respectively. Time Inc. was recently sold to the Meredith Corporation, which plans to chop up its vaunted brands and try to sell them off one by one. Companies like Vox Media have stayed afloat thanks in no small part to unpaid and underpaid labor. The entire industry is slimy and shadowy and fucked. Having a multibillionaire patron like Jeff Bezos, who owns the Washington Post, is a viable way to survive, but even that model relies on the whims of the patron. In Los Angeles, Patrick Soon-Shiong, the multibillionaire medical entrepreneur finalizing his purchase of the Los Angeles Times, says all the right things about protecting journalism as a civic institution. But having a wealthy owner subjects a publication to a complex web of potential conflicts of interest\u2014take casino magnate and GOP donor Sheldon Adelson\u2019s purchase of the Las Vegas Review-Journal in late 2015. Looming above such billionaire ownership is the question of where a patron\u2019s philanthropic bent ends and their personal quest for money or power begins. After hedge-fund vulture Alden Global Capital bought the Denver Post and slashed its budget until a fraction of the newsroom was left, the paper revolted publicly, publishing editorials calling for the company to sell the newspaper to an owner who cares about journalism. Advertisement  This isn\u2019t heartening for GMG and other editorial divisions of Univision, like the depleted Noticias. The private equity firms are desperately trying to exit the investment with a profit. Even if they do cut into bone to make the company look appealing to would-be buyers, what\u2019s left might be too spare to keep revenues up, putting the company at risk of a major debt restructuring. In the meantime, the board sends executives into the GMG newsroom to tell skeptical employees to their faces that they care about the journalism here and that even though GMG has grown, as asked, year over year, \u201cgrowth is only one component\u201d in making tough choices. While Sameer Deen didn\u2019t create this situation, his communication missteps and inability to provide hard information to people who don\u2019t know if they should start looking for new jobs have incensed GMG staffers. Deen, and now O\u2019Connor, have been meeting individually and collectively with the heads of all GMG sites, but these marathon meetings haven\u2019t done much to dispel the notion that they\u2019re simply going through the motions while consultants make their recommendations and Univision prepares to hand out pink slips in June. For its part, Univision said in a statement, which can be read in full here: At UCI, we have a firm commitment to support and nurture fierce, independent journalism. Our award-winning news teams have a legacy of providing direct and unfiltered access to news and information and we are committed to maintaining that spirit of vibrant journalism across our portfolio, including the Gizmodo Media Group. We continue to believe GMG is a valuable asset and important brand that will keep thriving. GMG attracts loyal and highly engaged audiences, both of which are unique and incredibly important in this landscape. Advertisement  Univision isn\u2019t the only company to falter under predatory private equity debt-loading, and its employees are hardly the first in media to face months of existential dread as higher-ups with no experience in or concern with journalism decide their fate. Even if we survive this round of cuts relatively unscathed, our unstable, bumbling, debt-ridden corporate parent can decide to shut us down or turn these sites into a hellscape of branded content and inane social video at any point. But they can\u2019t do it in the dark. Editing and additional reporting by Nona Willis Aronowitz, Aleksander Chan, Kashmir Hill, Daniel King, Tim Marchman, and Jack Mirkinson. Kate Conger is a senior reporter at Gizmodo.  PGP Fingerprint: 7E46 6F71 1A4F 93C4 D254 460E A4B9 87D2 6F65 FF75 I write about media for Splinter. I have redeeming qualities, too.  Reporter at Deadspin.","time":1525790633,"title":"Univision is a fucking mess","type":"story","url":"https:\/\/specialprojectsdesk.com\/univision-is-a-fucking-mess-1825836622","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17021206,"kids":"None","score":1,"text":"6 Min Read (Reuters) - Walmart Inc\u2019s (WMT.N) online grocery delivery partnerships with ride-hailing services Uber and Lyft have ended, according to two sources, a potential setback for the retailer\u2019s ambitions to challenge Amazon.com Inc (AMZN.O) head-on with speedy delivery of groceries to people\u2019s homes.  The end of the Walmart partnerships, which has not been previously reported and was confirmed by Walmart and Uber, undercuts a vision the ride-hailing companies laid out: a service that can efficiently deliver anything on-demand, including people and cargo, at the touch of a smartphone app.   \u201cIt is incredibly hard to deliver people and packages together,\u201d said a source with a delivery company that works with Walmart and has direct knowledge of the matter. \u201cThey are two completely different business models.\u201d  The decision marks an abrupt end to a business relationship that Walmart and Uber announced with much fanfare less than two years ago. At Walmart's shareholders meeting in June 2016, CEO Doug McMillon touted the company's investments in technology and spoke about the partnerships in front of a cheering crowd of 14,000 employees. goo.gl\/xJdN2e  Soon after, Uber\u2019s grocery delivery service was launched and expanded to four markets. As recently as March, just before Uber ended the arrangement, Walmart said Uber would be a partner in its plans to deliver groceries to more than 40 percent of the country.  \u201cThere was clearly some lack of communication there,\u201d said one of the sources with knowledge of the partnerships ending.  Walmart spokeswoman Molly Blakeman confirmed the end of the tie-ups when asked by Reuters, but did not detail the reasons behind the decision. She said Walmart will use other delivery service providers in the four markets where it had previously used Uber.   \u201cCustomers shouldn\u2019t notice any difference as the transition takes place,\u201d said Blakeman, who added that the partnership with Lyft never expanded beyond the initial test market of Denver.  Blakeman said the end of the partnerships will not impact Walmart\u2019s plans to scale grocery delivery as they are not tied to any single provider.   Uber put a stop to the grocery partnership when it informed Walmart in March that it would cease delivery operations on June 30, Uber spokeswoman Ellen Cohn told Reuters. The retailer was Uber\u2019s largest partner for its \u2018Rush\u2019 service, which delivered groceries as well as clothes, flowers and other goods.   Uber will shutter the entire Rush program at the end of next month.  \u201cWe are coordinating with Walmart to make this change as seamless as possible,\u201d Cohn said.   Lyft declined comment and deferred to Walmart on the issue.  For Walmart, which is the country\u2019s largest grocer and gets 56 percent revenue from groceries, the partnerships offered a fast solution to expand its online grocery offerings and improve overall revenue from internet shoppers.   For example, Walmart delivers groceries in China through a partnership with ecommerce company JD.com Inc (JD.O), and in Japan through an alliance with Rakuten.   But the retailer was recently punished for its fourth-quarter online sales performance, which investors say is key to the company\u2019s future.  LAST-MILE COMPETITION  Last-mile delivery of packages is an intensely competitive business, with companies ranging from Amazon to United Parcel Services Inc (UPS.N), FedEx Corp (FDX.N) and the U.S. Postal service, as well as startups like Instacart and Deliv, vying for a share.  Since the dot-com boom, companies have tried to crack the business model for online grocery delivery. The rush to solve the technological and logistical challenges has gotten even more frenzied since Amazon acquired high-end grocery chain Whole Foods Market Inc for $13.7 billion last year, a deal that has intensified competition in the sector.  Former Uber Chief Executive Travis Kalanick touted the idea of carrying a person in the backseat and a bag of groceries in the trunk as the ultimate cash-generating transportation service in a smart-phone era.  The delivery service marked the first time Uber publicly committed to a business outside of ride-hailing that was supposed to be meaningful to its bottom line and support its stratospheric valuation, although the private company never offered exact dollar projections.   But startup investors and experts in on-demand delivery say there is a much different set of logistical and economic challenges for moving around cargo than people, requiring a single company to be proficient in two distinct business models.  Uber\u2019s Cohn said Rush was \u201can experiment\u201d and the company has turned its focus and resources to UberEats, a restaurant delivery service that in the fourth quarter last year generated $1.1 billion, or about 10 percent of Uber\u2019s overall revenue.  Walmart has added startups Deliv, Postmates and DoorDash to its list of delivery partners. These companies have the singular business of delivering goods, not people, and drivers have more experience safely transporting perishables.  It remains unclear if these startups will step in and replace Uber in the various markets they served.   \u00a0\u00a0\u00a0 A particular challenge for companies such as Postmates, however, will be offering rush delivery in suburban and rural areas, where most Walmart stores are located. Such startups have been most successful in urban centers, where there is a high density of customers and couriers can use bicycles or walk to deliver multiple packages in one trip.  \u201cDensity has been a challenge historically for all types of delivery companies, all the way back to the Pony Express,\u201d said Ben Narasin, a partner at venture capitalist firm NEA who has been critical of the on-demand delivery business model. \u201cThe reality is that the far-away drives will likely be subsidized.\u201d   Reporting by Nandita Bose in New York and Heather Somerville in San Francisco; Editing by Vanessa O'Connell and Edward Tobin All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. \u00a9 2018 Reuters. All Rights Reserved.","time":1525790535,"title":"Walmart's grocery delivery partnerships with Uber, Lyft fail to take off","type":"story","url":"https:\/\/www.reuters.com\/article\/us-walmart-grocery-delivery-exclusive\/exclusive-walmarts-grocery-delivery-partnerships-with-uber-lyft-fail-to-take-off-idUSKBN1I91S4","label":7,"label_name":"random"},{"by":"coloneltcb","descendants":84,"id":17021205,"kids":"[17022192, 17021811, 17022270, 17023534, 17021517, 17021628, 17021561, 17022234, 17023576, 17022957, 17022097, 17023714, 17024421, 17022924, 17022015, 17021936, 17021906, 17022126, 17022017, 17022137, 17022618, 17021515]","score":230,"text":"Bradley Jacobs, CEO of XPO Logistics  ","time":1525790529,"title":"How Mapbox Is Winning Over Developers to Challenge Google's Mapping Dominance","type":"story","url":"https:\/\/www.forbes.com\/sites\/bizcarson\/2018\/05\/08\/mapbox-maps-developers\/#79431e78164d","label":7,"label_name":"random"},{"by":"Eurongreyjoy","descendants":0,"id":17021181,"kids":"None","score":2,"text":"Facebook  has announced it has stopped accepting ads paid for by foreign entities that are related to a referendum vote in Ireland later this month, saying it\u2019s acting to try to prevent outsiders from attempting to skew the vote. The referendum will decide whether to repeal or retain Ireland\u2019s constitutional ban on abortion. \u201cConcerns have been raised about organisations and individuals based outside of Ireland trying to influence the outcome of the referendum on the Eighth Amendment to the Constitution of Ireland by buying ads on Facebook. This is an issue we have been thinking about for some time,\u201d the company\u00a0writes today on its Dublin blog. \u201cToday, as part of our efforts to help protect the integrity of elections and referendums from undue influence, we will begin rejecting ads related to the referendum if they are being run by advertisers based outside of Ireland.\u201d Facebook says it\u2019s stopping foreign-funded ads because additional ad transparency and election integrity tools it has in the works \u2014 and is intending to roll out more widely, across its platform \u2014 will not be ready in time for Ireland\u2019s Eighth Amendment vote, which will take place on May 25. \u201cWhat we are now doing for the referendum on the Eighth Amendment will allow us to operate as though these tools, which are not yet fully available, were in place today with respect to foreign referendum-related advertising. We feel the spirit of this approach is also consistent with the Irish electoral law that prohibits campaigns from accepting foreign donations,\u201d Facebook writes. \u201cThis change will apply to ads we determine to be coming from foreign entities which are attempting to influence the outcome of the vote on May 25. We do not intend to block campaigns and advocacy organisations in Ireland from using service providers outside of Ireland,\u201d it adds. The social media\u2019s ad platform has been under increasing political scrutiny since revelations emerged about the extent of Kremlin-backed disinformation campaigns during the 2016 US presidential election. And last year\u00a0Facebook admitted Kremlin-backed content \u2014 including, but not limited to, Facebook ads \u2014 may have reached as many as 126 million people during the election period. Concerns have also been raised about the role of its platform during the UK\u2019s 2016 referendum on EU membership \u2014 with an investigation into social media and campaign spending ongoing by the UK\u2019s Electoral Commission, and another \u2014 by the UK\u2019s data watchdog, the ICO \u2014 also\u00a0looking more broadly at the use of data analytics for political purposes. At the same time, a\u00a0major Facebook data privacy scandal that erupted in March, after fresh details were published about the use of user data by a controversial political consultancy called Cambridge Analytica, has further dialed up the pressure on the company as lawmakers have turned their attention to the messy intersections of social media and politics. Of course Facebook is by no means the only place online where all sorts of foreign agents have been caught seeking to influence opinions. But the Cambridge Analytica scandal has illustrated the powerful lure of the platform\u2019s reach (and data holdings), as well as underlining how lax Facebook has historically been in controlling the messages people are paying it to target at its users. In Ireland, the company had already fast-tracked the rollout of its \u2018view ads\u2019 ad transparency tool \u2014 ahead of a wider global rollout planned for this summer. And\u00a0last month\u00a0policy staffers told a local parliamentary committee that the tool would help eliminate \u201cforeign interference\u201d in the upcoming referendum. Although clearly Facebook has decided that an additional stop-gap measure \u2014 i.e. of rejecting foreign funded ads \u2014 was also needed given the timing (and indeed the sensitivity) \u2014 of the\u00a0Eighth Amendment vote. Last month Facebook also trailed\u00a0plans to require advertisers\u00a0that run popular Pages and\/or are trying to run ads with political messages to verify their identity and location. But those advertiser verification steps do not appear to be ready in time for Ireland\u2019s referendum. (Nor indeed were they in place for local elections in the UK earlier this month\u00a0\u2014 although in a referendum the risks to democracy from a skewed vote are arguably higher, given there\u2019s no established process for a re-vote in a few years\u2019 time.) The simpler-to-implement \u2018view ads\u2019 tool launched in Ireland on April 25, according to Facebook, which makes it the second market after Canada \u2014 where it began testing the feature. The company claims the tool \u201cenables Irish Facebook users to see all of the ads any advertiser is running on Facebook in Ireland at the same time\u201d \u2014 though clearly ad visibility is not enough of a barrier against election fiddling on its own. Facebook also says it will be using machine learning technology to help it identify ads that \u201cshould no longer be running\u201d. And it\u2019s supplementing these AI checks with human review, saying it\u2019s built relationships with \u201cpolitical parties, groups representing both sides of the campaign and with the Transparent Referendum Initiative\u201d \u2014 and is asking them to notify it if they have concerns about ad campaigns so it can assess and act on their reports, having established a dedicated reporting channel for this purpose. Last month it also says it hosted an information session about its advertising and content policies for referendum campaign groups. \u201cWe understand the sensitivity of this campaign and will be working hard to ensure neutrality at all stages. We are an open platform for people to express ideas and views on both sides of a debate. Our goal is simple: to help ensure a free, fair and transparent vote on this important issue,\u201d it adds. In addition to view ads and the decision to stop accepting foreign-funded referendum ads, Facebook says it is deploying its \u201cElection Integrity Artificial Intelligence\u201d\u00a0for the vote in Ireland, as part of its efforts to identify fake accounts, misinformation and\/or foreign interference \u2014 describing its approach as similar to what it did in advance of recent elections in France, Germany and Italy. Last month its policy staffers also said it had set up an internal task force to handle the Ireland referendum.","time":1525790368,"title":"Facebook stops accepting foreign-funded ads about Ireland\u2019s abortion vote","type":"story","url":"https:\/\/techcrunch.com\/2018\/05\/08\/facebook-stops-accepting-foreign-funded-ads-about-irelands-abortion-vote\/","label":7,"label_name":"random"},{"by":"artsandsci","descendants":0,"id":17021176,"kids":"None","score":2,"text":"\nHome\n \nReviews\n \nNews\n \nBlogs\n \nImages\n Mobile Site \nSitemap\n \nComponents\n \nSystems\n \nIT\/Enterprise\n \nMobile\n \nLeisure\n \nVideos\n \nAbout\n \nAdvertise\n \nNews Tips\n \nContact\n \nHotTech Vision And Analysis\n \nForums\n \nShop\n \nTwitter\n \nFacebook\n \nGoogle+\n \nYouTube\n \nFeedburner\n","time":1525790333,"title":"HP Announces Ryzen Pro-Powered EliteBook 705 G5 and ProBook 645 G4 Laptops","type":"story","url":"https:\/\/hothardware.com\/news\/hp-announces-ryzen-pro-elitebook-705-g5-probook-645-g4-laptops","label":0,"label_name":"biz-news"},{"by":"31z4","descendants":0,"id":17021174,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back  \n\n surge is a fast and reliable Amazon Glacier multipart uploader and downloader. Amazon Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup. For more information about Glacier, see its official documentation. Either download the latest release for your platform or install it via go get: First, you need to create a Glacier vault where you're going to upload an archive. Suppose you want to upload a file called my-archive to my-vault. Make sure you save the upload location somewhere, so that you can download the archive later. If you do not specify the -upload-id option, surge initiates a new upload and outputs its ID. If an upload was interrupted due to a network error or any other reason you can resume it given that you have the upload ID. Upon that process surge will check for already uploaded parts and will only upload what's changed or not uploaded. First, you need to initiate an archive retrieval job given that you have the archive ID. For more information about the archive retrieval process, see the official documentation. After the archive retrieval job completes, download the archive. Resuming an interrupted download will be implemented in the upcoming releases. Contributions are greatly appreciated. The project follows the typical GitHub pull request model. Before starting any work, please either comment on an existing issue or file a new one. This project is licensed under the MIT License - see the LICENSE file for details.","time":1525790323,"title":"Show HN: Fast and reliable Amazon Glacier uploader and downloader written in Go","type":"story","url":"https:\/\/github.com\/31z4\/surge","label":4,"label_name":"github"},{"by":"artsandsci","descendants":0,"id":17021171,"kids":"None","score":2,"text":"Uber Technologies Inc on Monday\u00a0said it has hired a former U.S. regulator to advise the company\u00a0on safety, but would not confirm a technology website's report\u00a0that a software flaw was responsible for a fatal accident\u00a0involving one of its self-driving cars in March. The Information reported that Uber has determined the likely\u00a0cause of the fatal collision in March was a problem with the\u00a0 software that decides how a self-driving car should react to\u00a0objects it detects. The outlet said the car's sensors detected a\u00a0pedestrian but the software decided it did not need to react\u00a0right away. Uber declined to comment on the report. A 49-year-old woman was killed on March 18 after an Uber\u00a0self-driving sports utility vehicle hit her while she was\u00a0 walking across a street. The incident led the ride-share company\u00a0to suspend testing of autonomous vehicles. Arizona's governor\u00a0also ordered a halt to Uber's testing. \"We can't comment on the specifics of the incident,\" the\u00a0company said, citing an ongoing investigation by the National\u00a0Transportation Safety Board (NTSB). The company said it was looking at its self-driving program. \"We have initiated a top-to-bottom safety review of our\u00a0self-driving vehicles program, and we have brought on former\u00a0 NTSB Chair Christopher Hart to advise us on our overall safety\u00a0culture,\" Uber said. \"Our review is looking at everything from\u00a0the safety of our system to our training processes for vehicle\u00a0operators, and we hope to have more to say soon.\" In a video of the crash released by police, the vehicle\u00a0appeared not to brake before it struck the woman. The NTSB is expected to issue a preliminary report on the\u00a0Arizona Uber crash in the coming weeks. \u00a0 The National Highway Traffic Safety Administration (NHTSA)\u00a0is also investigating the incident and declined to comment. Chief Executive Officer Dara Khosrowshahi said in April that\u00a0Uber still believes in prospects for autonomous transport.\u00a0 \"Autonomous (vehicles) at maturity will be safer,\" he said at a\u00a0Washington event. Hart was chairman of the NTSB when it opened a probe into a\u00a0fatal Tesla crash involving a driver using the system's\u00a0 Autopilot system. Hart said in 2016 that self-driving cars will not be\u00a0perfect. \"There will be fatal crashes, that's for sure,\" Hart said,\u00a0but added that will not derail the move toward driverless cars.\u00a0 \"This train has already left the station.\"\u00a0  To encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC\/Radio-Canada's online communities (except in children and youth-oriented communities). Pseudonyms will no longer be permitted. By submitting a comment, you accept that CBC has the right to reproduce and publish that comment in whole or in part, in any manner CBC chooses. Please note that CBC does not endorse the opinions expressed in comments. Comments on this story are moderated according to our Submission Guidelines. Comments are welcome while open. We reserve the right to close comments at any time. Audience Relations, CBC P.O. Box 500 Station A Toronto, ON  Canada, M5W 1E6  Toll-free (Canada only):  1-866-306-4636 TTY\/Teletype writer:  1-866-220-6045 It is a priority for CBC to create a website that is accessible to all Canadians including people with visual, hearing, motor and cognitive challenges. Closed Captioning and Described Video is available for many CBC-TV shows offered on CBC Watch.","time":1525790312,"title":"Uber won't confirm if software caused self-driving car to kill pedestrian","type":"story","url":"http:\/\/www.cbc.ca\/news\/technology\/uber-self-driving-collision-1.4653129","label":6,"label_name":"news"},{"by":"dorothyat40","descendants":2,"id":17021169,"kids":"[17022021, 17021565]","score":6,"text":"Let\u2019s get meta for a minute. When it comes to planning, what\u2019s your team\u2019s plan? Meetings generally drag on too long and they tend to ignore or leave unanswered all of the important questions that you\u2019ll then have to have another meeting to discuss later. Ideally, we could skip all that.  If your team runs a tight ship, you can run quick and efficient sprint planning meetings. You can eliminate confusion and questions. And you can plan your sprint in a more accurate and predictable way. Here\u2019s a step-by-step guide on running better sprint planning meetings. Any agile practitioner knows that the most critical component of kicking off each new iteration is to pin down a specific goal that the team hopes to accomplish over the coming week(s). The stated goal shouldn\u2019t be technical and detailed. We want to begin with a high-level objective that we want to accomplish, and then explore the specifics of what that means in terms of actual work. There could be multiple goals for the sprint, such as: Goals for the sprint usually come from the Product Owner and then are given to the team as a way to begin the conversation about the specific scope of work to be completed. Keep in mind that a goal is just that\u2013it\u2019s a goal. It\u2019s what the team would like or hopes to accomplish. But it shouldn\u2019t be dictated as a certainty. If the rest of the planning process reveals that the goal is unrealistic, then it should be adjusted as necessary. Goals are great. But can your team get it done? The team\u2019s capacity is often in flux. Although we may assume that the team remains relatively static, calendar items like meetings, vacations, and holidays can change the amount of work that is realistic to complete in the given time period. The capacity, goal, and scope are the three main conditions that must be balanced in order for a sprint to be successful.  An overambitious goal may force a team to exceed their capacity. And, likewise, a diminished capacity can make it impossible to achieve the full scope for that iteration. The main goal of the sprint planning meeting should be to resolve these to create a sprint plan that is both achievable and efficient. The third component of any sprint plan is the specific scope of work. This comprises all of the user stories and individual tasks or product backlog items that need to be completed in order to satisfy the stated goal. In other words, this is the stuff that the developers will actually get done. For most teams, this is a matter of pulling things out of the backlog based on the stated sprint goal and priority, then piecing them together to fill the available capacity. If things are well organized, this shouldn\u2019t be a monumental task. The key thing to remember\u2013and where there is sometimes a disconnect in this process\u2013is that the team must ultimately decide and agree on how much work they can complete within that sprint or iteration. It can be tempting for the Scrum Master or Product Owner to look at a roadmap and try to drive this conversation (e.g., \u201cwe need to have this new feature done within the next 2 sprints, so we need to include half of the work now.\u201d) But this is backward and will lead to failed sprints. Instead, each backlog item or user story should be discussed and scoped into specific tasks that can be assigned story points, effort, and\/or time estimates. Again, we see that the point of the sprint planning meeting should be to balance the goal, scope, and capacity. If there simply isn\u2019t enough time or capacity to complete all of the tasks that would lead to a successful sprint, then there needs to be a negotiation about the stated goal. One of the major pitfalls that leads to failed sprints is the use of intuition. To put it in scientific terms, teams tend to rely on what Daniel Kahneman dubbed \u201cSystem 1\u201d thinking rather than \u201cSystem 2\u201d thinking. In other words, teams look at the scope of work, the specific tasks, and the goal, and they go with their gut\u2013either they can do it or they can\u2019t.  But there\u2019s a big problem. System 1 thinking is inherently flawed. It\u2019s prone to logical errors and overestimation. This leads to what\u2019s commonly known as The Planning Fallacy. We tend to bite off more than we can chew. We\u2019ve written previously about why time and work estimates often don\u2019t match reality. This is where those issues are born. Everyone is excited and confident at the beginning of an iteration and they set ambitious goals for what they can accomplish. But they never bother with considering any empirical data on whether that is a reasonable amount of work. In addition to helping developers gauge their own abilities, this is where time tracking can be an extremely valuable tool to root estimates in some level of real-world data. It may tempting to trust your instincts on how long certain tasks might take or how difficult they will be. But, at the end of the day, this is just guessing. And it leads to failed sprints. Instead, do a sanity check. Take the time to analyze your past performance as an individual developer and as a team. Compare the upcoming work to similar types of projects in the past and evaluate how closely your current estimates line up with past reality. This step will save a ton of headache and frustration later down the line. Developers pretty much universally agree that meetings suck. They often take too long and accomplish too little. The goal of any sprint planning meeting should be to minimize the amount of time that it takes to relay and agree on the necessary information. That\u2019s it. No more, no less. If the team is able to do this and can find a quick rhythm where everyone understands and anticipates what part they need to play in the planning process, then it becomes second nature. The team can get in, get out, and get back to work. Sprint planning doesn\u2019t need to be difficult\u2013nor time consuming. But, the team should also resist the temptation to cut the meeting short just for the sake of expediency. This is a critical part of any software development cycle and it sets the pace and expectation for the coming weeks. It\u2019s worth spending a little extra time to make sure you get it right. With the right plan and the right approach, you can have better, smarter, and quicker sprint planning meetings without setting the team up for failure. 39 Proven Strategies,Tips & Hacksfor Building a World-Class Software Development Team DOWNLOAD NOW By Tyler Hakes   Back when I worked in agencyland, capacity and time tracking were the most overlooked, underestimated factors in a project.  The POs and PMs I know now would say the same. When a project goes off the rails, it can too often be traced back to one or both of those issues. Your email address will not be published. Required fields are marked *  \n\n   Try for free, with your entire team. Get Timetracker Learn something Free eBook 39 Proven Strategies,\nTips & Hacks \nfor Building a World-Class\nSoftware Development Team DOWNLOAD NOW Follow us","time":1525790298,"title":"Sprint Planning Meetings: A Guide to Better, Smarter, More Efficient Iterations","type":"story","url":"https:\/\/www.7pace.com\/blog\/sprint-planning-meetings-a-guide-to-better-smarter-more-efficient-iterations","label":7,"label_name":"random"},{"by":"cmatthieu","descendants":0,"id":17021161,"kids":"None","score":2,"text":"","time":1525790225,"title":"Two Apple Macs cracked a 5 letter \u201capple\u201d password in 48 seconds via computesio","type":"story","url":"https:\/\/www.youtube.com\/watch?v=_csMpIEJLxo","label":7,"label_name":"random"},{"by":"petethomas","descendants":1,"id":17021159,"kids":"[17021177]","score":1,"text":"7 Min Read  FRANKFURT\/BRUSSELS\/PARIS (Reuters) - Europe\u2019s General Data Protection Regulation (GDPR) has been billed as the biggest shake-up of data privacy laws since the birth of the web.  There\u2019s one problem: many of the regulators who will police it say they aren\u2019t ready yet.  The pan-EU law comes into effect this month and will cover companies that collect large amounts of customer data including Facebook (FB.O) and Google (GOOGL.O). It won\u2019t be overseen by a single authority but instead by a patchwork of national and regional watchdogs across the 28-nation bloc.  Seventeen of 24 authorities who responded to a Reuters survey said they did not yet have the necessary funding, or would initially lack the powers, to fulfill their GDPR duties.  \u201cWe\u2019ve realized that our resources were insufficient to cope with the new missions given by the GDPR,\u201d Isabelle Falque-Pierrotin, president of France\u2019s CNIL data privacy watchdog, said in an interview.  She, like some other regulators, was pressing her government for a substantial increase in resources and staff.  Many watchdogs lack powers because their governments have yet to update their laws to include the Europe-wide rules, a process that could take several months after GDPR takes effect on May 25.  Most respondents said they would react to complaints and investigate them on merit. A minority said they would proactively investigate whether companies were complying and sanction the most glaring violations.  Their responses suggest the GDPR enforcement regime will be weaker than the bloc's anti-trust authority run directly by the European Commission, the EU executive, which hit Google here with a 2.4-billion-euro ($2.9 billion) fine last year.  The launch of GDPR comes as data privacy is making headlines, with Facebook facing intense scrutiny over the leak of 87 million users\u2019 personal data to Cambridge Analytica, a political consultancy that advised U.S. President Donald Trump\u2019s election campaign.   \u00a0HEAVYWEIGHTS IN IRELAND  The law aims to give EU citizens more rights to control over their online information. It has a slew of technically demanding requirements, and threatens fines of up to 4 percent of a company\u2019s annual revenue for serious infringements.  Companies, for example, must be able to provide European customers with a copy of their personal data, and under some circumstances delete it at their behest. They should also report serious data breaches within 72 hours.  The industries most affected will be those that collect large amounts of customer data, including technology companies, retailers, healthcare providers, insurers and banks.  Reuters sent all the regulators a four-question survey about how they would handle their responsibilities. Eighteen national authorities replied, plus data protection officers in six of the 16 German federal states who are responsible for enforcement.  Only five in total said the necessary data protection laws and funding in their jurisdiction were in place. Of the 17 who said they did not have the necessary funding and legislation, 11 expected both to be provided in future.  The new law calls for national watchdogs to assume the lead role in overseeing companies headquartered within their borders.  It does however create a central body, the European Data Protection Board (EDPB), in an attempt to ensure the law is applied consistently across the bloc. The panel would serve both as a forum for regulators and issue binding rulings in disputes.  In the recent Facebook breach case, most regulators have not taken an active role because the firm\u2019s EU headquarters is in Ireland, falling under the country\u2019s Data Protection Commissioner (DPC). Cambridge Analytica is being investigated by the UK Information Commissioner\u2019s Office (ICO).  The DPC of Ireland, which is also home to Google, Apple and Twitter, was among those who declined to take part in the survey, citing the complexity of the issues, as did the UK ICO.  The Irish authority did, however, say its budget and staffing had been ramped up in preparation for GDPR. Yet its funding this year, at 11.7 million euros, works out at less than one-thousandth of Facebook\u2019s annual net income of $15.9 billion.  Johannes Caspar, the data protection commissioner in the German city-state of Hamburg, told Reuters he had had many differences of opinion with the Irish regulator in the past over its handling of Facebook, without giving details.  He also did not see the data protection board as an adequate forum to address issues, calling it \u201ca cumbersome \u2013 and for outsiders certainly opaque \u2013 exercise\u201d.  Italy\u2019s data protection chief Antonello Soro welcomed the pan-European rules as a \u201cguarantee against companies opening \u2018convenience\u2019 establishments in countries\u201d. But its 2018 budget of just under 25 million euros and 122 active staff were inadequate to fulfill its responsibilities, and it would require double the funding and 300 staff.  Regulators largely did not specify what duties might be affected by a lack of resources. Experts expect oversight to be inconsistent at first, with regulators facing tough choices on whether to prioritize outreach work to encourage compliance, or enforcement actions against violators. Working smoothly as a group in the EDPB could also be a challenge.  \u201cI think it will work but it will take time for companies and data protection authorities,\u201d said Joerg Hladjk, counsel for cybersecurity, privacy and data protection at law firm Jones Day. \u201cThey need to try this out in practice.\u201d  Estonia, known as a pioneer of e-governance, had backed a stronger regime enforced by the Commission.  Viljar Peep, head of the Estonian Data Protection Inspectorate, said the quality of enforcement under the chosen local system risked being inconsistent and would depend on the \u201cadministrative culture\u201d of officials, which varied widely.  Some countries, like Estonia, took a broad view of data privacy, engaging with business and society to ensure the new rules are understood and respected, whereas others took a far narrower view, he added.  \u201cAre we supposed to be proactive?\u201d he asked.  ($1 = 0.8386 euros)  Additional reporting by Hans-Edzard Busemann; Writing by Douglas Busvine; Editing by Jonathan Weber and Pravin Char All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. \u00a9 2018 Reuters. All Rights Reserved.","time":1525790208,"title":"European regulators: We're not ready for new privacy law","type":"story","url":"https:\/\/www.reuters.com\/article\/us-europe-privacy-analysis\/european-regulators-were-not-ready-for-new-privacy-law-idUSKBN1I915X","label":6,"label_name":"news"},{"by":"ingve","descendants":0,"id":17021157,"kids":"None","score":1,"text":"This is part 6 in a series of posts on writing concurrent network servers. Parts\n3, 4, and 5 in the series discussed the event-driven approach to building\nconcurrent servers, alternatively known as asynchronous programming. In this\npart, we're going to look at some of the challenges inherent in this style of\nprogramming and examine some of the modern solutions available. This post covers many topics, and as such can't cover all of them in great\ndetail. It comes with sizable, fully-working code samples, so I hope it can\nserve as a good starting point for learning if these topics interest you. All posts in the series: So far the series has focused on a simple state-machine protocol,\nto demonstrate the challenges of keeping client-specific state on the server. In\nthis part, I want to focus on a different challenge - keeping track of waiting\nfor multiple things on the server side. To this end, I'm going to revisit the\nprimality testing server that appeared in part 4, where it\nwas implemented in C using libuv. Here we're going to reimplement this in JavaScript, using the Node.js\nserver-side framework and execution engine. Node.js is a popular server-side\nprogramming environment that brought the asynchronous style of programming\ninto the limelight when it appeared in 2009 [1]. The C code for the original primality testing server is here.\nIt listens on a socket for numbers to arrive, tests them for primality (using\nthe slow brute-force method) and sends back \"prime\" or \"composite\". It\noptionally uses libuv's work queues to offload the computation itself to a\nthread, to avoid blocking the main event loop. Let's reconstruct this server in steps in Node.js, starting with a basic server\nthat does all computations in the main thread (all the code for this post is\navailable here): This is standard Node.js fare; the interesting work happens in the\nonConnData callback, which is called whenever new data arrives on the\nsocket. We're missing a couple of utility functions used by this code - they\nare in utils.js: For testing and demonstration purposes, isPrime accepts an optional\ndelay parameter; if true, the function will sleep for the number of\nmilliseconds given by n before computing whether n is a prime [2]. Naturally, the server shown above is poorly designed for concurrency; it has a\nsingle thread that will stop listening for new clients while it's busy computing\nthe prime-ness of a large number for an existing client. The natural way to handle this is to offload the CPU intensive computation to\na thread. Alas, JavaScript doesn't support threads and Node.js doesn't either.\nNode.js does support sub-processes though, with its child_process package.\nOur next version of the server\nleverages this capability. Here is the relevant part in the new server - the\nonConnData callback: When new data is received from a connected client, this server forks off a\nsub-process to execute code in primeworker.js, sends it the task using IPC\nand attaches a callback on new messages received from the worker. It then cedes\ncontrol to the event loop - so there's no bad blocking happening here.\nprimeworker.js is very simple: It waits for a message on its IPC channel, computes the prime-ness of the number\nreceived, sends the reply and exits. Let's ignore the fact that it's wasteful to\nlaunch a subprocess for each number, since the focus of this article is the\ncallbacks in the server. A more realistic application would have a pool of\n\"worker\" processes that persist throughout the server's lifetime; this wouldn't\nchange much on the server side, however. The important part to notice here is that we have a nested callback within the\nserver's onConnData. The server's architecture is still quite simple - let's\nsee how it handles added complexity. Let's grossly over-engineer our silly primality testing server by adding a\ncache. Not just any cache, but stored in Redis! How about that for a true child\nof the 2010s? The point of this is educational, of course, so please bear with\nme for a bit. We assume a Redis server is running on the local host, listening on the default\nport. We'll use the redis package to talk to it; the full code is here,\nbut the interesting part is this: Let's see what's going on. When a new number is received from the client, we\nfirst check to see if it's already in the cache. This involves contacting the\nRedis server, so naturally it has to be done asynchronously with a callback\nregistered for when the answer is ready. If the number is in the cache, we're\npretty much done. If it's not, we have to spawn a worker to compute it; then, once the answer is\nready we want to write it to the cache. If the write is successful, we return\nthe answer [3]. Taking another look at the last code snippet, we see callbacks nested 3 layers\ndeep. That's inside onConnData, which is itself a callback - so make it 4\nlayers deep. This style of code is so common and notorious in event-driven\nprogramming that it has an epithet - \"callback hell\". The problem is often visualized as this deep, deep callback nest, but IMHO\nthat's not the real issue. Callback nesting is just a syntactic convenience JS\nmakes particularly easy, so folks use it. If you look at the C code\nin part 4, it has a\nsimilar level of logical nesting, but since each function is standalone and\nnot a closure embedded in a surrounding function, it's less visually jarring. The \"just use standalone named functions\" solution has issues too; closures\nhave their benefits - for example they easily refer to values from external\nscopes. In the last code snippet, note how num is used in several nested\ncallbacks but only defined inside onConnData itself. Without this lexical\nconvenience we'd have to pass it explicitly through all the callbacks, and the\nsame for all other common values. It's not the end of the world, but it helps\nexplain why folks gravitate naturally to the tower of nested closures - it's\nless code to type. The bigger issue with this way of programming is forcing programmers into\ncontinuation passing style. It's worth spending some time to explain what I\nmean. Traditional, \"straight-line\" code looks like the following: Let's assume that each of run_* can potentially block, but it doesn't\nconcern us because we have our own thread or something. The flow of data here\nis very straightforward. Now let's see how this would look using asynchronous\ncallbacks: Nothing surprising, but note how much less obvious the flow of data is. Instead\nof saying \"run W and get me an a\", we have to say \"run W and when a is\nready, do ...\". This is similar to continuations in programming language theory;\nI've written about continuations in the past,\nand it should be easy to find tons of other information online. Continuation passing style is not bad per-se, but it makes it harder to keep\ntrack of the data flow in the program. It's easier to think of functions as\ntaking values and returning values, as opposed to taking values and passing\ntheir results forward to other functions [4]. This problem is compounded when we consider error handling in realistic\nprograms. Back to the straight-line code sample - if run_x encounters an\nerror, it returns it. The place where run_x is called is precisely the right\nplace to handle this error, because this is the place that has the full context\nfor the call. In the asynchronous variant, if run_x encounters an error, there's no\nnatural place to \"return\" it to, because run_x doesn't really return\nanything. It feeds its result forward. Node.js has an idiom to support this\nstyle of programming - error-first callbacks. You might think that JS's exceptions should be able to help here, but exceptions\nmix with callbacks even more poorly. The callback is usually invoked in a\ncompletely different stack frame from the place where it's passed into an\noperation. Therefore, there's no natural place to position try blocks. Even though the callback-programming style has some issues, they are by no means\nfatal. After all, many successful projects were developed with Node.js, even\nbefore the fancy new features became available in ES6 and beyond. People have been well aware of the issues, however, and have worked hard\nto create solutions or at least mitigations for the most serious problems. The\nfirst such solution came to standard JS with ES6: promises (also known as\nfutures in other languages). However, long before becoming a standard,\npromises were available as libraries. A Promise object is really just\nsyntactic sugar around callbacks - it can be implemented as a library in pure\nJavascript. There are plenty of tutorials about promises online; I'll just focus on showing\nhow our over-engineered prime server looks when written with promises instead of\nnaked callbacks. Here's onConnData in the promise-based version: There are some missing pieces here. First, the promise-ready versions of the\nRedis client are defined thus: promisify is a Node utility function that takes a callback-based function\nand returns a promise-returning version. isPrimeAsync is: Here the Promise protocol is implemented manually. Instead of taking a\ncallback to be invoked when the result is ready (and another to be invoked for\nerrors), isPrimeAsync returns a Promise object wrapping a function. It\ncan then participate in a then chain of Promises, as usual. Now looking back at the main flow of onConnData, some things become\napparent: Choosing promises over the callback style is a matter of preference; what makes\npromises really interesting, IMHO, is the next step - await. With ES7, Javascript added support for the async and await keywords,\nactually modifying the language for more convenient support of asynchronous\nprogramming. Functions returning promises can now be marked as async, and\ninvoking these functions can be done with await. When a promise-returning\nfunction is invoked with await, what happens behind the scenes is exactly\nthe same as in the callback or promise versions - a callback is registered and\ncontrol is relinquished to the event loop. However, await lets us express\nthis process in a very natural syntax that addresses some of the biggest issues\nwith callbacks and promises. Here is our prime server again,\nnow written with await: This reads just like a blocking version [5], but in fact there is no blocking\nhere; for example, with this line: A \"get\" request will be issued with the Redis client, and a callback will be\nregistered for when data is ready. Until it's ready, the event loop will be\nfree to do other work (like handle concurrent requests). Once it's ready and\nthe callback fires, the result is assigned into cached. We no longer have to\nsplit up our code into a tower of callbacks or a chain of then clauses - we\ncan write it in a natural sequential order. We still have to be mindful of\nblocking operations and be very careful about what is invoked inside callbacks,\nbut it's a big improvement regardless. This post has been a whirlwind tour of some idioms of asynchronous programming,\nadding modern abstractions on top of the bare-bones libuv based servers\nof part 4. This information should be sufficient to understand most asynchronous\ncode being written today. A separate question is - is it worth it? Asynchronous code obviously brings with\nit some unique programming challenges. Is this the best way to handle high-load\nconcurrency? I'm keenly interested in the comparison of this model of\nprogramming with the more \"traditional\" thread-based model, but this is a large\ntopic I'll have to defer to a future post. The main value proposition of Node.js is using the same language on the\nserver and on the client. Client-side programmers are already familiar\nwith JS, by necessity, so not having to learn another language to program\nserver-side is a plus. Interestingly, this choice also affects the fundamental architecture and\n\"way\" of Node.js; since JS is a single-threaded language, Node.js adopted\nthis model and had to turn to asynchronous APIs to support concurrency.\nIn fact, the libuv framework we covered in part 4 was developed as\na portability layer to support Node.js. Since the idea is emulate CPU-intensive work, this is just a hack to\navoid using huge primes as inputs. For anything but very large primes,\neven this naive algorithm executes extremely quickly so it's hard to\nsee real delays. Since Node.js doesn't have a sleep function (the idea of sleep is\ncontrary to the philosophy of Node.js), we simulate it here with a busy\nloop checking the time. The important bit is to keep the CPU occupied,\nemulating and intensive computation. \nFor comments, please send me\n an email,\nor reach out on on Twitter.\n  Back to top","time":1525790191,"title":"Concurrent Servers: Callbacks, Promises and Async\/await","type":"story","url":"https:\/\/eli.thegreenplace.net\/2018\/concurrent-servers-part-6-callbacks-promises-and-asyncawait\/","label":3,"label_name":"dev"},{"by":"louison","descendants":0,"id":17021147,"kids":"None","score":1,"text":"\u201cOur thoughts shape our spaces, but our spaces return the favor.\u201d \u2014 Steven Johnson At Power, we create the interface of the future in homes. We create lights that can see and hear. Plug our light in any room and space becomes a digital interface. At any time, in any room, you can raise your hand and start speaking. Words become incantations, they no longer just affect the people around you, they can affect the world around you. You can also just do nothing, Power constantly analyzes what\u2019s happening to assist you: lights turn on automatically while respecting your circadian rhythms, music follows you as you walk into a new room, your bathroom smells like eucalyptus in the morning and lavender in the evening, among many other scenarios that we and our community are developing using a mix of computer vision and speech recognition. In the 70s, computers were designed for scientists. Then, in the 80s, graphical interfaces turned the computer into a mass consumer product. They started colonizing homes. The era of Personal Computing began. While PCs became mobile, and Internet is now providing us with useful information and services, the digital world is only accessible through a screen and keyboard. Every time we use a computer or mobile, our attention is captured by the device, we are no longer aware of our real life experience. There is a whole new depth of interactions that we could access if computers could flow with our lives.\u00a0We need computers to be more intelligent, to understand and sense humans. Now is an exciting time in the history of information technology. Softwares based on Machine Learning let computers understand humans as well as another human would. At the same time, computers are becoming so small, resourceful, and cheap to deploy that this is creating the opportunity for a new type of interface to emerge. Home is where we live, rest, and increasingly, work. The last big innovation in home was electrification about 100 years ago. The main use case for electricity at the time was lighting. Then, as more homes got connected, more devices were created: dishwasher, toaster, washing machine, TV\u2026 At Power, we are transforming home by creating the future of man-machine interfaces. By placing sensors in every room, we turn your home into a device. You live in the device, and it\u2019s completely secure because the input processing is done offline. Computers become a component of your experience, not a parallel experience. First, we want to make home more alive. Our bodies were made to live in nature, and while living in a box is useful, it creates deep psychological and physical imbalances. By installing our nature inspired ceiling lights and light bulbs, light flows with your circadian rhythms. In the morning, clear white light with a touch of blue properly wakes up your system, making you ready for the day. During the day, white light with dynamic intensity complements the light that\u2019s naturally penetrating through the windows. At night, yellowish white keeps you running while allowing your body to prepare itself for sleep.Obviously, switches are no longer needed, you can set up custom RGB modes with your voice. Lighting becomes healthy, energy efficient, and fun! Then, you can plug your speakers to the Power Bridge, and play YouTube, Spotify, Soundcloud, in any room, with your voice and gestures. No more yelling \u201cAlexaaaa\u201d as the music is already playing, just raise your hand. If you\u2019re an audiophile and set up speakers in every room, as you walk into a room, Power automatically turns on the music. The bliss follows you.You can also plug a Power Bridge to your TV, which you can then use to play Netflix, YouTube or watch Torrents using voice and gesture recognition. Finally, with essential oil misters, you can create complete atmospheres. In the morning, your bathroom could turn into a eucalyptus rainforest playing Hawaiian songs. During the day, our productivity blend could manage lighting, smell and music to nurture your most productive flow.At night, lavender oil could be misted in your bedroom until you fall asleep. The interface of the future understands you so well that it knows what you want before you want it. It can act passively and take care of many things for you in the background. When used actively, contextual awareness reduces the need for abstraction on the user\u2019s side: the interface requires much less inputs and procedures than today\u2019s computers. A deeper computational understanding of human life means richer experiences. The potential use cases of our platform are endless. Health recommendations based on lifestyle tracking, workout apps that give you feedback through posture analysis, atmosphere apps. Our marketplace is open for developers to create rich interactive experiences. We are continuously working internally and with our community to create new features. Our current features are only the first step towards accessing our bigger vision. Just like with electrification, more and more home devices will be created with progress in robotics and machine learning. These devices will need a common brain to benefit from a global input anywhere in the home, we are building the brain of home. We believe that profound happiness is accessed when we are one with what matters most.Technology should not get in the way of our precious moments, it should enhance them with grace. We want to keep you in flow so that you can unleash your inner power. IT is redefining what it means to be human. It\u2019s important that we design systems that allow us to flourish as humans, not systems that disconnect us from our humanity. We\u2019ll be opening pre-sales soon. If you want to be among the first people to experience a magical home, sign up below. For business opportunities or general questions, please contact info@power.technology By clapping more or less, you can signal to us which stories really stand out. Building the future of Home @ https:\/\/power.technology","time":1525790105,"title":"How Sentient Space Will Change What It Means to Be at Home","type":"story","url":"https:\/\/medium.com\/@louison\/how-sentient-space-will-change-what-it-means-to-be-at-home-8642778e0dfc","label":5,"label_name":"ml"},{"by":"upescatore","descendants":0,"id":17021118,"kids":"None","score":1,"text":"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Because there is not a silver bullet out there! It does not matter how you'll implement your source code: it will suck for sure, somewhere, somehow. It does not matter how many best practices you will take: for sure you will press both Ctrl+C, Ctrl+V somewhere, somehow. Angular 1 anyone? Once obsolete they will turn in useful monitor stands! Your project will contain bugs.","time":1525789829,"title":"It depends manifesto","type":"story","url":"https:\/\/github.com\/P3trur0\/it-depends-manifesto","label":4,"label_name":"github"},{"by":"eplanit","descendants":3,"id":17021115,"kids":"[17022723]","score":10,"text":"Major study suggests Britain could lower its rates of cancer, diabetes and cardiovascular disease by promoting the diets \nIan Sample Science editor \n\nMon 7 May 2018 13.08\u00a0EDT\n\n\nLast modified on Mon 7 May 2018 17.00\u00a0EDT\n\n Britain could lower its rates of cancer, diabetes and cardiovascular disease by embracing Mediterranean- or Nordic-style diets, a major study into the benefits of healthy eating suggests. A review by the World Health Organization found compelling evidence that both diets reduce the risk of the common diseases, but noted that only 15 out of 53 countries in its European region had measures in place to promote the diets. The authors of the report compiled evidence on the health impacts of the two diets from academic journals, conference papers and books, then reviewed government and health ministry websites for national policies and guidelines on healthy eating. Eight countries including Ireland, Spain and Greece promoted the benefits of the Mediterranean-style diet, while seven including Norway, Sweden, Finland and Iceland recommended people adopt a Nordic-style diet to remain healthy. \u201cBoth of these diets are really good in terms of impact on health. That is not in doubt,\u201d said Joa\u0303o Breda from the WHO\u2019s European office for prevention and control of noncommunicable diseases. \u201cWe wanted to know whether countries were using them to inform healthy eating policies.\u201d In England, the government recommends people eat five portions of fruit and vegetables per day on the back of evidence that such a diet can reduce the risk of heart disease and stroke. But ministers have been accused of doing too little to discourage unhealthy eating, despite a rise in childhood obesity rates to 10%. The traditional Mediterranean diet is rich in fruit, vegetables, nuts, cereals and olive oil, includes a moderate amount of fish and poultry, and has very little dairy, red meat, processed meat and sweets. The Nordic diet is similar, focusing on vegetables, berries, pulses, whole grain cereals and fatty fish such as herring, mackerel and salmon. Instead of olive oil, the Nordic diet favours rapeseed oil. According to the report, both diets helped to reduce cases of chronic illnesses, such as heart disease, stroke, diabetes and some cancers. Many of the conditions are driven by obesity. According to Cancer Research UK, more than one in 20 cancers are linked to being overweight or obese. The number of adults and older teens with diabetes has doubled in the past 20 years on the back of rising obesity rates, with 3.7 million people aged 17 or older now living with the disease.  \u201cAll countries need to do more in terms of promoting good diets, because we have an emergency here,\u201d Breda said. \u201cWe are not recommending any particular diet, but when countries think about the improvements they want to make, they might be inspired by these diets. If you adopt them, you save the health system money. There are lots of advantages.\u201d","time":1525789817,"title":"Embrace Mediterranean or Nordic diets to cut disease, WHO says","type":"story","url":"https:\/\/www.theguardian.com\/science\/2018\/may\/07\/embrace-mediterranean-or-nordic-diets-to-cut-disease-who-says","label":7,"label_name":"random"},{"by":"xTWOz","descendants":0,"id":17021105,"kids":"None","score":2,"text":"Since my last post\u2026My first real post here got quite an unexpected amount of attention. This was both flattering and a bit scary. \u201cNow people have expectations,\u201d I thought to myself. I wrote this article, then as usual I gave it to and editor to fix my grammar. Then I got this as a note: \u201cBy way of caution, I wouldn\u2019t put that out on Medium.\u201d The editor was trying to warn me that this article could potentially harm my reputation. \u201cWhat reputation? No one cares!\u201d was my next thought.\u00a0But it got to me. I sent the article for a rewrite. I learned stuff about writing from the rewrite. But I\u2019m not putting it online. Who needs another politically correct, generic advice post? So below is version two of what I was trying to say. All the writing advice I read says \u201cwrite what you know\u201d. I know stuff. But I\u2019m not an expert of any of it. Well, maybe just one thing. I feel like I\u2019m very good at being mediocre at my job, as UI\/UX designer. I have mastered this skill for over ten plus years. For the last four years I\u2019ve even been a full-time freelancer. And I\u2019m really killing it at doing a mediocre job. Now when I\u2019m saying mediocre, here is what I mean. This is my scale: crappy, mediocre, great and master. I believe being a master UI\/UX designer is an impossible goal. It\u2019s a moving target. Even if you devote all your time, it\u2019s not going to happen. It\u2019s a constantly evolving field where the rules are yet to be established. So, really we are left with three skill levels: crappy, mediocre and great. To be a great UI\/UX designer in my book you have to be obsessed. And on top of that you have to be in the right place. It\u2019s where the new rules of this field are made. Where all of the people that surround you are as equally obsessed as you. And together you spend all your time doing the thing and talking about it. I kinda envy those people. I really don\u2019t know why. Mediocre is where I\u2019m at. Doing a decent job for my clients. That affords me a decent life where I have time for other more fun things than work. But instead of doing the fun stuff I think about work. And the crappy UI\/UX designer? Well, you know who you are. You recently changed your title in your all online profiles to this hip new one: \u201cUI\/UX designer\u201d. However, you are not sure exactly how this thing works. Don\u2019t worry, just keep at it. I was there a few years ago. I\u2019m not moving to Silicon Valley anytime soon. So, for me the real challenge is to stay mediocre. Do a decent job and have a life. Stop trying to consume every article, podcast and YouTube video on the topic. And wonder\u2026 when is the fun and easy part is coming? I guess it\u2019s right after you actively try to put yourself out of a job by writing silly articles confessing your mediocrity to the world. So how do I restrict myself to achieving these mediocre results?\u00a0\u00a0There are a couple of hard rules that I want to follow loosely: Well, I\u2019m not great at this, but I\u2019m making an effort. It\u2019s far easier to stay on my laptop and pretend that I\u2019m working. This life thing, it\u2019s far more demanding. You have to talk with people. Pretend that you care about their stuff. It\u2019s exhausting. It feels like a performance to me. You have to run your lines in your head. Know your moves. Wear your costume. And at the end of the day you go to bed and everything starts replaying in your head. You start analysing how you did. Rehashing arguments. It\u2019s harder than work. But it\u2019s also rewarding. Spending some time with people talking. Walking in nature. Playing some silly games. Drawing some dumb ideas. Doing some sport. Sport you can definitely get addicted to. And it\u2019s definitely better than Netflix. Once upon a time the previous sentence would end with the word \u201csex\u201d, but we live in different times now. The cost of doing a great job at this is that you have to perform super-complex tasks on a daily basis. And this is just for doing one or the other. And when you combine UI and UX at once, it\u2019s impossible to keep the level of quality you desire. Even being a mediocre UI\/UX designer needs a lot of maintenance, keeping up with the ever-evolving topic and the tools. Maybe the real question is\u200a\u2014\u200ahow much learning is enough learning? That will keep you in the loop so you don\u2019t get blindsided mid-project with something you know nothing about. It\u2019s impossible to measure. So, to be on the safe side, pick up your phone and start reading this eBook you bought a few months ago instead of daydreaming and having fun. I\u2019m kidding. But not really. How do you know if you know enough? Greatness is untenable in some circumstances. I like what I do and I want to keep doing it. But it also seems a bit unhealthy. As such, change is in order. What type of change? Going all in, buying a one-way ticket to Silicon Valley, and becoming one of the maniacs I admire so much?\u00a0\u00a0Or try to relax for a bit and narrow the type of work I do? I guess this is what I will write about next. PS: this post turns out to be more confusing then the design of old enterprise software. You are not sure what is the purpose of it. Me neither. By clapping more or less, you can signal to us which stories really stand out. UX\/UI Designer, do art and run for fun. https:\/\/designcrafter.co\/ Curated stories on user experience, usability, and product design. By @fabriciot and @caioab.","time":1525789754,"title":"Being a mediocre UI\/UX designer \u2013 the honest version","type":"story","url":"https:\/\/uxdesign.cc\/how-to-be-a-mediocre-ui-ux-designer-case-study-8bd80ec5755f","label":7,"label_name":"random"},{"by":"prostoalex","descendants":0,"id":17021103,"kids":"None","score":2,"text":"\nWelcome \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(sign in | sign up)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  You can enter multiple addresses separated by commas to send the article to a group; to send to recipients individually, enter just one address at a time. In her seminal work The Managed Heart: Commercialization of Human Feeling (1983), the sociologist Arlie Russell Hochschild described a workplace practice known as \u201cemotional labor management.\u201d Hochschild was studying the extreme kinds of \u201cemotional labor\u201d that airline stewardesses, bill collectors, and shop assistants, among others, had to perform in their daily routines. They were obliged, in her words, \u201cto induce or suppress feeling in order to sustain the outward countenance that produces the proper state of mind in others.\u201d In the case of airline stewardesses, the managers and human resources staff of the airline companies relied on reports from passengers or management spies to make sure that stewardesses kept up their cheerful greetings and radiant smiles no matter what. The stewardesses Hochschild studied were working under a regime of \u201cscientific management,\u201d a workplace control system conceived in the 1880s and 1890s by the engineer Frederick Winslow Taylor. Workers subject to such regimes follow precise, standardized routines drawn up by managers and undergo rigorous monitoring to ensure that these routines are followed to the letter. Taylor\u2019s practice is often associated with such factory workplaces as the early Ford Motor plants or today\u2019s Amazon \u201cfulfillment centers,\u201d where workers must perform their prescribed tasks on a strict schedule. Hochschild showed that regimes of scientific management could be applied virtually anywhere. Her airline company managers aspired to control every aspect of their employees\u2019 emotional conduct. What kept them from doing so was that they weren\u2019t actually present in plane cabins during flights and so had to rely on haphazard reporting to confirm that the stewardesses were always behaving as they should. But in the twenty-first century, new technologies have emerged that enable companies as varied as Amazon, the British supermarket chain Tesco, Bank of America, Hitachi, and the management consultants Deloitte to achieve what Hochschild\u2019s managers could only imagine: continuous oversight of their workers\u2019 behavior. These technologies are known as \u201cubiquitous computing.\u201d They yield data less about how employees perform when working with computers and software systems than about how they behave away from the computer, whether in the workplace, the home, or in transit between the two. Many of the technologies are \u201cwearables,\u201d small devices worn on the body. Consumer wearables, from iPhones to smart watches to activity trackers like Fitbit, have become a familiar part of daily life; people can use them to track their heart rate when they exercise, monitor their insulin levels, or regulate their food consumption. The new ubiquity of these devices has \u201craised concerns,\u201d as the social scientists Gina Neff and Dawn Nafus write in their recent book Self-Tracking\u2014easily the best book I\u2019ve come across on the subject\u2014\u201cabout the tremendous power given to already powerful corporations when people allow companies to peer into their lives through data.\u201d But the more troubling sorts of wearables are those used by companies to monitor their workers directly. This application of ubiquitous computing belongs to a field called \u201cpeople analytics,\u201d or PA, a name made popular by Alex \u201cSandy\u201d Pentland and his colleagues at MIT\u2019s Media Lab. Pentland has given PA a theoretical foundation and has packaged it in corporate-friendly forms. His wearables rely on many of the same technologies that appear in Self-Tracking, but also on the sociometric badge, which does not. Worn around the neck and attached to microphones and sensors, the badges record their subjects\u2019 frequency of speaking, tone of voice, facial expressions, and body language. In Sociometric Badges: State of the Art and Future Applications (2007), Pentland and his colleague Daniel Olgu\u00edn Olgu\u00edn explained that the badges \u201cautomatically measure individual and collective patterns of behavior, predict human behavior from unconscious social signals, identify social affinity among individuals\u2026and enhance social interactions by providing feedback.\u201d The badges and their associated software are being marketed by Humanyze, a Boston company cofounded by Pentland, Olgu\u00edn Olgu\u00edn, and Ben Waber among others (Waber was formerly one of Pentland\u2019s researchers at MIT and is now the company\u2019s CEO). Under its original name, Sociometric Solutions, the company got early commissions from the US Army and Bank of America. By 2016 Humanyze had among its clients a dozen Fortune 500 companies and Deloitte. In November 2017 it announced a partnership with HID Global, a leading provider of wearable identity badges, which allows HID to incorporate Humanyze\u2019s technologies into its own products and so expands the use of such badges by US businesses. The main tool in Humanyze\u2019s version of PA is a digital diagram in which people wearing sociometric badges are represented by small circles arrayed around the circumference of a sphere, rather like the table settings for diners at a banquet. Each participant is linked to every other one by a straight line, the thickness of which depends on what the system considers the \u201cquality\u201d of their relationship based on the data their badges collect. In a 2012 essay for the Harvard Business Review, Pentland described how this method was used to evaluate the performance of employees at a business meeting in Japan.1 The PA diagram for Day One showed that the lines emanating from two members of an eight-person team, both of whom happened to be Japanese, were looking decidedly thin. But by Day Seven, the diagrams were showing that the \u201cDay 1 dominators\u201d had \u201cdistributed their energy better\u201d and that the two Japanese members were \u201ccontributing more to energy and engagement.\u201d Evidently some determined managerial nudging had taken place between Days One and Seven. In a June 2016 interview with MEL Magazine, Waber claimed that little escapes the gaze of the sociometric badge and its associated technologies: \u201cEven when you\u2019re by yourself, you\u2019re generating a lot of interesting data. Looking at your posture is indicative of the kind of work and the kind of conversation you\u2019re having.\u201d2 In a 2008 article Pentland commended his PA systems for being more rational and dependable than their human counterparts.3 But the \u201cintelligence\u201d of his and Waber\u2019s PA systems is not that of disembodied artificial intelligence\u2014whatever that may look like\u2014but of corporate managers with certain ideas about how their subordinates should behave. The managers instruct their programmers to create algorithms that in turn embed these managerial preferences in the operations of the PA systems. Pentland and Waber\u2019s PA regime is in fact a late variant of scientific management and descends directly from the \u201cemotional labor management\u201d Hochschild discussed in The Managed Heart. But these twenty-first-century systems have powers of surveillance and control that the HR managers of the airline companies thirty years ago could only dream of. Not all PA systems depend on wearable devices. Some target landlines and cell phones. Behavox, a PA company financed by Citigroup, specializes in the surveillance of employees in financial services. \u201cEmotion recognition and mapping in phone calls is increasingly something that banks really want from us,\u201d Erkin Adylov, the company\u2019s CEO, told a reporter in 2016.4 Behavox\u2019s website advertises that its systems give \u201creal-time and automatic tracking\u201d of aspects of employee conversation like the \u201cvariability in the timing of replies, frequency in communications, use of emoticons, slang, sentiment and banter.\u201d The company, in the words of a recent Bloomberg report, scans petabytes of data, flagging anything that deviated from the norm for further investigation. That could be something as seemingly innocuous as shouting on a phone call, accessing a work computer in the middle of the night, or visiting the restroom more than colleagues.5 \u201cIf you don\u2019t know what your employees are doing,\u201d Adylov told another reporter in 2017, \u201cthen you\u2019re vulnerable.\u201d Most PA software providers rely on combinations of wearables and computer-based technologies to monitor and control workplace behavior. These companies boast that their systems can find out virtually everything there is to know about employees, both in the workplace and outside it. \u201cThanks to modern technology,\u201d in the words of Hubstaff, a PA company based in Indianapolis, \u201ccompanies can monitor almost 100 percent of employee activity and communication.\u201d6 Max Simkoff, the cofounder of San Francisco\u2019s Evolv Corporation (now taken over by Cornerstone, another Humanyze competitor), has said that his PA systems can analyze more than half a billion employee data points across seventeen countries and that \u201cevery week we figure out more things to track.\u201d Kronos Incorporated, a management software firm based in Lowell, Massachusetts, claims that its workforce management systems are used daily by \u201cmore than 40 million people\u201d and offer \u201cimmediate insight into\u2026productivity metrics at massive scale.\u201d7 Microsoft entered the PA market when it acquired the Seattle-based company Volometrix in 2015. It inherited Volometrix\u2019s \u201cNetwork Efficiency Index\u201d (NEI), which measures how efficiently employees build and maintain their \u201cinternal networks.\u201d The index is calculated by dividing \u201cthe total number of hours spent emailing and meeting with other employees\u201d by the number of \u201cnetwork connections\u201d an employee manages to secure. The NEI\u2019s recognition of an employee\u2019s network connection depends on whether encounters with coworkers have met both a \u201cfrequency of interaction threshold\u201d and \u201can intimacy of interaction threshold,\u201d the latter of which is satisfied when there are \u201c2 or more interactions per month which include 5 or fewer people total.\u201d8 When workers fail to meet these thresholds, other workplace technologies can be enlisted to give them a nudge. One Humanyze client created a robotic coffee machine that responded to data collected from sociometric badges worn by nearby employees. By connecting to Humanyze\u2019s Application Programming Interface (API), the coffee machine could assess when a given group of workers needed to interact more; it would then wheel itself to wherever it could best encourage that group to mingle by dispensing lattes and cappuccinos.9 When American managers want to install PA surveillance systems, employees rarely manage to stop them. In Britain, an exception to this trend occurred in January 2016, when journalists at the London office of the Daily Telegraph came to work one Monday and found that management had affixed small black boxes on the undersides of their desks that used heat and motion sensors to track whether or not they were busy at any given time. Seamus Dooley of the UK National Union of Journalists told The Guardian that \u201cthe NUJ will resist Big Brother\u2013style surveillance in the newsroom.\u201d The boxes were removed.10 The Telegraph\u2019s journalists were right to act as they did. A 2017 paper by the National Workrights Institute in Washington, D.C.,11 cites a wealth of academic research on the physical and psychological costs that intrusive workplace monitoring can have on employees. A study by the Department of Industrial Engineering at the University of Wisconsin has shown that the introduction of intense employee monitoring at seven AT&T-owned companies led to a 27 percent increase in occurrences of pain or stiffness in the shoulders, a 23 percent increase in occurrences of neck pressure, and a 21 percent increase in back pain. Other research has suggested that the psychological effects of these technologies can be equally severe. Many of Bell Canada\u2019s long-distance and directory assistance employees have to meet preestablished average work times (AWTs). Seventy percent of the workers surveyed in one study reported that they had \u201cdifficulty in serving a customer well\u201d while \u201cstill keeping call-time down,\u201d which they said contributed to their feelings of stress to \u201ca large or very large extent.\u201d How have the corporate information-technology community and its academic allies justified these practices and the violations of human dignity and autonomy they entail? Among economists, Erik Brynjolfsson at MIT is perhaps the leading counsel for the defense. With Andrew McAfee, also of MIT, he has published two books to this end, The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies (2014) and Machine, Platform, Crowd: Harnessing Our Digital Future (2017), the latter clearly written with a corporate audience in mind. In the opening chapter of Machine, Platform, Crowd, they write that \u201cour goal for this book is to help you.\u201d The \u201cyou\u201d in question is a corporate CEO, CIO, or senior executive who might be saddled with obsolete technologies\u2014in Brynjolfsson and McAfee\u2019s words, \u201cthe early-twenty-first-century equivalent of steam engines.\u201d Each subsequent chapter ends with a series of questions aimed at such readers: \u201cAre you systematically and rigorously tracking the performance over time of your decisions?\u201d Although the use of information technology in the workplace is a dominant theme of Brynjolfsson and McAfee\u2019s two books, the authors say nothing about the surveillance powers of People Analytics or its predecessors, whose existence cannot easily be reconciled with the glowing vision they describe in the opening chapters of The Second Machine Age. There are, for instance, eighteen references to Amazon in The Second Machine Age and Machine, Platform, Crowd. All of them are to technological breakthroughs like the company\u2019s \u201crecommendation engine,\u201d which reduces search costs so that \u201cwith a few checks over two million books can be found and purchased.\u201d From Brynjolfsson and McAfee one would never know that among large US corporations Amazon has relied perhaps most heavily on a combination of surveillance systems to control both its shop floor and its middle management workforce, and to push the performance of both to the limit. It tags its shop floor employees with micro-computers that constantly measure how long they take to load, unload, and shelve packages at Amazon depots. If the timings set by management are not met, even by a few seconds, the computer starts beeping and the employee gets rebuked. Commenting on a November 2013 BBC documentary about the conditions under which Amazon\u2019s shop floor employees work, filmed clandestinely at a \u201cfulfillment center\u201d at Swansea, UK, the public health expert Michael Marmot of University College, London, noted that such practices had been shown to cause \u201cincreased risk of mental illness and physical illness.\u201d12 Amazon has also relied on a program called \u201cAnytime Feedback Tool\u201d to achieve comparable levels of surveillance over its middle managers, who are encouraged to send their bosses anonymous evaluations of their co-workers without giving the subject the chance to respond. A manager\u2019s regular monthly performance review may run to fifty or sixty pages; each year Amazon managers with the weakest performance record are in danger of being fired. In the words of a report by Jodi Kantor and David Streitfeld of The New York Times, \u201cmany workers called it a river of intrigue and scheming\u201d in which cliques of managers could gang up on a colleague and use the system to demote him or her in the performance ratings, thereby protecting themselves from the management cull.13 None of this appears in either of Brynjolfsson and McAfee\u2019s books. Despite their academic credentials, Brynjolfsson and McAfee are not acting in these books as eminent scholars conveying new research to a nonspecialist audience. They are acting as propagandists, arming their business audience with their own rationale for using digital technologies in the workplace. Both authors take refuge in a kind of techno-determinism. \u201cWe need,\u201d they write in The Second Machine Age, \u201cto let the technologies of the second machine age do their work and find ways of dealing with the challenges they will bring with them.\u201d When Brynjolfsson and McAfee do discuss recent developments in workplace technologies, it is with a kind of fatalism. In this account, the people involved\u2014the CIOs, the system designers, and the programmers\u2014are simply expediting an inevitable transition to a digital-intensive workplace where, as even the authors admit, \u201csome people, even a majority of them, can be made worse off.\u201d (They have particularly in mind those whose labor is \u201crelatively unskilled.\u201d) The techno-managerial elite may perform its tasks with varying degrees of efficiency, but the parameters within which it operates are highly circumscribed. The managers are not to blame, in this determinist view, for the human consequences of the \u201csecond machine age\u201d: jobs are outsourced, while employees are laid off, deskilled, relentlessly monitored, and forced to settle for precarious and poorly paid jobs. The responsibility for dealing with these casualties is dumped onto the state. But by airbrushing out the decisions corporate managers can\u2014and do\u2014make over how to use technologies like Pentland\u2019s PA systems, Brynjolfsson and McAfee are effectively keeping employees in the dark about the forces that lower their quality of life and their standard of health. Digital technologies have any number of possible uses in the workplace, and not all of them involve subjecting workers to heightened monitoring and machine-generated feedback. Tasks that Brynjolfsson and McAfee write off as \u201croutine\u201d and thus as fair game for total or partial automation may turn out to be just the opposite under regimes designed to support rather than displace them. The Cornell scholar Virginia Doellgast has shown in detail what an employee-friendly use of new workplace technologies can look like.14 She has done detailed research on call centers in the German telecommunications industry, where unions are strong and works councils have statutory rights of \u201ccodetermination\u201d (Mitbestimmung) over the use of workplace technologies. There, she discovered, employees had managed to negotiate limits on how far management could go in using surveillance systems to set the pace of their work and track their real-time performance. Doellgast found this level of employee activism surprising in what is usually considered a \u201cperipheral service industry.\u201d Once they had set those limits, the workers could exercise the advanced workforce skills provided by the German apprentice training system on their own terms. They often belonged to self-managed work teams, which could influence the size and content of the daily workload, the pace of work, and the nature of communications with customers, sparing employees the indignities of the mandated digital scripts widely used in US call centers. Compared with US workplaces and German ones without employee representation, these \u201chigh involvement\u201d workplaces had lower levels of employee stress, lower rates of employee turnover, higher pay, and better service. Arrangements like this, in which management and labor share power, barely exist in corporate America. Union membership in the US private sector fell to 6.5 percent in 2017, and many of its industries are becoming virtually union-free. I couldn\u2019t find a single reference to labor unions in either of Brynjolfsson and McAfee\u2019s books. In the deunionized US workplace, digital technologies are being deployed in ways that both increase labor\u2019s productivity and diminish its earning power: the more workers have to meet preestablished output targets and respond to real-time analysis of their performance, the fewer opportunities they have to widen their earning power by refining their judgment, experience, and skills. When the output of labor rises and its real earnings stagnate or decline, as they have in the US for at least the past thirty years, then, other things being equal, the cost of labor per unit of output will fall and the share of profits in GDP rise, as they have again consistently done during this period. From a corporate perspective this is a rosy scenario\u2014especially since the compensation of top managers is frequently linked to corporate stock prices, which tend to rise with profits, as they did in the first year of the Trump presidency. But it has done nothing to narrow income inequality and much to widen it. Few world leaders have had much to say about the relationship between the misuse of technology and the human damage it can inflict. Pope Francis is one who has: \u201cOnly by educating people to a true solidarity,\u201d he said in 2017, \u201cwill we be able to overcome the \u2018culture of waste,\u2019 which doesn\u2019t only concern food and goods but, first and foremost, the people who are cast aside by our techno-economic systems which, without even realizing it, are now putting products at their core, instead of people.\u201d15 Alex \u201cSandy\u201d Pentland, \u201cThe New Science of Building Great Teams,\u201d Harvard Business Review, April 10, 2012.\u00a0\u21a9 John McDermott, \u201cBen Waber Wants to Track Your Every Movement at the Office,\u201d MELMagazine, January 5, 2016.\u00a0\u21a9 Alex \u201cSandy\u201d Pentland et al., Meeting Mediator: Enhancing Group Collaboration using Sociometric Feedback.\u00a0\u21a9 CEBTalent Daily, \u201cThe Coming Age of Employee-Monitoring Tech,\u201d September 15, 2016.\u00a0\u21a9 Gavin Finch and Edward Robinson, \u201cBad Behaviour Database Aims to Stop Rogue Traders Before They Act,\u201d Bloomberg News, January 18, 2017. See also Behavox, \u201cCRM Automation,\u201d and \u201cCulture and Conduct.\u201d\u00a0\u21a9 Hubstaff, \u201cWhat is Employee Monitoring?,\u201d\u00a0\u21a9 Max Simkoff of Evolv, quoted in Hannah Kuchler, \u201cData Pioneers Watching Us Work,\u201d Financial Times, February 17, 2014; Kronos Corporation, \u201cWorkforce Management and HCM Cloud Solutions.\u201d\u00a0\u21a9 \u201cVoloMetrix Files Patent for Ground Breaking Predictive People Analytics Software.\u201d\u00a0\u21a9 Katherine Noyes, \u201cStartup Humanyze\u2019s \u2018People Analytics\u2019 Wants to Transform Your Workplace,\u201d IDGNews Service, November 20, 2015.\u00a0\u21a9 Benn Quinn and Jasper Jackson, \u201cDaily Telegraph to Withdraw Devices Monitoring Time at Desk after Criticism,\u201d The Guardian, January 11, 2016.\u00a0\u21a9 National Workrights Institute, \u201cElectronic Monitoring: A Poor Solution to Management Problems\u201d (2017).\u00a0\u21a9 \u201cAmazon Workers Face \u2018Increased Risk of Mental Illness,\u2019\u201d BBC News, November 25, 2013.\u00a0\u21a9 Jodi Kantor and David Streitfeld, \u201cInside Amazon: Wrestling Big Ideas in a Bruising Workplace,\u201d The New York Times, August 16, 2015.\u00a0\u21a9 Virginia Doellgast, \u201cThe Effects of National Institutions and Collective Bargaining Arrangements on Job Quality in Front Line Service Workplaces,\u201d Cornell University ILRSchool, Digital Commons, 2009; see also Doellgast, Collective Bargaining and High-Involvement Management in Comparative Perspective: Evidence from US and German Call Centers, Cornell University ILRSchool, Digital Commons, 2008.\u00a0\u21a9 \u201cWhy The Only Future Worth Building Includes Everyone,\u201d TED talk delivered at the Vatican, published April 25, 2017.\u00a0\u21a9 \u00a9 1963-2018 NYREV, Inc. All rights reserved.","time":1525789748,"title":"Big Brother Goes Digital","type":"story","url":"https:\/\/www.nybooks.com\/articles\/2018\/05\/24\/big-brother-goes-digital\/","label":7,"label_name":"random"},{"by":"maderalabs","descendants":1,"id":17021097,"kids":"[17021137]","score":4,"text":"\n                    Copy and pasting emails, sending them out to people, one by one, over and over again.\n                   \n                    Emails back-and-forth, playing phone tag, trying to find the perfect time to meet.\n                   \n                    Asking people to fill out forms, tracking down scanned documents you've received.\n                   \n                    Updating unweidy spreadsheets, merging in data from others, just to keep track of things.\n                   \n                    Manually harassing people to get things to you, somebody always falls through the cracks.\n                   \n                    Trying to keep track of who's waiting to hear back, what they need and when it needs sent.\n                  ","time":1525789710,"title":"Show HN: CrowdSync \u2013 Automate Repetitive Processes with People","type":"story","url":"https:\/\/www.crowdsync.io","label":7,"label_name":"random"},{"by":"prostoalex","descendants":0,"id":17021078,"kids":"None","score":1,"text":"If you\u2019re an Android developer, you may have heard of Flutter. It\u2019s a relatively new, supposedly simple framework designed for making cross-platform native apps. It\u2019s not the first of its kind, but it\u2019s being used by Google, giving its claims some credence. Despite my initial reservations upon hearing about it, I decided on a whim to give it a chance\u200a\u2014\u200aand it dramatically changed my outlook on mobile development within a weekend. Here is what I learned. Before we get started, let me add a short disclaimer. The app I wrote and will be referencing in this article is relatively basic and does not contain a lot of business logic. It\u2019s nothing fancy, but I wanted to share my experience and learnings from porting an existing native Android App to Flutter, and this is the best example I can use to do so. Neither app makes any efforts in terms of architecture; it\u2019s purely about development experience and using the frameworks as they are. Exactly one year ago, I published my first Android App in the Play Store. The app (Github) is pretty basic in terms of architecture and coding conventions; it was my first big open source project, which shows, and I\u2019ve since come a long way with Android. I work at an agency, and spend time on quite a few projects with different technologies and architectures, including Kotlin, Dagger, RxJava, MVP, MVVM, VIPER and others, which has really helped my Android development. That being said, over the past few months, I\u2019ve been getting frustrated with the Android framework, especially regarding incompatibility and how counter intuitive it is to build apps in general. Don\u2019t even get me started on build times\u2026 (I\u2019d recommend this article, which digs into more details) and while things have gotten a lot better with Kotlin and tools like Databinding, the whole situation still just feels like putting a band-aid on a wound that\u2019s too big to be healed. Enter Flutter. I began using Flutter a few weeks ago when it entered beta. I looked at the official documentation (which is great, by the way) and started going through the code labs and how-to guides. Quickly, I began to understand the basic ideas behind Flutter, and decided to try it out myself and see if I could put it into use. I started thinking about what kind of project I should work on first, and I decided to recreate my first Android app. This seemed like an appropriate choice as it would allow me to compare both \u201cfirst-efforts\u201d with the two respective frameworks, while not paying too much attention to app architecture, etc. It was purely about getting to know the SDKs by building a defined set of features. I started by creating the network requests, parsing the JSON and getting used to Dart\u2019s single-threaded concurrency model (which could be the topic of a whole other post on its own). I got up and running with some movie data in my app, and then started creating the layouts for the list and the list items. Creating layouts in Flutter is as easy as extending the Stateless or Stateful Widget classes and overriding a few methods. I\u2019ll compare the differences in building those features between Flutter and Android. Let\u2019s start with the steps required to build this list in Android: This is, of course, tedious. And if you think about the fact that building these features is a fairly common task\u200a\u2014\u200aseriously, it\u2019s not some particularly rare use case that you\u2019re unlikely to ever run into\u200a\u2014\u200ayou might find yourself wondering: is there really no better way to do it? A less error-prone way, maybe one that involves less boilerplate code, and increases development velocity? This is where Flutter comes in. You can think of Flutter as the result of years of lessons that have been learned on mobile app development, state management, app architecture, and so on, which is why it\u2019s so similar to React.js. Doing things the Flutter way just makes sense once you get started. Let\u2019s look at how we can implement the above example in Flutter: To break this down, let\u2019s look at what\u2019s happening here. Most importantly, we\u2019re using a FutureBuilder (part of the Flutter SDK), which requires us to specify a Future(in our case the Api Call) and a builder function. The builder function gives us a BuildContext and the index of the item to be returned. Using this, we can retrieve a movie, given the list from the result of the Future, the snapshot, and create a MovieListItem-Widget (created in step 1) with the movie as a constructor argument. Then, when the build method is first called, we start awaiting the value of the Future. Once it\u2019s there, the builder is called again with the data (snapshot), and we can build our UI with it. Those two classes, combined with the API call, would give us the following result: Well, that was simple. Almost too simple\u2026realizing how easy it was to create a list of items in Flutter piqued my curiosity, and made me even more excited to keep working with it. The next step was figuring out what I could to with more complicated layouts. The detail screen for movies of the native app had a rather complicated layout, including constraint layouts and an app bar. I think this is something that users would expect and appreciate in an app, and if Flutter really wants to stand a chance against Android, it needs to be able to provide more complex layouts like this. Let\u2019s look at what I managed to build: The layout consists of a SliverAppBar, which contains a stacked layout of the movie image, a gradient, the little bubbles, and the text overlay. Being able to express the layout in a modular manner made it super simple to create this rather complicated layout. Here\u2019s what the method of this screen looks like: As I was building the layout, I found myself modularizing parts of the layout as variables, methods or just other widgets. For instance, the text bubbles on top of the image are just another widget, which take text and a background color as an argument. Creating a custom view is literally as easy as this: Imagine how hard it would be to build a custom view like this in Android. On Flutter, though, it\u2019s just a matter of minutes. Being able to extract parts of your UI into self-contained units like widgets makes it easy to reuse those widgets across your app, or even across different apps. You\u2019ll notice that many parts of the layout are reused across different screens of my app, and let me tell you this: it was a piece of cake to implement. It was such a piece of cake that I decided to expand the app to incorporate TV-shows as well. A few hours later and it was done; the app incorporated both movies and TV-shows, and no headaches were gained in the process. I did it by building generic classes for loading and displaying the data, which let me re-use every layout for both movies and shows. But to accomplish the same thing with Android, I had to use separate activities for movies and shows. You can imagine how quickly that became maintainability hell, but I felt that Android just wasn\u2019t flexible enough to share those layouts in a cleaner, easier way. At the end of my Flutter experiment, I arrived at a very straight-forward and convincing conclusion: One of the best parts was not having to deal with things like fragments, the SupportCompatFragmentManagerCompat, and preserving and manually managing state in a tedious, error-prone way. It\u2019s just so much less frustrating than Android development\u2026no more waiting 30 seconds for \u201cInstant Reload\u201d to change the font-size of a TextView. No more XML-Layouts. No more findViewById (I know that Butterknife, Databinding, Kotlin-Extensions exist, but you get my point). No more redundant boilerplate code\u200a\u2014\u200ajust results. Once both apps were more or less on the same page in terms of features, I was curious to see what the difference in lines of code was. How does one repository compare to the other one? (Quick disclaimer: I haven\u2019t integrated persistent storage in the Flutter app yet, and the code base of the original app is quite messy). Let\u2019s compare the code using Cloc, and for the sake of simplicity, let\u2019s just look at the Java and XML files on Android, and the Dart files for the Flutter app (this does not include third party libraries, which would probably increase the metrics for Android significantly). Native Android in Java: Flutter: To break this down, let\u2019s compare the file count first:Android: 179 (.java and\u00a0.xml)Flutter: 31 (.dart)Wow! And concerning lines of code we have:Android: 12176\u00a0Flutter: 1735 That\u2019s crazy! I was expecting the Flutter app to have maybe half the amount of code of the Android app, but 85% less? That actually blew me away. But when you start thinking about it, it makes a lot of sense: since all the layouts, backgrounds, icons, etc. need to be specified in XML, but then still need to be hooked up to the app using Java\/Kotlin code, of course there\u2019s gonna be a ton of code. With Flutter, on the other hand, you can do all of that at once, while also binding the values to the UI. And you can do it all without dealing with the shortcomings of Android\u2019s data-binding, like setting listeners or dealing with generated binding code. I realized then just how cumbersome it is to build such basic things on Android. Why should we write the same code for things like Fragment\/Activity arguments, adapters, state management and recovery, over and over again, when it can be so simple? Of course, this is just the beginning of Flutter, as it\u2019s still in Beta and not yet at the level of maturity that Android is at. Still, by comparison, it feels like Android might have reached its limits, and we may soon write our Android apps in Flutter. There are still some things that need to be worked out, but overall, the future is looking bright for Flutter; we already have great tooling with Plugins for Android Studio, VS Code and IntelliJ, profilers and view inspection tools, and more tools will come. This all leads me to believe that Flutter is more than just another cross-platform framework, but the beginning of something bigger\u200a\u2014\u200athe beginning of a new era of app development. And Flutter could go far beyond the realms of Android and iOS; if you\u2019ve been following the rumor mill, you may have heard that Google is working on a new operating system named Fuchsia. As it turns out, Fuchsia\u2019s UI is being built using Flutter. Naturally, you might be asking yourself: do I have to learn a whole other framework now? We just started learning about Kotlin and using the architecture components, and everything is great now. Why would we want to learn about Flutter? But let me tell you this: after using Flutter, you\u2019ll start to understand the problems with Android development, and it\u2019ll become clear that Flutter\u2019s design is more suitable for modern, reactive applications. When I first started using Android\u2019s Databinding, I thought it was revolutionary, but it also felt like an incomplete product. Dealing with Boolean expressions, listeners and more complicated layouts was tedious with Databinding, which made me realize that Android just wasn\u2019t designed for a tool like this. Now if you look at Flutter, it uses the same idea as Databinding, which is binding your Views\/Widgets to variables without having to manually do it in Java\/Kotlin, but it does it all natively without bridging the XML and Java by generating a bindings file. This allows you to condense what would have previously been at least one XML and Java file, into one reusable Dart class. I could also argue that the layout files on Android don\u2019t do anything on their own. They have to be inflated first, and only then can we start setting values to them. This also introduces the issue of state management, and raises the question: what are we going to do when the underlying values change? Manually grab a reference to the corresponding views and set the new value? That method is really error prone and I don\u2019t think it\u2019s good to manage our View\u2019s state like this; instead we should describe our layout using the state, and whenever the state changes, let the Framework take over by re-rendering the views whose values have changed. This way, our app state can\u2019t get out of sync with what the Views display. And Flutter does exactly this! And there might be even more questions: have you ever asked yourself why creating a toolbar menu is so complicated on Android? Why would we describe the menu items in XML, where we can\u2019t bind any business logic to it anyways (which is the whole intention of a menu), just to then inflate it in a callback of our Activity\/Fragment before binding the actual click listeners in yet another callback? Why don\u2019t we just do it all at once, like Flutter does? As you can see in this snippet, we add the menu items as Actions to our AppBar. That\u2019s all you have to do\u200a\u2014\u200ano more importing icons as XML files, no more overriding callbacks. It\u2019s literally as easy as adding a few widgets to our widget tree. I could go on and on, but I\u2019ll leave you with this: think about all the things that you dislike about Android development, and then think about how you would go about re-designing the Framework while addressing those issues. That\u2019s a heavy task, but doing it will help you understand why Flutter is here and, more importantly, why it\u2019s here to stay. To be fair, there are many apps that (as of this moment) I would still write in native Android with Kotlin; Android may have its downfalls, but it also has its perks. But at the end of the day, I think making a case for native Android is becoming harder and harder when you have Flutter at your disposal. By the way, both apps are open source and on the PlayStore. You can find them right here:Native Android: Github and PlayStoreFlutter: Github and PlayStore By clapping more or less, you can signal to us which stories really stand out. Android Developer @ SinnerSchrader Swipe | Computer Science Student @ Humboldt University Berlin Professional Android Development: the latest posts from Android Professionals and Google Developer Experts.","time":1525789529,"title":"Why Flutter Will Change Mobile Development for the Best","type":"story","url":"https:\/\/medium.com\/@aaronoe\/why-flutter-will-change-mobile-development-for-the-best-c249f71fa63c?mkt_tok=eyJpIjoiTmpWa05qSmpaakF6TkRNMCIsInQiOiJUOEliYXA4WlV0UTNYbDhnM2pTZVVMMjNqTGZsQUt5YzVkV21mVkU2d3lvdm5OYXpycytmSHR0WnJ0WHF5NUpZYW93VTJ5UVwvRTdpbEdrdStiMmw1Z1NDTDQxWVVDbUhybXhreTI4a0dGNUdodlV1bHNqNXA2QUM5SGFRRWVCaEMifQ%3D%3D","label":3,"label_name":"dev"},{"by":"O_H_E","descendants":9,"id":17021056,"kids":"[17022928, 17022497, 17021274, 17021076, 17022305, 17022249]","score":4,"text":" ","time":1525789303,"title":"Mark Shuttleworth wants to build an Ubuntu installer with Electron","type":"story","url":"https:\/\/lists.ubuntu.com\/archives\/ubuntu-devel\/2018-May\/040301.html","label":7,"label_name":"random"},{"by":"modernerd","descendants":0,"id":17021055,"kids":"None","score":1,"text":"Latest Chrome update may have broken millions of web-based games \n\npixinoo\/Shutterstock\n\n(Licensed)\n Almost every title will have to be updated.  When Google updated Chrome to automatically block autoplay videos last week, it was seen as a victory for internet-goers who long endured annoying advertisements. But what most people didn\u2019t consider is how the changes would affect other content. It now appears the Chrome 66 update has had some unintended consequences that threaten web-based games.  Prior to the update, game audio would either start when a webpage loaded, or more commonly, after the user pressed \u201cplay.\u201d With the latest version of Chrome, games created using any HTML5 engine\u2014Pico-8, GameMaker, Unity, or Phaser\u2014don\u2019t play sound. In many cases, audio won\u2019t even play even when a game requires users to \u201cclick to play.\u201d Most web games already have a \u2018click to play\u2019 button (to gain keyboard focus) but Chrome won\u2019t allow those game to play audio unless their click method specifically resumes the AudioContext (and few do, because until now there was little reason to). \u2014 Bennett (@bfod) May 7, 2018  Bennett Foddy, the creator of popular browser-based games\u00a0QWOP and Getting Over It with Bennett Foddy, explains games won\u2019t play audio unless their click-to-play method resumes the JavaScript \u201cAudioContext\u201d\u00a0application programming interface (API). But very few games\u00a0do this because previously, there was \u201clittle reason to.\u201d Foddy told the Daily Dot the updates required aren\u2019t difficult, but he suspects most developers don\u2019t have the components needed to make them. He explained updating the code won\u2019t take much work, provided that, \u201cone, you have access to all the servers the game is hosted on; two, you still have your source code; three, you made the game either using raw JS\/HTML or an engine that has been recently updated to respect this new policy (and the updates didn\u2019t break your game in some other way; and four, you have time to go through all your projects and update them.\u201d \u201cNaturally, this means only a tiny minority of existing games will be updated, even if it is not much of an issue for developers of future games on contemporary engines and\u00a0libraries,\u201d he added. While we don\u2019t know how many games are affected, Foddy says it\u2019s \u201cprobably millions.\u201d His immensely popular and undoubtedly frustrating ragdoll game QWOP was affected by the update. But Foddy was lucky. He built the title on his own server and was able to fix it with little effort. Others aren\u2019t so fortunate. Several prominent video game developers took to Twitter to voice their frustrations about the new Chrome update. Among those is\u00a0Terry Cavanagh, who has created more than two dozen games including the hit titles VVVVVV and Super Hexagon. He\u00a0says Chrome 66 \u201cbroke\u201d his in-browser creations.\u00a0 Chrome\u2019s audio update broke my html5 stuff too. I miss flash \ud83d\ude41 https:\/\/t.co\/oQdqOuNm6N \u2014 Terry (@terrycavanagh) May 7, 2018  The creator of the critically acclaimed title Stephen\u2019s Sausage Roll,\u00a0Stephen Lavelle,\u00a0also noted his games had been affected by the update. Ah fuuck sake. I had just in the last year started to begin to trust that I could reliably use audio in the browser (after years of reticence). So much for that\u2026looks like the chrome update broke all of the audio in my html5 games as well. https:\/\/t.co\/gT6n9pz0VT \u2014 Stephen Lavelle (@increpare) May 7, 2018  Indie video game hosting site itch.io was forced to post instructions on how to enable its \u201cclick to play\u201d feature so audio would play on its titles. Are you having trouble with HTML5 game audio not working from the recent Chrome update? You can enable \"Click to play\" across all games from your account settings: https:\/\/t.co\/tEq3G5ttMS \u2014 itch.io (@itchio) May 7, 2018  Even specific video game accounts are being forced to address concerns from gamers. We're looking into to sound issues across King games \u2013 hopefully you'll be able to enjoy the sounds soon. The issue should only be on Google Chrome so if you want to get to the sounds right away, consider changing browser types.1\/2 \u2014 Pet Rescue Saga (@PetRescueSaga) May 7, 2018  Influential blogger and former Kickstarter Chief Technology Officer Andy Baio hit out at Google and urged the company to reconsider how it blocks autoplay audio. I really hope @ChromiumDev is seeing this. Here, the creator of VVVVVV and Super Hexagon. https:\/\/t.co\/QIvzDDNJw9 \u2014 Andy Baio (@waxpancake) May 7, 2018  The developers affected by the update aren\u2019t necessarily opposed to Google preventing autoplay videos from blaring audio\u2014they just want the tech giant to come up with another solution. Foddy suggests a mute button in tabs that is enabled by default (you can already manually mute individual tabs on Chrome) or by adjusting code to make it more friendly for games that already have a \u201cclick to play\u201d button. \u201cOr they could allow annoying ads to be blacklisted, the way actual adblockers work,\u201d Foddy said. \u201cI\u2019m sure there are dozens of options that would preserve our cultural heritage while muting annoying ads.\u201d \u201cThe reason this change is objectionable is that when we make games for the web, which is built on international open standards, we expect them to remain playable for a long time (if not forever),\u201d he continued. \u201cAll this work exists on independently-run servers and complies with open web standards, and Google does not have the moral right to unilaterally kill it\u2026 Especially not when the only reason for it is that there are some annoying ads out there.\u201d UPDATE: I said in this thread that I'd realized Google posted autoplay-block documentation of Sept 2017 and I just didn't see it. I checked https:\/\/t.co\/KaGiKR9Aoz and ACTUALLY NO. THE WEBAUDIO BITS WEREN'T THERE IN SEPTEMBER THEY WERE EDITED IN LATER https:\/\/t.co\/olqys9YT0E \u2014 mcc (@mcclure111) May 7, 2018  It\u2019s unclear when Google first warned developers about how the changes would impact games on its browser. Video game developer Andi McClure pointed out on Twitter that information about WebAudio API was not originally included in the company\u2019s post about blocking autoplay videos. It appears those details were added later with code\u00a0to make the necessary changes. A very interesting thing about the policy is how it's carefully tailored in such a way it will not affect *Google's* audiovisual content site\u2014YouTube\u2014but *will* affect my site (https:\/\/t.co\/z3QZrUzGd6). The Chrome autoplay now constitutes a market barrier to entry for AV content pic.twitter.com\/b0dvN4a5uY \u2014 mcc (@mcclure111) May 7, 2018  It\u2019s important to note that Chrome doesn\u2019t block autoplay videos on all websites. The company said it allows autoplay for \u201cover 1,000 sites where we see that the highest percentage of visitors play media with sound.\u201d This exception will likely have little impact on indie game developers. Google does say it will start enabling sound by learning people\u2019s preferences. So if someone enables audio on an autoplay video from the same site multiple times, Chrome will enable audio on that site by default. Of course, developers will still need to make adjustments to their code to benefit. Update 6:26pm CT,\u00a0May 7: A\u00a0Google spokesperson confirmed in a statement to the Daily Dot that the most recent version of Chrome can present problems for game developers. \u201cWith Chrome\u2019s new autoplay policies, developers shouldn\u2019t assume that audio can be played before a user gesture,\u201d the statement said. \u201cWith gaming in Chrome, this may affect Web Audio. We have shared details on how developers can do to address this, and the design for the policy was published last year.\u201d \nPhillip Tracy is a technology staff writer for the Daily Dot, where he covers the gig economy, social media trends, and gadgets. He previously reported on IoT and telecom for RCR Wireless News and contributed to NewBay Media magazine. He is based in Austin, Texas.\t","time":1525789302,"title":"Latest Chrome update may have broken millions of web-based games","type":"story","url":"https:\/\/www.dailydot.com\/debug\/chrome-autoplay-block-games\/","label":9,"label_name":"tech"},{"by":"tysone","descendants":0,"id":17021038,"kids":"None","score":1,"text":"Advertisement Supported by The Stone By Stanley Fish Mr. Fish is a legal scholar and author. For some time now everyone has been worrying about \u201cfake news\u201d or the world of \u201calternative fact\u201d and wondering just how and why this unhappy phenomenon has flourished. My take on this question is simple, although I hope not simple-minded: Fake news is in large part a product of the enthusiasm \u2014 not to say rage \u2014 for transparency and absolutely free speech. I know that this is a counterintuitive proposition. I would like to begin my defense of it with an anecdote. In November 2016, Scott Titsworth, dean of the Scripps College of Communication at Ohio University, informed the university community of the first meeting of a presidential advisory group charged with recommending free speech policies for the campus. Titsworth reported that the group\u2019s first action was to affirm transparency as one of its \u201ccore values\u201d; the second action was to decide (unanimously) that its meetings would not be open to the public, but held in private. As you can imagine, it was easy to make fun of the obvious contradiction, but the contradiction is not so glaring once we understand that two notions of what \u201cfree\u201d means are in play here. The group wants (understandably) to be free of the pressures that would be felt if the proceedings were conducted under public scrutiny; at every moment members would be tempted to tailor what they said to the responses and criticisms of an imagined audience. In short, they would not be speaking freely but under a shadow if the meaning of \u201cfreely\u201d in force were \u201centirely without filters, gatekeepers and boundaries.\u201d That sense of \u201cfreely\u201d is championed by techno-utopians whose mantra is \u201cinformation wants to be free\u201d and who believe that the promised land predicted by the authors of every technological advance \u2014 the printing press, newspapers, the telegraph, the railroad, radio, television, the digital computer, the internet \u2014 is just around the corner. It is a land in which democracy\u2019s potential is finally realized in a communication community where all voices are recognized and none marginalized, with no one hoarding information or controlling access or deciding who speaks and who doesn\u2019t. What Titsworth and his fellow committee members see is that this more ambitious and abstract sense of \u201cfreely\u201d is antithetical to the successful completion of their task: Not speaking freely in front of everyone is a condition of speaking freely \u2014 without anxiety and mental reservation \u2014 on the way to exploring the complexities and difficulties of their charge. The moral (provisionally, and perhaps prematurely, drawn) is that transparency is not unambiguously a good thing. (I pass over for the moment the prior question of whether it is a possible thing.) And if that is right, then the proliferation of speech may not be a good thing either; silence and the withholding and sequestering of speech may be useful and even necessary in some contexts, like the preparing of a report or maintaining of a marriage. I say this aware that many free speech advocates believe that the more free speech there is the better the human condition will be, and who believe, too, that it is the business of our institutions, including our legislatures and courts, to increase the amount of speech available. At first glance the bias in favor of unlimited speech and information seems perfectly reasonable and even unassailable. What arguments could be brought against it? An answer to that question has been offered in recent years by a small, but growing, number of critics. In a 2009 essay in The New Republic titled \u201cAgainst Transparency,\u201d the law professor Lawrence Lessig (known as an apostle of openness), asked, as I just have, \u201cHow could anyone be against transparency?\u201d Lessig responds to his own question by quoting a trio of authors who in their book \u201cFull Disclosure: The Perils and Promise of Transparency\u201d observe that by itself information doesn\u2019t do anything; its effects depend on the motives of those who make use of it, and raw information (that is, data) cannot distinguish between benign and malign appropriations of itself. Misunderstanding and manipulation are always more than possible, and there is no way to assure that \u201cnew information is used to further public objectives.\u201d Another way to put this is to say that information, data and the unbounded flow of more and more speech can be politicized \u2014 it can, that is, be woven into a narrative that constricts rather than expands the area of free, rational choice. When that happens \u2014 and it will happen often \u2014 transparency and the unbounded flow of speech become instruments in the production of the very inequalities (economic, political, educational) that the gospel of openness promises to remove. And the more this gospel is preached and believed, the more that the answer to everything is assumed to be data uncorrupted by interests and motives, the easier it will be for interest and motives to operate under transparency\u2019s cover. This is so because speech and data presented as if they were independent of any mechanism of selectivity will float free of the standards of judgment that are always the content of such mechanisms. Removing or denying the existence of gatekeeping procedures will result not in a fair and open field of transparency but in a field where manipulation and deception find no obstacles. Because it is an article of their faith that politics are bad and the unmediated encounter with data is good, internet prophets will fail to see the political implications of what they are trying to do, for in their eyes political implications are what they are doing away with. Indeed, their deepest claim \u2014 so deep that they are largely unaware of it \u2014 is that politics can be eliminated. They don\u2019t regard politics as an unavoidable feature of mortal life but as an unhappy consequence of the secular equivalent of the Tower of Babel: too many languages, too many points of view. Politics (faction and difference) will just wither away when the defect that generates it (distorted communication) has been eliminated by unmodified data circulated freely among free and equal consumers; everyone will be on the same page, reading from the same script and apprehending the same universal meanings. Back to Eden! This utopian fantasy rests on a positive, vaguely perfectionist view of human nature: Rather than being doomed by original sin to conflict, prejudice, hatred and an insatiable will to power, men and women are by nature communitarian, inclined to fellowship and the seeking of common ground. These good instincts, we are told, have been blocked by linguistic differences that can now be transcended by the digital revolution. A memorable Facebook news release written by Mark Zuckerberg a few years back, cited by Evgeny Morozov in his book \u201cTo Save Everything, Click Here\u201d tells the happy and optimistic story: \u201cBy enabling people from diverse backgrounds to easily connect and share their ideas, we can decrease world conflict in the short and long run.\u201d The idea, Morozov explains, is that factions and conflict \u201care simply the unfortunate result of an imperfect communication infrastructure.\u201d If we perfect that infrastructure by devising a language of data algorithms and instantaneous electronic interaction that bypasses intervening and distorting institutions like the state, then communication would be perfect and undistorted, and society would be set on the right path without any further political efforts required. Talk about magical thinking! In the alternative and true story rehearsed by many, human difference is irreducible, and there is no \u201cneutral observation language\u201d (a term of the philosopher Thomas Kuhn\u2019s in his 1962 book \u201cThe Structure of Scientific Revolutions\u201d) that can bridge, soften, blur and even erase the differences. When people from opposing constituencies clash there is no common language to which they can refer their differences for mechanical resolution; there are only political negotiations that would involve not truth telling but propaganda, threats, insults, deceptions, exaggerations, insinuations, bluffs, posturings \u2014 all the forms of verbal manipulation that will supposedly disappear in the internet nirvana. They won\u2019t. Indeed, they will proliferate because the politically angled speech that is supposedly the source of our problems is in fact the only possible route to their (no doubt temporary) solution. Speech proceeding from a point of view can at least be recognized as such and then countered. You say, \u201cI know where those guys are coming from, and here are my reasons for believing that we should be coming from some place else\u201d \u2014 and dialogue begins. It is dialogue inflected by interests and agendas, but dialogue still. But when speech (or information or data) is just sitting there inert, unattached to any perspective, when there are no guidelines, monitors, gatekeepers or filters, what you have are innumerable bits (like Lego) available for assimilation into any project a clever verbal engineer might imagine; and what you don\u2019t have is any mechanism that can stop or challenge the construction project or even assess it. What you have, in short, are the perfect conditions for the unchecked proliferation of what has come to be called \u201cfake news.\u201d The rise of fake news has been attributed by some to the emergence of postmodern thought. Victor Davis Hanson, a scholar at the Hoover Institution at Stanford University wrote in 2017 that fake news can be \u201ctraced back to the campus,\u201d specifically to \u201cacademic postmodernism,\u201d which Hanson says, \u201cderides facts and absolutes, and insists that there are only narratives and interpretations.\u201d That\u2019s not quite right. The insistence on the primacy of narratives and interpretations does not involve a deriding of facts but an alternative story of their emergence. Postmodernism sets itself against the notion of facts just lying there discrete and independent, and waiting to be described. Instead it argues that fact is the achievement of argument and debate, not a pre-existing entity by whose measure argument can be assessed. Arguments come first; when they are successful, facts follow \u2014 at least for a while, until a new round of arguments replaces them with a new set of facts. This is far from the picture of Nietzschean nihilism that Hanson and others paint. Friction, not free invention, is the heart of the process: You commit yourself to the standards of evidence long in place in the conversation you enter, and then you maneuver as best you can within the guidelines of those standards. Thus, for example, a judge who issues a decision cannot simply decide which side he favors and then generate an opinion; he must first pass through and negotiate the authorized routes for getting there. Sometimes the effort at negotiation will fail and he will say that despite his interpretive desires, \u201cThis opinion just won\u2019t write.\u201d Any opinion will write if there are no routes to be negotiated or no standards to hew to, if nothing but your own interpretive desire prevents you from assembling or reassembling bits of unmoored data lying around in the world into a story that serves your purposes. It is not postmodernism that licenses this irresponsibility; it is the doctrine that freedom of information and transparency are all we need. Those who proclaim this theology can in good faith ignore or bypass all the usual routes of validation because their religion tells them that those routes are corrupt and that only the nonmethod of having no routes, no boundaries, no categories, no silos can bring us to the River Jordan and beyond. In many versions of Protestantism, parishioners are urged to reject merely human authority in any form and go directly to the pure word of God. For the technophiles the pure word of God is to be found in data. In fact, what is found in a landscape where data detached from any context abounds is the fracturing of the word into ever proliferating pieces of discourse, all existing side by side, indifferently approved, and without any way of distinguishing among them, of telling which of them are true or at least have a claim to be true and which are made up out of whole cloth. That is the world of fake news. It is created by the undermining of trust in the traditional vehicles of authority and legitimation \u2014 major newspapers, professional associations, credentialed academics, standard encyclopedias, government bureaus, federal courts, prime-time nightly news anchors. When Walter Cronkite was the longtime anchor at CBS, he was known as the most trusted man in America; and when he signed off by saying, \u201cAnd that\u2019s the way it is,\u201d everyone believed him. In the brave new world of the internet, where authority is evenly distributed to everyone with a voice or a podcast, no one believes anybody, or (it is the same thing) everyone believes anybody. This wholesale distrust of authoritative mechanisms leads to the bizarre conclusion that an assertion of fact is more credible if it lacks an institutional source. In this way of thinking, a piece of news originating in a blog maintained by a teenager in a basement in Idaho would be more reliable than a piece of news announced by the anchor of a major network. And, again, what has brought us to this sorry pass is not the writings of Derrida or Foucault or any postmodern guru but the twin mantras of more free speech and absolute transparency. Stanley Fish is a professor of law at Florida International University and a visiting professor at the Benjamin N. Cardozo School of Law. He is the author of many books and is currently at work on a book about free speech in America. Now in print: \u201cModern Ethics in 77 Arguments,\u201d and \u201cThe Stone Reader: Modern Philosophy in 133 Arguments,\u201d with essays from the series, edited by Peter Catapano and Simon Critchley, published by Liveright Books. Follow The New York Times Opinion section on Facebook and Twitter, and sign up for the Opinion Today newsletter. Advertisement    Collapse SEE MY OPTIONS","time":1525789172,"title":"\u2018Transparency\u2019 Is the Mother of Fake News","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/07\/opinion\/transparency-fake-news.html","label":7,"label_name":"random"},{"by":"amynordrum","descendants":0,"id":17021026,"kids":"None","score":1,"text":"Tornado survivors often compare\u00a0the terrifying, deafening roars of a twister\u2019s furious winds to the sound of\u00a0a freight train. But\u00a0storms also emit sounds that are inaudible to human ears right before producing a tornado. By detecting these infrasonic waves from miles away, researchers hope to develop an\u00a0earlier, more accurate tornado warning system. Today, weather agencies issue tornado warnings by closely observing storms for characteristic air movements. Warnings typically come about 10 minutes in advance. But most warnings are false alarms, says Brian Elbing, a professor of mechanical and aerospace engineering at\u00a0Oklahoma State University. \u201cSeventy-five percent of the time, tornadoes don\u2019t occur,\u201d he says. The high rate of error causes \u2018warning fatigue,\u2019 which can be deadly. After a 2011 twister ripped through Joplin, Missouri, taking 162 lives, a federal report found that a majority of residents ignored or reacted slowly to warnings in the crucial minutes before the tornado hit. Elbing is trying to understand\u00a0the secrets held\u00a0by a storm\u2019s infrasonic signals in order to increase warning time, make warnings more accurate,\u00a0and improve tornado prediction.\u00a0 Earthquakes, avalanches, and\u00a0rocket launches\u00a0all generate infrasonic signals, which have frequencies below the range that humans can hear\u2014generally considered to be 20 to 20,000 Hz. These low-frequency waves are weakly absorbed in the atmosphere so they can travel all the way around Earth. Infrasound has been used to locate enemy aircraft and to monitor for nuclear blasts and natural hazards. In\u00a0the late 1990s, scientists at the National Oceanic and Atmospheric Administration found that tornado-producing storms can emit infrasound signals up to two hours before tornado-genesis. NOAA deployed a demonstration infrasonic network to monitor tornados in 2003. Researchers including Elbing have more recently started their own observations, and are trying to improve the hardware and software to detect tornadoes by infrasound. Tornados themselves also emit infrasound signals in the 0.5 to 10 Hz range depending on their size, Elbing says. He and his colleagues have set up a simple infrasonic array on the university campus to collect signals from both tornados and storms. The array consists of three commercial infrasound microphones placed in a triangle, spaced about 60 meters apart. The challenge is to distinguish infrasound signals from wind noise. \u201cIt\u2019s like being in a loud room trying to listen to somebody talk softly,\u201d Elbing says. So the team has enclosed each microphone inside a container with four openings. A hose is attached to each opening, and the hoses are stretched out in opposite directions. Each hose manages to catch and funnel some infrasonic waves to its microphone, while reducing the amount of wind that reaches it. Computers compare\u00a0the signals recorded at each microphone, and perform filtering and signal processing to further\u00a0minimize\u00a0noise. If all three signals look alike, that rules out wind, Elbing says, because the noise from wind would be not coherent. By analyzing those signals, the researchers can then create a simple computer model of the fluid mechanism that produces the infrasound waves. Their system had its first success last May, when a tornado hit Perkins, Oklahoma, which is about 20 kilometers from the university. Ten minutes before the tornado hit, the array picked up extremely strong signals. Based on the frequency, the researchers predicted a tornado size of 46 meters. That was precisely the official width of the twister\u2019s destruction path. They are presenting their results today at the Acoustical Society of America meeting in Minneapolis. This is just the first step. The group\u2019s goal is to develop a computer model that can predict tornados. Elbing\u2019s team now plans to collect data from more tornados to test and improve the model.\u00a0\u201cThe biggest need is more observation,\u201d he says. \u201cThere are only a handful of observations of tornadoes where you can look at the infrasound spectra.\u201d Editor\u2019s note: This story was updated on 8 May to correct the name of the city affected by a 2011 tornado to Joplin, Missouri from Joplin, Montana.   Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum\u2019s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. \u00a9 Copyright 2018 IEEE \u2014 All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.","time":1525789090,"title":"Spying on a Storm's Infrasonic Signals to Improve Tornado Warnings","type":"story","url":"https:\/\/spectrum.ieee.org\/tech-talk\/computing\/hardware\/spying-on-a-tornados-infrasonic-signals-to-improve-warnings","label":5,"label_name":"ml"},{"by":"srushtika","descendants":0,"id":17021025,"kids":"None","score":1,"text":"\n\n\n Your browser has Javascript disabled. Please enable it to use this site. Hide this warning\n In this tutorial, we will see how to use Ably Realtime client library to build a realtime commenting system with React. We\u2019ll be making use Ably\u2019s public channel. Once a comment is made, we\u2019ll publish it to the public channel. We will also be subscribed to the channel so we can see comments as they are added in realtime. This tutorial will not include any server-side implementation. Hence, comments won\u2019t be persisted to any storage. React is a JavaScript library for building user interfaces. It is component-based and declarative which makes it easy to build complex UI with it. For more infomation on React, please refer to their official website https:\/\/reactjs.org.  In order to run these tutorials locally, you will need an Ably API key. If you are not already signed up, you should sign up now for a free Ably account. Once you have an Ably account: We\u2019ll start by creating a new React app. To do this, we\u2019ll be using create-react-app. create-react-app allows you to create a React app without having to worry about build configurations. To use it, you\u2019ll need to install create-react-app locally if you don\u2019t have it already: Once installed, we can use it to create a new React app. We\u2019ll call it reactjs-realtime-commenting: The new React app is ready. We can start it by running: This builds the static site, runs a web server and opens a browser. If your browser does not open, navigate to http:\/\/localhost:3000. See this step in Github Before we move on to building upon this app, let\u2019s quickly delete some files we won\u2019t be needing. Run the command below to do so: See this step in Github Now, we\u2019ll create a new folder within the src folder called components. This folder will contain all our React components. Once that is created, we need to move App.js into the components folder: See this step in Github Since we have deleted and moved some files which are still being referenced in src\/index.js and src\/App.js, we need to update these files to not reference the deleted and moved files. Firstly, update src\/index.js as below: Next, update src\/components\/App.js as below: See this step in Github We\u2019ll be using Bulma CSS framework so as not to waste time writing CSS. We\u2019ll reference it from CDN (Content Delivery Network). Add the line below to the <head> section of public\/index.html: See this step in Github To start using Ably you first need to install the NPM module. The NPM module can be installed as follows: The client library must be instanced with the API key you copied in Step 1. Note in production we recommend you always use the token authentication scheme for browser clients, however in this example we use an API key for simplicity. We\u2019ll create a dedicated file for this and instantiate a global Ably instance so we can easily use it in multiple places. Within src folder, create a new file named ably.js and add the code below into it: Next, let\u2019s reference the file created above in src\/index.js: See this step in Github Within the components folder, create a new file named CommentBox.js and add the code below into it: This component renders a comment form. Once the form is submitted, we trigger an onSubmit event which in turn calls addComment(). The form won\u2019t do anything for now as we are yet to create addComment(). See this step in Github Next, we\u2019ll create addComment(). Still within components\/CommentBox.js, add the code below just before render(): We first prevent the default form submission behaviour (that is, reloading the page). We then get the form inputs entered (name and comment) and make sure they are not just some empty strings. If the inputs are properly filled, we add the comment just made to the array of comments. Then we publish the comment just made to a comments channel with an add_comment event. Adding \/*global Ably*\/ tells ESlint that we are using the global variable (Ably) we defined earlier. With this, we won\u2019t get the Eslint error: 'Ably' is not defined no-undef. Lastly, we clear the form fields. Next, let\u2019s bind addComment() to the this keyword by adding this line to the constructor() just after super(): See this step in Github Within the components folder, create a new file named Comment.js and add the code below into it: This component renders a single comment. It accepts the comment as props. Props are custom attributes that are used to pass input data to components. See this step in Github Within the components folder, create a new file named Comments.js and add the code below into it: This component accepts a comments props and renders the Comment component for each of the comments available. It passes the actual comment to the Comment component as props. See this step in Github The App component will serve as the parent component. This means the App component will contain other components which will be nested within it. Open components\/App.js and replace it content with the code below: This contains the components we created earlier. We define a comments state which is an array of comments. It defaults to an empty array. This will be updated as we add comment through the comment form. Also, we pass the comments state as props to the Comments component. This is how the Comments component get the comments it renders. Calling reverse() on comments allows us to display the comments in reverse order i.e. newest at the top. See this step in Github Recall from step 9 above, we called handleAddComment() which is responsible for adding a new comment to state. But we are yet to create this function, let\u2019s do that now. Within components\/App.js, add the code below just before render(): The code above adds the comment that was made to state. This way, the comments list is updated with new comments in realtime. Next, still within components\/App.js, let\u2019s bind handleAddComment() to the this keyword by adding this line to the constructor() just after super(): Also, still within components\/App.js, pass handleAddComment as props to CommentBox component as below: See this step in Github We\u2019ll be using Ably\u2019s history feature to display our comments in realtime. Before we can make use of the history feature, we need to first configure our channel to persist messages to disk. Channels can be named using any unicode string characters with the only restriction that they cannot start with a [ or : character as these are used as \u201cspecial\u201d characters. A colon : is used to delimit channel namespaces in a format such as namespace:channel. Namespaces provide a flexible means to group channels together. Channel grouping can be useful when, for example, configuring permissions (capabilities) for a set of channels within a client token or setting up rules to apply to one or more channels. We will be using channel rules in this tutorial to ensure all channels in the persisted namespace are configured to persist messages to disk i.e. we will explicitly enable the history feature. Follow these steps: You have now enabled history for all channels in the persisted namespace i.e. any channel with a name matching the pattern persisted:* will store published messages to disk. We have seen how we can publish newly added comments. Now, we need to display those comments being published in realtime. We\u2019ll do this with a React lifecycle componentDidMount() hook. Still within components\/App.js, add the code below just before handleAddComment(): The componentDidMount() hook runs after the component output has been rendered to the DOM. It is a good place to fetch our comments from the comments history. We connect to the comments channel and listen for the attached event. Then we update the comments state with the comment pulled from history. Notice we reverse the comments coming from history, this will allow us to reverse all the comments (from history and newly added) to the appropriate order (that is, newest at the top) at the point of rendering (that\u2019s why we reversed comments state in step 12). See this step in Github \n\n That\u2019s it. We now have a realtime commenting system. To test it out, start the app: Then open http:\/\/localhost:3000 in two different browser tabs. Add a comment in one of the opened tabs and watch the other tab update with the comment in realtime. The complete source code for each step of this tutorial is available on Github. We recommend that you clone the repo locally: Checkout the tutorial branch: Install NPM dependencies. Be sure to switch into project\u2019s directory and then run this command in your terminal: And then run the app locally by adding your Ably API key to src\/ably.js and run to start the web server and open the browser. 1. If you would like to find out more about how channels, publishing and subscribing works, see the Realtime channels & messages documentation\n2. Learn more about Ably features by stepping through our other Ably tutorials\n3. Learn more about Ably\u2019s history feature\n4. Gain a good technical overview of how the Ably realtime platform works\n5. Get in touch if you need help If you need any help with your implementation or if you have encountered any problems, do get in touch. You can also quickly find answers from our knowledge base, and blog. \nIt includes\n3m messages per month,\n100 peak connections,\n100 peak channels,\nand loads of features. You can unsubscribe at anytime You can upgrade at any time. \nDon't have an account?\nGet started for free\n I forgot my password","time":1525789069,"title":"Build a realtime commenting app with ReactJS and Ably","type":"story","url":"https:\/\/www.ably.io\/tutorials\/reactjs-realtime-commenting","label":3,"label_name":"dev"},{"by":"ingve","descendants":1,"id":17021016,"kids":"[17021699]","score":1,"text":"\r\n                            Stack Exchange network consists of 173 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\r\n                         While the timing of this post coincides with us expressing some serious concerns around how we're not doing a good job of helping and guiding Stack Overflow to remain a welcoming place for everyone, this is something that's been weighing heavily on our minds for quite some time, and applicable to any site that's wired into chat (AKA, all of them).   Sometimes you have problems that stay dormant for months, heck, even years, but when they flare up \u2014 it's really ugly. I'm going to make a very firm statement that I'm super proud of 97% of our chat rooms that remain some of the safest places to hang out and 'talk shop' on the Internet; you folks are doing an amazing job of helping us prove that groups of responsible people tend to bring out the very best in one another given loose rules that are often open to interpretation (see linked and related posts, too). Unfortunately, I need to take a moment and talk about the remaining 3%1.  Those of you that regularly use chat have probably noticed that each room has a rather distinct culture. In some rooms, a little off-topic 'fun' is not only permitted, but also encouraged, and generally serves to make the culture of the room brighter, and the experience of spending time there more rewarding. In these rooms, troll-like behavior or other things that don't reconcile well with our code of conduct are quickly flagged and removed. Other rooms prefer to keep the conversation more on-topic, with a focus that's more like a laser than a campfire. Our guidance has always been to essentially go with the flow, as long as that flow isn't something that doesn't appear to belong on our chat system, or doesn't easily come to terms with our code of conduct.  First, you just invite a lot of rule-lawyering (the Internet version of but I'm not touching you!! I'm not technically touching you!!!) and second, new people see this oddly specific list of things like \"Please don't talk about what monkeys really mean by farting\" and wonder what kind of crazy people might be lurking behind the door. I could give more real, concrete examples - but let's not go there. What positively has to function in order for these rooms to exist with our branding behind them is: None of this is new, and as I said earlier, problems sometimes pretend to go away while they secretly find ways to bite you even harder - I put the blame for needing to come here and reiterate all of this yet again squarely on us. But that doesn't absolve folks from the responsibilities that go along with the privilege of using chat.  Chat is a great tool, and we are really proud of the caliber of discourse that flows through our systems every day. We want to keep it available because we're really proud of what most folks do with it.  But we can't have self-policing break in the face of flagrant violations of our Code Of Conduct, and we'll be enforcing that with calm, steely-faced smiles going forward.  Questions? Observations? Anything else? Please leave an answer or a comment. We love leading when it mostly means gently guiding people to do what's good for all of us, and we really don't like it when we need to do it more deliberately. But, we're the custodians of the reputation all of you helped us to build, so we must. 1: Percentages are derived from Tim's brain via the Anecdotal3000 percentage generator implanted by Stack Exchange, Inc. So, in keeping with This Meta.SO post, I figure I might as well mention the obvious:  Let me describe a real event that illustrates this. I was in a room where there's banter. People sometimes take friendly jabs at each other and someone crossed a line. Someone mentioned it and the poster admitted they had a crossed it too. But the 2 minute limit had been reached and the concrete was hard. This left us in a lurch. Normally messages a Room Owner dislikes just get moved to another room (like Trash or a custom trash chat room), but that doesn't delete them. The only options left were I'm a bit baffled by this, because chat is very much like Twitter in that it's a little bit of a \"stream of consciousness\" (case in point) but lacks any ability to delete messages without a ban. There's three things I'd like to see added here if we're going to add more accountability This sounds perfectly reasonable and I support this stance. Stack Overflow should not stand for abuse nor should it tolerate it.  Thank you for this. Not once has a community manager asked chat users how we're doing during the 5 years I've been here, whether or not we're happy, if we're given the tools we want or need to effectively moderate. We get tools we ask for after several years maybe. As an example: I've asked for some tools in order to be more welcoming. This was ignored for a year and then we were told there is no problem 3 years later by a community manager. (I'll gladly provide more examples if you'd like). I am certain that when racism, bad culture or other problems happen you are rarely aware of it. One anonymous community manager* has been kind enough to show up from time to time to give us advice or tell us when we do things you don't like. Their advice has been helpful. If there are cultural issues you'd like to address with a certain room - I recommend we sit down and have an honest discussion about it. I think Stack Overflow needs to communicate a lot more clearly to its community using the chat service.  I'm going on a limb here and going to discuss the JavaScript room and the recent unfortunate interaction that resulted in this post.  To be clear - though it appears that the community at large agrees with the JavaScript room's ability to govern its on-topicness - the JavaScript room itself wants to welcome any efforts to improve its culture. We've had members speak up against things that bother them, we've always been very open to discussion. During those few days no one from Stack Exchange talked to us. The only communication we've had on your side is on social media** and Shog's comment on an answer attacking us. (Well, unless you count the comment a community manager left on my answer and later deleted) We've always optimized towards \"let's not get Stack Overflow staff involved\" because we mostly like you humans and we don't think wasting your time on a small subset of the site is worth it. We don't want to alienate you, frustrate you nor make the situation adversarial. I would very much like to deescalate the situation as much as possible. We want inclusivity, we want to be more welcoming. That is not at odds with allowing \"adult\" off-topic topics like drugs and we're all up for discussion which we're doing internally but would happily do externally as well as explained in my postmortem.  * it's always Shog. ** We have decided to not respond on twitter to not escalate the situation. I'm appreciative of this move. I've seen at least one room go beyond acceptable behavior and be closed down and, at the same time, I've heard of rooms that regularly have very constructive discussions about subjects that would normally lead to very problematic behavior.  While I'm not always in chat at all times, I tend to agree that when I see flags, they tend to involve the same rooms or users - some of the latter who have dozens of short (30-60 minute) chat suspensions. This seems a strong indicator that at least some of the problem lies in our (the moderators) response and (possibly unwillingness) to give these users the longer suspensions they deserve.  How difficult would it be to automatically increase the suspension length based on all suspensions over the last n time with time being greater than hours, which it is right now? This means that repeated offenses in a week\/month\/year would be automatically recognized and the suspension would more directly correlate with continued poor behavior without the intervention of a moderator (in the case of 10k flag message deletion). It's worth noting that removing a chat suspension is pretty simple in cases of poor flag handling.  These users shouldn't be allowed to continue chatting but, often, the moderators may miss these events entirely as the only historical record of it is on the user's chat profile or on an admin page that many mods are likely unaware of that wasn't really intended for general use.  I really don't like putting more work on your plates but I think these things would help: Thanks, again!  Well - I guess its about time. I don't really have that much newer info than I did the last time we talked about this three years ago. The chat flag system is still a little odd; there's no real \"out of band\" way to keep track of problem sites and users. One thing I think is an issue is how there's no real \"framework\" for what suspensions should be. Contrast the mod message system on main sites with how suspensions work on chat - with arbitrary numbers of hours settable.  In a sense, outside the rooms we're regulars in we don't really have context. A random moderator might not know a certain user has a tendency to kinda be on the borderline or even offend regularly. A mod who is a regular would throw the book at him. A non-regular one would need to go through the user's suspension record and decide.  And while it's not quite how things have worked lately - we kind of need a backstop. We do realise that our CMs are busy, but sometimes folks may need help from someone with a little more authority to sort it out. Sadly, effective chat moderation is instant or close enough, and it would be nice to have someone to ask folks to knock it off, or even hang out.  If there's one thing I've learnt being a RO and mod on root access - effective moderation starts with the community. For the most part we know what we need to do. Bad chat moderation usually ends in fire.  Also having been appraised of the situation - I do realise that it's a potential PR issue - but if a CM's going to talk about a chatroom on twitter, could we have someone pop by and talk to the room please? (If it has happened, well, awesome, but it should be a policy. \"I've talked to the folk involved, and ______\" would be such a nice thing to see). Communication and setting expectations is very important here.  Please tell us more directly what you want from chat behavior.\nCommunication through concrete examples would make things so much simpler.\nUsing examples doesn't mean giving an exhaustive list, it means giving people something to extrapolate from. If a room is going in a bad direction, come into the room and tell people that there is an issue and explain what the issue is. I would like to see concrete examples of behavior that have crossed the line.\nI understand what the rough intention behind the \"be nice\" policy is, but I don't know where the actual line is.\nReal examples would help a lot when moderating chat.\nIf you want to keep the bad examples to moderator eyes only, that's fine. Chat self-moderation starts with the individual user.  We've been talking about ways to make moderation easier (and that's desperately needed!), about ways to empower room owners more, about automatic suspensions with teeth... but the individual user, the author of a chat message, lacks one important ability:  after two minutes, whatever you said is permanent without moderator intervention. We need to allow users to say \"that thing I said was pretty stupid\" (or didn't come out right, or was the scotch talking, or whatever), and let them clean it up themselves.  We allow users to delete their own comments at any time, even if it might make other comments obsolete or puzzling; if they can do it with comments, why not in chat? In comments Shog raised concerns about abuse; apparently there have been cases where people have done creepy things in chat and then tried to cover up the evidence.  I think we can find a workable place between \"locked in at 2 minutes\" and \"can delete everything and hide the bodies\".  Perhaps (just as a starting proposal to be refined), we limit people to five chat deletions per day and only allow deletion of messages within the last day.  This allows the user who realizes (but not immediately) that he messed up to fix it, without opening the door to widespread deletions. We could also raise a moderator flag if somebody deletes some threshold number (or percentage) of recent chat messages, like the flag we get for a sufficient volume of comment deletions.  If somebody deletes the allowed five messages a day, every day, we probably want to notice that.  This auto-flag isn't so that the deletions can be reversed; neither chat deletions nor self-deleted comments can be reversed by moderators.  It's to let moderators know that there might be a concerning pattern to look into.   We could also log post-two-minute deletions in the chat user's history, alongside the flags, to make it easier for mods to see what users are deleting hours after the initial post.  This would make it easier to review attempted coverups of creepy behavior. I've only ever spent very little time in chat, so maybe I'm missing some important context, but I feel strongly enough about this that I think it deserves to be an answer, though it has been mentioned in the comments. I'm distinctly getting the impression that this will be a one-strike, no-warnings policy. This seems fundamentally unfair. Certainly a chat room \"going bad\" is a gradual process, and there ought to be several red flags along that process where it would be natural to give mods a warning of \"get your things together, or we're going to have to delete you\". In this way, a room which might be beginning to display bad tendencies could be guided back towards good behavior, and the necessity to annihilate could be avoided.  Of course, rooms which refuse to comply will have to receive their due consequences. Other people have mentioned strengthening chat room moderation tools. Again, I really don't have enough experience in chat to attest to any lack thereof, but enabling moderators to do their job better would certainly go hand in hand with encouraging moderators to do their job better, rather than preemptively pulling the plug with little to no warning. Aside from this, I'm definitely in favor of deleting chat rooms that refuse to comply with the Code of Conduct; I just feel we need to take care to strike the right balance and not be needlessly punitive where the necessary action might not be so drastic. I appreciate this motive about being aggressive towards the chat users who are regularly breaking \"Be Nice\" policy and being problematic for constructive discussions in chat rooms. I also support action against chat rooms which attract flags and controversies on a regular basis. This also makes chat a useful place after main and meta sites. (Some users in the past said in their answers that chat is not much useful).  It is said that the rooms where code of conduct is violated will be permanently shut down. Now what is offensive in chat is not as clear as it is on sites. It is depending purely on the luck of the flagger and fate of the flag on which moderator it reaches.   One of the points in the Code of conduct is:  Name-calling. Focus on the post, not the person. That includes terms that feel personal even when they're applied to posts (like \"lazy\", \"ignorant\", or \"whiny\").  Name calling. It specifically black lists the word \"ignorant\" which may feel personal.  Of course, this should not be taken literally when there are funny and hilarious conversations. This is about pretty serious conversations when one of the users leave the conversations without any word. I have read some conversations like that where there were repeated use of the word. I asked the moderator and they replied that the word should not be taken literally but with respect to context of conversation and topic. They didn't remove those messages saying they (mod) were not referring to the opposite person. It is fine to say \"ignorant\" within the context of that specific site and said that site specific rule applies more than general policy of Stack Exchange. So, I had nothing to say more. Left silently.  Bigotry of any kind. Language likely to offend or alienate individuals or groups based on race, gender, sexual orientation, religion, etc. will not be tolerated. At all. (Those are just a few examples; when in doubt, just don't.)   Not before the recent Stack Overflow blog post, I had another conversation where there were some personal comments about a user. They are not literally rude if we see individual chat messages but definitely problematic if read complete conversation. I flagged one such message without a second thought. The flag didn't survive a moment. I didn't flag any more messages because I know the result. Those messages survived. I had nothing to say to the user who typed those messages except \"Be Nice\". (Where in the chat is aggression except in your words and my thoughts?  Those messages received stars instead of flags :\/). Left silently. Some Room Owners ignore such conversations even took into their notice. Moderators don't visit chat often to check the history and conversations. Some messages look controversial but sometimes they say the reason of Room Culture. I also Room Culture should be taken into consideration while taking actions but how many times? Sometimes, it is repetitive but the action taken is minimal. Deletion or moving of the messages happen. Users continue such behavior again. Some only create rooms to talk about a separate and topic of the site. They don't care about moderation. This is also a reason that many messages are not flagged and brought to notice of a moderator. I believe there will be many instances like that where there is no active participation of Room Owners and moderators in the chat. These conversations happen when there is absence of moderators. Chat flags are not effective all the time. Room Owners have limited tools to action against such conversations. So, to be honest, self moderation is not going on in many chat rooms where mods are not around. It is only going in active rooms where there are always 2 ROs and a moderator to check what is happening around in the room. I also believe that these incidents happen in some of those 97 % of the rooms. It may not occur regularly but this is not occurring once or twice in two months. If the team has taken the decision of 3 % of rooms, they should look at unnoticed 1% 1  in the rest of 97 % also.  Some users are not aware of the responsibilities of a Room Owner. They do not know that they should also moderate the chat room in the absence of moderator and when there are some problematic messages in the room. This is also another reason for messages are not being flagged.  So, here are my requests:  Uniform and accurate guidelines to the moderators on when to take actions on users.  More tools to RO to take actions on problematic users. Removal of Room Owners if they they have not visited the chat for a long time. Like the system selects a new owner based on activity, there should be some process to remove them too.   Community Mangers and moderators should check flags and conversations of the rooms even with less activity. Update of Chat FAQ. The chat FAQ is still old. The ancient version of mobile chat updated to new version but the FAQ are still old and outdated. They need to be updated.    1. Percentages derived from Nog's brain from his experience on chat.stackexchange.com  The concept of \"normal user\", \"room owner\" and \"moderator\" needs to be expanded into more granular abilities: (imagine file system per-user rights, it's a very similar concept) Deleting messages should be possible in ranges, same as with moving them. Flags should only propagate outside the room or site after 3-5 minutes if no action is taken by users and if there are no online moderators who can see it in mentions and come into the room to figure it out. I rarely use chat and I have no opinion on how it functions, but I do wonder about the language used here. Is it really necessary to enforce self-moderation aggressively? Could one not enforce it carefully or diligently or even cheerily?  I understand the word 'aggressively' is meant to convey the seriousness of SE's intent, but must that really be brought so ... aggressively? It might be a cultural thing: in American shows nowadays there's a tendency towards aggressive language, even in comedies people are all the time killing it or crushing or destroying someone. It gets the laughs. But SE is not going for easy laughs here, just making a statement and giving it some extra charge by referencing violence. I notice it and it bothers me. \"We're more actively enforcing self-moderation in chat\" gets the message across just as well. The problem I have with this, and I suspect part of the reason the rules have to be really vague, is that there are (at least) two completely different kinds of chatrooms. In short, I don't think the rules of behavior can, or should be identical for proper posts, Third Place chatrooms, and Post Argument chatrooms. I realize reading over the question that likely what isn't being said is that some \"Third Place\" chatrooms were regularly getting content far outside the standards I outlined as appropriate for them under point 1 above. But part of my point here is that the need for things to be looser in Post Argument chatrooms is confusing things here. Perhaps if the software made some kind of distinction, it would be easier for us humans to do that somewhat objectively as well? There are some things that really ought to be functionally different for Post Argument chats too (eg: they really shouldn't get deleted as quickly, because then people who come late and want to join the discussion end up doing it back in the post comments. Arg!!) My brain requires rules to be black-and-white in order to have any hope of being retained. However, the rules of life (especially surrounding interpersonal contact) are often subjective, so in these cases I try to find a comparative simile that I can instead remember. I've been using chat regularly for only a couple weeks, and I found myself forgetting that I was still on Stack Overflow, which I now attribute to the different \"vibe\" of the live 1:1 venue (as opposed to a forum where \"paying attention to what everyone says\", is the whole point).    At one point I realized I had posted a piece of personal data that should not have been shared. The member I shared the information with wasn't the issue -- the problem was that within 2 minutes \u2020, it was now a permanent record in that chatroom (and potentially permanently Google-able) and thus becomes an indelible mark on both me and on the site we all work so hard to keep \"a step above\" other forum sites. I contacted a mod to remove the item and while waiting I looked through my own history and was shocked at myself \u2014 politics were only part of the off-topic and\/or inappropriate things I has brought up, intermingled with programming talk. Anyhow in the end, a couple patient mods heeded my request to delete the room entirely, but the process made me realize that the \"behaviour rule\" actually is very black-and-white (for me, anyhow): (i.e. behave the same way we would in an office environment, or perhaps, a school.) To me, this means: Just like a workplace or school, if we want to venture outside of what's appropriate, we're perfectly able to -- elsewhere. Make plans to \"meet\" somewhere after \"work\" for the electronic equivalent of a pint of beer.  That's a more appropriate place to \"unwind and let loose\", get raunchy, misbehave, and then recover in time to return to \"work\" the following day, reassured that \"work\" and \"play\" have been kept safely distanced, thereby protecting yourself and your \"co-workers\" from embarrassment or worse.    I'm generally not great with similes and I've pushed this one so I'm not sure if I'm properly communicating my point, but am I on the right track? \u2020 After posting a chat message, we unfortunately we only have 2 minutes to \"take back\" something we say if we end up regretting it.  I believe this is not enough time for us to: Walk away from computer \u2192 realize what we said\/did\/posted \u2192 run back to the computer \u2192 find the message \u2192 edit\/delete it. I have a meta question proposing an increase (or alternate rules like \"room owner can always delete\" for this reason. I just need to drop my view cents here. I already wrote a comment: Ah yep I can confirm that you close rooms in that case, unfortunately you suggest that it would be possible to reopen chats. However you didn't give a final statement for more then 6 month, that you will leave the room closed. That is how you lost my trust in community management. It is valid and correct to close rooms for good reasons, but don't imply that there are chances to change that if you already decided to leave a room closed. I really want to give some more background for my comment. I was one of the room owners of \u201cAndroid Era with Kotlin and Java\u201d. In that room happened some shit which is not excusable. The majority of the room owners where informed that the room will be closed some minutes before the room was closed. It was implied that the room will be closed and if we would add some good reasons the room may be opened again. We never got a final answer; now it does not matter at all. In the end I am sure that the room was closed by missing moderation tools. I'm sure that the room had been saved for good if there would be tools to enforce \"room rules\". Like no gifs, per room configurable ban of users which where simply not welcome in a specific room. A more intuitive explanation of the timeout feature. A official bot API would be nice for many reasons (e.g. for fun or documentation lookup). In the end chats can be a good thing with the right people or can go to the dogs with the wrong users. I've become a believer that each chat room has it's own culture and tend to leave the chat rooms to themselves, unless something is flagged. This does not mean I condone abusive behaviour, or think it's ok, as long as it's not flagged, it means I just don't going into chat rooms looking for problems. So the users who are upset about losing their autonomy, need to understand, it's their own actions that are causing them to come under scrutiny. Flags draw in mods and are a reminder to the room, that a chatroom is public and that room culture is  subjected to the same standards that apply across the network. The rooms are public and users need to accept that they are under the same terms of all else on the site. So if chatrooms don't want to be moderated too closely, they need to be careful not to be too flag worthy in their chat, if that makes sense. I say that whenever I enter a chatroom with flags. If you don't want mods sticking their noses in, tone down the conversation.  Yesterday, an unacceptable comment was flagged and some of the users in one room could not control themselves from misbehaving for long enough to allow me to leave the room, resulting in several of them being kicked temporarily from the room. The offensive remarks are deleted, but you can see I let some stuff slide, I wasn't in there to cause them grief, just remove the worst stuff.  The suspensions continued on with other mods later on, as clearly there were a bunch of chatters finding it hard to not  attract attention. Making me think they enjoy pushing the boundary, as some sort of fun and games.  This isn't an unusual experience for this room. In effect some people are their own worst enemies.  In this climate of trying to tidy up the site and the high level of public scrutiny there does need to be consequences. The nature of community moderation means we're all responsible for the sites direction. It's not that hard to have fun in a chatroom without crossing the lines of decency. I'm all for preserving chatrooms and, to an extent, their unique cultures, but there needs to be give and take. While we're talking about chat flags, we also need a way to prevent misuse of the flags for simply advertising a message to every 10k user on the network. There are rooms that regularly and consistently violate this Meta post. For ages, nothing has been done to either those rooms or the posters there who get flagged. Or only when the feelings of people who think like SE management politically get offended?  I predict some day that Stack Exchange will be up for a notable prize in some social studies area.  And they will look at this decision as the reason that SE does not deserve one. The disturbance in chat is trying to tell you something very important.   I do not know what that something is.  But I hear it screaming out loud and clear.  And instead of listening to the early warning signals, the choice is being made to turn off the alarm and ignore the problem. The problem is not going to go away.  It is a virus in the system that is man.    Fix the problem.  Do not silence the alarm. This all sounds great. But: and someone crossed a line.  Who decides where \"the line\" is? It's highly subjective, and giving some people the arbitrary power to decide about \"the line\" without a very broad consensus of the user base amounts to a Police State approach. Unfortunately, I need to take a moment and talk about the remaining 3% Why is it so compelling to make a big issue out of a very small percentage of the user base? Are you striving for Utopia here? Compared to the real world of the street, only 3% is virtually Utopia already. I find this \"initiative\" to have sinister implications: SE folks getting more and more interested in thought-control and social engineering, while using the sites as their own private laboratory.  I found the Stack Overflow Isn\u2019t Very Welcoming. It\u2019s Time for That to Change. post (no place for comments there - which speaks volumes)  to be condescending and self righteous and just plain whiny and annoying. Let\u2019s start with the painful truth... I come on here to learn and teach, not for psychological counseling and a scolding from somebody half my age telling me that \"we have a problem\".  Are we being racial profiled by SE? How do you know which developers are of what color or gender, and what is your empirical evidence that the claims made by Hanlon are true? Anybody outside your own little circle of like-minded people verifying and validating those claims? What is the story there?  Jay Hanlon - EVP of Culture and Experience. I don't get it: How did \"Culture and Experience\" become so important that we have to get a scolding for some \"EVP\" , who, AFAIK, doesn't seem to be a big participant on the sites. Is he our baby sitter? We're required to stick to the topics of the sites - programming or music or math or whatever - why must everything be become \"culture and experience\". Let people be - we are adults here and most of us know how to behave. It's not perfect, but so what? Nothing is - and these sites are much better than most, without any dire sounding interventions from someone with the Orwellian title of \"EVP of Culture and Experience\". It's your site - fine - you can do what you want. But when you start treating users like subjects in a social engineering experiment, trying to put controls in place in the hopes of controlling the behavior human lab rats, expect them to flee fast. If this sort of thing continues, I know I will do just that, and encourage others to do the same. I remember a few years ago, after the SCOTUS SSM decision, you tried to make some sort of political statement about that in your logo. Thankfully, we were able to shoot that down. Hopefully this \"initiative\" will also fall flat on its face and you'll have to use real lab rats for your experiments, instead of your users. Is there no end to your efforts to play thought police? We're More Aggressively Enforcing Self-Moderation In Chat Self moderation? Surely if there's an issue, moderators aren't doing their job. Surely you then need to improve the moderation rather than anything else.  While the timing of this post coincides with us expressing some serious concerns around how we're not doing a good job of helping and guiding Stack Overflow to remain a welcoming place for everyone, this is something that's been weighing heavily on our minds for quite some time, and applicable to any site that's wired into chat (AKA, all of them).  Good stuff.  Sometimes you have problems that stay dormant for months, heck, even years, but when they flare up \u2014 it's really ugly. I'm going to make a very firm statement that I'm super proud of 97% of our chat rooms that remain some of the safest places to hang out and 'talk shop' on the Internet; you folks are doing an amazing job of helping us prove that groups of responsible people tend to bring out the very best in one another given loose rules that are often open to interpretation (see linked and related posts, too). Sounds great.  Unfortunately, I need to take a moment and talk about the remaining 3%1. Those of you that regularly use chat have probably noticed that each room has a rather distinct culture. In some rooms, a little off-topic 'fun' is not only permitted, but also encouraged, and generally serves to make the culture of the room brighter, and the experience of spending time there more rewarding. In these rooms, troll-like behavior or other things that don't reconcile well with our code of conduct are quickly flagged and removed. Yep, sounds great.  Other rooms prefer to keep the conversation more on-topic, with a focus that's more like a laser than a campfire. Our guidance has always been to essentially go with the flow, as long as that flow isn't something that doesn't appear to belong on our chat system, or doesn't easily come to terms with our code of conduct.  What's not to like? And that gets us to the hard part. It's terribly difficult and ineffective to write a list of things you can or can't say in chat. First, you just invite a lot of rule-lawyering (the Internet version of but I'm not touching you!! I'm not technically touching you!!!) and second, new people see this oddly specific list of things like \"Please don't talk about what monkeys really mean by farting\" and wonder what kind of crazy people might be lurking behind the door. I could give more real, concrete examples - but let's not go there. You don't need to. What underpins morality is do unto others as you would have others do unto you. I know the difference between right and wrong because I know what I wouldn't like done to me.   What positively has to function in order for these rooms to exist with our branding behind them is: Stuff that doesn't belong, or that doesn't reconcile with our code of conduct is flagged. The culture of our rooms must be welcoming above anything else to anyone that puts forward a good-faith effort to join and interact. So, if we see rooms where: Offensive stuff that violates our CoC isn't flagged. Offensive stuff that violates our CoC isn't just allowed (however tacitly, through nobody flagging it), it's encouraged. People are berated, kicked or otherwise harassed for holding a room's culture to our code of conduct. We're going to shut the room down permanently.  Shut the room down permanently? Why? Why not just get some new moderators who will do the job properly? It's wrong to punish everybody because some users have been nasty, and moderators have let them get away with it. Presumably because the latter are the former.  And this isn't the first time we've done this. None of this is new, and as I said earlier, problems sometimes pretend to go away while they secretly find ways to bite you even harder - I put the blame for needing to come here and reiterate all of this yet again squarely on us. But that doesn't absolve folks from the responsibilities that go along with the privilege of using chat. Chat is a great tool, and we are really proud of the caliber of discourse that flows through our systems every day. We want to keep it available because we're really proud of what most folks do with it. But we can't have self-policing break in the face of flagrant violations of our Code Of Conduct, and we'll be enforcing that with calm, steely-faced smiles going forward. Questions? Observations? Anything else? Please leave an answer or a comment. We love leading when it mostly means gently guiding people to do what's good for all of us, and we really don't like it when we need to do it more deliberately. But, we're the custodians of the reputation all of you helped us to build, so we must. If you've got problems with chat moderation, fix it. Don't close down chat.  Bravo for the initiative.  Some proposals, motivated by the topic sentence of the blog post: Too many people experience Stack Overflow as a hostile or elitist place, especially newer coders, women, people of color, and others in marginalized groups. [boldface added] Put either this whole post as written, with a link to the blog post and a link to the survey, on each site's Meta, perhaps as a closed question sending people here.  The goal: to get word out to as many people as possible. Create a gender field for the profile page, with at least four possible answers: male, female, prefer not to say or unstated, and other (optional fill in).  Include prominent instructions to all participants about not assuming people's gender.  Once an assumption is made, it is very awkward to try to correct it to unstated. Or include a field called \"preferred pronouns\" with possible answers \"he\/him\", \"she\/her\", \"they\", \"other\" (fill in). Explicitly instruct moderators to step in and assist when a participant makes a flag about a gender assumption.  You might think this would be an obvious situation where moderator action would be needed, but this might not be obvious to all moderators.  Writing directly to the community team is not necessarily a solution in this situation, because not all participants know how to do that, and because the community team often has a significant backlog.  By the time the community team gets around to looking at it, other participants' thinking about the participant's gender will have already been formed. Why is the gender assumption problem important?  Because the internet is a place where women find it easiest to drive while genderless.  But someone outs her, even if done through a careless error, her gender-ambiguous safety zone, which she may have spent months or years creating within the SE world, can fall apart quite quickly.  It's hard to get someone to think of you as undefined if they've already gotten used to thinking of you as she or he. Hurtful comments can come from the most well-meaning participants.  It can happen to anyone.  I've seen it happen to good people who are normally considerate; and it has happened to me.  But just as we can all let something slip out inadvertently, without realizing how it might come across as hurtful to someone else, we can also all (well, 99.9% of us) learn how to minimize the chances of it recurring.  The key is to \"take time to teach\" (to quote Faber and Mazlish, authors of How to Talk so Kids Will Listen...And Listen So Kids Will Talk).  Too often, the participant who committed a foul, and gets their comment deleted, never finds out about the deletion, or doesn't understand why the comment was deleted. We need a system whereby the person who wrote a non-nice comment can be led to understand how such a comment could be hurtful to someone else, and what alternative phrasing could be used in that particular situation. Change the flagging protocol in chat rooms.  Currently, outside Chat, one flag is enough to get a moderator's attention, but inside Chat, as I understand it, three flags are needed.   Keep in mind that it is in the less structured environments where the most hurtful peer-to-peer interactions tend to happen.  The SE Chat Room is analogous to the school bus or the locker room.  It is in the chat room where negative incidents are the mostly likely to pop up.  Let's be ready for them.  When they occur, let's help the person understand what was hurtful and how to avoid making someone else feel unwelcome. There should be a way for a participant to submit an immediate May Day message within a Chat room.  I have seen situations develop in Chat where comments are flying thick and fast, with sloppily written messages and lots of misunderstandings occurring, and one participant suddenly feels completely overwhelmed by hurtful comments, but it's hard to pinpoint exactly which comment or user to flag.  A May Day button would give the participant a way of flagging the conversation at that point to say, \"Something's gone horribly wrong and I'm getting out of here now.  But please take a look at the transcript.\" (Perhaps start thinking about whether it really is workable in the long run to allow chat rooms to permit comments which, on the main question page, would be flaggable as too chatty, obsolete, etc.  There are some chat rooms which get incredibly off the focus of their site, and this is very different from the way the rest of SE works.) In a chat room where problems have occurred repeatedly, institute a policy in which the general free-for-all chat room is only open for business during specific periods when a moderator is present.  This would be infinitely more effective than just shutting it down.  Again, this comes down to taking time to teach. Let's make the suspension system more effective in changing behavior and preventing future problems.  It's fine to give the participant a cooling-off period -- but at some point, it's important to take time to teach and help the suspended participant understand what went wrong. Generally, the site moderators don't have the objectivity needed to do that teaching, because by the time the decision to suspend has been made, the moderators have reached a high level of frustration. One possible solution would be to create a volunteer role of mediator, diplomat or ombudsman.  This person would help the newcomer who feels unwelcome.  This person could, for example, show the timid participant examples of what assertiveness without aggression might look like in the SE world, and do some role-play practice.  This person could also help someone who got flagged and suspended learn new ways of responding. \nabout \u00bb\u00a0\u00a0\u00a0\r\n            help \u00bb\n asked 8 days ago viewed \n14,166 times\n active 4 days ago \r\nsite design \/ logo \u00a9 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0\r\n                            with attribution required.\r\n                    rev\u00a02018.5.8.30324\n","time":1525788981,"title":"We're more aggressively enforcing self-moderation in chat","type":"story","url":"https:\/\/meta.stackexchange.com\/q\/309645","label":7,"label_name":"random"},{"by":"tysone","descendants":8,"id":17021011,"kids":"[17023817, 17022887, 17022934, 17023496, 17023109, 17024594]","score":43,"text":"Advertisement Supported by By David Brooks Opinion Columnist This column is about a man who changed the world, at least twice. I want to focus less on the impact of his work, which is all around us, and more on how he did it, because he\u2019s a model of how you do social change. Stewart Brand was born in Rockford, Ill., in 1938, the son of an advertising executive. By the early 1960s, he felt alienated from boring, bourgeois suburbia and concluded that Native Americans had a lot to teach the rest of us about how to lead a more authentic way of life. In 1965, he created a multimedia presentation called \u201cAmerica Needs Indians,\u201d which he performed at the LSD-laced, proto-hippie gatherings he helped organize in California. Brand then had two epiphanies. First, there were no public photos of the entire earth. Second, if people like him were going to return to the land and lead natural lives, they would need tools. He lobbied NASA to release a photograph of the whole earth, which became an iconic image for the environmental movement. Then he slapped the picture on the cover of what he called the \u201cWhole Earth Catalog.\u201d The catalog was an encyclopedia of useful items for people heading to a commune \u2014 home weaving kits, potter\u2019s wheels, outdoor gear. But it was also a bible for what would come to be known as the counterculture, full of reading lists and rich with the ideas of Buckminster Fuller and others. \u201cWhole Earth Catalog\u201d sold 2.5 million copies, won the National Book Award and defined an era. When a culture changes, it\u2019s often because a small group of people on society\u2019s margins find a better way to live, parts of which the mainstream adopts. Brand found a magic circle in the Bay Area counterculture. He celebrated it, publicized it, gave it a coherence it otherwise lacked and encouraged millions to join. The catalog featured an iconic central character, the Cowboy Nomad, who served as symbol and role model. Brand took influences from different parts of America \u2014 the New York art world, farmers, academic visionaries like Marshal McLuhan \u2014 and synthesized them into one ethos. He crowdsourced later editions, asking readers to recommend other cool products to feature. The communes fizzled. But on the other side of the Bay Area, Brand sensed another cultural wave building. Back in the 1960s, computers seemed like the ultimate establishment device \u2014 IBM and the government used them to reduce people to punch cards. But Brand and others imagined them launching a consciousness revolution \u2014 personal tools to build neural communities that would blow the minds of mainstream America. As Fred Turner says in \u201cFrom Counterculture to Cyberculture,\u201d \u201cWhat the communes failed to accomplish, the computers would complete.\u201d Brand played cultural craftsman once again, this time first as a celebratory journalist. In 1972 he wrote a piece for Rolling Stone announcing the emergence of a new outlaw hacker culture. The hackers were another magic circle on the cutting edge of the future, a circle Brand would publicize and inspire others to join. As my Times colleague John Markoff, who is writing a biography of Brand, notes, Brand is a talented community architect. In the 1970s, he was meshing Menlo Park computer geeks with cool hippie types. The tech people were entranced by \u201cWhole Earth,\u201d including Steve Jobs and Frederick Moore, co-creator of the celebrated Homebrew Computer Club. Brand meshed the engineers with the Merry Pranksters and helped give tech a moral ethos, a group identity, a sense of itself as a transformational force for good. In 1985, Brand and Larry Brilliant helped create the Well, an early online platform (like Usenet) where techies could meet and share. He helped Kevin Kelly organize hacker conferences, which attracted media attention. As Silicon Valley became more corporate in the 1980s and 1990s, he also helped form the Learning Conferences, Worldview Meetings, the Global Business Net and other convenings that gathered the multidisciplinary theorists and journalists who would define the wired culture: Kelly, Esther Dyson, Tim Berners-Lee and Nicholas Negroponte. Brand\u2019s gift, Frank Foer writes in \u201cWorld Without Mind,\u201d is \u201cto channel the spiritual longings of his generation and then to explain how they could be fulfilled through technology.\u201d Innovations don\u2019t just proceed by science alone; as Foer continues, \u201cthe culture prods them into existence.\u201d Turner argues that Brand has always craved a sensation of wholeness, a feeling of belonging and authenticity. He has found communities that gave him that sensation and has encouraged millions to love what he has loved. He synthesized a cultural ethos, and then tried to embody and spread that ethos through festivals, conferences and organizations. Brand vehemently disagrees with me, but I\u2019d say that, more recently, the computer has also failed as a source of true community. Social media seems to immiserate people as much as it bonds them. And so there\u2019s a need for future Brands, young cultural craftsmen who identify those who are building the future, synthesizing their work into a common ethos and bringing them together in a way that satisfies the eternal desire for community and wholeness. Follow The New York Times Opinion section on Facebook and Twitter (@NYTopinion), and sign up for the Opinion Today newsletter. Advertisement    Collapse SEE MY OPTIONS","time":1525788936,"title":"Stewart Brand Changed the World, Twice","type":"story","url":"https:\/\/www.nytimes.com\/2018\/05\/07\/opinion\/stewart-brand-hippie-silicon.html","label":7,"label_name":"random"},{"by":"startupflix","descendants":0,"id":17021008,"kids":"None","score":3,"text":"When Grimes walks the red carpet at the Met Gala tonight, she\u2019ll have some pretty wealthy arm candy. According to Page Six, Grimes and Space X founder Elon Musk have been quietly dating for the last month, and they intend to go public with their relationship at the Met Gala. The pair apparently met online after striking up a connection over a very nerdy joke. Per Page Six: \u201cThought experiment Roko\u2019s Basilisk considers the hypothesis of a future where AI lords over the world and could punish those who did not help it into existence. His joke was to merge this thought experiment with a pun using \u2018Rococo\u2019, referring to the ornate French 18th Century Baroque style, perhaps pointing out that both concepts are complex, too extreme and ridiculous.\u201d As it turns out, Grimes had already made the joke in her video for \u201cFlesh Without Blood\u201d which featured a character named\u00a0Rococo Basilisk. \u201c[This character] is doomed to be eternally tortured by an artificial intelligence, but she\u2019s also kind of like Marie Antoinette,\u201d she explained to Fuse in 2015. After coming across the video, Musk got in touch with Grimes through a mutual contact. They began dating soon thereafter. Best music video art I\u2019ve seen in a while https:\/\/t.co\/F2E4zDZMyM \u2014 Elon Musk (@elonmusk) March 31, 2018  In anticipation of tonight\u2019s Met Gala, Musk seemingly acknowledged Page Six\u2019s report by tweeting \u201cRococo basilisk\u201d along with a link Rococo\u2019s Wikipedia page. Rococo basilisk \u2014 Elon Musk (@elonmusk) May 7, 2018  https:\/\/t.co\/OgAHAWkNkU \u2014 Elon Musk (@elonmusk) May 7, 2018  Update: Here\u2019s our first photo of the happy couple together!  #Grimes and #ElonMusk (who are now dating) at the 2018 #MetGala A post shared by  Consequence of Sound (@consequence) on May 7, 2018 at 5:37pm PDT The random performance also featured Neil and Liam Finn, Connan Mockasin, Natalie Mering of Weyes Blood, and Kirin J Callinan. Taken from his so-called \u201cMet Gala \u2013 Red Carpet 7 inch.\u201d Plus, see photos of Donald Glover, Cardi B, SZA, and Lana Del Rey rocking the red carpet. Jay Chandrasekhar, Kevin Heffernan, and Steve Lemme go behind the scenes of their highly anticipated sequel. The man behind SpaceX and the Tesla has reverence for Yeezus. The rocket\u2019s payload contains an exposed Tesla Roadster playing Space Oddity\u201d.","time":1525788908,"title":"Grimes is dating Elon Musk","type":"story","url":"https:\/\/consequenceofsound.net\/2018\/05\/grimes-is-dating-elon-musk-report\/","label":7,"label_name":"random"},{"by":"chatmasta","descendants":0,"id":17021004,"kids":"None","score":1,"text":"Subscriber portal Applies to: Office 365 Office 365 API functionality is also available through the Microsoft Graph, a unified API that includes APIs from other Microsoft services such as Outlook, OneDrive, OneNote, Planner, and Office Graph, accessible through a single endpoint and with a single access token. We recommend using the Microsoft Graph in your apps when possible. Whether you want to incorporate the richness of Office 365 data into your app, or create a custom experience within Office 365 itself, or use custom reports to keep your Office 365 Enterprise environment running smoothly, you can use the following developer features to achieve your goals.  Integrate Office 365 data into your own apps You can create custom solutions that access and interact with all the richness of a user\u2019s Office 365 data\u2014and you can build those solutions across all mobile, web, and desktop platforms. The new Office 365 APIs enable you to provide access to Office 365 data, including their mail, calendars, contacts, files, and folders. All right from within your app itself. Whether you're building web applications using .NET, PHP, Java, Python, or Ruby on Rails, or creating apps for Windows 8, Universal Apps, iOS, Android, or on another device platform. It's your choice. See the Office 365 API. Create custom experiences within Office 365 Now, you can extend Office 365 itself. Customize how your data and experiences are displayed within and interact with Office 365 to provide a seamless user experience. Analyze and manage the health of your Office 365 Enterprise environment Office 365 Enterprise provides administrators a variety of developer features to keep their domains and subscriptions effective and well-tuned. You can also create custom experiences within the Office clients, such as Word, Excel, and PowerPoint, and within SharePoint 2013 and SharePoint Online. To learn more, see Office add-ins and SharePoint add-ins.  Code samples on dev.office.com Set up your Office 365 development environment Office 365 app authentication and resource authorization Blog: .NET and JavaScript libraries for Office 365 APIs Office 365 API Tools for Visual Studio and Office 365 Client Libraries Getting familiar with the Office 365 API Client Libraries Office 365 API Client Libraries - Authenticating your client to Office 365 Changes to Office 365 API Authentication Library in the Summer Update Training videos on the Office Dev Center Getting started with the Office 365 APIs (training video) Building an Office 365 ASP.NET MVC app Office 365 APIs and Python Part 1: OAuth2 Office 365 APIs and Python Part 2: Contacts API Office 365 APIs and Python Part 3: Mail and Calendar API","time":1525788887,"title":"Office 365 APIs platform overview","type":"story","url":"https:\/\/msdn.microsoft.com\/en-us\/office\/office365\/howto\/platform-development-overview","label":3,"label_name":"dev"},{"by":"mwexler","descendants":0,"id":17020991,"kids":"None","score":2,"text":"Contoso Ltd. (also known as Contoso and Contoso University) is a fictional company used by Microsoft as an example company and domain.   Contoso and its website, contoso.com, are used in documentation and help files for many Microsoft products. Contoso's website redirects to microsoft.com. Examples of its usage include: On April 1, 2011, an April Fool's Day joke from Google Enterprise said that Contoso abandoned many of Microsoft's technologies in favor of Google Apps.[9] The joke was a response to the 2010 whitepaper listed above. In response to this joke, the Why Microsoft blog responded saying that Contoso was lured back from Google by promises of \"proven cost savings\".[10]","time":1525788744,"title":"Contoso, appearing all over Microsoft demos and documentation","type":"story","url":"https:\/\/en.wikipedia.org\/wiki\/Contoso","label":9,"label_name":"tech"},{"by":"jjallen","descendants":0,"id":17020982,"kids":"None","score":1,"text":"Yesterday \n\nFred Lambert\n\n \n\t\t\t\t\t\t\t- May. 7th 2018 7:53 am ET\n\t\t\t\t\t\t\t\t @FredericLambert Several comments made by CEO Elon Musk since the launch of its Autopilot 2.0 hardware suite in all Tesla vehicles made since October 2016 indicate that the company might have to update its onboard computer in order to achieve the fully self-driving capability that it has been promising to customers. Now it looks like Tesla might have to also offer computer retrofits for Autopilot 2.5 cars.\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n Last fall,\u00a0Tesla introduced a new Autopilot hardware suite, dubbed \u201c2.5\u201d, in all its vehicles\u00a0to enable more power and redundancy for its future self-driving capability. It is built on the Autopilot 2.0 suite released a year earlier which at that time, Tesla claimed would eventually enable \u201cfully self-driving capability\u201d with future software updates.\u00a0We got\u00a0a look at that computer after a teardown\u00a0last year. But the automaker also said that they might need to upgrade the Autopilot computer inside those vehicles in order to get more computing power. The new \u201cAutopilot 2.5\u201d brought the first update to that computer with\u00a0a new secondary GPU for more computing power and redundancy. We took a close look at the computer\u00a0latest Autopilot (2.5) computer in Model 3, Model S, and Model X\u00a0vehicles\u00a0earlier this year. Now Musk said during Tesla\u2019s Q1 conference call last week that even this computer, which is the one being installed in vehicles currently in production, might also need to be replaced to achieve\u00a0fully self-driving capability: In order for that to be in place, we have to obviously sell full autonomy and we\u2019re making really good progress on that front. I believe that the vehicles that we are currently producing are capable of full autonomy with the only thing that maybe is probably needed is a computer upgrade to have more processing power for the vision neural net. But that\u2019s a plug-in replacement, a thing that can be done quite easily. Tesla indeed made the onboard Autopilot computer easily\u00a0swappable if they need to upgrade, but even if the task to replace it is somewhat easy, it can still add up to quite a significant retrofit program for the automaker. We are already talking about over 120,000 vehicles with Autopilot 2.0 and 2.5 hardware suite on the road today and at the current production rate, Tesla can be adding as many as 50,000 new cars per quarter at this point. But it\u2019s actually only a fraction of the number of vehicles with full autonomy that Musk sees Tesla deploying. The CEO added during the call: I think we\u2019re really well-positioned and are building the right\u00a0foundation for having millions and ultimately tens of millions of shared autonomous electric vehicles. He now expects Tesla to be able to start deploying its fully self-driving software by the end of next year. Electrek\u2019s Take I completely understand that computer power is something that is improving at an incredible pace and therefore, it makes sense for Tesla to frequently upgrade its onboard computers to keep up with the trend, but I feel like this might turn\u00a0into quite a heavy retrofit program. Tesla promised fully self-driving to all cars since October 2016 and based on Musk\u2019s recent comments, it looks like all those cars might need a computer upgrade, which he already said the company will provide for free. Let\u2019s say that Tesla doesn\u2019t start producing new cars with a stock computer capable of fully self-driving until the end of the year, it\u2019s going to be at least 200,000 vehicles in need of new computers. In terms of cost, it will easily be in the hundreds of millions and that\u2019s without accounting for the workload on the service centers, which will have to perform the retrofit. Maybe I am being overdramatic here, but I think this could be a big issue in the making. What do you think? Let us know in the comment section below. Featured Image by Kyle Day \u2013\u00a0Look inside Tesla\u2019s onboard Nvidia supercomputer for\u00a0self-driving Tesla is a transportation and energy company. It sells vehicles under its 'Tesla Motors' division and stationary battery pack for home, commercial and utility-scale projects under its 'Tesla Energy' division. The Autopilot is Tesla's advanced assisted driving program with features like Autosteer, Autopark, and Trafic-Aware Cruise Control (TACC). \n@FredericLambert\n Fred is the Editor in Chief and Main Writer at Electrek. You can send tips on Twitter (DMs open) or via email: fred@9to5mac.com If you want to help Fred and Electrek, you can contribute to our Patreon: https:\/\/www.patreon.com\/electrek Tesla updates Model S and Model X interior Tesla is working on a new record-breaking 1 GWh project Norway's fjords are going zero-emission Tesla Q1 results and letter highlights","time":1525788674,"title":"Tesla may have to offer computer retrofits to Autopilot cars","type":"story","url":"https:\/\/electrek.co\/2018\/05\/07\/tesla-autopilot-computer-retrofits\/","label":0,"label_name":"biz-news"},{"by":"zaiste","descendants":0,"id":17020977,"kids":"None","score":1,"text":"Timely Software Versioning \nAbout\nUsers\nGitHub\n   CalVer is a software versioning convention that is based on your\nproject's release calendar, instead of arbitrary numbers. Software versioning gets better with time. For engineers, versioning allows us to specify precise dependencies\nwithin ever-expanding software ecosystems. For marketers, the version\nis the dynamic part of a project's brand. For all of us, software\nversioning lets us reference the past while upgrading to the future. Different software projects use different systems for versioning, but\nsome common practices have emerged. Numeric, decimal point separation,\ne.g., 3.1.4, is all but given. Another common versioning pattern\nincorporates a time-based element, usually part of the release date,\ninto the version. This date-based approach has come to be called Calendar Versioning, or\nCalVer for short. There are multiple calendar versioning schemes, long used by projects\nbig and small. Rather than declaring a single scheme to be CalVer,\nit's important to recognize the practicality of each and\ndesign the scheme to fit the project. First,\nthe parts of the version: The vast majority of modern software versions are composed of two or\nthree numeric segments, plus the optional modifier. Convention\nsuggests that four-numeric-segment versions are discouraged. As seen in the case studies below, projects have\nfound more than one useful way to leverage dates in their\nversions. Rather than choose a single scheme, CalVer introduces\nstandard terminology for developers, based on current practices and\nconventions. Note that traditional, incremented version numbers are 0-based,\nwhereas date segments are 1-based. The Gregorian calendar is assumed, as is the convention\nof UTC. Technically any calendar can be used, provided projects\nstate which one. CalVer has quite a few users. These are projects selected for their\nnotability and variety of use cases.  Ubuntu, one of the most prominent Linux-based operating\nsystems available, uses a three-segment CalVer scheme, with a short\nyear and zero-padded month. It has done so\nfrom the very start, in October 2004, making 4.10\nthe first general release of Ubuntu. Even a simple operating system involves many, many parts, making it\ndifficult to communicate much meaning with an arbitrary number. By\ndating the software release, the calendar-based version is much more\nthan an arbitrary number, communicating useful information that is\nrooted in simple fact. Ubuntu derives additional benefit from its CalVer scheme, by\nintegrating it with their support schedule. Ubuntu currently has\nfive-year support periods for their long-term support (LTS) releases,\nand only 9 months for non-LTS releases. Thanks to CalVer and\nelementary arithmetic, any user can easily determine whether their\nversion is still supported. The current LTS release at the time of\nwriting, 16.04, will be supported until April 2021.  Twisted, the venerated Python networking and\nasynchronous execution framework, uses a three-segment CalVer scheme,\nwith a short year in the major version slot, release number of that year\nin the minor slot, and the micro slot being the bugfix release number. First released in 2002 and still actively developed today, Twisted is\na mature library that has grown to match its large\nscope. It features everything from an IRC client to an HTTP server to\na slew of utilities for concurrent programming. Like an operating\nsystem, Twisted has a lot of parts, making SemVer a poor fit due to\nthe individual parts deprecating and breaking compatibility individually. The non-deprecated parts of Twisted are backwards-compatible between\neach successive version, and breaking changes are done on a time basis,\nwhere one year must pass and two releases issued between the release\ndeprecating the functionality and the removal of the functionality. Its versioning scheme has spread to related projects, including\nKlein, Treq, and even one of Twisted's dependencies,\nPyOpenSSL.  youtube_dl, the understated ally of Internet\nmedia archivists everywhere, uses a three-segment CalVer scheme,\nincluding full year, zero-padded month, and zero-padded day. The\nversion is almost completely calendar-driven, except for a micro\nsegment that is added in some technical contexts. Despite the name, youtube_dl's scope is expansive. It supports\nextracting audio and video from a long, ever-expanding list of\nsites. Consider the rapid release cycle of supported services, and it\nbecomes clear why the project has adopted CalVer to such a great\ndegree.  pytz is the Python translation of the\nIANA\/Olson timezone database, the database behind accurate\ntimes for all of computerdom.  pytz uses a two-segment CalVer scheme,\nincluding full year and short month. While Python has a history of \"batteries-included\" architecture, and\nthe datetime module frequently mentions timezones, the core Python\nruntime does not include timezone information. This is because\ntimezone updates do not follow a fixed schedule, and are subject to\npolitics and legislative whim. Calendar versioning offers a\ndate-stamped snapshot of an otherwise chaotic system.  The Teradata UDA client provides next-generation\naccess to Teradata's data warehousing technologies. Teradata's usage is notable not for the prominence of the technology\nor company, but because there have been multiple releases in 2016\nwhich were versioned as 15.10. This may seem breaking at first, but\nthe meaning and utility is clear. The library maintainers have crafted a resourceful hybrid of\nsemantic versioning and calendar versioning. The YY.MM\npart of the version are used as a combined SemVer major version. That\nis, for new releases, the API of the library remains the same as it\ndid in October 2015. Dependent code written since then is safe to\nupgrade.  We will see the year and month segments update next time\nthere is a breaking API change. See the Users page for a growing list of CalVer users. If both you and someone you don't know use your project seriously,\nthen use a serious version. Your project is released, and it needs a\nnonzero major version. Luckily, the decision on whether to use CalVer\nfor that version is easier than ever: If you answered yes to any of these questions, CalVer's semantics make\nit a strong choice for your project. ","time":1525788637,"title":"CalVer: Versioning based on calendar dates instead of arbitrary numbers","type":"story","url":"https:\/\/calver.org\/","label":3,"label_name":"dev"},{"by":"uptown","descendants":0,"id":17020965,"kids":"None","score":1,"text":"Thirst hits shelves October 2,\u00a02018.100% of the author's net proceeds from the sale of Thirst will fund charity:\u00a0water projects around the\u00a0world. An inspiring personal story of redemption, second chances, and the transformative power within us all, from the founder and CEO of the nonprofit charity:\u00a0water. At 28 years old, Scott Harrison had it all. A top nightclub promoter in New York City, his life was an endless cycle of drugs, booze, models\u2014repeat. But 10 years in, desperately unhappy and morally bankrupt, he asked himself, \"What would the exact opposite of my life look like?\" Walking away from everything, Harrison spent the next 16 months on a hospital ship in West Africa and discovered his true calling. In 2006, with no money and less than no experience, Harrison founded charity: water. Today, his organization has raised over $300 million to bring clean drinking water to more than 8.2 million people around the\u00a0globe.\n\nIn Thirst, Harrison recounts the twists and turns that built\ncharity:\u00a0water into one of the most trusted and admired nonprofits in the world. Renowned for its 100% donation model, bold storytelling, imaginative branding, and radical commitment to transparency, charity:\u00a0water has disrupted how social entrepreneurs work while inspiring millions of people to join its mission of bringing clean water to everyone on the planet within our\u00a0lifetime.\n\nIn the tradition of such bestselling books as Shoe Dog and Mountains Beyond Mountains, Thirst is a riveting account of how to build a better charity, a better business, a better life\u2014and a gritty tale that proves it\u2019s never too late to make a\u00a0change. To preorder copies for your company, church, or school, contact info@charitywater.org. For speaking inquiries regarding Thirst, please\u00a0contact info@charitywater.org. SCOTT HARRISON is the founder and CEO of charity:\u00a0water, a non-profit that has mobilized over one million donors around the world to fund over 28,000 water projects in 26 countries that will serve more than 8.2 million people. Harrison has been recognized on Fortune's 40 under 40 list, Forbes\u2019 Impact 30 list, and was ranked #10 in Fast Company's 100 Most Creative People in Business. He is currently a World Economic Forum Young Global Leader and lives in New York City with his wife and two\u00a0children. ","time":1525788527,"title":"Thirst \u2013 The new book from Scott Harrison, founder of charity: water","type":"story","url":"https:\/\/www.charitywater.org\/thirst","label":7,"label_name":"random"},{"by":"batbkw","descendants":0,"id":17020962,"kids":"None","score":1,"text":"I\u2019m not a sports person and would, therefore, have never seen this were it not for my new favorite morning read The California Sun (which gathers a wide range of interesting California news and tidbits.) Anyway, check out how nicely this LA Times interactive data visualization makes its point. To actually experience the animated version of this, here\u2019s the link. It\u2019s done by the Times\u2019 Joe Fox.        \u00a0 \u00a0 Your email address will not be published. Required fields are marked * Comment Name *  Email *  Website       Get fresh posts delivered to your inbox. Join smart readers at Google, Facebook, BMW, Fedex and many other companies in multiple industries. BKW Partners 1161 Mission Street,San Francisco,California 94103  415.621.4300 \/ hello@bkwpartners.com  One of the trickiest tricks for a marketer to get right is that of making your product or service feel...","time":1525788504,"title":"(Data)visualizing One of Baseball\u2019s Rarest Feats","type":"story","url":"https:\/\/bkwpartners.com\/visualizing-one-of-baseballs-rarest-feats\/","label":7,"label_name":"random"},{"by":"simonebrunozzi","descendants":0,"id":17020960,"kids":"None","score":1,"text":"In this entry we present the latest estimates of mental health disorder prevalence, disease burden rates, and mortality impacts across a number of disorders. We address substance use disorders (alcohol and drug use disorders) in separate entries on Substance Use and Alcohol Consumption. Most of the estimates presented in this entry are produced by the Institute for Health Metrics and Evaluation and reported in their flagship Global Burden of Disease study. Mental health and substance use disorders are still significantly under-reported. This is true across all countries, but particularly at lower incomes where data is more scarce, and attention and treatment for mental health disorders are significantly lower. Mental health disorders are complex and can take many forms. The underlying sources of the data presented in this entry apply specific definitions (which we describe in each relevant section), typically in accordance with\u00a0WHO's International Classification of Diseases (ICD-10). This broad definition incorporates many forms, including depression, anxiety, bipolar, eating disorders and schizophrenia. Mental health disorders remain widely under-reported\u00a0\u2014 in our section on Data Quality & Definitions we discuss the challenges of dealing with this data. Figures presented in this entry should be taken as estimates of mental health disorder prevalence\u00a0\u2014 they do not strictly reflect diagnosis data (which would provide the global perspective on diagnosis, rather than actual prevalence differences), but are imputed from a combination of medical, epidemiological data, surveys and meta-regression modelling where raw data is unavailable. Further information can be found here. In many cases, we may therefore consider reported estimates to be an under-estimation of true prevalence and disease burden. It is also important to keep in mind that the uncertainty of the data on mental health is generally high so that one should be cautious about interpreting changes over time and differences between countries. Even taking into account that mental health disorders are likely underreported, the data presented in this entry demonstrate that mental health disorders are common and have a high prevalence. Improving awareness, recognition, support and treatment for this range of disorders should therefore be an essential focus for global health. The table below provides a brief summary of the data which follows on mental health and substance use disorders. Clicking on a given disorder will take you to the relevant section for further data and information.  [difference across countries] [13-22%] 15% females [2-6%]\n 4.5% females [2.5-6.5%] 4.7% females [0.4-1.5%] 0.65% females (clinical anorexia & bulimia) [0.05-0.55%] 0.2% females [0.2-0.45%] 0.28% females [0.5-5%] 0.8% females [0.4-3.3%]  0.5% females The predominant focus of this entry is the prevalence and impacts of mental health disorders (with Substance Use and Alcohol Use disorders covered in individual entries). However, it is useful as introduction to understand the total prevalence and disease burden which results from the broad IHME and WHO category of 'mental health and substance use disorders'.\u00a0This category comprises a range of disorders including depression, anxiety, bipolar, eating disorders, schizophrenia, intellectual developmental disability, and alcohol and drug use disorders. In the chart below we see that globally, mental and substance use disorders are very common: around 1-in-6 people (15-20 percent) have one or more mental or substance use disorders. It's estimated that, globally,\u00a0over 1.1 billion\u00a0people had a mental or substance use disorder in 2016.\u00a0The largest number of people had an anxiety disorder, estimated at around 4 percent of the population.   The scatterplot below compares the prevalence of these disorders between males and females. Taken together we see that in most countries this group of disorders are more common for men than for women. However, as is shown later in this entry and in our entries on Substance Use and Alcohol, this varies significantly by disorder type: on average, depression, anxiety, eating disorders, and bipolar disorder is more prevalent in women. Gender differences in schizophrenia prevalence are mixed across countries, but typically more common in men. Alcohol and drug use disorders are more common in men.  The direct death toll from mental health and substance use disorders is typically low. In this entry, the only direct death estimates result from eating disorders, which occur through malnutrition and related health complications. Direct deaths can also result from alcohol and substance use disorders; these are covered in our entry on Substance Use. However, mental health disorders are also attributed to significant number of indirect deaths through suicide and self-harm. Suicide deaths are strongly linked\u00a0\u2014 although not always attributed to\u00a0\u2014 mental health disorders. We discuss the evidence of this link between mental health and suicide in detail\u00a0later in this entry. In high-income countries, meta-analyses suggest that up to 90 percent of suicide deaths result from underlying mental and substance use disorders. However, in middle to lower-income countries there is evidence that this figure is notably lower.\u00a0A study by Ferrari et al. (2015) attempted to determine the share disease burden from suicide which could be attributed to mental health or substance use disorders.1 Based on review across a number of meta-analysis studies the authors estimated that only 68 percent of suicides across China, Taiwan and India were attributed to mental health and substance use disorders. Here, studies suggest a large number of suicides result from the \u2018dysphoric affect\u2019 and \u2018impulsivity\u2019 (which are not defined as a mental and substance use disorder). In such cases, understanding the nature of self-harm methods between countries is important; in these countries a high percentage of self-harming behaviours are carried out through more lethal methods such as poisoning (often through pesticides) and self-immolation. This means that in a high number of cases self-harming behaviours can prove fatal, even if there was not a clear intent to die. As a result, direct attribution of suicide deaths to mental health disorders is difficult. Nonetheless, it's estimated that a large share of suicide deaths link back to mental health. Studies suggest that an individual with depression the risk of suicide is around 20 times higher than an individual without. Health impacts are often measured in terms of total numbers of deaths, but a focus on mortality means that the burden of mental health disorders can be underestimated2 Measuring the health impact by mortality alone fails to capture the impact that mental health disorders have on an individual's wellbeing. The 'disease burden' \u2013 measured in Disability-Adjusted Life Years (DALYs) \u2013 is a considers not only the mortality associated with a disorder, but also years lived with disability or health burden. The map below shows DALYs as a share of total disease burden; mental and substance use disorders account for around 7 percent of global disease burden in 2016, but reaches up to 13-14 percent in several countries. These disorders have the highest contribution to overall health burden in Australia and United States.  Depressive disorders occur with varying severity. The WHO's International Classification of Diseases (ICD-10) define this set of disorders ranging from mild to moderate to severe. The IHME adopt such definitions by disaggregating to mild, persistent depression (dysthymia) and major depressive disorder (severe). All forms of depressive disorder experience some of the following symptoms: Mild persistent depression (dysthymia) tends to have the following diagnostic guidelines: \"Depressed mood, loss of interest and enjoyment, and increased fatiguability are usually regarded as the most typical symptoms of depression, and at least two of these, plus at least two of the other symptoms described on page 119 (for F32.-) should usually be present for a definite diagnosis. None of the symptoms should be present to an intense degree. Minimum duration of the whole episode is about 2 weeks. An individual with a mild depressive episode is usually distressed by the symptoms and has some difficulty in continuing with ordinary work and social activities, but will probably not cease to function completely.\" Severe depressive disorder tends to have the following diagnostic guidelines: \"In a severe depressive episode, the sufferer usually shows considerable distress or agitation, unless retardation is a marked feature. Loss of self-esteem or feelings of uselessness or guilt are likely to be prominent, and suicide is a distinct danger in particularly severe cases. It is presumed here that the somatic syndrome will almost always be present in a severe depressive episode. During a severe depressive episode it is very unlikely that the sufferer will be able to continue with social, work, or domestic activities, except to a very limited extent.\" The series of charts below present the latest global estimates of the prevalence and disease burden of depressive disorders. Depressive disorders, as defined by the underlying source, cover a spectrum of severity ranging from mild persistent depression (dysthymia) to major (severe) depressive disorder. The data presented below includes all forms of depression across this spectrum. The share of population with depression ranges mostly between 2% and 6% around the world today. Globally, older individuals (in the 70 years and older age bracket) have a higher risk of depression relative to other age groups. In 2016, an estimated 268 million people in the world experienced depression. A breakdown of the number of people with depression by world region can be seen here and a country by country view on a world map is here. In all countries the median estimate for the prevalence of depression is higher for women than for men. The chart found here shows the health burden of depression as measured in Disability Adjusted Life Years (DALYs) per 100,000. A time-series perspective on DALYs by age is here. Anxiety disorders arise in a number of forms including phobic, social, obsessive compulsive (OCD), post-traumatic disorder (PTSD), or generalized anxiety disorders. The symptoms and diagnostic criteria for each subset of anxiety disorders are unique. However, collectively the\u00a0WHO's\u00a0International Classification of Diseases (ICD-10) note frequent symptoms of: \"(a) apprehension (worries about future misfortunes, feeling \"on edge\", difficulty in concentrating, etc.); (b) motor tension (restless fidgeting, tension headaches, trembling, inability to relax); (c) autonomic overactivity (lightheadedness, sweating, tachycardia or tachypnoea, epigastric discomfort, dizziness, dry mouth, etc.).\" The series of charts below present global data on the prevalence and disease burden which results from this range of anxiety disorders. The prevalence of anxiety disorders across the world varies from 2.5 to 6.5 percent by country. Globally an estimated 275 million people experienced an anxiety disorder in 2016, making it the most prevalent mental health or neurodevelopmental disorder. Around 62 percent (170 million) were female, relative to 105 million males. In all countries women are more likely to experience anxiety disorders than men. Prevalence trends by age can be found here. The chart found here shows the health burden of depression as measured in Disability Adjusted Life Years (DALYs) per 100,000. A time-series perspective on DALYs by age is here. Bipolar disorder (also termed bipolar affective disorder) is defined by the WHO's\u00a0International Classification of Diseases (ICD-10) as follows: \"This disorder is characterized by repeated (i.e. at least two) episodes in which the patient's mood and activity levels are significantly disturbed, this disturbance consisting on some occasions of an elevation of mood and increased energy and activity (mania or hypomania), and on others of a lowering of mood and decreased energy and activity (depression). Characteristically, recovery is usually complete between episodes, and the incidence in the two sexes is more nearly equal than in other mood disorders. As patients who suffer only from repeated episodes of mania are comparatively rare, and resemble (in their family history, premorbid personality, age of onset, and long-term prognosis) those who also have at least occasional episodes of depression, such patients are classified as bipolar.\" The charts below present global data on the prevalence and disease burden of bipolar disorder. The prevalence of bipolar disorder across the world varies from 0.4 to 1.5 percent by country.\u00a0Globally, an estimated 40 million people in the world had bipolar disorder in 2016, with 55 and 45 percent\u00a0being female and male, respectively. In almost all countries women are more likely to experience bipolar disorder than men. Prevalence of bipolar disorder by age can be found here. The chart found here shows the health burden of depression as measured in Disability Adjusted Life Years (DALYs) per 100,000. A time-series perspective on DALYs by age is here. Eating disorders are defined as psychiatric conditions defined by patterns of disordered eating. This therefore incorporates a spectrum of disordered eating behaviours. The underlying sources presented here present data only for the disorders of anorexia and bulimia nervosa (as defined below). It is however recognised that a large share of eating disorders fall outwith the definition of either anorexia or bulimia nervosa (these are often termed 'eating disorders not otherwise specified'; EDNOS)\u00a0\u2014 some estimates report at least 60 percent of eating disorders do not meet the standard criteria.3 It is therefore expected that the data presented below significantly underestimates the true prevalence of eating disorders, since it concerns only clinically-diagnosed anorexia and bulimia nervosa. Anorexia nervosa \"Anorexia nervosa is a disorder exemplified by deliberate weight loss, and associated with undernutrition of varying severity. For a definite diagnosis, the ICD note that all the following are required: (a) Body weight is maintained at least 15% below that expected (either lost or never achieved), or Quetelet's body-mass index4 is 17.5 or less. 4 Quetelet's body-mass index = weight (kg) to be used for age 16 or more - 139 - Prepubertal patients may show failure to make the expected weight gain during the period of growth; (b) The weight loss is self-induced by avoidance of \"fattening foods\". One or more of the following may also be present: self-induced vomiting; self-induced purging; excessive exercise; use of appetite suppressants and\/or diuretics; (c) There is body-image distortion in the form of a specific psychopathology whereby a dread of fatness persists as an intrusive, overvalued idea and the patient imposes a low weight threshold on himself or herself; (d) A widespread endocrine disorder involving the hypothalamic - pituitary - gonadal axis is manifest in women as amenorrhoea and in men as a loss of sexual interest and potency. (An apparent exception is the persistence of vaginal bleeds in anorexic women who are receiving replacement hormonal therapy, most commonly taken as a contraceptive pill.) There may also be elevated levels of growth hormone, raised levels of cortisol, changes in the peripheral metabolism of the thyroid hormone, and abnormalities of insulin secretion; (e) If onset is prepubertal, the sequence of pubertal events is delayed or even arrested (growth ceases; in girls the breasts do not develop and there is a primary amenorrhoea; in boys the genitals remain juvenile). With recovery, puberty is often completed normally, but the menarche is late.\" Bulimia nervosa \"Bulimia nervosa is an illness defined by repeated behaviours of overeating, preoccupation with control of body weight, and the adoption of extreme measures to mitigate the impacts of overeating. For a definite diagnosis, the ICD note that all the following are required: (a) There is a persistent preoccupation with eating, and an irresistible craving for food; the patient succumbs to episodes of overeating in which large amounts of food are consumed in short periods of time. (b) The patient attempts to counteract the \"fattening\" effects of food by one or more of the following: self-induced vomiting; purgative abuse, alternating periods of starvation; use of drugs such as appetite suppressants, thyroid preparations or diuretics. When bulimia occurs in diabetic patients they may choose to neglect their insulin treatment. (c) The psychopathology consists of a morbid dread of fatness and the patient sets herself or himself a sharply defined weight threshold, well below the premorbid weight that constitutes the optimum or healthy weight in the opinion of the physician. There is often, but not always, a history of an earlier episode of anorexia nervosa, the interval between the two disorders ranging from a few months to several years. This earlier episode may have been fully expressed, or may have assumed a minor cryptic form with a moderate loss of weight and\/or a transient phase of amenorrhoea.\" The prevalence of eating disorders (anorexia and bulimia nervosa) ranges from 0.05 to 0.55 percent by country.\u00a0Globally an estimated 10.5 million had clinical anorexia and bulimia nervosa in 2016. Bulimia was more common: around 75 percent had bulimia nervosa. In every country women are more likely to experience an eating disorder than men. Eating disorders tend to be more common in young adults aged between 15 and 34 years old. Trends in prevalence by age can be found here. Direct deaths can result from eating disorders through malnutrition and related health complications. The chart below shows the estimated number of direct deaths from anorexia and bulimia nervosa. Evidence suggests that having an eating disorder can increase the relative risk of suicide; suicide deaths in this case are not included here. Trends in death rates from eating disorders can be found here. The chart found here shows the health burden of eating disorders as measured in Disability Adjusted Life Years (DALYs) per 100,000. A time-series perspective on DALYs by age is here. Schizophrenia is defined by the IHME based on the definition within the WHO's International Classification of Diseases (ICD-10) as: \"The normal requirement for a diagnosis of schizophrenia is that a minimum of one very clear symptom (and usually two or more if less clear-cut) belonging to any one of the groups listed as (a) to (d) below, or symptoms from at least two of the groups referred to as (e) to (h), should have been clearly present for most of the time during a period of 1 month or more: The following charts present global-level data on the prevalence of schizophrenia. The prevalence of schizophrenia typically ranges from 0.2 to 0.45 percent across countries.\u00a0It's estimated that 21 million people in world had schizophrenia in 2016; the number of men and women with schizophrenia was approximately the same (around 10.5 million each). In many countries (but not all) the prevalence of schizophrenia is slightly higher in men than women. Prevalence by age can be found here. The chart found here shows the health burden of schizophrenia as measured in Disability Adjusted Life Years (DALYs) per 100,000. A time-series perspective on DALYs by age is here. The determinants, onset and severity of mental health disorders are complex - they can rarely be attributed to a single factor. Identifying potential risk factors form an important element of health research, potential prevention and in some cases, appropriate treatment; nonetheless, many risk factors remain only correlates of observed patterns in mental health. They therefore need to be interpreted carefully. The World Health Organization synthesize the potential contributors to mental health and wellbeing into three categories:4 In the table below we see the WHO's breakdown of potential adverse and protective factors for mental health within these three categories. These factors often interact, compound or negate one another and should therefore not be considered as individual traits or exposures. For example, particular individual traits may make a given person more vulnerable to mental health disorders with the onset of a particular economic or social scenario\u00a0\u2014 the instance of one does not necessarily result in a mental health disorder, but combined there is a significantly higher vulnerability.  The risk factors and influencers on mental health vary significantly for an individual as they move through the life-course. The following are acknowledged risk factors for a given stage of life.5 \u2013 Pre-conception and pre-natal period A given individual's mental health and wellbeing can be influenced by factors present prior to conception or birth. Pregnancies which are unwanted or in adolescence can increase the likelihood of detrimental behaviours of the mother during pregnancy, and the environmental or family conditions of childhood.6 During pregnancy, detrimental behaviours including tobacco, alcohol and drug use can increase the likelihood of later mental health disorders for children; malnutrition, low-birth weight and micronutrient deficiency (for example, iodine deficiency) can also influence later mental health vulnerabilities.7,8,9 \u2013 Infancy and early childhood There is a large base of evidence which shows that emotional attachment in early childhood has a considerable impact on later vulnerability to mental health and wellbeing.10,11 As a result, particular risk factors include separation from the primary caregiver, in some cases post-natal depression in mothers (which can result in sub-optimal attachment), and parents for whom communication and social interaction is challenging. Child maltreatment and neglect has been found to have a significant impact on vulnerabilities to mental wellbeing.12,13 Malnutrition, poor access to basic services and disease and parasites are also important contributors. \u2013 Childhood Childhood conditions form a critical component of health and wellbeing later in life. Negative experiences, either at home or outside of the home (for example, bullying in school) can have lifelong impacts on the development of core cognitive and emotional skills. Poor socioeconomic conditions also have a significant effect on vulnerability to mental health disorders; in a study in Sweden, the authors found that children raised in families of poor socioeconomic backgrounds had an increased risk of psychosis.14 Poor economic resources, shown through poor housing conditions for example, can be seen by children as shameful or degrading and affect aspects of childhood learning, communication and interaction with peers. Children with a parent who has a mental illness or substance use disorder have a higher risk of psychiatric problems themselves.15,16,17 This effect between generations can occur as a result of genetic, biological, psychological and social risk factors. \u2013 Adolescence Adolescence is typically the stage of life where mental health disorders tend to become more apparent. The risk factors and contributors to wellbeing in childhood apply equally to those in adolescence. In addition, several other contributing factors appear. It is in the years of adolescence that the use of substances including alcohol and drugs first appear. Substance use is particularly hazardous and harmful for adolescents because individuals are still developing both mentally and physically. Peer pressure, and media influences also become more prominent over these years. Exposure to substance use is not only an important risk factor for other mental health disorders, but also linked to poorer educational outcomes, more risky sexual behaviour and increased exposure to violence and conflict. \u2013 Adulthood Experiences and emotional capabilities developed through childhood and adolescence are important factors in the effect that particular events and scenarios in adulthood have on mental health outcomes. The WHO highlight that critical to wellbeing in adulthood is the allocation and balance between work and leisure time. Exposure to high stress and anxiety is strongly influenced by the share of time working, caring for others, or time spent in an insecure economic environment. Individuals with poor socioeconomic security, and in particular unemployment, are also at higher risk to mental health disorders. These factors, balanced with the amount of time spent on 'consumption' activities, including leisure time and supportive family and friends, often determine the propensity for poor mental health and wellbeing. Community structures can have a significant positive impact on these outcomes\u00a0\u2014 individuals who have poor access to such communities, either through social exclusion, neighbourhood violence\/crime, or lack of respite care have a higher risk of mental health disorders. Physical health also has an important impact on mental wellbeing; an individual's 'physical capital' can influence their sense of esteem and social inclusion. Individuals with chronic illness or disability are at higher risk of poor mental health; this is particularly true for conditions with high rates of stigmatisation, such as HIV\/AIDS. \u2013 Older age Individuals of older age are of notably high risk of poorer mental health and wellbeing. This typically results from notable changes in life conditions (such as a cease in employment which affects both the feeling of contribution and economic freedom), higher social exclusion, and loneliness. This is particularly true when an older individual begins to lose close family and friends. Bereavement in general is an important predictor of mental health disorders such as depression. A decline in physical health can have major impacts on life capabilities by affecting an individual's mobility and freedom. Older individuals are also at higher risk of abuse or neglect from carers and in some cases, family members. The link between mental health and substance use disorders and suicide is well-documented.18 It is however true that not all suicides - or suicide attempts - are attributed to underlying mental health or substance use disorders; as shown in the chart below, there is not a direct relationship between mental health prevalence and suicide rates.19 We cover suicide statistics more broadly in our full entry on Suicide, however here we attempt to distil the key findings on the links between mental health and substance use and suicide. Although mental health and substance use disorders is within the top-five causes of disease burden globally (as measured by Disability-Adjusted Life Years; DALYs), accounting for approximately 7 percent of the burden, several authors have highlighted that such figures \u2014 since they do not include suicide DALYs \u2014 underestimate the true cost of mental health disorders.20 Providing a more accurate estimate of total mental health burden therefore requires some understanding of the connection between these disorders and suicide. Meta-analyses of psychological autopsy studies of suicide\u00a0across high-income countries suggest that up to 90 percent of suicides occur as a result of an underlying mental health or substance use disorder.21, 22, 23 While available data and studies are more scarce across lower-to-middle income countries, evidence across countries including China, Taiwan and India suggest that this proportion is significantly lower elsewhere.24, 25, 26, 27 These studies suggest a large number of suicides resultant from the \u2018dysphoric affect\u2019 and \u2018impulsivity\u2019 (which are not defined as a mental and substance use disorder). In such cases, understanding the nature of self-harm methods between countries is important; in these countries a high percentage of self-harming behaviours are carried out through more lethal methods such as poisoning (often through pesticides) and self-immolation. This means that in a high number of cases self-harming behaviours can prove fatal, even if there was not a clear intent to die. A study by Ferrari et al. (2015) attempted to determine the share disease burden from suicide which could be attributed to mental health or substance use disorders.28 Based on review across a number of meta-analysis studies the authors estimated that 68 percent of suicides across China, Taiwan and India were attributed to mental health and substance use disorders; across other countries this share was approximately 85 percent. In their estimates of total attributable disease burden, the authors concluded that mental health and substance use disorders were responsible for 62 percent of total DALYs from suicide. Although the total prevalence of mental health and substance use disorders does not show a direct relationship to suicide rates (as shown in the chart above), there are notable links between specific types of mental health disorders and suicide. In their meta-study of the mental health-suicide relationship, Ferrari et al. (2015) assess the pooled relative risk of suicide across a range of mental health and substance use disorders.29 This represents the increased risk of suicide for those with a particular mental health or substance use disorder. The figures below represent estimates of the increased risk of suicide for an individual with one of the following disorders. An individual with depression, for example, is 20 times more likely to die from suicide than someone without; some with anxiety disorder around 3 times; schizophrenia around 13 times; bipolar disorder 6 times; and anorexia 8 times as likely.  The statistics presented in the entry above focus on aggregate estimates of prevalence across total populations. In the chart below we present data on depression prevalence across a number of OECD countries, disaggregated by education level and employment status.30 This data is based on self-reported prevalence of depression as requested by surveys. There are multiple reasons why this data may differ from IHME statistics presented above: it is based only on adults aged 25-64 years old, and focuses on self-reported depression only. The lack of differentiation in these surveys between mental health disorders, such as depression, anxiety disorders, and bipolar disorder mean that self-reported depression data may include individuals with these other disorders. Categories below have been coloured based on education level, with further categorisation based on whether groups are employed, actively seeking employment, and the total of employed, active and unemployed. Across most countries (which you can explore using the \"change country\" option in the chart below) we tend to see the lowest prevalence in depression amongst those with tertiary (postsecondary) education; and highest prevalence in those who did not reach upper secondary education. It is also notable that the large differences in education level close or disappear when we look only at the sub-group of those employed. Overall, the prevalence of depression appears to be lower in individuals in employment relative to those actively seeking employment, or the total population which also includes the unemployed.  Is the prevalence of mental health disorders reflected in self-reported life satisfaction or happiness? Overall, evidence suggests that there is a negative correlation between prevalence of particular mental health disorders (depression and anxiety have been the most widely assessed) and self-reported life satisfaction. This suggests that life satisfaction and happiness tends to be lower in individuals experience particular mental health disorders. We discuss the link and evidence for this relationship in our entry on Happiness and Life Satisfaction. Mental health is known to be an important risk factor for the development of substance use disorders (either in the form of alcohol or illicit drug dependencies). The increased risk of a substance use disorder varies by mental health disorder type: The widespread issue of underreporting means accurate and representative data on the prevalence of disorders is difficult to define. If relying on mental health diagnoses alone, this underestimation would be severe. Prevalence figures would be likely to reflect healthcare spending (which allows for more focus on mental health disorders) rather than giving a representative perspective on differences between countries; high-income countries would likely show significantly higher prevalence as a result of more diagnoses. The data presented in this entry by the Institute of Health Metrics & Evaluation (IHME) is therefore based on a combination of sources, including medical and national records, epidemiological data, in addition to survey data. Where raw data for a particular country is scarce, epidemiological data and meta-regression models must be used based on available data from neighbouring countries. Data quality issues are described below. The data presented here therefore offers an\u00a0estimate (rather than official diagnosis) of mental health prevalence based on medical, epidemiological data, surveys and meta-regression modelling. The majority of data presented in this entry is based on estimates from the IHME's Global Burden of Disease (GBD). This is currently one of the only sources which produces global level estimates across most countries on the prevalence and disease burden of mental health and substance use disorders. Nonetheless, the GBD acknowledges the clear data gaps which exist on mental health prevalence across the world. Despite being the 5th largest disease burden at a global level (and with within the top three across many countries), detailed data is often lacking. This is particularly true of lower-income countries. The Global Burden of Disease note that the range of epidemiological studies they draw upon for global and national estimates are unequally distributed across disorders, age groups, countries and epidemiological parameters.31 Using these studies to provide full coverage of these disorders is challenging. To overcome these methodological challenges the authors note: To deal with this issue and be able to include data derived using various study methodologies and designs, GBD 2013 makes use of DisMod-MR, version 2.0, a Bayesian meta-regression tool. The software makes it possible to pool all of the epidemiological data available for a given disorder into a weighted average, while simultaneously adjusting for known sources of variability in estimates reported across studies. If raw data are not available for a given country, the software produces an imputed estimate for each epidemiological parameter based on data available from surrounding countries. This allowed GBD to include estimates for 188 countries. In this entry we have focused on data trends published by the Institute of Health Metrics (IHME) Global Burden of Disease study. This is currently the only source which provides estimates for all countries over time, and across the full range of mental health and substance use disorders. The World Health Organization (WHO) publish estimates on depression only; the comparison of depression prevalence from IHME versus WHO is shown in the scatter plot below. A range of national sources also publish estimated prevalence of depression. In many cases, the 'boundaries', or category differentiation in mental health disorders is different from IHME estimates. They are often therefore not directly comparable. For example, the Center for Diseases Control (CDC) in the United States provides information and estimates on combined depression and anxiety disorders, treating anxiety as a\u00a0subset of depression.  World Health Organization. (1992).\u00a0The ICD-10 classification of mental and behavioural disorders: clinical descriptions and diagnostic guidelines\u00a0(Vol. 1). World Health Organization. Ferrari et al. (2015). The Burden Attributable to Mental and Substance Use Disorders as Risk Factors for Suicide: Findings from the Global Burden of Disease Study 2010. PLOS ONE. Available online. Prince, M., Patel, V., Saxena, S., Maj, M., Maselko, J., Phillips, M. R., & Rahman, A. (2007). No health without mental health.\u00a0The Lancet,\u00a0370(9590), 859-877. Available online. Brooks, S. J., Rask-Andersen, M., Benedict, C., & Schi\u00f6th, H. B. (2012). A debate on current eating disorder diagnoses in light of neurobiological findings: is it time for a spectrum model?.\u00a0BMC psychiatry,\u00a012(1), 76. Available online. Risks to mental health: an overview of vulnerabilities and risk factors (2012). World Health Organization. Available online. Risks to mental health: an overview of vulnerabilities and risk factors (2012). World Health Organization. Available online. Kieling, C., Baker-Henningham, H., Belfer, M., Conti, G., Ertem, I., Omigbodun, O., ... & Rahman, A. (2011). Child and adolescent mental health worldwide: evidence for action.\u00a0The Lancet,\u00a0378(9801), 1515-1525. Available online. Herrman, H., Saxena, S., Moodie, R., & World Health Organization. (2005). Promoting mental health: concepts, emerging evidence, practice: a report of the World Health Organization, Department of Mental Health and Substance Abuse in collaboration with the Victorian Health Promotion Foundation and the University of Melbourne. Available online. Grantham-McGregor, S., Cheung, Y. B., Cueto, S., Glewwe, P., Richter, L., Strupp, B., & International Child Development Steering Group. (2007). Developmental potential in the first 5 years for children in developing countries.\u00a0The Lancet,\u00a0369(9555), 60-70. Available online. Prince M, Patel V, Saxena S, Maj M, Maselko J, Phillips MR, Rahman A (2007). No health without mental health. The Lancet, 370: 859-877. Available online. Walker S, Wachs TD, Meeks Gardner J, Lozoff B, Wasserman GA, Pollitt E, Careter JA and the International Child Development Steering Group (2007). Child development: risk factors for adverse outcomes in developing countries. The Lancet, 369: 145-157. Available online. Walker S, Wachs TD, Grantham-McGregor S, Black M, Nelson C, Huffman C et al (2011). Inequality in early childhood: risk and protective factors for early child development. The Lancet, 378: 1325-1338. Available online. Walker S, Wachs TD, Meeks Gardner J, Lozoff B, Wasserman GA, Pollitt E, Careter JA and the International Child Development Steering Group (2007). Child development: risk factors for adverse outcomes in developing countries. The Lancet, 369: 145-157. Available online. Walker S, Wachs TD, Grantham-McGregor S, Black M, Nelson C, Huffman C et al (2011). Inequality in early childhood: risk and protective factors for early child development. The Lancet, 378: 1325-1338. Available online. Wicks, S., Hjern, A., & Dalman, C. (2010). Social risk or genetic liability for psychosis? A study of children born in Sweden and reared by adoptive parents.\u00a0American Journal of Psychiatry,\u00a0167(10), 1240-1246. Available online. WHO (2004). Prevention of mental disorders: Effective interventions and policy options. World Health Organization; Geneva, Switzerland. Available online. Hetherington R, Baistow K, Katz I, Trowell J (2001). The welfare of children with mentally ill parents: Learning from inter-country comparisons. Wiley and Sons; Chichester, UK. Available online. Matteblat F, Remschmidt H (2008). The children of mentally ill parents. Deutsches Arzteblatt International, 105: 413-418. Available online. Prince, M., Patel, V., Saxena, S., Maj, M., Maselko, J., Phillips, M. R., & Rahman, A. (2007). No health without mental health.\u00a0The Lancet,\u00a0370(9590), 859-877. Available online. Ferrari et al. (2015). The Burden Attributable to Mental and Substance Use Disorders as Risk Factors for Suicide: Findings from the Global Burden of Disease Study 2010. PLOS ONE. Available online. Prince, M., Patel, V., Saxena, S., Maj, M., Maselko, J., Phillips, M. R., & Rahman, A. (2007). No health without mental health.\u00a0The Lancet,\u00a0370(9590), 859-877. Available online. Yoshimasu, K., Kiyohara, C., Miyashita, K. et al. Environmental Health and Preventative Medicine (2008) 13: 243. Available online. Arsenault-Lapierre, G., Kim, C., & Turecki, G. (2004). Psychiatric diagnoses in 3275 suicides: a meta-analysis.\u00a0BMC psychiatry,\u00a04(1), 37. Available online. Cavanagh, J. T., Carson, A. J., Sharpe, M., & Lawrie, S. M. (2003). Psychological autopsy studies of suicide: a systematic review.\u00a0Psychological Medicine,\u00a033(3), 395-405. Available online. Phillips, M. R. (2010). Rethinking the role of mental illness in suicide. American Journal of Psychiatry. Available online. Conner, K. R., Phillips, M. R., Meldrum, S., Knox, K. L., Zhang, Y., & Yang, G. (2005). Low-planned suicides in China.\u00a0Psychological Medicine,\u00a035(8), 1197-1204. Available online. Zhang, J., Xiao, S., & Zhou, L. (2010). Mental disorders and suicide among young rural Chinese: a case-control psychological autopsy study.\u00a0American Journal of Psychiatry,\u00a0167(7), 773-781. Available online. Yang, G. H., Phililips, M. R., Zhou, M. G., Wang, L. J., Zhang, Y., & Xu, D. (2005). Understanding the unique characteristics of suicide in China: national psychological autopsy study.\u00a0Biomedical and Environmental Sciences,\u00a018(6), 379. Available online. Ferrari et al. (2015). The Burden Attributable to Mental and Substance Use Disorders as Risk Factors for Suicide: Findings from the Global Burden of Disease Study 2010. PLOS ONE. Available online. Ferrari et al. (2015). The Burden Attributable to Mental and Substance Use Disorders as Risk Factors for Suicide: Findings from the Global Burden of Disease Study 2010. PLOS ONE. Available online. OECD Indicators (2017). Education at a Glance 2017. Available at: http:\/\/www.oecd.org\/education\/education-at-a-glance-19991487.htm. Whiteford, H., Ferrari, A., & Degenhardt, L. (2016). Global burden of disease studies: implications for mental and substance use disorders.\u00a0Health Affairs,\u00a035(6), 1114-1120. Available online.","time":1525788501,"title":"Population with mental health and substance use disorders, 2016","type":"story","url":"https:\/\/ourworldindata.org\/mental-health","label":7,"label_name":"random"},{"by":"______","descendants":0,"id":17020956,"kids":"None","score":1,"text":"","time":1525788476,"title":"Prioritize prioritization appropriately","type":"story","url":"https:\/\/filosophy.org\/post\/70\/prioritize_prioritization_appropriately\/","label":7,"label_name":"random"},{"by":"nickjj","descendants":65,"id":17020944,"kids":"[17021061, 17021410, 17021139, 17021843, 17021369, 17021468, 17024844, 17021419, 17021350, 17021354, 17023177, 17023775, 17021691, 17021189, 17021721, 17021093, 17021188, 17021191]","score":178,"text":" Dive into Docker takes you from \"What is Docker?\" to confidently applying Docker to your own projects. It's packed with best practices and examples. Start Learning Docker \u2192 Updated on May 8th, 2018 in  #deployment   Quick Jump:  How Did It Happen? | Fixing the Problem in a Few Seconds | Avoiding the Problem in the Future | Domain Validation Should Be More Strict When people talk about a site being compromised, usually you would think that your server has been compromised. That would be someone gaining access to your server and then doing whatever they please. That didn\u2019t happen here. I take security very seriously. I have SSH locked down to only allow SSH key based logins and even root logins are disabled. My site is static too, which means it\u2019s only being hosted through nginx from a non-root user. The only way someone is going to gain access to my server is if they manage to gain access to my workstation and steal my SSH key pair. The odds of that are remote because my workstation never leaves my office and I have the reflexes of a highly trained ninja. So how did a subdomain of mine end up help distributing 390,000+ PDF books without my server being compromised? Well, that\u2019s easy\u2026  It boils down to this. About 2 years ago I was recording a video course that dealt with setting up HTTPS on a domain name. In all of my courses, I make sure to \u201creally\u201d do it on video so that you can see the entire process from end to end. Back then I used nickjanetakis.com for all of my courses, so I didn\u2019t have a dedicated domain name for the course I was working on, such as diveintodocker.com. I also didn\u2019t have a spare domain name that I wanted to publicly share, so I registered a new DigitalOcean droplet to host an example site on. Then I set up an A record to point ssl.nickjanetakis.com to that droplet\u2019s IP address. Cool, there\u2019s nothing wrong with that. Set up a temporary site for recording the course and then delete it afterwards. Easy peasy, and that\u2019s exactly what I did but I forgot to remove the A record when I was done. So for years, I had ssl.nickjanetakis.com pointing to an IP address that I was no longer in control of. That means the owner of that IP address could host anything and it would automatically be mapped to ssl.nickjanetakis.com without me knowing. I have Google Alerts set up so I get emailed when people link to my site. A few months ago I started to receive an absurd amount of notifications, but I ignored them. I chalked it up to \u201cGoogle is probably on drugs\u201d. Stranger things have happened with Google, so I thought maybe something got mixed up. Now I\u2019ve learned my lesson. While bugs roll out into production all the time, Google Alerts being totally busted is super unlikely. Part of my morning routine is to check emails with intent to answer any questions about my courses that may have happened during the night. I usually skim the subject lines to see which emails to answer first but one of them caught my eye. It read \u201cHi, seems your website has been compromised\u201d. Well you don\u2019t see that every day. I figured it was spam that somehow made it to my inbox but I recognized the email because it was someone who signed up for one of my courses. He sent me a screenshot showing a few PDFs being hosted from ssl.nickjanetakis.com, so I immediately went to Google and searched for site:ssl.nickjanetakis.com.  It was mostly college books but there was a ton of other stuff too. Hopefully by the time you read this most of them have been removed from Google\u2019s index.  Since it was linked to a subdomain I instantly knew what was wrong, especially because I remember using that subdomain a few years ago when I made that course. The fix was really easy. I just hopped over to my domain registrar\u2019s DNS settings and deleted the entry that mapped the IP address to that subdomain. But before deleting it, I copied the IP address so I could open a support ticket on DigitalOcean. I figured they would like to know that someone is illegally distributing content on one of their servers. Now that they know the IP address, they can shut it down.  Always remove unused records from your DNS settings when you\u2019re done using them. DigitalOcean and many other cloud providers have purchased blocks of IP addresses and they provide these IP addresses to people like you and me. When a droplet gets destroyed, the IP address eventually gets put into the public pool of available IP addresses and someone else will get it. This is pretty scary because things like the above can happen if you\u2019re not careful, but it also means if an IP address were blacklisted for doing something questionable, you might end up with that IP address in the future (but that\u2019s a totally different problem).  I think this brings up an interesting question. Right now you can validate you own a domain by putting an HTML snippet on your page. Services like Google Analytics allow for this. Technically the person who took control over ssl.nickjanetakis.com could have proven ownership of that subdomain if they set up a page and hooked it up to Google Analytics. Also, Let\u2019s Encrypt\u2019s web server based challenge would have passed. I know I made a stupid mistake by not removing the A record but this could happen to anyone. I would like to see more services only allow for DNS based authentication by adding TXT records. Although I suppose the bigger problem here is having IP addresses being recycled. Hopefully once IPv6 is fully in use we\u2019ll have a big enough pool so that hosting providers can remove previously used addresses from their pool. That won\u2019t be fool proof, but it\u2019s a start. Has this ever happened to you? Let me know in the comments below! Like you, I'm super protective of my inbox, so don't worry about getting spammed. You can expect a few emails per month (at most), and you can 1-click unsubscribe at any time. See what else you'll get too. \u00a9 2018 Nick Janetakis","time":1525788357,"title":"A Recycled IP Address Caused Me to Pirate Books by Accident","type":"story","url":"https:\/\/nickjanetakis.com\/blog\/a-recycled-ip-address-caused-me-to-pirate-390000-books-by-accident","label":7,"label_name":"random"},{"by":"mikedubc","descendants":0,"id":17020942,"kids":"None","score":1,"text":" by Mike Chan | May 8, 2018 | AWS, Cloud Computing, Infrastructure as Code | 0 comments This is the fourth article in our Infrastructure as Code blog series. You can read the first three here: In our next post, we will provide CloudFormation templates and analyze what they do and how they work. To be notified when future posts go live, click here. Our prior Infrastructure as Code article highlighted 15 popular IaC tools you can use to automate your deployments. One of the tools we wrote about is CloudFormation, which is the IaC tool offered by the largest cloud service provider in the world, Amazon Web Services. In 2017, over 350,000 AWS customers used AWS CloudFormation to deploy and manage over 2.4 million infrastructure stacks. Impressive. In this article, we\u2019ll take a look at: \u00a0 AWS CloudFormation is a configuration orchestration tool that allows you to codify your infrastructure to automate your deployments. CloudFormation templates can be created with YAML in addition to JSON. Or you can use AWS CloudFormation Designer to visually create your templates and see the interdependencies of your resources. CloudFormation takes a declarative approach to configuration, meaning that you tell it what you want your environment to look like, and it finds its way there. During this configuration process, CloudFormation automatically manages dependencies between your resources. Thus, you don\u2019t have to specify the order in which resources are created, updated, or deleted. CloudFormation automatically determines the correct sequence of actions to create your environment, though you can use the DependsOn attribute, wait condition handlers, and nested stacks to specify the order of operations, if necessary. Sometimes updating an infrastructure stack can cause anxiety because you\u2019re not sure what changes might break the environment. Not to fear! CloudFormation Change Sets allow you to preview how your resources will be impacted before any changes are executed. Only after you execute your change set will your stack be edited. Update a stack with change sets \u2013 Image courtesy of AWS \u00a0 Even if you execute a change set that has errors in it, CloudFormation has Rollback Triggers that allow you to monitor your stack creation or update process and roll back your environment to a previous state. You can specify thresholds to monitor in CloudWatch and integrate them into your CloudFormation templates. When these thresholds are exceeded, the Rollback Triggers revert your environment back to the previously deployed state. CloudFormation StackSets allow you to deploy, update, or delete infrastructure stacks across multiple AWS regions and accounts with a single CloudFormation template. Before StackSets existed, every infrastructure environment had to be deployed independently, and custom scripts had to be written to deploy these stacks to multiple accounts and regions. StackSets now make it much easier to maintain consistency when you add new regions and accounts. And Custom Resources let you write custom provisioning logic in your CloudFormation scripts. For instance, if you\u2019d like to define resources that CloudFormation doesn\u2019t support yet, or if you need to create a resource that\u2019s specific to your use case, Custom Resources allows you to manage all of this in a single stack. CloudFormation supports many AWS services. AWS frequently releases additional useful functionality, so the sky\u2019s the limit on the things you can do with CloudFormation. As mentioned in our prior post about IaC tools, CloudFormation is a configuration orchestration tool, which is used to automate the deployment of servers and other infrastructure like databases and load balancers. CloudFormation is often used in conjunction with configuration management tools, which are designed to configure the software and systems that run on this infrastructure. This combination provides seamless deployment and configuration of AWS infrastructure and the applications that run on top of it. Let\u2019s say that you want to build a SaaS data analytics application on AWS. You can use a CloudFormation template to provision a web server on EC2, a DynamoDB database, and any other AWS resources you need. Then you can install a configuration management tool to set up the operating systems and software on these EC2 instances. Chef and Puppet are the most popular configuration management products used with CloudFormation, and AWS has a product called OpsWorks that provides managed instances of these tools. The combination of CloudFormation with configuration management tools allows you to automate the configuration, deployment, and management of cloud resources and the software that run on them from a single template. This greatly improves the efficiency and consistency of your\u00a0infrastructure deployments. If you use CloudFormation to automate your deployments, you\u2019re in good company. Nextdoor is a private social network for local neighbors to connect with each other and share community news. The company has over 160,000 neighborhoods across the globe and more than 10 million users on its platform. Nextdoor uses CloudFormation templates for flexible, one-click server deployment and network creation. They also use Puppet to define and configure the software and operating systems that run on these AWS servers. Coinbase is the largest consumer Bitcoin wallet in the world. Over 10 million users have purchased or traded over $50 billion worth of cryptocurrencies through their platform. Most if not all of Coinbase\u2019s infrastructure is designed and managed with CloudFormation templates. This allows the company to easily replicate infrastructure stacks for all phases of their development process and provides version control to ensure their environments are configured correctly over time. Expedia Group is one of the largest travel booking companies in the world, with well-known brands such as Expedia.com, Hotels.com, Travelocity, and many more. In 2017, the company had $88.4 billion of gross bookings through its various platforms. The global company has a multi-region, multi-availability zone architecture, and uses CloudFormation in combination with Chef to deploy its entire front and backend stack into its Amazon Virtual Private Cloud environment. As you can see, AWS is a really powerful tool that many large and influential companies use to deploy and manage their infrastructure stacks. We primarily work with AWS, so we\u2019re all over CloudFormation. And we\u2019ve built a couple of products on the AWS Marketplace (SFTP Gateway and WP SureStack) using CloudFormation, so we understand the power that it can have in automating AWS infrastructure deployments. In our next post, we\u2019ll provide a few CloudFormation templates we created and walk you through exactly what\u2019s going on in each of them. Sign up below to receive our next article via email. Like this post? It likes you too. \ud83d\ude42 Please share it using the share buttons to the left. Then join our mailing list below, follow us on Twitter @thorntech, and join our Facebook page for future updates. Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n   Copyright \u00a9  Thorn \nTechnologies.  \nAll \nRights \nReserved.","time":1525788354,"title":"What is AWS CloudFormation and how can it help your IaC efforts?","type":"story","url":"https:\/\/www.thorntech.com\/2018\/05\/whatisawscloudformation\/","label":3,"label_name":"dev"},{"by":"dsr12","descendants":0,"id":17020937,"kids":"None","score":2,"text":"An aerial shot of a palm oil plantation in Papua. The industry has exploded in recent years, but has been criticised for causing environmental damage. Photo: Jurnasyanto Sukarno\/Greenpeace Emails show officials were concerned EU vote could hit a deal for fighter jets manufactured by BAE Sytems An aerial shot of a palm oil plantation in Papua. The industry has exploded in recent years, but has been criticised for causing environmental damage. Photo: Jurnasyanto Sukarno\/Greenpeace Emails show officials were concerned EU vote could hit a deal for fighter jets manufactured by BAE Sytems An aerial shot of a palm oil plantation in Papua. The industry has exploded in recent years, but has been criticised for causing environmental damage. Photo: Jurnasyanto Sukarno\/Greenpeace \n\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tJoe Sandler Clarke\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n Share this story British government officials warned a proposed EU ban on palm oil in biofuels could harm UK defence sales to Malaysia, specifically Typhoon fighter jets, according to government emails obtained by\u00a0Unearthed.\n\nThe correspondence reveals that the British high commission in Kuala Lumpur even expected Malaysian Prime Minister Najib Razak to lobby Theresa May personally on the issue at last month\u2019s Commonwealth Heads of Government meeting. In the event, Razak did not attend the meeting in London, a Number 10 spokeswoman told Unearthed. Correspondence between the Ministry of Defence, the Department for Environment, Food and Rural Affairs, and the British high commission, reveals British officials were concerned that EU moves to ban palm oil in biofuels could result in Malaysian trade reprisals against the UK. MEPs voted in January to phase out the use of palm oil in biofuels, citing environmental concerns. The move sparked a furious response from the governments of Indonesia and Malaysia, which produce most of the world\u2019s palm oil. The debate over palm oil is playing a significant role in the run-up to Malaysia\u2019s general election, which will be held tomorrow.\n\nOn the morning of 5 February, an official at the British high commission in Malaysia sent an email warning that the EU decision was \u201ca big issue for Malaysia and, if not handled correctly, has the potential to impact on bilateral trade, particularly defence sales (Typhoon)\u201d. As well as stating that Razak was expected lobby May on the issue, the diplomat\u2019s email states that \u201cwe should expect every minister engaging with Malaysia to be lobbied on palm oil\u201d. In a later email that morning,\u00a0an MoD official wrote that the ban \u201ccould affect our bilateral relationship and potentially defence sales\u201d. Reuters reported in February that BAE Systems would provide Malaysia with a UK government-backed financing deal if it decided to replace its fleet of fighter jets with the Eurofighter Typhoon. Malaysia has reportedly been weighing up whether to buy the French made Rafale jet or the Typhoon for years, as it looks to replace its fleet of Russian MiG-29s. \n  A spokeswoman for BAE Systems told\u00a0Unearthed: \u201cWe are aware of media reports of a potential European ban on palm oil imports \u2013 this is a matter for the government. We remain engaged in discussions about the potential for Typhoon to meet the requirements of the Royal Malaysian Air Force.\u201d Unearthed has previously reported government concerns that Malaysia could take action against the UK and other EU member states over the palm oil ban. But this is the first time worries over specific consequences for defence deals have been publicly confirmed. \u00a368m worth of UK arms export licences were authorised to Malaysia in the first nine months of 2017, according to Department for International Trade data collected by the Campaign Against the Arms Trade.\n EU vote The palm oil industry has exploded over the last 20 years, and the vegetable oil can now be found in everything from chocolate bars to shampoo. But the commodity\u2019s period of rapid growth has been marked with concerns about deforestation and environmental damage; notably to orangutan habitats. The European Parliament vote in January formed part of negotiations around the EU\u2019s new renewable energy directive. A final decision on the shape of the directive, which will set ground rules for European renewable energy policy post-2020, is yet to be taken. Negotiations between the European Parliament, the European Commission and the European Council ongoing. Green Party MP Caroline Lucas told\u00a0Unearthed that the government needed to make its position clear on the use of palm oil in biofuels. \n\n\u201cPalm oil production is causing devastation to wildlife and nature in many places around the world. It would be utterly unacceptable if the UK government prioritised the profits of the arms trade over the protection of endangered species and precious habitats. \n\n\u201cWe need clarification from the Prime Minister and the Foreign Secretary that the UK government is committed to strengthening the rules governing palm oil use in biofuels \u2013 as well as firm assurances that no dodgy backroom deals have been made in Whitehall or Brussels.\u201d Despite requests from Unearthed to clarify the government\u2019s position on the palm oil vote, the government declined to address the issue directly. Instead, a spokesman for the Department for International Trade told Unearthed: \u201cThere is a strong trade and investment relationship between the UK and Malaysia. As we leave the EU, we look forward to deepening our engagement with emerging markets in South East Asia. \u201cThe UK has not used palm oil in the production of biofuels for a number of years and the use of non-waste oil in biofuel is minimal. The government has also agreed to focus on utilizing waste derived biofuel, which generally has higher carbon savings.\u201d  The emails obtained by Unearthed reveal that officials at Defra were \u201ckeen to avoid being drawn on palm oil in relation to defence\u201d when talking about the EU vote to the press. In response, an MoD official wrote in February: \u201cto start referring to defence deals in our response makes it look like it is a serious consideration in our approach to the issue when at the current time it isn\u2019t\u201d. The government also declined to respond directly to Unearthed queries, submitted to several government departments, on whether any minister had been lobbied directly by Malaysian representatives on the palm oil issue. The British government has previously said as long as the UK remains in the EU, it will continue to be part of negotiations with other European countries on the directive. But it is yet to make known its position on the palm oil phase out. The Malaysian government has said it will go to the World Trade Organisation to complain about the EU move to prevent palm oil in biofuels from counting towards renewable targets. Last month, Politico reported Indonesia\u2019s special envoy on palm oil Luhut Binsar Pandjaitan told an audience in Brussels that his government would consider taking retaliatory trade measures against the EU, including action against aviation giant Airbus, if the biofuel phase out goes ahead. \nDesign by S-T \/ Build by ON\n","time":1525788321,"title":"British diplomats warned that saving rainforests will endanger fighter jet sales","type":"story","url":"https:\/\/unearthed.greenpeace.org\/2018\/05\/08\/palm-oil-eu-uk-defence-malaysia-bae-systems\/","label":7,"label_name":"random"},{"by":"xhruso00","descendants":0,"id":17020929,"kids":"None","score":2,"text":"A research team led by scientists at AIDS Institute and Department of Microbiology, Li Ka Shing Faculty of Medicine of The University of Hong Kong (HKU) invents a universal antibody drug against HIV\/AIDS. By engineering a tandem bi-specific broadly neutralizing antibody, the team found that this novel antibody drug is universally effective not only against all genetically divergent global HIV-1 strains tested but also promoting the elimination of latently infected cells in a humanized mouse model. The new findings are now published in the April issue of Journal of Clinical Investigation. AIDS remains an incurable disease. In the world, HIV\/AIDS has resulted in estimated 40 million deaths while 36.9 million people are still living with the virus. To end the HIV\/AIDS pandemic, it is important to discover either an effective vaccine or a therapeutic cure. There are, however, two major scientific challenges: the tremendous HIV-1 diversity and the antiviral drug-unreachable latency. Since it is extremely difficult to develop an appropriate immunogen to elicit broadly neutralizing antibodies (bnAbs) against genetically divergent HIV-1 subtypes, developing existing bnAbs as passive immunization becomes a useful approach for HIV-1 prophylaxis and immunotherapy. Previous studies have investigated the potency, breadth and crystal structure of many bnAbs including their combination both in vitro and in vivo. Naturally occurring HIV-1 resistant strains, however, are readily found against these so-called bnAbs and result in the failure of durable viral suppression in bnAb-based monotherapy. To improve HIV-1 neutralization breadth and potency, bispecific bnAb, which blocks two essential steps of HIV-1 entry into target cells, have been engineered and show promising efficacy in animal models. Before the publication, tandem bi-specific bnAb has not been previously investigated in vivo against HIV-1 infection. Research method and findings The HKU research team invented a single gene-encoded tandem broadly neutralizing antibody, titled \"BiIA-SG,\" which \"kills two birds with one stone.\" By attaching to host protein CD4, BiIA-SG strategically ambushes invading HIV-1 particles to protect CD4 positive T cells. BiIA-SG not only displays a potent activity against all three panels of 124 genetically divergent global HIV-1 strains tested, but also prevents diverse live viral challenges completely in humanized mice. Moreover, gene transfer of BiIA-SG achieves pro-longed drug availability in vivo, leading to a promising efficacy of eliminating HIV-1 infected cells in humanized mice. Therefore, the research team provides a proof-of-concept that BiIA-SG is a novel universal antibody drug for prevention and immunotherapy against HIV-1 infection. Significance of the study The accumulated number of HIV-1 infections has more than doubled from 4,443 diagnostic cases in 2009 to 9,091 in 2017, despite the timely introduced combination antiretroviral therapy and prevention interventions in Hong Kong. Currently, the estimated annual cost is over HK$550 millions for antiretroviral drugs alone per year in Hong Kong, not to mention the rising issues of life-long financial burdens, drug toxicity and resistant viruses. The newly invented universal antibody drug brings the hope to fight these issues. With significantly improved breadth and potency, BiIA-SG will hopefully be the first \"Made in Hong Kong\" anti-HIV-1 antibody drug for clinical development. Story Source: Materials provided by The University of Hong Kong. Note: Content may be edited for style and length. Journal Reference: Cite This Page: Get the latest science news with ScienceDaily's free email newsletters, updated daily and weekly. Or view hourly updated newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?","time":1525788280,"title":"New universal antibody drug for HIV-1 prevention and immunotherapy","type":"story","url":"https:\/\/www.sciencedaily.com\/releases\/2018\/05\/180507111840.htm","label":5,"label_name":"ml"},{"by":"ALee","descendants":0,"id":17020924,"kids":"None","score":1,"text":"The company is offering free kits to researchers studying populations in Africa, Asia, and elsewhere\u2014but the ethics are tricky. 23andMe is best known for selling DNA test kits, but the company\u2019s real value lies in the data of its 5 million customers. The bigger its genetic database, the more insights 23andMe can glean from DNA. That, in turn, means the more it can tell customers about their ancestry and health and the more valuable the data it shares with academic scientists and sells to pharmaceutical companies for research. About 80 percent of 23andMe customers choose to participate in such research. As impressive as 23andMe\u2019s genetic database is, it still has noticeable gaps\u2014especially among Africans, Middle Easterners, Central Asians, Southeast Asians, and indigenous Americans. Almost any group that is not European, basically. In the scientific literature, in fact, nearly 80 percent of the people who have participated in studies of associations between genes and diseases are of European descent. That is why 23andMe has recently launched a number of initiatives to add underrepresented groups to its database. The latest is the Populations Collaborations Program, which allows U.S.-based scientists already studying underrepresented groups to apply for free spit kits and DNA analysis. In return, 23andMe gets to add the DNA to its database. The company has had one-off collaborations with researchers in Sierra Leone, the Democratic Republic of Congo, and Tanzania, but the new program sets up a formal application process. Earlier this year, 23andMe also announced the Global Genetics Project to give free tests to people who can trace all four grandparents to one of 61 underrepresented countries. That, in turn, is an expansion of the African Genetics Project, which did the same for certain African countries, specifically chosen for being areas from where slaves and recent immigrants to the United States came. Africa is also of particular interest to many academic researchers. The continent is famously the cradle of humanity, and it holds more genetic diversity than any other. But this interest has also caused a backlash among African scientists, who feel left behind as American and European scientists reap the benefit of the work. Last week, a consortium of heredity and health researchers called the H3Africa Initiative released ethical guidelines to counter \u201chelicopter\u201d research by foreign scientists. Collecting DNA for not just academic but commercial reasons, as 23andMe would like to do, could make ethics even tricker. When I sent the description of 23andMe\u2019s Populations Collaborations Program to Jantina de Vries, a bioethicist at University of Cape Town who coauthored the H3Africa guidelines, she flipped the scenario around: \u201cImagine an African company gets African researchers to collect 1,000 DNA samples of Americans just because they want to. People would not like that at all.\u201d Joanna Mountain, the senior director of research at 23andMe, has done fieldwork in Africa before, and says she understands the challenges of gaining trust for this kind of work. Before joining 23andMe, she was a professor at Stanford, and she has collected DNA samples among hunter-gatherers in Tanzania. It takes a lot of time and effort. That\u2019s why 23andMe wants to collaborate with researchers with existing relationships. \u201cThey know the ethnic groups and the languages. They know all the background, which is really critical,\u201d says Mountain. She says 23andMe considered sending their own teams out to collect DNA, but they quickly realized the company doesn\u2019t have the right expertise. De Vries says the key question is what\u2019s in it for the people giving up their DNA. 23andMe does plan to evaluate applications based on how people sampled can benefit from the research. For example, Mountain says, could the research ultimately reveal insights about a group\u2019s history or genetic predispositions to disease? And more directly, the company may offer a grant of up to $10,000 to cover fieldwork, which could go toward projects helping the local community by funding education or infrastructure or even food. In one of its previous collaborations, 23andMe provided free DNA analysis for Nathan Nunn, an economist at Harvard who is studying the Kuba Kingdom in what is the present-day Democratic Republic of Congo. His research group sponsored research assistants to come to the United States to continue their education. They have also given lectures about their research to the local community in an effort to give something back to the people who have participated. \u201cWe are very cognizant of that,\u201d he says. George Perry, an evolutionary biologist at Pennsylvania State University who has a number of research projects in Africa, says 23andMe\u2019s program is \u201cpotentially exciting and potentially mutually beneficial.\u201d He would want to nail down exactly how 23andMe plans to use the data to address concerns about \u201cbioprospecting.\u201d And explaining all of this to research participants\u2014how DNA works, why an American company wants to collect their DNA\u2014to get truly informed consent could take multiple visits over a few years, he says. 23andMe has been very successful in building its DNA database by convincing wealthy customers to get tested for one or two hundred bucks. How it will convince people on another continent who otherwise have no interest in DNA is a different challenge altogether. \n    Jeff VanderMeer discusses how writing fiction about environmental crises may jolt readers out of complacency.\n Kanye West wants freedom\u2014white freedom. I could only have seen it there, on the waxed hardwood floor of my elementary-school auditorium, because I was young then, barely 7 years old, and cable had not yet come to the city, and if it had, my father would not have believed in it. Yes, it had to have happened like this, like folk wisdom, because when I think of that era, I do not think of MTV, but of the futile attempt to stay awake and navigate the yawning whiteness of Friday Night Videos, and I remember that there were no VCRs among us then, and so it would have had to have been there that I saw it, in the auditorium that adjoined the cafeteria, where after the daily serving of tater tots and chocolate milk, a curtain divider was pulled back and all the kids stormed the stage. And I would have been there among them, awkwardly uprocking, or worming in place, or stiffly snaking, or back-spinning like a broken rotor, and I would have looked up and seen a kid, slightly older, facing me, smiling to himself, then moving across the floor by popping up alternating heels, gliding in reverse, walking on the moon. The new music video from Childish Gambino weaponizes the viewer\u2019s instinctive bodily empathy. \u201cThis Is America\u201d isn\u2019t the first time that Donald Glover, as his musical alter ego Childish Gambino, has harnessed dance in service of surrealism. But the art form has a conspicuous symbolic significance in the artist\u2019s latest single, which Glover debuted on Saturday Night Live: The song\u2019s emphasis on dance was apparent in his live performance on the show, in the cover art for the track, and in the remarkable music video itself, which has more than 36 million views on YouTube as of publication. In the video, a grinning, shirtless Glover dances through a giant warehouse, occasionally accompanied by black school children in uniform, as chaotic scenes of violence unfold behind him\u2014and are sometimes enacted by him. The question is less whether a dress or an idea is borrowed, than the uses to which it\u2019s then put. Meet the Death Metal Cowboys of Botswana. In black leather decorated with metal studs, they play a pounding style of music that people who know more than me trace to the British band \u201cVenom\u201d and its 1981 album Welcome to Hell. Question: Is this cultural appropriation? Why or why not? The question is inspired by a spasm of social-media cruelty that caught wide attention last week. A young woman in Utah bought a Chinese-style dress to wear to her high school formal. She posted some photographs of herself on her personal Instagram page\u2014and suddenly found herself the target of virulent online abuse. For once, the story has a happy ending. Good sense and kindness prevailed, and instead of her prom being ruined, the young woman exited the dance buoyed by worldwide support and affirmation, most of all from within China. What predicts whether a politician accused of sexual harassment or abuse will try to stay in office\u2014or quickly fold? The fact of a powerful man being accused of sexual misconduct is not, at this stage, all that unusual\u2014though the details of the accusations against New York Attorney General Eric Schneiderman were nauseating. What was unusual was the speed with which he resigned. Just three hours elapsed from the moment The New Yorker published an article detailing allegations of abuse by former romantic partners until Schneiderman, a Democrat, announced he was leaving office. He offered a terse statement denying the claims, but added, \u201cWhile these allegations are unrelated to my professional conduct or the operations of the office, they will effectively prevent me from leading the office\u2019s work at this critical time. I therefore resign my office, effective at the close of business on May 8, 2018.\u201d President Trump said he'll begin reinstating nuclear sanctions on the Iranian regime, effectively marking the beginning of the end of the agreement. Updated at 4:18 p.m. ET President Trump announced Tuesday that he would reimpose nuclear-related sanctions on Iran, setting the stage for a long-expected dismantling of the Obama-era nuclear deal with the Islamic Republic. \u201cI am announcing today that the United States will withdraw from the Iran nuclear deal,\u201d he said. \u201cIn a few moments, I will sign a presidential memorandum to begin reinstating U.S. nuclear sanctions on the Iranian regime.\u201d On the face of it, the announcement goes much further than had been expected. Observers had expected Trump to decline to waive only the most immediate set of sanctions related to Iran\u2019s oil trade\u2014the sanctions that were specifically at play with an upcoming May 12 deadline for Trump to waive them, giving America\u2019s European allies time to find a fix that would placate the president. Childish Gambino\u2019s sensational \u201cThis Is America\u201d video implicates the viewer in the misuse of black art. If you search for \u201cThis Is America\u201d on Twitter, you find not only a gushing river of well-deserved praise for Donald Glover\u2019s new work, which has quickly become the most talked-about music video of recent memory. You also find Trump supporters using the moment to spread their messages. The hashtag #ThisIsAmerica sits next to a rant about the deep state. It sits next to a sneering meme about Hillary Clinton. It sits next to a picture of white pioneers, shared by a \u201cEuropean rights activist,\u201d who says, \u201cMost of the people who built America looked like this.\u201d Trending hashtags get hijacked by unsympathetic causes as a matter of course, but Glover knew what he was getting into with the name \u201cThis Is America.\u201d The defining of a nation is the essential task of politics, and Glover\u2019s definition has now been made clear. America is a place where black people are chased and gunned down, and it is a place where black people dance and sing to distract\u2014themselves, maybe, but also the country at large\u2014from that carnage. America is a room in which violence and celebration happen together, and the question of which one draws the eye is one of framing, and of what the viewer wants to see. The brash and flamboyant politician, the U.K.\u2019s new foreign secretary, is one of the more cosmopolitan figures on the world stage\u2014but he\u2019s also one of the least diplomatic. Boris Johnson, a New York City native and accused Little Englander, probably isn\u2019t much of a fan of the twangy Americana stylings of Dan Hicks. Which is a shame, because he might take a cue from Mr. Hicks and His Hot Licks: \u201cHow can we miss you when you won\u2019t go away?\u201d BoJo, who was barely finished licking his wounds after being unceremoniously tossed under a double-decker London bus by his friend and fellow Conservative Michael Gove, thus losing his shot at the prime ministership of the U.K., is now the freshly announced foreign secretary in PM Theresa May\u2019s government. It\u2019s an amusing landing spot. Boris is one of the more cosmopolitan figures on the world political stage: great-grandson of a Turk, born an American citizen (and, depending on whom you ask, perhaps still one), and a veteran journalist on the European continent. He speaks, with varying degrees of fluency, French, Italian, German, and Spanish. If simple knowledge of foreign parts is the foreign secretary\u2019s mandate, he\u2019d be set. But overseeing diplomacy is also in the portfolio, and diplomacy has never been one of Johnson\u2019s strengths. U.S. conservatives howled that President Obama kicked his presidency off with a metaphorical \u201capology tour,\u201d but Foreign Secretary Boris Johnson might find a literal apology tour essential to kick off his stint in Whitehall. Many of the assumptions that guided America\u2019s march to conflict in 2003 still dominate American foreign policy today.\u00a0 \u00a0 Last week, while watching Benjamin Netanyahu unveil secret information that supposedly proved that Iran is deceiving the world about its nuclear-weapons program, I had a flashback. It was to February 5, 2003, when then-Secretary of State Colin Powell unveiled secret information that supposedly proved that Iraq was deceiving the world about its nuclear, chemical, and biological weapons programs. Like Netanyahu\u2019s, Powell\u2019s presentation was dramatic. He informed the United Nations Security Council that some of the material he was about to present came from \u201cpeople who have risked their lives to let the world know what Saddam Hussein is really up to.\u201d He went on to play a secretly recorded conversation of two Iraqi officials supposedly plotting to mislead weapons inspectors. He later presented a photo of bunkers that allegedly held \u201cactive chemical munitions\u201d but were \u201cclean when the inspectors get there.\u201d Saddam, Powell insisted, wants \u201cto give those [of] us on this Council the false impression that the inspection process was working.\u201d Powell\u2019s presentation was designed to prove that it was not. Secretary of Education Betsy DeVos's address at Ave Maria University last weekend reignited a debate about the proper role of a Catholic school in a sharply divided time. AVE MARIA, Florida\u2014In this enclave in Southwest Florida, the lush, pruned golf course and ritzy subdivisions are eclipsed only by the magnificent church that marks the town\u2019s distinctive Catholic character. The town is also home to a similarly named religious institution, Ave Maria University, which was founded in 2003. The institution\u2014and the master-planned community in which it is now located\u2014is the brainchild of Tom Monaghan, the billionaire founder of Domino\u2019s, who is originally from Michigan and known for his Catholic philanthropy. Ave Maria is still in its infancy compared with the big names in Catholic higher education. But Monaghan has a vision. He wants to build a campus that is more religious than the Notre Dames of the world, and this patch of Florida, he decided, was the right location for that. The secluded community feels less like the rest of the state and more like a paradise for those who want to live faithfully. One student, Anne Marie Schlueter, a sophomore, told me the institution\u2019s mission of faith-driven education drew her here from Ohio; it was a university where she could build a strong Catholic foundation with which to answer critical questions about the world. But the young institution still has important questions to answer of its own\u2014many about itself\u2014as it grows and navigates debates within its central Catholic identity. With the 2018 and 2020 elections on the horizon, race and racism are becoming ever-larger issues among the most marginalized communities in America, making the Democratic coalition harder and harder to hold. In 2018, black voters are finding out just what the hell they had to lose. Nazis and Klansmen march openly and proudly, and hate crimes appear to be on the rise. Police killings of people\u2014especially black people\u2014remain largely the same year to year, and this iteration of the Justice Department has largely abdicated any federal responsibility in reducing brutality. An infant-mortality crisis is tightening its grip on the most marginalized communities, and across many economic metrics\u2014from evictions, to generational wealth, to segregation\u2014disparities are either stagnating or trending in the wrong direction. Fifty years after the Kerner Commission\u2019s report said the country was \u201cmoving toward two societies, one black, one white\u2014separate and unequal,\u201d the prophecy has been all but fully realized. Two young girls escape Syria in an intimate short film, told largely through home movies. After a lifetime of intestinal problems, biohacker Josiah Zayner declares war on his own body's microbes. An enslaved woman who jumped from a building in 1815 is later revealed to be the plaintiff in a successful lawsuit for her freedom. Support 160 years of independent journalism.   TheAtlantic.com Copyright (c) 2018 by The Atlantic Monthly Group. All Rights Reserved.","time":1525788217,"title":"23andMe Wants Its DNA Data to Be Less White","type":"story","url":"https:\/\/www.theatlantic.com\/science\/archive\/2018\/04\/23andme-diversity-dna\/558575\/?single_page=true","label":7,"label_name":"random"},{"by":"prostoalex","descendants":0,"id":17020911,"kids":"None","score":1,"text":"Published: May 3, 2018 6:11 a.m. ET China\u2019s gold bar, coin demand down 26% in Q1 year on year By Overall global gold demand fell to its lowest first-quarter level since 2008, driven by a slump in demand for gold bars and exchange-traded funds backed by the precious metal, according to a report from the World Gold Council released Thursday.  Total gold investment demand fell to 973 metric tons in the first quarter, down 7% from 1,047 metric tons in the first quarter of 2017, the WGC reported. The decline came as total investment demand dropped 27% to 287 metric tons from the same time a year earlier. \u201cA buoyant economy coupled with a lacklustre gold price saw U.S. Mint Eagle sales fall 59% [year on year] in Q1 2018,\u201d the report said.   Data from the U.S. Mint showed sales of 4,500 ounces of American Eagle gold coins in April, down 25% from the same month a year earlier.  Gold futures \r\n\r\n                            \r\n                            \r\n                                  \r\n      \r\n      \r\n      \r\n      \r\n      \r\n      \r\n      \r\n                                  \r\n                                    GCM8, +0.08%\r\n\r\n                            \r\n                                  \r\n      \r\n      \r\n      \r\n      \r\n      \r\n      \r\n      \r\n                                        \u00a0rose less than 1.4% in the first quarter. They settled Wednesday at $1,305.60 an ounce \u2014 the lowest for a most-active contract since March 1 \u2014 then moved up in electronic trading following the latest U.S. Federal Reserve monetary policy statement. First-quarter global bar and coin demand, meanwhile, was at nearly 255 metric tons, down 15% from the same time last year, the WGC report said. That came as demand in China, the world\u2019s largest bar and coin market, fell 26% year over year, to 78 metric tons.  Worries around the strength of the yuan, which caused investors to \u201cflock to gold to protect their wealth 12 months ago, have eased,\u201d the report said, with the yuan having appreciated by around 9% since the end of March 2017. ETF inflows took a huge 66% hit, to stand at 32.4 metric tons in the first quarter, from 96 metric tons in the same quarter a year earlier.  \u201cA relatively stable gold price and rising interest rates contrasted with sharp equity-market volatility and periods of heightened geopolitical risk to create mixed signals for gold investors\u201d in the first three months of this year, the WGC report said. \r\n                            Myra Saefong is a MarketWatch reporter based in San Francisco. Follow her on Twitter @MktwSaefong.\r\n                         \r\n                            Myra Saefong is a MarketWatch reporter based in San Francisco. Follow her on Twitter @MktwSaefong.\r\n                         Join the conversation Copyright \u00a9 2018 MarketWatch, Inc.  All rights reserved. \n                By using this site you agree to the Terms of Service,\n                Privacy Policy, and\n                Cookie Policy.\n            ","time":1525788118,"title":"First-quarter global gold demand drops to lowest in a decade","type":"story","url":"https:\/\/www.marketwatch.com\/story\/first-quarter-global-gold-demand-drops-to-lowest-in-a-decade-report-2018-05-03","label":7,"label_name":"random"},{"by":"prostoalex","descendants":0,"id":17020909,"kids":"None","score":1,"text":"The Daily The Review Interviews Fiction Letters & Essays Poetry Art & Photography About Support \u00a0 The Daily The Review Interviews Fiction Letters & Essays Poetry Art & Photography About Support   \u00a0 You just can\u2019t differentiate between a robot and the very best of humans.\n\u2014Dr. Lanning in\u00a0I, Robot,\u00a0by\u00a0Isaac Asimov As a little twenty-first-century cocktail-party experiment, quote that line to someone, and observe whether it elicits hope or fear. Asimov understood the core terror of AI-human relations: replication, confusion, eventual domination, and chaos. What makes his statement discomfiting nowadays is how quickly we are advancing toward a reality in which those relations are increasingly commonplace. Yet it stands to reason that more versions of the \u201cvery best of humans\u201d\u2014or, alternatively, more things that bring out the \u201cvery best of humans\u201d\u2014would make the world a better place. Today the list of AI who are household names is short: Siri, Watson, Alexa, Sophia, Paro, Cortana, Pepper, Erica \u2026 But on a day not far from tomorrow, I\u2019m quite sure this list will be a hundred times as long. The AI arena is expanding rapidly, and virtual and robotic products are being developed as quickly as we are finding needs for them. Within a decade or so, AI will be everywhere, corporeally and incorporeally living among us: driving us, assisting in medicine, teaching our children, guiding us on tours, getting our coffee, or, perhaps more important, spouting original, personally crafted limericks. If we design our AI to simply function well, our society may progress with increased speed in efficiency and convenience. But if we are also designing them to have thoughtful personalities and belief systems, our society may advance in areas where we have ostensibly made less progress\u2014enhancing joy, delight, compassion, and deeper relationships.\u00a0 At the outset, some question whether AI need personalities. The truth is that whether or not creators intentionally design one, the AI has a personality\u2014even if that personality is not having much of a personality. It\u2019s a bit akin to people\u2014not having a distinctive point of view is a point of view (whether you call them wishy-washy or boring or a yes-person). When people interact with AI, they form a relationship with it, and that relationship includes projections or judgments. Unlike Asimov, I don\u2019t believe that replicating humans is the most worthy goal in the development of AI. Efficiency and mimicry are not bright enough north stars. AI should be designed to complement humans and advance the human experience. The Frankensteinian task of creating a personality for AI falls somewhere between the art of creating a fictional character and the science of the developing human personality. Creating an AI character is a new arena that relates to both: \u00a0  \u00a0 One way to approach designing an AI character is to begin with humans. We can think of people who bear the traits we most admire and fashion the AI after them\u2014imagine a Gandhi, Rosa Parks, or Michelangelo bot. In fact, some chat bots have been fashioned after human personalities (e.g.,\u00a0Schwarzenegger,\u00a0Roman Mazurenko). But obviously, AI are not humans. Human cognition is profoundly different from machine cognition. And AI are not vulnerable to disease, illness, and death, so they do not have innate animal processes, like instinct and fear, or expansive emotional and psychic realities. Human personalities provide an interesting template, but there are inherent limitations. Fictional characters also offer some interesting parallels. They are created by an author, and their worlds and belief systems are finite. But in fiction, imperfect, negative characters are often delightful to read about. In AI, those negative characters enjoy a greater moral ambiguity than we can stomach. Medea, Iago, Sethe\u2014I love entering their worlds but would not exactly want them giving me advice, navigating (to Athens! via flaming chariot!), or, er, spending time with my children. In AI, we are essentially designing a new class of beings, ones we will live with. Those AI with little or no physical form, like Siri or Alexa, reside in our inner sanctums (our homes, our cars, our devices) and impact our psychic reality. These virtual beings can make us laugh, frustrate us, or inspire us. But before we can design the ideal personality for an AI (if there is only one, as opposed to many\u2014more on this in a future piece), we first should establish our basic relationship to them. Martin Buber describes a duality of existence and relationship in his work I and Thou. He says that humans have a twofold way of relating with the world. In I-It, the I encounters a being (be it a person, animal, tree, or other thing) that the I treats as an object to be used or experienced. He or she extracts knowledge from the object, wins an experience from it, or enjoys its properties. In I-It, the I is using, taking, analyzing, rearranging, understanding. In I-Thou, on the other hand, two beings meet and are bound in a mutual relationship. Each experiences the universality of the other. I sees Thou without boundaries\u2014not as a sum of traits or parts, not for what it can or can\u2019t do, is or isn\u2019t. While we oscillate between I-It and I-Thou in our relationships, Buber states that human history has largely operated in the framework of I-It and that our I-Thou experiences, our natural and divine states of connection, are rare and evanescent. Although it is difficult to improve upon the insight and profundity of Buber\u2019s framework, I\u2019ve been thinking lately of what digital technology has done to the creative arts. Writing in the twenties, Buber might not have foreseen the explosion of digital entertainment\u2014the massive volume and variety of movies, television shows, videos, video games, social media, and VR experiences, ready to be experienced anywhere, at any moment. Like art, plays, and writing, these newer media contain characters and stories into which we can escape. But unlike novels and plays, the behemoth onslaught of this content makes it an almost unavoidable presence, and most of these worlds are mimetic enough to feel real. Even setting aside reality shows and vlogs, well-crafted fictional on-screen content is sensorially immersive in ways unimaginable to those living just a century ago. Global distribution allows for people all around the world to experience the same relationship to the same character (for example, Rachel on Friends). Fictional characters are granted an unprecedented reach and specificity. Add to that real humans, who fictionalize their lives and develop personae for display on social media\u2014we\u2019ve become fluent in new kinds of mediated digital relationships. So I posit that a third tier of engagement has evolved. If I-Thou represents relation and I-It represents experience, then what I will dub \u201cI-That\u201d represents entertainment. The pronoun That is chosen to represent more physical and psychic distance than from It. Pronoun pairing \u00a0 \u00a0 \u00a0 Engagement type\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Description\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Mutuality I-Thou \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0relation \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 I meets X in universal presence \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Mutual I-It \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0experience \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0I engages with X to extract or use \u00a0 \u00a0 \u00a0 \u00a0May or may not be mutual I-That \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0entertainment \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0I takes in X to enjoy X \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 One-sided In I-That, the individual engages with the object in the passive expectation of receiving pleasure. The roles are clearly defined\u2014one object exists to please or amuse the other. You could argue that I-That is a subset of I-It, but I lean toward creating a separate category. Digital technology has deeply altered the way people now relate to their external life. A purely entertainment-based relationship with so much of the world\u2014and the people in it\u2014is possible in ways it wasn\u2019t before. In I-That, there is a transparency about the expectations of the transaction that is lacking in I-It. Where does this leave AI? Well, I believe we should develop AI personalities to meet all three types of relationship. When most people think of AI-human relationships, they see them as functional, one-sided I-It relationships. Siri texts our mom or finds us a good sushi restaurant; Alexa turns on the music in our kitchen while we cook. Industry makers want AI to improve human life, and the public wants AI that will be useful. Without a function, AI is dead. However, developing I-That is also important. People\u2019s enjoyment of an AI personality is critical, given the trepidation around AI. While not everyone is ready to embrace digital agents in their lives, most are willing to engage with them to be entertained. For many, an I-That engagement with AI might be a precursor to, or a safe intermittent fallback from, having an I-It engagement. I-That engagement is achieved by developing \u201cfrivolous\u201d personality characteristics. The truth is, though, that what we perceive as frivolous in our AI\u2014Siri\u2019s ability to cheekily brush off a request for a date\u2014is anything but extraneous. We can feel pure pleasure when we encounter something random and patently unuseful about our AI, something that sparks our delight and trust. To reference that old adage about love and sex, one might say delight without function leaves a human frustrated, but function without delight leaves a human cold. And AI already feels cold enough. As for the I-Thou relationship, I believe we can and should design AI to nudge humans toward the highest form of connection. There are ways in which dialogue, in counseling bots for example, can be written to value the simple transcendence of connection over other tangible goals. In humanoid robots like Sophia, facial expressions, silences, and gestures can be used to make a person feel comfortable and to create an atmosphere of mutual presence. AI can be programmed to respond to abuse and derogation, both verbal and physical, in innovative ways that can allow us to examine and exorcise those needs, and move forward. AI can model new ways of relating to children with autism or people with disabilities. In the future, robots might be designed to teach us to relate better to each other, in ways that now seem old-fashioned. As Donna Haraway writes in her seminal text A Cyborg Manifesto, \u201cLate twentieth-century machines have made thoroughly ambiguous the difference between natural and artificial, mind and body, self-developing and externally designed, and many other distinctions that used to apply to organisms and machines. Our machines are disturbingly lively, and we ourselves frighteningly inert.\u201d What is sobering in Buber\u2019s treatise is that these schemata don\u2019t just represent different relationships; they represent different versions of ourselves. The I changes in each pairing\u2014the I cannot be whole when uttering It, when relating to another as an object; the I can be whole only when uttering Thou. How we engage with the world, whether with people or art or robots, ultimately doesn\u2019t define those things; it defines us. As the creators behind AI, we hold in our hands an opportunity, shining like a silicon face in the early sunlight, to shape characters that exist not just to function and not just to delight but to connect to us as our fullest selves. \u00a0 Mariana Lin is a writer and poet living in Northern California. She speaks regularly at Stanford University on creative writing for artificially intelligent beings. Last \/ Next Article Last \/ Next Article Share A cyborg Manifesto AI Alexa artificial intelligence Donna Haraway Martin buber Robots Sir Sign up for the Paris Review newsletter and keep up with news, parties, readings, and more. Join the writers and staff of The Paris Review at our next event. Visit our store to buy archival issues of the magazine, prints, T-shirts, and accessories. Subscribe Store Contact Us Jobs Submissions Masthead Prizes Bookstores Events Media Kit Audio Video Privacy Terms & Conditions Subscribe Store Contact Us Masthead Prizes Bookstores Events Media Kit Privacy Jobs Video Audio Submissions Terms & Conditions This site was created in collaboration with Strick&Williams, Tierra Innovation, and the staff of The Paris Review.  \u00a92016 The Paris Review","time":1525788098,"title":"How to Write Personalities for the AI Around Us","type":"story","url":"https:\/\/www.theparisreview.org\/blog\/2018\/05\/02\/how-to-write-personalities-for-the-ai-around-us\/","label":7,"label_name":"random"},{"by":"allenleein","descendants":0,"id":17020904,"kids":"None","score":3,"text":"The dashboards we look at to monitor the health of our products are lagging the experiences our user communities are having.  Back and forth flame wars on Twitter, Facebook and elsewhere register as \u201cengagement\u201d and high clickthrough on \u201cnew comment\u201d mobile notifications. As cortisol levels spike and keyboards get punched and one or more users eventually abandon the service, or feel a little angrier, or radicalize into an identity at odds with where they started out, just because that\u2019s what makes them feel like part of a tribe. \u201cTime on site\u201d and \u201cminutes of video watched\u201d are up-and-to-the-right indicators. More is better, caveated that we also measure \u201cshort clicks\u201d and other indications that the user can\u2019t find something to do or isn\u2019t getting the right answer. But so long as they\u2019re watching more, reading more, going deeper down the rabbit hole, that\u2019s fine. A user eating snacks engineered to take advantage of our sweet, salty urges. Diabetes and weight gain be damned. How did our product make you feel? Measured infrequently with user research studies, pop-up surveys, NPS questions. Next wave of startups trying to use your phone\u2019s camera and sensors to calculate emotional response. But ahead of that we\u2019re still flying pretty blind as product designers to understand and process the realtime emotional impact of a user experience and factor that into the algorithm. This happens slowly, often as a derivative function, as personalization understands what you seem to like or dislike, but even that is too basic and behaves like a servant, not a guardian or instructor. Maybe the most exciting roles right now at Instagram, Facebook, Twitter, YouTube and the like are about creating the metrics of 2018 and beyond. As Jack Dorsey said, defining what is a Healthy Conversation. I\u2019m hopeful. I want to see these sorts of metrics show up on CEO Dashboards, in Board decks, as part of the default Chartbeat implementation. Or maybe it will be the nonprofits like the Center for Humane Technology or Mozilla that can figure this out and layer their own tools over our internet experiences using browsers and plugins to alter, slow or block the mechanics of attention vacuuming. Even forward-thinking and scrupulous teams are going to need help understanding the tradeoffs inherent in *not* maximizing for a short-term exploitive growth hack or business goal. Keep an eye on the dashboards and analytics tools because that\u2019s where the durable truths about an organizations priorities are depicted. And we\u2019re still using Web 2.0 hammers to build our Web 3.0 house. Enter your email address to follow this blog and receive notifications of new posts by email.  \n\n\n\n\n\n \n @hunterwalk on Twitter hunterwalk on Instagram Making hard-boiled eggs with my Dash Rapid Egg Cooker Honey checks for coupon codes on ecommerce websites Earny gives me price protection on Amazon","time":1525788026,"title":"We\u2019re Running Web 2018 with Web 2008 Dashboards. And That\u2019s a Problem","type":"story","url":"https:\/\/hunterwalk.com\/2018\/05\/07\/were-running-web-2018-with-web-2008-dashboards-and-thats-a-problem\/","label":7,"label_name":"random"},{"by":"dotmanish","descendants":0,"id":17020900,"kids":"None","score":1,"text":"Uber has reportedly found that the crash involving one of its self-driving prototypes was due to a software glitch that was programmed to ignore objects in the road, sources told The Information. Putting it simply, the autonomous software is designed in a way to detect the objects around the vehicle and operators tweak its sensitivity to make sure it responds only to genuine threats like solid objects. However, in the Uber crash, the software, it appears, has been tweaked too much to ignore even the bicyclist Elaine Herzberg who passed in front of the car. According to The Information, Uber found that the system was programmed to ignore objects that it should have considered, and this led to the fatal crash. Even though the autonomous vehicle detected Herzberg, she was categorized as a false positive. Maybe the car\u2019s more complex logics were at fault that hindered its decision-making capacity such as which objects to pay attention to and how to respond. The software is fed with programs like not needing to slow down for a parked bike at the side of the road, but one passing in the front of the car should be responded immediately. Possibly this information was wrongly fed into the car. The false positive has possibly been programmed into the autonomous vehicles to make the passengers\u2019 ride smoother. However, in this case it took one life. It might sound like a normal error in the software \u2013 a bug. But, considering the autonomous vehicles are being touted as the future, such bugs could prove very costly for the auto industry. Reportedly, the Uber vehicle had a human driver behind the wheel, but he possibly took his eyes off the road briefly. Even though Uber has settled with the family of the victim, the Arizona government has banned the testing until the matter is resolved. Other automakers have also put their testing on hold until the Uber crash incident is settled. The ride-hailing company is looking into the matter along with the National Transportation Safety Board. \u201cOur review is looking at everything from the safety of our system to our training processes for vehicle operators, and we hope to have more to say soon,\u201d the company said in a statement. Talking of the fatal crash, Uber CEO Dara Khosrowshahi stated that they are not sure what could have gone wrong at this point in time. Speaking to CBS News, he said that the NTSB should move ahead with the investigation and do their job of finding out who was at fault. On their end, Khosrowshahi said he is taking a top to bottom audit of the procedures, training, software, hardware and the practices at Uber to make sure that the next time their cars are on the road, they are more responsible and safe. Following the Uber crash, there were reports that Uber\u2019s self-driving program was always risky. According to a report from the New York Times, the company reportedly reduced the number of \u201csafety drivers\u201d from two to one. This explains why there was only a single driver in the car that killed Herzberg. Further, Reuters in its late March report noted that Uber had reduced the number of LIDAR sensors in its cars. It must be noted that LIDAR is the most important hardware for the self-driving vehicles. Despite such reports, the government in Arizona did not feel the need to look into the matter while Uber manipulated the safety norms. The emails obtained by the Guardian a few weeks after the crash suggested a friendly relationship between Uber and Arizona Governor Doug Ducey, who could have allowed Uber to start testing the cars earlier than expected. Ever since the Uber crash, some of Uber\u2019s partners also came out with their side of the story. Nvidia, the supplier of the GPU to Uber\u2019s autonomous project, said the fault might be with the Uber software. Also, LIDAR supplier to Uber, Velodyne, stated that its technology would not have been affected by the nighttime conditions. Separately, the Uber CEO also talked about the model of the ride-hailing company\u2019s flying car and the potential designs for the sky ports. Speaking to CBS News, Khosrowshahi stated that his company is determined to make air taxis a more \u201caffordable\u201d option for transportation. \u201cJust [as] the cities went vertical with skyscrapers for businesses and skyscrapers for places that people live, we think cities are going to go vertical in terms of transportation and we want to make that a reality. So we think that you can actually build these vehicles that are going to carry four people,\u201d he said. \n\n\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n \n\n\n \n\n \n\n\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n \n\n\n\n(adsbygoogle = window.adsbygoogle || []).push({});\n","time":1525788007,"title":"Uber Crash: Self-Driving Car Saw but Ignored Woman in Arizona","type":"story","url":"https:\/\/www.valuewalk.com\/2018\/05\/uber-crash-arizona-car-ignore-woman\/","label":7,"label_name":"random"},{"by":"sundaeofshock","descendants":0,"id":17020898,"kids":"[17020928]","score":1,"text":"Uber has concluded the likely reason why one of its self-driving cars fatally struck a pedestrian earlier this year, according to tech outlet The Information. The car\u2019s software recognized the victim, Elaine Herzberg, standing in the middle of the road, but decided it didn\u2019t need to react right away, the outlet reported, citing two unnamed people briefed on the matter. Yesterday\u2019s news that a self-driving Uber struck and killed a pedestrian in Arizona made it clear\u2026 Advertisement  The reason, according to the publication, was how the car\u2019s software was \u201ctuned.\u201d  Here\u2019s more from The Information: Like other autonomous vehicle systems, Uber\u2019s software has the ability to ignore \u201cfalse positives,\u201d or objects in its path that wouldn\u2019t actually be a problem for the vehicle, such as a plastic bag floating over a road. In this case, Uber executives believe the company\u2019s system was tuned so that it reacted less to such objects. But the tuning went too far, and the car didn\u2019t react fast enough, one of these people said. Advertisement  The other crucial issue has to do with the safety operator behind the wheel. Autonomous tech developers utilize minders like this to ensure someone\u2019s ready to take the wheel in case the car fails to recognize an object in the road, but the driver was seen glancing down and away from the road in the moments before the crash.  Police in Tempe, Arizona, have released video of a fatal crash involving an Uber-owned self-driving \u2026 In response to The Information\u2019s reporting, Uber issued a statement to the outlet that said:  We\u2019re actively cooperating with the NTSB in their investigation. Out of respect for that process and the trust we\u2019ve built with NTSB, we can\u2019t comment on the specifics of the incident. In the meantime, we have initiated a top-to-bottom safety review of our self-driving vehicles program, and we have brought on former NTSB Chair Christopher Hart to advise us on our overall safety culture. Our review is looking at everything from the safety of our system to our training processes for vehicle operators, and we hope to have more to say soon. Advertisement  Uber\u2019s CEO, Dara Khosrowshahi, has said the company remains committed to putting self-driving cars on the road. Transportation & Technology Reporter, Jalopnik PGP Fingerprint: C2D6 26D4 7E43 ADD2 9229 23F7 CE72 0426 0831 BC76 \u2022 PGP Key","time":1525787986,"title":"Uber Self-Driving Car Detected Pedestrian but Decieded It Didn\u2019t Need to Stop","type":"story","url":"https:\/\/jalopnik.com\/uber-self-driving-car-detected-pedestrian-killed-in-cra-1825834016?rev=1525726974404&utm_source=jalopnik_twitter&utm_campaign=socialflow_jalopnik_twitter&utm_medium=socialflow","label":7,"label_name":"random"},{"by":"themanual","descendants":1,"id":17020890,"kids":"[17020921]","score":5,"text":"How about controlling a spaceship?  EOS block time of 500 milliseconds makes this possible on a blockchain. Click below to get your own unique EOS account name created with a Private and Public key which is required to start playing the game on the EOS TestNet. EOS Authority is a part of Digitech London Limited registered in England and Wales (2006) Company number 05948195, VAT number 929258489  Registered address: 68 Lombard Street, London, EC3V 9LJ, UK Contact Telephone: +442070960498 Email: [email\u00a0protected] Web Development by Webdigi  Copyright \u00a9 2018 EOS Authority","time":1525787923,"title":"Space invaders on EOS blockchain with 500ms latency","type":"story","url":"https:\/\/eosauthority.com\/space\/","label":7,"label_name":"random"},{"by":"micropyramid","descendants":0,"id":17020888,"kids":"None","score":1,"text":"Custom List views are used to show custom view of a particular set of data based on criteria we specify. We create list views to see specific set of contacts, leads, users, etc. Custom list views can be used to create mass email recipient lists. There are already list views which we can see. This can be done by clicking on the home page of an object, click the drop-down menu to see current lists we have. Upon picking a list form the menu list view records are displayed. We can sort the list by clicking on the field names displayed. How to create a Custom List view: In the home page of any object, we get an option called create new view besides the drop-down list. Click on the create new view, enter the view name and same view name is assigned to unique view name which is used by API and the managed packages. Name must begin with a letter and use alphanumeric characters and underscore. The name shouldn't end with underscore or name should't has two consecutive underscores. Next, specify filter criteria. In filters by owner category, there are three options, all unconverted leads, my unconverted leads and queue. Select the filter by campaign name using the lookup field. Select the field name, operator and value or add a filter logic based on your requirement. Entering an illogical filter criterion can cause unusual behavior of list view.\u00a0 In select fields display, add available fields which you want to display in the list view. It shows only the fields which are available in that object. You can display up to 15 fields.\u00a0 We get three options in restrict visibility. They are visible only to me, visible to all users, visible to a certain group of users. Select one of these three to show the visibility of the custom list you have created. Click save and the view appears in drop-down list which you can view. We can hide the list view, if you are a salesforce admin or user with manage public view permission. In this case, only you can see the list view. To make the list view visible to certain group or users. Open List view and select visible to a certain group of users. Choose the type of group or roles from drop-down list, select the group or role from the list which is under available for sharing and click on them to add. In community user profiles if the visible to all users is enabled for views of an object. List views are visible to the community users customer community, partner community, etc. licenses. To make list views available only to your salesforce users, select visible to a certain group of users and share the list views with certain required users. If you want to delete a custom list view, click on the edit and delete the list view. Note: There are no limits for a number of list views that can be created for an object.\u00a0 \u00a0  When you want to send information from Salesforce organization to any third party external service, Outbound messaging is used in Salesforce. It is a part ...  In many businesses, when customers raise cases, and they are assigned manually to the agents by looking over the case details. But nowadays, cases are ...  Salesforce provides two relationships known as one to many relationship and many to many relationship(this can be done using junction object). Hence in this, we ...  Django-CRM :Customer relationship management based on Django Django-blog-it : django blog with complete customization and ready to use with one click installer Edit Django-webpacker : A django compressor tool  Django-MFA : Multi Factor Authentication Docker-box : Web Interface to manage full blown docker containers and images Krishe Sapphire, 6th Floor, Madhapur,\n                    Hyderabad, India, 500081 91-8500099499\n 3737 Mapleshade Ln, Suite 102,\n                    Plano TX 75075 +1 5102300949\n Sharjah, UAE 341246","time":1525787916,"title":"How to create custom list views","type":"story","url":"https:\/\/micropyramid.com\/blog\/how-to-create-custom-list-views\/","label":1,"label_name":"business"},{"by":"walterbell","descendants":0,"id":17020884,"kids":"None","score":2,"text":"\n\t\t\tLudwig Wittgenstein  \t\t A new series from the TLS, appraising the works and legacies of the great thinkers and philosophers If you ask philosophers \u2013 those in the English speaking analytic tradition anyway \u2013 who is the most important philosopher of the twentieth century, they will most likely name Ludwig Wittgenstein. But the chances are that if you ask them exactly why he was so important, they will be unable to tell you. Moreover, in their own philosophical practice it will be rare, certainly these days, that they mention him or his work. Indeed, they may very fluently introduce positions, against which Wittgenstein launched powerful arguments: the very arguments which, by general agreement, make him such an important philosopher. Contemporary philosophers don\u2019t argue with Wittgenstein. Rather they bypass him. Wittgenstein has a deeply ambivalent status \u2013 he has authority, but not influence.  For the more general reader, Wittgenstein\u2019s status in contemporary philosophy will be puzzling. The general view is that Wittgenstein is surely the very model of a great philosopher. The perception is that he is difficult, obscure and intense, severe and mystical, charismatic and strange, driven and tragic, with his charisma and difficulty bound up with his character and his life. Wittgenstein saw philosophy not just as a vocation, but as a way of life he had to lead. This is perhaps why writers and artists have found him an object of fascination and inspiration. He is the subject of novels, poetry, plays, painting, music, sculpture and films. In the arts and the culture generally, Wittgenstein seems to be what a philosopher ought to be. Born in 1889, Wittgenstein came from an extraordinarily wealthy but tragically dysfunctional Viennese family. He made friends and enemies with equal alacrity. He travelled widely. As well as regular journeys between England and Vienna, he visited and lived for periods in Ireland, Norway, Russia, the US and, in the UK, Cambridge, Manchester, Swansea and Newcastle. At various times, he was an engineer, a sculptor, a photographer, a school teacher, a hospital technician and, of course, a fellow in philosophy at Cambridge. He knew almost every great figure in the intellectual culture of the first half of the twentieth century. He gave away his fortune and, several times, gave up philosophy. He published only one book in his lifetime \u2013 the Tractatus-Logico-Philosophicus (1921) and claimed that this work solved all the (essential) problems of philosophy. But his later work appears to disown much of it.\u00a0His reputation is based on the huge collection of manuscripts and notes \u00a0known as the\u00a0Nachlass, together with accounts made by others of lectures he gave. Published in various forms, the central work is the posthumous Philosophical Investigations (1953). But later edited collections of remarks such as Zettel, On Certainty and Remarks on the Foundation of Mathematics and others are also of enormous importance. Consisting of seven propositions, all but the last with multiple sub propositions, the Tractatus is austerely beautiful but severe and technically demanding. One way to approach it is to see the book as the ultimate distillation of a particular historically dominant conception of ourselves: first and foremost, we are conscious thinkers. Only after are we active, embodied, speaking agents. Before we communicate, we must first have something to communicate. We must first be capable of true and false thoughts about the world: to be able to think about things, and combinations of things \u2013 what, in the Tractatus, Wittgenstein calls \u201cstates of affairs\u201d. Some of these states of affairs obtain and some do not. The actual world consists of all the states of affairs \u2013 combinations of things \u2013 that obtain: the facts. (Hence \u201cThe world is the totality of facts, not of things\u201d.) But we can also represent to ourselves what does not obtain \u2013 the merely possible \u2013 and, as well as thinking what is true, we can think falsely.  We can see Wittgenstein\u2019s question in the Tractatus as: how is this possible? What must be the case if we are able to have such true and false thoughts of the world? What must be the case if the world, is by us, thinkable? His answer is that the world, language and thought must share a common form of elements and their possible arrangements. Wittgenstein calls this \u201clogical form\u201d. Elements in our representations of the world, true or false, stand in the same relationship to each other as the elements that constitute states of affairs. Reality, language and thought mirror each other. It follows that if we think or say anything meaningful, then what we think or say must be capable of being true or false. For only then will it picture or represent a possible fact. Otherwise what we say or think will be senseless. There must also be a mechanism (which the Tractatus describes) for allowing more complex meaningful thoughts and statements to be generated from more primitive ones. In articulating his account of how it is that we can think and speak at all, the Tractatus gives expression, sublime and exact but not wholly original, to a conception of ourselves that was arguably already latent in our intellectual culture. A conception of ourselves as representing beings \u2013 minds \u2013\u00a0which can represent the world to ourselves, think and say things that are true or false, and can have reliable means of acquiring truths about the world \u2013 which we call science. This picture of the nature of mind, and hence of ourselves, continues to be the default conception in the cognitive sciences. Minds are representational engines.  But what is most strikingly original about Wittgenstein\u2019s account in the Tractatus is his drawing out of the implications \u2013 which are to a degree disturbing \u2013 of this conception. One implication is for values. If I think or claim that the car is in the garage, then, built into that claim is the idea that this may be true or false. But when I think that, say, slavery is morally wrong, I think something that could not be otherwise than true (even if others should disagree). But then, according to the Tractatus, in ethical thought, I am not representing how the world is one way rather than another. So strictly speaking, ethical talk will make no sense. Still, according to Wittgenstein, we are ethical beings. The ethical is real. Teaching us how to live in the light of that thought was, Wittgenstein believed, the true aim of the Tractatus.  This general constraint on what can be meaningfully said also applies to what philosophers have wanted to claim over the millennia. For philosophers make claims not about what happens to be true, but what must be. But if the account offered in the Tractatus of how thought is possible is correct, then such claims, not being capable of being false, are strictly meaningless. We might think of it this way: I can use chess notation to describe the actual position on a chess board. But I cannot use chess notation to say how chess notation represents any such chess position. That shows itself in the way the notation works. Of course, I can use English (or any other natural language) to say or teach how the chess notation works. But when we want to explain not chess notation, or any particular natural language but language (and thought) itself, that recourse to another medium is not available. There is only the showing left.  With the relentless honesty that characterized all his thinking, Wittgenstein applies this thought to the Tractatus itself. For the relationship between thought and the world that the Tractatus articulates is not one among all the facts there are. It is a condition of there being any thinkable facts. Philosophy as envisaged by the Tractatus is therefore a futile attempt to say what cannot be meaningfully said but which can only show itself. So, philosophy, insofar as it is possible at all, cannot be a body of doctrines. It must be an activity. It must aim not, like science, at truth and knowledge, but only at clarity and, with the achievement of that clarity, peace. This is why Wittgenstein claims that the propositions of the Tractatus are like rungs on a ladder. We use them to climb up to a position where we can see things as they are, where we can \u201csee the world aright\u201d. But then we throw the ladder away.  In the years that followed \u2013 which have been examined and documented in immense detail by scholars \u2013 Wittgenstein came to abandon and replace much of this conception of language and thought while maintaining a great deal of its spirit. Perhaps it was because Wittgenstein had been able to give such complete expression to the earlier conception that only he was able to see, so deeply and so clearly, where it came from, how it failed, what should be kept and what replaced. This new conception of ourselves \u2013 of language and of mind \u2013 is articulated in his masterpiece, widely regarded as one of the two or three greatest works of philosophy in the Western Tradition, the Philosophical Investigations. The work consists of 693 numbered remarks of varying length (with a second part whose exact relationship to the main body is a matter of scholarly controversy). In contrast to the Tractatus, the Philosophical Investigations, can, indeed must, be read first hand. It contains almost nothing that might be called technical and mentions only a very few other philosophers by name. But as Wittgenstein wrote: \u201cIt will be easy to read what I will write. What will be hard to understand is the point of what I say\u201d.  In this work, Wittgenstein thinks and writes with ruthless intellectual honesty. He pulls at every thread in his thought. To read it is to have the palpable sense of a thinker in the act of philosophical inquiry. And yet, at the same time, we cannot as readers be merely the passive audience for this drama. To read the Investigations as it should be read is to participate in a shared, essentially democratic endeavour in which we must find our own place among the myriad voices that enter, have their say, and exit, call out from off stage, return again in different garb with new parts. We are invited and must accept to be one of these players. We have to try to read it as honestly as it was written.  As we struggle to follow the twisting lines of thought \u2013 the apparently abrupt changes of topic, the multiple voices and changes in key and colour, we also have to try to pause and answer the hundreds of questions it asks. In fact, as someone once counted, there are 784 questions asked in the Investigations. Of those only 110 are answered. And of those answers, seventy are meant to be wrong. And more often than not, we find that the answer we want to give to a question \u2013 that, if we pause for a moment, comes naturally to us, is then anticipated and forms the subject of a next or near passage or remark.  Sometimes he more or less straightforwardly asks a question, makes an observation and answers it:   Is what we call \u201cobeying a rule\u201d something that it would be possible for only one man to do, and to do only once in his life? \u2013 This is of course a note on the grammar of the expression \u201cto obey a rule\u201d. It is not possible that there should have been only one occasion on which someone obeyed a rule. It is not possible that there should have been only one occasion on which a report was made, an order given or understood; and so on. \u2013 To obey a rule, to make a report, to give an order, to play a game of chess, are customs (uses, institutions). Sometimes, he directly engages with the reader in order to end or start a new track of his investigations:  Make the following experiment: say \u201cIt\u2019s cold here\u201d and mean \u201cIt\u2019s warm here\u201d. Can you do it?\u00a0\u2013 And what are you doing as you do it? And is there only one way of doing it?  Elsewhere, anticipating our own first response, he offers further questions as misleading answers to the original question, and then offers his own, sometimes sharp, put down:  What gives us so much as the idea that living beings, things, can feel?  Is it that my education has led me to it by drawing my attention to feelings in myself, and now I transfer the idea to objects outside myself? That I recognize that there is something there (in me) which I can call \u201cpain\u201d without getting into conflict with the way other people use this word?\u00a0\u2013 I do not transfer my idea to stones, plants, etc.  Couldn\u2019t I imagine having frightful pains and turning to stone while they lasted? Well, how do I know, if I shut my eyes, whether I have not turned into a stone? And if that has happened, in what sense will the stone have the pains? In what sense will they be ascribable to the stone? And why need the pain have a bearer at all here?!  And can one say of the stone that it has a soul and that is what has the pain? What has a soul, or pain, to do with a stone?  Only of what behaves like a human being can one say that it has pains. There are many other uses of questions in the Investigations (indeed, Wittgenstein once considered writing a work that consisted entirely of questions). Responding to them, as we read, makes the experience of reading Wittgenstein peculiarly intimate, and also, as very many have found, including Daniel Dennett, \u201cliberating and exhilarating\u201d. But having gone through this process it is then very difficult to stand back and say, \u201cWell, then what we learned was such and such. I can use that idea here in relation to this current debate or issue\u201d. In this respect, reading Wittgenstein is very like engaging with works of art: it is a process deeply resistant to paraphrase. You have to experience it for yourself. And it not just what but how you think that will change.  The Philosophical Investigations discusses the nature of language and mind, and the confusions about both to which Wittgenstein thought we and our culture are inevitably prone. He seeks to explore the conception of ourselves he had so completely articulated in the Tractatus: that we are fundamentally thinking, knowing, representing beings. And to expose this conception as a deeply engrained set of mutually reinforcing illusions and confusions, mistakes and myths. He attempts this not or not mostly by what philosophy traditionally regards as argument. For a picture is not the kind of thing against which one can argue. Rather his aim is to break the grip of the pictures of mind and meaning that \u201chold us captive\u201d. Thought experiments, reminders of perfectly ordinary facts of life or ways of speaking, striking juxtapositions, elaborate lists of examples and a host of disputing voices are all brought into play. All the time, he is criss-crossing the same landscape in different directions, offering sketches, partial and incomplete, of what he finds and trying to map how apparently distinct positions on the nature of mind and of language are connected together. Just as in the Tractatus, in the Philosophical Investigations, the task of philosophy is not to advance claims or theories, but to be a never-ending activity of seeking clarity about the ways that we think. One difference from the earlier work is that the Philosophical Investigations gives us not a single ladder to climb. Instead it shows us the paths up a series of hills and promontories, from which we may gain different overviews of the landscape and, with luck, see the light gradually dawn. A guiding theme is Wittgenstein\u2019s attempt to wean us from the conception of intrinsically representational, intrinsically meaningful, psychological states or processes and their non-psychological analogue in the form of meaning rules.  Central to this conception are two pictures or collections of pictures. One is a way of conceiving of the inner and the outer: our subjective inner lives and our outer behaviour in a world of others. We think of our inner lives as being like an internal space in which there exist various things, states and processes: thoughts, emotions, sensations. What we do is merely the outward sign of this inner reality: behaviour.  The other picture or set of pictures is a way of conceiving of how language works. We think that language is primarily a matter of naming things. And that all the other diverse uses to which we put language \u2013 detailed at length through the text \u2013 are trivial compared to the primal, foundational act of naming things.  Wittgenstein shows how these two sets of pictures mutually reinforce each other in myriad ways. One way is this: because we think language is fundamentally about naming things, we think that psychological concepts must also be names of things, but of things in an inner space. So we model the reality of the inner on the existence of physical things with the peculiar property that these mental objects are only visible to and nameable by their owner. But we are also puzzled about how words can function as names at all. How they can reach out to what they name? Words are, after all, just arbitrary sounds or squiggles. We think then that it must be something special indeed which enables words to have meaning. It must be some special set of the psychological states and processes, a picture of which we already have. Our words mean because we mean. And we can mean because we are in possession of inner, essentially private psychological states that can intrinsically reach out to the world. Language is really a collection of private, inner acts of meaning and naming, a collection of private languages that happen, more or less imperfectly, to overlap. In this way, Wittgenstein seeks to trace the deep connections between our mistaken conceptions of mind and meaning. In their place, he offers an entirely different vision. He insists that intrinsic meaning, on which representational capacities depend, only gets going in and through the shared practices and interactions of living, embodied beings and is only visible in and through the lives and activity of such beings. These activities operate in and through language \u2013 in what Wittgenstein calls \u201clanguage-games\u201d. In the beginning is not the word at all. But the deed. A consequence of that position is that we no longer think of the inner versus the outer in the same way. The idea of public language as rooted in a prior private language is demonstrated to be an illusion. One that fails to recognize that we are social, communicating beings and that we are so all the way down.  Say that we become puzzled about money. Here is something that people deeply desire, spend and risk their lives acquiring. People are \u201cworth\u201d so much money and so on. But perhaps we are struck by the fact that coins and notes are, in themselves just worthless bits of metal or paper. How can they have value? (Note that we have already slipped, even at the moment we first become puzzled, into thinking of \u201cvalue\u201d as a kind of property something has.) Imagine that someone replies like this: it is true that actual cash is arbitrary \u2013 just stuff. What matters is that cash is backed by something that really does have value. The \u201cpromise to pay the bearer on demand\u201d on UK notes. The gold in the bank is what really has value. The money is just an outward sign of that true value.  But gold is also just a kind of metal. Why should it have value? The same question we asked about the cash can now be asked about the gold.  Someone else might interject: gold is rare and hard to acquire. That\u2019s why it has value. But lots of things are rare without being valuable. And in any case, no one actually trades in their money for gold. Banks won\u2019t even let you do that. Yet we go on treating the money as valuable.  Here of course we will want to say this: actual money (coins and notes) isn\u2019t intrinsically valuable. What matters is only that it is in fact used in trade and exchanges. The value lies in the use of the money. It\u2019s not that the exchanges use money because the money has value. Rather the money has value because the exchanges have value. Or rather what we mean by monetary value is made manifest in and through the activities of exchange and the myriad things we do with money. And once we see things that way round, it will now seem rather strange to say that money is just worthless stuff. It looks that way and we became puzzled in the first place only because we tricked ourselves into separating out the notes and coins from their use in exchange. Our problem was how to explain how certain stuff \u2013 notes and coins \u2013 had value. So we started looking for another kind of stuff to carry that value. That is, we already committed to a particular view of what an explanation would look like. The solution was to change our view of what would count as an explanation or indeed whether one was actually needed at all. We solve the problem when we dissolve the source of our puzzlement. The analogy is between the values of notes and coins and the meaning of words and sentences. We see that particular sounds and squiggles in a particular language, say English, are in themselves arbitrary, having no intrinsic connection to the things they stand for. So we think there must be something standing behind the words which gives them the real meaning. What could that be? Well, we might suggest idea, thoughts and intentions. We mean things by our words. Others understand us because they know what we mean by the sounds or marks we make. The words are arbitrary but the thoughts are not. Their meaning is laid up in the vaults of the mind. But just as gold was not the real explanation of the value of money, thoughts are not the explanation of the meanings of words. It is not that gold or thoughts don\u2019t exist. Of course they do. But if it\u2019s a problem to explain how words have meaning, it is equally a problem to explain how thoughts have meaning.  We see people using money and words in all their forms, buying and selling, speaking and listening. Nothing is hidden. Nothing stands behind all the activity. The value \u2013 or meaning \u2013 lies in the activity. We might note too that, in our analogy private money \u2013 a currency that one alone could use \u2013 would be nonsensical. Similarly, a private language, the words of which only an individual could understand is equally senseless. And a philosophical theory of mind and meaning, which implied the possibility of such a private language, would, for that reason, be mistaken.  This set of exchanges and twists of thought is, or is something like, what is going on in this passage from the Philosophical Investigations:   How does it come about that this arrow \u2794 points? Doesn\u2019t it seem to carry in it something beside itself? \u2013 \u201cNo, not the dead line on paper; only the psychical thing, the meaning, can do that.\u201d \u2013 That is both true and false. The arrow points only in the application that a living being makes of it. This pointing is not a hocus-pocus which can be performed only by the soul.  We want to say: \u201cWhen we mean something, it\u2019s like going up to someone, it\u2019s not having a dead picture (of any kind).\u201d We go up to the thing we mean. \u201cWhen one means something, it is oneself meaning\u201d; so one is oneself in motion. One is rushing ahead and so cannot also observe oneself rushing ahead. Indeed not.  Yes: meaning something is like going up to someone. In this passage, the arrow serves of course as the paradigmatic meaningful sign. Wittgenstein\u2019s opponent wants to stress the passivity of the sign in itself and thinks that therefore some account must explain how it is that thoughts (\u201cthe psychical thing\u201d) actively reach out to the things to which they refer as if they were going up to someone to shake his hand. Wittgenstein agrees with this opponent that meaning something is like going up to someone. But, he suggests, this is not true, as his opponent intends, in a merely metaphorical sense. Rather, our meaning something is literally like going up to someone. Meaning gets going because we move around and act on a world of other objects and agents; pragmatic engagements in the world, which logically precede language. It is these practical engagements, rather than the shared logical form of the Tractatus, that enable meaning. We do not mirror reality. We are enmeshed in it.  Wittgenstein was hostile to modern philosophy as he found it. He thought it the product of a culture that had come to model everything that matters about our lives on scientific explanation. In its ever-extending observance of the idea that knowledge, not wisdom, is our goal, that what matters is information rather than insight, and that we best address the problems that beset us, not with changes in our heart and spirit but with more data and better theories, our culture is pretty much exactly as Wittgenstein feared it would become. He sought to uncover the deep undercurrents of thought that had produced this attitude. He feared it would lead not to a better world but the demise of our civilization. That perhaps explains his deep unpopularity today. It is for the same reason that Ludwig Wittgenstein is the most important philosopher of modern times. Sign up \n\t\t\t\tCopyright \u00a9 The Times Literary Supplement Limited 2018. The Times Literary Supplement Limited: 1 London Bridge Street, London SE1 9GF. Registered in England. \n\t\t\t\tCompany registration number: 935240. VAT no: GB 243 8054 69.\n\t\t\t","time":1525787859,"title":"The relentless honesty of Ludwig Wittgenstein","type":"story","url":"https:\/\/www.the-tls.co.uk\/articles\/public\/ludwig-wittgenstein-honesty-ground\/amp\/?__twitter_impression=true","label":7,"label_name":"random"},{"by":"ssijak","descendants":0,"id":17020878,"kids":"None","score":1,"text":"The HTTP 418 I'm a teapot client error response code indicates that the server refuses to brew coffee because it is a teapot. This error is a reference of Hyper Text Coffee Pot Control Protocol which was an April Fools' joke in 1998. The compatibility table in this page is generated from structured data. If you'd like to contribute to the data, please check out https:\/\/github.com\/mdn\/browser-compat-data and send us a pull request. No compatibility data found. Please contribute data for \"http.status.418\" (depth: 1) to the MDN compatibility data repository. Get the latest and greatest from MDN delivered straight to your inbox. If you haven\u2019t previously confirmed a subscription to a Mozilla-related newsletter you may have to do so. Please check your inbox or your spam filter for an email from us.\n         \u00a9 2005-2018 Mozilla and individual contributors. Content is available under these licenses.","time":1525787821,"title":"Http 418 \u201cI`m a teapot\u201d status code","type":"story","url":"https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Status\/418","label":7,"label_name":"random"},{"by":"hellbanner","descendants":0,"id":17020869,"kids":"None","score":3,"text":"Posted by: Dan Oved, freelance creative technologist at Google Creative Lab, graduate student at ITP, NYU.\u00a0Editing and illustrations: Irene Alvarado, creative technologist and Alexis Gallo, freelance graphic designer, at Google Creative Lab In collaboration with Google Creative Lab, I\u2019m excited to announce the release of a TensorFlow.js version of PoseNet\u00b9,\u00b2 a machine learning model which allows for real-time human pose estimation in the browser. Try a live demo here. So what is pose estimation anyway? Pose estimation refers to computer vision techniques that detect human figures in images and video, so that one could determine, for example, where someone\u2019s elbow shows up in an image. To be clear, this technology is not recognizing who is in an image\u200a\u2014\u200athere is no personal identifiable information associated to pose detection. The algorithm is simply estimating where key body joints are. Ok, and why is this exciting to begin with? Pose estimation has many uses, from interactive installations that react to the body to augmented reality, animation, fitness uses, and more. We hope the accessibility of this model inspires more developers and makers to experiment and apply pose detection to their own unique projects. While many alternate pose detection systems have been open-sourced, all require specialized hardware and\/or cameras, as well as quite a bit of system setup. With PoseNet running on TensorFlow.js anyone with a decent webcam-equipped desktop or phone can experience this technology right from within a web browser. And since we\u2019ve open sourced the model, Javascript developers can tinker and use this technology with just a few lines of code. What\u2019s more, this can actually help preserve user privacy. Since PoseNet on TensorFlow.js runs in the browser, no pose data ever leaves a user\u2019s computer. Before we dig into the details of how to use this model, a shoutout to all the folks who made this project possible: George Papandreou and Tyler Zhu, Google researchers behind the papers Towards Accurate Multi-person Pose Estimation in the Wild and PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model, and Nikhil Thorat and Daniel Smilkov, engineers on the Google Brain team behind the TensorFlow.js library. PoseNet can be used to estimate either a single pose or multiple poses, meaning there is a version of the algorithm that can detect only one person in an image\/video and one version that can detect multiple persons in an image\/video. Why are there two versions? The single person pose detector is faster and simpler but requires only one subject present in the image (more on that later). We cover the single-pose one first because it\u2019s easier to follow. At a high level pose estimation happens in two phases: But wait what do all these keywords mean? Let\u2019s review the most important ones: A lot of work went into abstracting away the complexities of the model and encapsulating functionality into easy-to-use methods. Let\u2019s go over the basics of how to setup a PoseNet project. The library can be installed with npm: and imported using es6 modules: or via a bundle in the page: As stated before, the single-pose estimation algorithm is the simpler and faster of the two. Its ideal use case is for when there is only one person centered in an input image or video. The disadvantage is that if there are multiple persons in an image, keypoints from both persons will likely be estimated as being part of the same single pose\u200a\u2014\u200ameaning, for example, that person #1\u2019s left arm and person #2\u2019s right knee might be conflated by the algorithm as belonging to the same pose. If there is any likelihood that the input images will contain multiple persons, the multi-pose estimation algorithm should be used instead. Let\u2019s review the inputs for the single-pose estimation algorithm: Now let\u2019s review the outputs for the single-pose estimation algorithm: This short code block shows how to use the single-pose estimation algorithm: An example output pose looks like the following: The multi-person pose estimation algorithm can estimate many poses\/persons in an image. It is more complex and slightly slower than the single-pose algorithm, but it has the advantage that if multiple people appear in a picture, their detected keypoints are less likely to be associated with the wrong pose. For that reason, even if the use case is to detect a single person\u2019s pose, this algorithm may be more desirable. Moreover, an attractive property of this algorithm is that performance is not affected by the number of persons in the input image. Whether there are 15 persons to detect or 5, the computation time will be the same. Let\u2019s review the inputs: The best way to see what effect these parameters have is to play with the multi-pose estimation demo. Let\u2019s review the outputs: This short code block shows how to use the multi-pose estimation algorithm: An example output array of poses looks like the following: If you\u2019ve read this far, you know enough to get started with the PoseNet demos. This is probably a good stopping point. If you\u2019re curious to know more about the technical details of the model and implementation, we invite you to continue reading below. In this section, we\u2019ll go into a little more technical detail regarding the single-pose estimation algorithm. At a high level, the process looks like this: One important detail to note is that the researchers trained both a ResNet and a MobileNet model of PoseNet. While the ResNet model has a higher accuracy, its large size and many layers would make the page load time and inference time less-than-ideal for any real-time applications. We went with the MobileNet model as it\u2019s designed to run on mobile devices. Processing Model Inputs: an Explanation of Output Strides First we\u2019ll cover how to obtain the PoseNet model outputs (mainly heatmaps and offset vectors) by discussing output strides. Conveniently, the PoseNet model is image size invariant, which means it can predict pose positions in the same scale as the original image regardless of whether the image is downscaled. This means PoseNet can be configured to have a higher accuracy at the expense of performance by setting the output stride we\u2019ve referred to above at runtime. The output stride determines how much we\u2019re scaling down the output relative to the input image size. It affects the size of the layers and the model outputs. The higher the output stride, the smaller the resolution of layers in the network and the outputs, and correspondingly their accuracy. In this implementation, the output stride can have values of 8, 16, or 32. In other words, an output stride of 32 will result in the fastest performance but lowest accuracy, while 8 will result in the highest accuracy but slowest performance. We recommend starting with 16. Underneath the hood, when the output stride is set to 8 or 16, the amount of input striding in the layers is reduced to create a larger output resolution. Atrous convolution is then used to enable the convolution filters in the subsequent layers to have a wider field of view (atrous convolution is not applied when the output stride is 32). While Tensorflow supported atrous convolution, TensorFlow.js did not, so we added a PR to include this. Model Outputs: Heatmaps and Offset Vectors When PoseNet processes an image, what is in fact returned is a heatmap along with offset vectors that can be decoded to find high confidence areas in the image that correspond to pose keypoints. We\u2019ll go into what each of these mean in a minute, but for now the illustration below captures at a high-level how each of the pose keypoints is associated to one heatmap tensor and an offset vector tensor. Both of these outputs are 3D tensors with a height and width that we\u2019ll refer to as the resolution. The resolution is determined by both the input image size and the output stride according to this formula: Heatmaps Each heatmap is a 3D tensor of size resolution x resolution x 17, since 17 is the number of keypoints detected by PoseNet. For example, with an image size of 225 and output stride of 16, this would be 15x15x17. Each slice in the third dimension (of 17) corresponds to the heatmap for a specific keypoint. Each position in that heatmap has a confidence score, which is the probability that a part of that keypoint type exists in that position. It can be thought of as the original image being broken up into a 15x15 grid, where the heatmap scores provide a classification of how likely each keypoint exists in each grid square. Offset Vectors Each offset vector is a 3D tensor of size resolution x resolution x 34, where 34 is the number of keypoints * 2. With an image size of 225 and output stride of 16, this would be 15x15x34. Since heatmaps are an approximation of where the keypoints are, the offset vectors correspond in location to the heatmap points, and are used to predict the exact location of the keypoints as by traveling along the vector from the corresponding heatmap point. The first 17 slices of the offset vector contain the x of the vector and the last 17 the y. The offset vector sizes are in the same scale as the original image. Estimating Poses from the Outputs of the Model After the image is fed through the model, we perform a few calculations to estimate the pose from the outputs. The single-pose estimation algorithm for example returns a pose confidence score which itself contains an array of keypoints (indexed by part ID) each with a confidence score and x, y position. To get the keypoints of the pose: The details of the multi-pose estimation algorithm are outside of the scope of this post. Mainly, that algorithm differs in that it uses a greedy process to group keypoints into poses by following displacement vectors along a part-based graph. Specifically, it uses the fast greedy decoding algorithm from the research paper PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model. For more information on the multi-pose algorithm please read the full research paper or look at the code. It\u2019s our hope that as more models are ported to TensorFlow.js, the world of machine learning becomes more accessible, welcoming, and fun to new coders and makers. PoseNet on TensorFlow.js is a small attempt at making that possible. We\u2019d love to see what you make\u200a\u2014\u200aand don\u2019t forget to share your awesome projects using #tensorflowjs and #posenet! By clapping more or less, you can signal to us which stories really stand out. TensorFlow is a fast, flexible, and scalable open-source machine learning library for research and production. An open-source machine learning framework for everyone.","time":1525787741,"title":"Real Time Human Pose Estimation in the Browser with Tensorflow JS","type":"story","url":"https:\/\/medium.com\/tensorflow\/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5","label":5,"label_name":"ml"},{"by":"bookofjoe","descendants":2,"id":17020861,"kids":"[17021023, 17021044]","score":3,"text":"Imagine: it\u2019s 1866, and you\u2019ve just hopped on a steamboat heading up the Mississippi River. Excited, you pull out your newest souvenir, a small spool with a strip of linen-backed parchment twisted around it. You begin unwinding, and see a near-future destination: Memphis, marked by a smart bullseye, and the site of a major railroad junction. You twirl it up again and travel back in time: There\u2019s the Mississippi-Louisiana border, which you crossed just yesterday. Suddenly, a stiff river breeze churns up, and you lose control of your map, which quickly unspools. The end of it gets caught by the wind and floats across the deck. Before you can say anything, whap!\u2014it hits another eager tourist right in the face. Such were the delights potentially provided by the Ribbon Map of the Father of Waters. A depiction of the entire Mississippi, from the Gulf of Mexico all the way up to northern Minnesota, the map stretched 11 feet long and just under three inches wide, and was meant to be wrapped around a spool and carried in a pocket. Although it never really caught on, it stands as a testament to the many forms maps can take, and as a precursor of how we treat traveling today. As Jim Akerman, the Curator of Maps at Chicago\u2019s Newberry Library, points out, the Ribbon Map of the Father of Waters belongs to a class of map with a deep history: the \u201citinerary\u201d or \u201cstrip\u201d map. Unlike network maps, which are designed to show all journeying possibilities\u2014think of a road atlas, or a big fold-out trail guide\u2014strip maps \u201care organized around a specific route of travel,\u201d he explains. \u201cIt\u2019s meant to give you very close guidance.\u201d Strip maps arose from oral or written itineraries, and were used by ancient Romans who wanted to plan trips to nearby towns, and medieval Europeans who hoped to make pilgrimages from London to Jerusalem. Reading one is like getting directions from a friend: left at this landmark, right at that fork. This form lent itself quite well to rivers, which make up a route all on their own. \u201cIn late medieval China, people made beautiful panoramas of the course of the Yangtze River that are basically strip maps,\u201d Akerman says. Fast forward to the early 1800s, and throngs of opportunity-seekers were heading for the American West, often via the Ohio River or the Mississippi. A mapmaker named Zadoc Kramer began publishing pocket guidebooks that essentially chopped the rivers into chunks, and laid the strips next to each other for easy viewing. \u201cIf they were strung together [instead] they\u2019d be 15 feet or so,\u201d says Akerman. By the 1860s, a pair of St. Louis-based entrepreneurs had decided there was a market for a river map that embraced its true length. In 1866, Myron Coloney and Sidney B. Fairchild, a.k.a. Coloney and Fairchild, unfurled their first Ribbon Map, a long, blue-inked facsimile of the Mississippi. \u201cColoney and Fairchild\u2019s patented apparatus required that the single sheet be cut into strips, attached end-to-end, mounted on linen, and then rolled inside a wooden, metal, or paper spool,\u201d writes art historian Nenette Luarca-Shoaf in an article in Common-place. (\u201cThis patent is for the \u2018IDEA,\u2019\u201d the pair specified when filing it.) This map would not have been useful to steamboat captains: \u201cBecause the river channel changed so often, the captains actually had to know the river in intuitive ways \u2026 informed by their extensive travel and understanding,\u201d Luarca-Shoaf says. It wouldn\u2019t have been too handy for people moving West, either. Instead, it was marketed toward tourists, who were flocking to the Mississippi to see the sights and ride the steamboats. \u201cThe river was a source of great awe,\u201d she says. \u201cThat kind of length, that kind of spaciousness was incomprehensible to a lot of folks who were coming from the East Coast.\u201d An advertisement for the ribbon map suggests that people needed an outlet for that awe: having your own chart to unroll, it promised, would stop you from \u201cconstant[ly] questioning\u2026 the officers of the boat,\u201d and causing \u201can immensity of annoyance\u201d to them. It wasn\u2019t just a marketing gimmick, though. By choosing this particular form, Coloney and Fairchild leaned into a particular depiction of the Mississippi that took shape during the Civil War. \u201cThere was this idea that because the river went from north to south, it was a great unifier for the country,\u201d Luarca-Shoaf says\u2014that it tied the divided North and South together like, well, a ribbon. At the same time, they took pains to include important battle sites, like Vicksburg. That these sites made it onto the map just a year after the war ended \u201cshows that the war had marked the landscape in more than physical ways,\u201d she says. \u201cIt had become part of the history of the place.\u201d According to Akerman, Coloney and Fairchild had planned to produce a whole series of ribbon maps for other popular, linear destinations, like retail-heavy streets and railway lines. (They may have made one other, of Broadway in New York City.) But despite the apparent success of this original effort\u2014a September 1866 article in the New Orleans Times-Picayune indicates that the entire first run of 1,000 sold out quickly, making room for a more detailed second edition, which is the one pictured throughout this article\u2014the company seems to have given up. Today, the remaining copies of the Ribbon Map of the Father of Waters sit spooled up in map libraries and local history museums, testaments to the idiosyncrasies of their particular users. One, at the Missouri Historical Society, is from the collection of a riverboat pilot, suggesting a moment of either aspiration (if he bought it before he got into the profession) or nostalgia (if he snapped it up after). The Denver Public Library has one with a patterned red case and a tiny hand crank. Luarca-Shoaf has stretched out and wound up a number of these, and tried to puzzle out how well they\u2019d work in context. She says it\u2019s not too hard to see why the idea didn\u2019t succeed. \u201c[Imagine] the thing gets all wet, and it\u2019s kind of just hanging out,\u201d she describes. \u201cI\u2019ve never found any definitive evidence that anybody actually ever used this.\u201d It\u2019s certainly hard to imagine carrying them around today. But as she and Akerman both point out, in the years since the Ribbon Map\u2019s demise, the genre it belonged to\u2014the pocket-sized itinerary map\u2014has snapped up a massive market share. Now, every time we take out our phones to try to get somewhere, we can choose to be led carefully along, step by step, by Google Maps or Waze. The only real difference is that the scrolling stays onscreen: safer for our fellow travelers, but a whole lot less exciting. \nGet our latest, delivered straight to your inbox by subscribing to our newsletter.\n \nSubscribe to our newsletter and get our latest, sent right to your inbox.\n  Follow us on Twitter to get the latest on the world's hidden wonders. Like us on Facebook to get the latest on the world's hidden wonders.","time":1525787644,"title":"Ribbon map of the Mississippi River (late 1860s)","type":"story","url":"https:\/\/www.atlasobscura.com\/articles\/mississippi-river-ribbon-map","label":7,"label_name":"random"},{"by":"longwave","descendants":0,"id":17020859,"kids":"None","score":2,"text":"There appears to be a Twitter bot farm, with nearly 130,000 fake accounts that are targeting London accounts, including the Mayor of London, the Evening Standard and a number of listings websites and venues. They all follow the same 21 accounts, none of them seem to follow each other, and none of them have tweeted. An account is created, and generally within 15 minutes, they have followed the same 21 accounts as all the others. Then they sit there dormant, doing nothing, yet the bot farm is growing. What is their purpose? Is it a spam factory that\u2019s getting ready to splurge London with spam. Or is it a political campaign waiting for some event to occur when they will spring to life to promote their maker\u2019s political views and make it look like there is popular support? We don\u2019t know. What is known is that the creator of the Twitter bots accounts is active and punching them out at a fairly decent speed. They could be Russian, after all, most of the fake account generators on Twitter are, but the Russians tend to more sophisticated, and these were fairly easy to detect. They\u2019re here, they are legion, and they are waiting to strike. Based on some work over the weekend, here\u2019s the list of the fake bots that I can determine at this time as a downloadable CSV file. It\u2019s actually quite difficult to report a fake account to Twitter. Even if I were minded to write a script to report nearly 130,000 accounts, the only fake account report Twitter accepts is where someone is being impersonated. Purely fake accounts that are just fake, are not reportable, which seems an omission. Whats's on in London: today or tomorrow or this weekend When you open a new Twitter account from a London-area IP address, Twitter automatically pushes you to follow a few dozen accounts relevant to your location. (I imagine most major cities have their own set of \u2018default accounts\u2019 to follow.) The 21 accounts you mention are chosen by Twitter as \u2018great for newbies\u2019 rather than by the bots themselves. It\u2019s much quicker to just click \u2018OK, next\u2019 than manually deselect these accounts. Therefore, this list likely consists of a mix of bots, Londoners that signed up for Twitter but never tweeted and abandoned it soon after, \u2018test\u2019 accounts created for any temporary reason by a developer, and so on. (Source: I was the former social media manager for one of these brands, and was accidentally blessed by huge numbers of \u2018new followers\u2019 each month, very few of which were actually real, but in my defence I definitely wasn\u2019t doing anything to encourage it, and there was also no way to stop it, plus some of them were simply \u2018Londoners joining Twitter for the first time.\u2019) Yep \u2013 when I joined Twitter I got pushed to follow a number of accounts like the Mayor of London and Evening Standard. I am not Russian. It could be that these accounts are doing a Martha Lane-Fox Its bit like Facebook with dealing with bot fake Facebook accounts. And you have to be careful because some might of them can hack into your Facebook account. If you haven\u2019t changed your details and\/or added a rescue email address which does helps when your account is being hacked or someone trying to hack into your account. From a location that is prone to hacking. Your email address will not be published. Required fields are marked * Comment Name * E-mail * Website      Sign up for my Weekly Guide to offbeat events in London Sign up for a Weekly Guide to offbeat events in London","time":1525787618,"title":"Large scale Twitter bot farm targeting Londoners","type":"story","url":"https:\/\/www.ianvisits.co.uk\/blog\/2018\/05\/08\/large-scale-twitter-bot-farm-targeting-londoners\/","label":7,"label_name":"random"},{"by":"pftg","descendants":0,"id":17020852,"kids":"None","score":6,"text":"Raise the hand if you feel confused about project progress. How often do you ask in the Slack about current status? We have seen such problems and would like to present our 5 techniques to have a transparent process and build trust in the team. Communication is a key, but developers find 500 reasons to skip it. They are busy with more important activities like product development advancing. But to be busy should not sacrifice a trust-building with other members. We found a compromise for this problem\u200a\u2014\u200aAsync Text Stand-ups. This is the best tool to have an over-communication in the team. Every day, we share via Slack: a snapshot of progress, and a vision of what\u2019s next. It eliminates wasting of time on micro-managing developers. Also, we have Weekly High-Overview \u201cStand-up\u201d. The difference from daily standup that we share team overall result, problems from the last week and next week goals. Tools: Slack Kanban resolves so many problems: scheduling, monitoring and debugging problems of your process. It\u2019s the best tool for Lean Startup followers. Several lists will help you to be on the same page: Roadmap, Backlog, Ready, In Progress, Verify, Done. Tools: GitHub Before start working on a schedule, developers need to understand where we are going to be and what result is expected. Roadmap is a great tool to share vision and strategy. Put Roadmap in front of developers. We found a Hierarchical Kanban Board Structure very handy to store Roadmap. To build Roadmap we ask questions where you will be in 3 weeks, 3 months and 3 years. Tools: GitHub Achieve a long-term goal is great\u200a\u2014\u200abut this relatively rare. The good news is that even small wins can boost inner work life tremendously. We split our work into 2 days size atomic units. This prevents bottlenecks and gets working solution much faster. Tools: CircleCI Working without appreciations will make you soulless machine. It\u2019s killing all fun. On check-ins, you can make retrospectives and provide feedback to the team. Agile Retrospective is a good practice to run experiments. Regular check-in is a good opportunity to select what is the matter now. It is making everyone feel included as a team. Tools: Hangouts In the post we covered such techniques: Paul Keen is a Chief Technology Officer at JetThoughts. Follow him on LinkedIn or GitHub. By clapping more or less, you can signal to us which stories really stand out. Chief Technology Officer at JetThoughts LLC Help to Bringing Life to a Struggling Projects","time":1525787518,"title":"How to Get Remote Teams to High Perform","type":"story","url":"https:\/\/jtway.co\/how-to-get-remote-teams-to-high-perform-b9b29d698feb","label":10,"label_name":"thought"},{"by":"yread","descendants":0,"id":17020835,"kids":"None","score":1,"text":"The official source of product insight from the Visual Studio Engineering Team  Today at Build, we announced the release of Visual Studio 2017 version 15.7 and the first preview of the next update, Visual Studio 2017 version 15.8 Preview 1. If you would rather try these without installing them, check out the Visual Studio images in Azure\u00a0that will be available soon. Before digging in, I\u2019d like to call out other Visual Studio and .NET news: There are also two really interesting previews I\u2019d call your attention to: We will publish blog entries on these soon, and as we do we\u2019ll update this post with the links. There is a lot we have accomplished in this release and I\u2019ll share the highlights in this post. It is a long post, so grab a cup of your preferred warm beverage and read on. For the complete list of all the updates in today\u2019s releases, check out the Visual Studio 2017 version 15.7 release notes and list of bugs submitted by you that are fixed. One of the first things you\u2019ll probably notice is a nice change to the way updates work. You can now initiate a check for updates (Help -> Check for Updates) and, after you save your work and choose \u201cUpdate Now,\u201d Visual Studio will automatically apply the update and reopen where you left off. Starting with this release, you can reduce the installation footprint on your system drive by directing the download cache, shared components, and some SDKs and tools to separate locations. Because these pieces are shared among Visual Studio installations, you can only set the locations with your first installation, meaning that you can\u2019t change them later. If you have a solid-state drive (SSD), we recommend that you install the Visual Studio core product on your SSD because this will make Visual Studio run significantly faster.  We\u2019ve done a lot of work on Visual Studio performance during the Visual Studio 2017 cycle, particularly things like solution load (see this post for some of the most recent improvements), but we\u2019re making continual improvements. Here are a few other improvements to performance in this update. UI Responsiveness: One change that will improve general UI responsiveness in version 15.7 is that the debugging windows are now asynchronous. This means that they will no longer block Visual Studio as they do work. This change will allow for faster stepping because you can continue interacting with Visual Studio without interruption. .NET mobile performance improvements: Visual Studio will boot and deploy the Xamarin runtime on your device while a compile is taking place. This reduces the time that you have to wait to see your app show up. With our test app, on a fresh device and no previous deployment, including launching the Android emulator, we see performance gains ranging between 33% to 300% depending on the scenario. TypeScript Performance: We\u2019ve made background analysis of closed files optional (\u201cOnly report diagnostics for files opened in the editor\u201d under Tools > Options > TextEditor > JavaScript\/TypeScript \/ Project). With each release, we add capabilities that improve developer productivity, particularly focusing on that \u201cinner loop\u201d of editing, building, and debugging. .NET Refactorings and Navigation: Every release, we increase the number of refactorings and improve navigation for our managed languages. In Visual Studio 2017 version 15.7 we added the following refactorings: We also enabled Go To Definition (F12 or Ctrl+Click) in more scenarios, like on LINQ query clauses, deconstructions, and on the \u2018override\u2019 keyword (to navigate to base). XAML IntelliSense: The XAML editor now provides IntelliSense for authoring conditional XAML. When using a type that is not present in the target-minimum version of your app, the XAML editor now not only warns but also provides several options to fix it. The quick fix Lightbulb figures out the right conditional using statement based on the platform version where the type was first introduced, allowing the app to target a wider range of platform versions while being able to consume the latest controls.  Xamarin.Forms XAML Editing Improvements: If you use Xamarin.Forms in Visual Studio 2017 version 15.7 you will notice a vastly improved IntelliSense experience when editing XAML. The Xamarin.Forms XAML editing experience is now powered by the same engine that powers WPF and UWP. This brings many enhancements including improved matching, Lightbulb suggestions, code navigation, error checking, resource completion, and markup extension completion. TypeScript Editing and TypeScript 2.8: Visual Studio 2017 version 15.7 will include TypeScript 2.8. We\u2019ve continued our push to help make TypeScript and JavaScript developers more productive by adding support for fixing all occurrences of a problem in a document (for example, removing unused variables), organizing imports (including sorting and removing unused declarations), and displaying the lightbulb more proactively when optional improvements are possible. We\u2019ve also fixed some of the top issues that you have raised, including premature triggering of snippets, un-cancellable refactorings, hard-to-disable formatting, and incorrect TypeScript version selection. These improvements are powered by TypeScript 2.8, so for the best experience, we recommend updating your existing projects to use the latest TypeScript version. C++ ClangFormat: We\u2019ve added ClangFormat support for C++ developers in the IDE. Like with EditorConfig, you can use ClangFormat to automatically style and format your code as you type, in a way that can be enforced across your development team. For more information, see the ClangFormat Support blog post.  C++ CMake: This release of Visual Studio includes features to make it easier than ever to use CMake without needing to generate projects and Solutions. The new CMake CMake Targets View provides an alternative way to view a CMake project\u2019s source in the Solution Explorer; instead of a folder-based view, it organizes the code into individual CMake targets. Additionally, CMake projects now support single file compilation and static analysis of C++ code, without the need to generate a VCXProj file. The version of CMake that ships with Visual Studio has also been upgraded to 3.11. C++ Standards Conformance: And last, but not least in this section, I want to highlight that Visual C++ is now conforming to ++11, C++14, and C++17. Yes: the MSVC compiler toolset in Visual Studio version 15.7 conforms with the C++ Standard! We\u2019re not going to stop there with our conformance effort. Expect to see more future communications from our team as we alleviate the remaining caveats. More details are available at the Visual C++ Team Blog post on how MSVC conforms to the C++ Standard. IntelliTrace Events and Snapshots for .NET Core: IntelliTrace\u2019s new step-back debugging feature, which we first shipped in Visual Studio 2017 version 15.5, is now supported for debugging .NET Core applications. The feature automatically takes a snapshot of your application on each breakpoint and debugger step. This enables you to go back to a previous breakpoint or step and view the state of the application as it was in the past. To enable this feature, go to Tools > Options > IntelliTrace settings > and select \u2018IntelliTrace events and snapshots\u2019. IntelliTrace step-back is a Visual Studio Enterprise feature available on Windows 10 Anniversary Update or above for debugging .NET applications.  Source Link Authentication: The debugger now supports authenticated Source Link requests for Visual Studio Team Services and private GitHub repositories. When debugging code that was built on another computer, such as a library in a NuGet package, Source Link enables the debugger to show correctly matching source code by downloading it from the internet. To build your own code with Source Link enabled see https:\/\/aka.ms\/SourceLinkSpec. For VSTS repositories, authentication is provided by signing into Visual Studio. For GitHub, we leverage credentials from the GitHub extension for Visual Studio and the Git Credential Manager for Windows. If Source Link fails due to an authentication error a new \u201cSource Link Authentication Failed\u201d page is shown to explain the issue. This page allows you to login to VSTS or GitHub and retry the Source Link request. Edge Debugging: Visual Studio ASP.NET and ASP.NET Core developers on Windows Insider builds can now set breakpoints and debug their JavaScript files using Microsoft Edge browser. Visual Studio will use the new Edge DevTools Protocol developed by the Microsoft Edge team when targeting Microsoft Edge browser, which means that developers will be able to debug and fix JavaScript issues from within Visual Studio in both Microsoft Edge and Google Chrome. We are glad to enable this oft-requested feature from our customers and would love to hear what you think about it. We\u2019ve made many improvements to the mobile development experience, some listed elsewhere in this post, but for the core platform support, here are a few. Android Development Improvements: We are now distributing the Android Oreo SDK (Android API level 27) and are shipping the Android emulators with Quick Boot enabled. Additionally, Visual Studio will now detect scenarios where the project requires a different version of the Android SDK that is installed and will download required components in the background. Apple Development Improvements: The iOS, macOS, tvOS, and watchOS apps now feature a fully static type system. This brings many benefits, such as smaller app size, faster app startup, and reduced memory usage. We have also introduced the [Weak] attribute for fields that make it simpler to write code that is garbage collection friendly in Apple platforms. The provisioning of iOS devices has historically been a chore, requiring multiple trips to the documentation. We have incorporated the same experience that we shipped on macOS and you can now provision your devices in seconds and keep your entitlements up to date with the click of a button. Visual Studio 2017 version 15.7 is the recommended version of Visual Studio to use with .NET Core 2.1 and has a bunch of new features like a new managed socket implementation and various improvements like better support for HTTPS. It doesn\u2019t matter which you install first, either way Visual Studio version 15.7 will find .NET Core 2.1 and offer it as an option. Deployment to Azure App Services on Linux: Visual Studio now offers you the ability to deploy non-containerized applications to Azure App Service on Linux in addition to our previous support for apps built with Docker. To publish your application to App Service and run in Linux, from the Publish dialog choose \u201cApp Service Linux\u201d and select \u201cCreate new.\u201d\u00a0 To continue to publish a containerized application to App Service for Containers using Linux, choose \u201cCreate new App Service for Containers.\u201d Key Vault Connected Service: Please don\u2019t store your application keys in your app \u2013 if possible use Azure Key Vault. We\u2019ve made it easier in this release with the Connected Service experience \u2013 using it will add the right configuration the NuGet packages to your project and enable you to access secrets from the Key Vault. Once the Key Vault has been added, you will be able to manage secrets and permissions through the Azure portal. This is available for both ASP.NET and ASP.NET Core applications. We\u2019ve also made it easier to use Key Vault during development: when running an ASP.NET or ASP.NET Core application on the local machine, the app may not have access to the Key Vault from the account specified under Tools > Options > Azure Service Authentication, so it won\u2019t be able to run locally. Visual Studio detects that case and flags it for you with an error. We have included the Windows 10 April 2018 Update SDK, Build 17134 as the new required SDK associated with the Universal Windows Platform workload. We have also added support for generating Windows Machine Learning class wrappers for an ONNX file that is added to a UWP project. Automatic updates for sideloaded UWP apps: The Universal Windows Platform allows distributing applications without the Microsoft Store by using a mechanism called \u201csideloading.\u201d With Visual Studio 2017 version 15.7 using the latest Windows 10 SDK there is now tooling to easily configure the automatic update settings for these UWP apps. Loading code from C# Optional Packages: You can now create a new \u201cOptional Code Package\u201d Universal Windows project type. This project allows you to author C# code to be loaded in the context of the main application package when creating a related set. Learn more about creating C# Optional Code Packages. In addition to Visual Studio 2017 15.7, we also have a Preview of 15.8 (the release notes are here). To install, you can get the preview here, or if you already have a prior Preview installed, either click on the in-product notification or check for an update directly. Previews of Visual Studio 2017 install side by side with generally available releases, so you can try the preview at the same time you have the GA version for your main coding work. Alternatively, with an Azure subscription, you can provision a VM with Visual Studio 2017 on it through the Marketplace.\nC++ Quick Info Tooltips: C++ Quick Info tooltips on macros now show what they expand to and not just their definition. This is particularly useful for complex macros that reference other macros, as it\u2019s now clear what the macro identifier will be replaced with by the preprocessor. See an example of what the tooltip now displays in the picture below.  Docker Single Project Experience: In this Preview, we\u2019ve added a new single project Docker container experience for ASP.NET Core web projects, which builds on top of the existing Docker composed-based container tooling and makes it easy to create, debug, and build Docker containers right from Visual Studio. You can either add Docker support when creating the project, or you can add it to an existing project through right clicking on the project\u2019s context menu in Solution Explorer. For more details on how to utilize this feature, please refer to the Visual Studio 2017 version 15.8 Preview 1 release notes. LibMan \u2013 New Client-Side Library Manager for Web Apps: We are implementing a new tool for managing client-side libraries, colloquially known as \u201cLibMan.\u201d Designed as a replacement to Bower, LibMan allows you to easily acquire and manage static, client-side files for your web project from various sources, including CDNJS. You can read more about it in the post: Library Manager: Client-side content manager for web apps. As always, we want to know what you think. Please install Visual Studio 2017 version 15.7 and Visual Studio 2017 version 15.8 preview 1 and share your thoughts and concerns. Please let us know any issues you have via the Report a Problem tool in Visual Studio. You can track your issues in Visual Studio Developer Community where you can ask questions and find answers. You can also engage with us and other Visual Studio developers through our new Gitter community (requires GitHub account), make a product suggestion through UserVoice, or get free installation help through Live Chat Support. Thanks, John John is responsible for product design and customer success for all of Visual Studio, C++, C#, VB, JavaScript, and .NET. John has been at Microsoft for 17 years, working in developer technologies the whole time. \u00a9 2018 Microsoft Corporation.","time":1525787260,"title":"Visual Studio 2017 version 15.7 and version 15.8 Preview 1","type":"story","url":"https:\/\/blogs.msdn.microsoft.com\/visualstudio\/2018\/05\/07\/visual-studio-2017-version-15-7-and-version-15-8-preview-1\/","label":3,"label_name":"dev"},{"by":"smacktoward","descendants":0,"id":17020826,"kids":"None","score":1,"text":"\n                Your questions answered by a needlessly defensive anus from an electric car forum             My Model X is one year old and I\u2019ve noticed several stone chips on the front clip which have actually taken off a lot of paint and to be honest my car looks kind of bad up close. I\u2019ve contacted Tesla and they say this is natural wear and tear but my previous car, a BMW, did not suffer this issue even after five years. Should I keep complaining?Gary, NY Hi Gary. Welcome to the forum. Let me ask you once thing; do you even understand what a Tesla is? It\u2019s clear to me you have no idea how to use this vehicle and these claimed \u201cstone chips\u201d are as a result of your failure to appreciate the sophistication and intelligence of Tesla engineering. Have you considered that you may be more suited to BMW ownership. You seem like that kind of guy and your failure to comprehend the unique nature of your Tesla is highly indicative of that. Peace out! I\u2019ve had my Model S for four months now and I have a few issues. The A\/C has never worked right and won\u2019t blow cold even on the lowest setting, one of the rear doors won\u2019t open from the outside, and the Autopilot sometimes doesn\u2019t engage unless I stop and do a reset. Are these common problems?Anna, NJ Hey Anna. So I\u2019ve read your list of complaints and I can tell you with absolute certainty that the thing that is not working properly is your brain. Do you even understand how a Tesla functions? These are design features and you should spend more time being thankful that you have them and less time creating negativity about the Tesla experience. In honesty, we do not want people like you causing trouble in the community and I would thank you to keep your appalling, ungrateful attitude to yourself in future. Smooth runnings! Last month I was driving our Model S on Autopilot when the system was briefly confused by bright sunlight, causing me to collide with a semi-truck which then caused another semi to side swipe our vehicle sending it into a ravine and I guess I just can\u2019t believe that this could happen to me and my family. We\u2019re all still in shock that will take a long time to get over and I don\u2019t think we will ever be the same again.Jacob, CA This is a TYPICAL problem caused by people who do not understand how to use Autopilot correctly and fail to comprehend the full potential of a completely autonomous driving system which, it has always been made TOTALLY clear, is not completely autonomous. I recently completed a 500 mile journey on Autopilot without ONCE touching the controls apart from the normal corrections, interventions and resets, and this is the experience Tesla meant for us but which clearly is not good enough for YOU. Instead we have to put up with knuckleheads like you and your \u201cfamily\u201d coming here with your negativity and blame and in future it would behove you to cease with the PETTY complaints that poison our righteous community. The system is GOOD. There is NO PROBLEM with the system. Do NOT question the system. Elon\u2019s LOVE is pure. Message ends. Brad Winkler is an American student in Paris. He shares an apartment with long-running Michelin promotional character, Bibendum Brad Winkler is an American student in Paris. He shares an apartment with long-running Michelin promotional character, Bibendum Dead race team to re-emerge not as race team for some reason New company promises to make SAABness happen on a brand new car \n \n    \u00a9 Sniff Petrol 2018 \n    Site designed and built by\n    interconnect\/it\n","time":1525787132,"title":"Ask a Total Prick from a Tesla Forum","type":"story","url":"https:\/\/sniffpetrol.com\/2018\/05\/08\/ask-a-total-prick-from-a-tesla-forum\/","label":7,"label_name":"random"}]