[{"by": "alexjv89", "descendants": 0, "id": 17020656, "score": 1, "time": 1525785424, "title": "Top blog posts by Moz", "type": "story", "url": "https://blog.massivetimesaver.com/2018/05/08/top-10-blog-posts-by-moz/", "text": "The top 10 project 1) How to Do a Content Audit [Updated for 2017]\n\u00a09.99\u00a0MTS score\n\u00a05844\u00a0mentions on twitter\n\u00a03\u00a0mentions on twitter by domain experts 2) 4 Ways to Build Trust and Humanize Your Brand\n\u00a09.99\u00a0MTS score\n\u00a05280\u00a0mentions on twitter\n\u00a03\u00a0mentions on twitter by domain experts 3) 5 Hacks for Creating and Promoting the Right Content\n\u00a09.99\u00a0MTS score\n\u00a05183\u00a0mentions on twitter\n\u00a02\u00a0mentions on twitter by domain experts 4)\u00a0My Single Best SEO Tip for Improved Web Traffic\n\u00a09.99\u00a0MTS score\n\u00a04389\u00a0mentions on twitter\n\u00a01\u00a0mentions on twitter by domain experts 5)\u00a0Your Google Algorithm Cheat Sheet: Panda, Penguin, and Hummingbird\n\u00a09.99\u00a0MTS score\n\u00a04220\u00a0mentions on twitter\n\u00a03\u00a0mentions on twitter by domain experts 6)\u00a0When Is a Blog the Right Form of Content Marketing?\n\u00a09.99\u00a0MTS score\n\u00a04082\u00a0mentions on twitter\n\u00a03\u00a0mentions on twitter by domain experts 7)\u00a0How to Perform the Ultimate Local SEO Audit\n\u00a09.99\u00a0MTS score\n\u00a04076\u00a0mentions on twitter\n\u00a00\u00a0mentions on twitter by domain experts 8)\u00a0The Illustrated SEO Competitive Analysis Workflow\n\u00a09.98\u00a0MTS score\n\u00a04065\u00a0mentions on twitter\n\u00a02\u00a0mentions on twitter by domain experts 9)\u00a010 Predictions for 2016 in SEO & Web Marketing\n\u00a09.98\u00a0MTS score\n\u00a04041\u00a0mentions on twitter\n\u00a04\u00a0mentions on twitter by domain experts MTS scores a blog post based on its mentions on twitter. Listed above are the top 10 blog posts in a particular blog. When on any page, use\u00a0MTS extension to figure out if a page is worth your time. \u00a0 \u00a0 Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website   \n\n  "}, {"by": "blueatlas", "descendants": 0, "id": 17020650, "score": 1, "time": 1525785361, "title": "I-95 corridor tests tracking drivers by usage", "type": "story", "url": "https://wtop.com/dc-transit/2018/05/95-corridor-tests-tracking-drivers-charge-per-mile-rather-gas-tax/", "text": "WASHINGTON \u2014 States up and down the East Coast are taking their first look at charging drivers per mile \u2014 even on roads ostensibly without tolls \u2014 to raise money for road and bridge projects as existing gas taxes become less effective. \n A second phase starting later this year will expand the group and cover the trucking industry. There have been somewhat more extensive studies of charging people per mile driven on the West Coast, and the new study funded by a $3.1 million federal grant focuses in part on the complexity of driving on the East Coast where crossing state lines is much more common, I-95 Corridor Coalition Executive Director Patricia Hendren said. \u201cAlso we want to look at the difference between urban and rural,\u201d Hendren said. \u201cWe want to make sure that this is a fair system going forward, and, also, there are a lot of concerns about privacy.\u201d For now, the 140 people participating are mainly transportation officials, lawmakers and others interested in the idea often referred to as VMT for vehicle miles traveled or as the coalition puts it MBUF for mileage-based user fee. \u201cWhat\u2019s happened over time is as cars get more fuel efficient and we have more electric vehicles on our roads, the connection between how much you use something and pay for it has been broken,\u201d Hendren said. Those in the pilot program are using all the technology and are being tracked by a private company but are only getting \u201cfaux invoices.\u201d The invoices list the number of miles driven in each state in a given period, how much the driver is estimated to have paid in gas taxes and how much they might pay under a theoretical miles-based charge. For now, the invoice assumes the goal would be to charge an average driver the same amount as the existing gas tax, which in Delaware would mean a charge of about 1.05 cents per mile, in the District would be 1.07 cents per mile, in Virginia would be 1.02 cents per mile and in Maryland would be 1.52 cents per mile. The study analysis is also slated to look at how the fee would fit in with existing toll facilities. Ultimately, any state legislature that took the jump to adopt the fee would set its own rate and those hypothetical rates are simply an example. The private company running the pilot tracks users cars directly or tracks a cellphone GPS signal when the phone is synced with a device left in the car to identify when the driver is in his or her own vehicle as opposed to riding with a friend or taking the bus or train. If the fees expanded, they could be charged to drivers without the GPS tracking or from out of state through photo enforcement or E-ZPass-like accounts. \u201cIf we go to a system like this in the future, there has to be choices. So we want to have a device in the car, on your phone, or have no technology at all as an option and have it be more of a paper process, so we\u2019re looking at it all,\u201d Hendren said. \u201cWe\u2019re really kind of testing some key concerns on privacy, equity, and then just the complexity of the East Coast.\u201d The group of 140 participants will expand to 1000 later in a second phase that also includes a separate six month analysis of how a mileage-based fee might work in the trucking industry alongside other, existing requirements. \u201cIt\u2019s a very different type of question than passenger vehicles,\u201d Hendren said. Like WTOP on Facebook and follow @WTOP on Twitter to engage in conversation about this article and others. \u00a9 2018 WTOP. All Rights Reserved. Need a break? Play a quick game of solitaire or Sudoku. Or take one of our fun quizzes!"}, {"by": "eevilspock", "descendants": 1, "id": 17020649, "kids": [17020657], "score": 1, "time": 1525785357, "title": "The Pleasure and Pain of Being California, the World\u2019s 5th-Largest Economy", "type": "story", "url": "https://www.nytimes.com/2018/05/07/us/california-economy-growth.html", "text": "Advertisement By THOMAS FULLERMAY 7, 2018\n SAN FRANCISCO \u2014 When a transportation agency said two years ago that rush hours were a thing of the past on a major highway in the San Francisco Bay Area, it was not good news. \u201cFor the first time on record, the morning and evening peak periods have merged,\u201d said a spokesman for the agency, the Metropolitan Transportation Commission, \u201ccreating a continuously congested freeway from 5:30 in the morning until nearly 8 o\u2019clock at night.\u201d It has only gotten worse. California\u2019s economy has soared into the stratosphere, but not without inflicting some pain. Paralyzing traffic is one symptom; the increasingly absurd price of putting a roof over one\u2019s head is another. One person from the Midwest devised a quick formula to calculate the price of a house in the Bay Area: See how much a similar house would cost in Minnesota and then add a million dollars. Advertisement Every few weeks there seems to be another story in the California news media about a dilapidated shack in an ordinary neighborhood selling for seven figures, just to be torn down. It has become common enough to lose its shock value. Advertisement California recorded another milestone last week, one reflecting a prouder facet of the state\u2019s success. If the state were an independent country, its economy would rank as the fifth-largest in the world, ahead of Britain\u2019s (which has been crawling lately). California held that spot once before, but it slipped a bit during the Great Recession a decade ago. As the state has blossomed, outpacing many others, it has reinforced a liberal narrative about growth, that a state can have big government and a booming economy, too. (Texas is the conservatives\u2019 counterexample: a big, fast-growing economy under laissez-faire government.) California has strict environmental protections, a progressive tax system and an ascendant minimum wage, now $10.50 an hour and set to rise in stages to $15 in 2023. The state welcomes immigrants, celebrates ethnic and linguistic diversity, and actively tries to combat climate change. And with all that, its economy continues to soar. \u201cWe have raised income taxes and imposed increasingly high fees to reduce greenhouse emissions,\u201d said Stephen Levy, director of the Center for Continuing Study of the California Economy. \u201cNone of that has overridden the attractiveness of this state for talent and innovation and entrepreneurship.\u201d California\u2019s economic success underpins the state\u2019s audacity and its defiance of President Trump. It is an invisible buttress when the governor and attorney general harangue the Trump administration, as they did recently at a news conference in Sacramento, for \u201cbasically going to war against the state of California.\u201d California is not the only state doing well, of course. The federal Bureau of Economic Analysis produced a map last week showing a somewhat lopsided pattern of prosperity in America. The economies of states like Kansas and Louisiana shrank slightly last year, while those in the West thrived: Nevada grew by 3.5 percent, Washington by 4.4 percent, Arizona by 3.2 percent. Even among its booming neighbors, though, California, which saw 3 percent growth last year, stands out for the diversity and sheer size of its economy. Every sector contributed to the state\u2019s growth last year except agriculture, according to Irena Asmundson, the chief economist of the California Department of Finance. Financial services and real estate led the pack, and even manufacturing, often said to be in decline in America, grew significantly, contributing $10 billion in output to the $127 billion the state added over all. Advertisement \u201cMost of this is a lot of relatively small firms that are very specialized,\u201d Ms. Asmundson said of the growth in manufacturing. Another barometer of growth is the surge in people using California\u2019s airports, especially the regional ones. Airports in Long Beach, San Jose, San Luis Obispo and Sonoma all saw double-digit percentage increases in passenger traffic in 2017.  Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. View all New York Times newsletters. Unsurprisingly, Silicon Valley is a big part of California\u2019s success. One of the state\u2019s technology giants, Apple, brought in more revenue in its latest fiscal year \u2014 $229 billion \u2014 than the entire economic output of Wyoming, five times over. All of that money pouring in to California\u2019s tech and entertainment industries produces a big wealth effect, ballooning what the state\u2019s workers can spend \u2014 and not just those who work directly in those fields. Facebook revealed last month that the median pay of its employees was $240,430 a year. But the fire chief in San Ramon has been doing pretty well, too, with total pay and benefits of $516,344 in 2016, according to the website Transparent California. And nearly 200 police officers across the state make more than $300,000 a year, when overtime and benefits are included. Like many states, California has persistent worries about how it will cover its pension obligations down the road, and those high rates of pay for public sector workers do not help matters. But in these boom times, California\u2019s bright fiscal position is a world away from the federal government\u2019s. The state treasury is flush with cash, and is socking billions away in a rainy-day fund. When Gov. Jerry Brown returned to office in 2011, he faced a budget deficit of $27 billion. Now, after eight years of economic expansion, the state has a surplus of $6 billion, and its tax revenues are running well ahead of projections. Yet it is hard to overlook the pain that prosperity has brought: traffic, property prices, homelessness. Advertisement Those last two issues are increasingly seen as sides of the same coin. In 2017, California saw the fastest growth in its homeless population of any state (14 percent), and also had the highest proportion of them unsheltered: 68 percent of the state\u2019s 134,000 homeless people sleep outdoors. All economic booms run out of steam sooner or later, and some Californians say they might welcome a little relief from this one. William Yu, an economist with the Anderson Forecast at the U.C.L.A. Anderson School of Management, recalls a panel discussion a month ago with real estate developers. \u201cOne developer was asked, \u2018Are you worried about a recession coming?\u2019\u201d Mr. Yu said. \u201cThe developer said, \u2018I\u2019m not worried at all. I\u2019m waiting for it.\u2019\u201d Why? So he can snap up some properties at cheaper prices. A version of this article appears in print on May 8, 2018, on Page A12 of the New York edition with the headline: The Pleasure and Pain of Being California, the World\u2019s 5th-Largest Economy.  Order Reprints| Today's Paper|Subscribe\n\n We\u2019re interested in your feedback on this page. Tell us what you think. Go to Home Page \u00bb"}, {"by": "koconder", "descendants": 1, "id": 17020626, "kids": [17020641], "score": 1, "time": 1525785059, "title": "Ostemper \u2013 Automatic Hardening and Tweaks for OS X", "type": "story", "url": "https://github.com/koconder/ostemper", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back     OS Temper Automatic Hardening and Tweaks for OS X This script is an \"all-in-one\" solution for a large number of security fixes, recommended tweaks and first time setup for Mac OS X operating system. This has only been tested for OSX version 10.13.x  Platform mac_os_x OS Versions Supported 10.13.x For quick setup and running based on the default configuration as per the current release in Github. If you would like to modify please feel free to fork and make any changes as needed. Open Terminal app and run: If you have pulled the script down using Git you can run the script directly. Auto-Harden A number of to-do's in progress: OS Temper is built upon the works and ideas from other scripts and community discussions, you can find a list of the sources below. Automated Installs: System and Security Tweaks: Developer Friendly Tweaks: OS Temper terminal logic and functionality: If you find something interesting or would like to contribute, please open issue and start disccussion. Feel free to fork and pull request.\nIf OS Tamper has helped you out feel free to donate via BTC/ETH or to the EFF Author: Vincent Koc (vincent@loophole.eu)\nCopyright: 2018, Vincent K. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. OSTemper logo based upon Fingerprint by LAFS from the Noun Project under Creative Commons licence 3.0: https://creativecommons.org/licenses/by/3.0/"}, {"by": "ralmidani", "descendants": 0, "id": 17020610, "score": 1, "time": 1525784965, "title": "Prototyping a declarative data modeling and validation library (with TypeScript)", "type": "story", "url": "https://medium.com/@ralmidani/prototyping-a-declarative-data-modeling-and-validation-library-in-5-days-with-typescript-e86d6d1af7d5", "text": "I enjoy working with declarative libraries and frameworks\u200a\u2014\u200athat is, tools which do the heavy lifting for you, and let you focus on the unique aspects of your application rather than on writing procedural boilerplate. Some of the best tools I have seen and used include Django, a server-side Python framework for building web applications, and React, a JavaScript library for building user interfaces. Both of these tools help developers build elegant applications without having to get bogged down in details like how to compose and execute an SQL query, or when/how to update the DOM. One of the problems I have encountered as a developer is the lack of data modeling and validation tools which can be used easily on both client and server. For example, if a developer wants to build a data-driven application using Node/Express on the server and React on the client, she may find herself having to write modeling/validation logic in several places. First, she may define her database schemas using Sequelize, then tell her React components to check the data on the client with PropTypes and then, if she wants to provide useful feedback to a user filling out a form, write even more validations in her HTML input elements. As a student at Fullstack Academy, I decided to investigate this problem for my Hackathon (or, more affectionately, \u201cStackathon\u201d) project. Having used Django\u2019s ORM in the past and enjoyed it thoroughly, I lamented not being able to define my models in one place, and then reuse these models and their validations throughout my application. My goal was to build something which could potentially enable developers to craft isomorphic JavaScript applications. Borrowing heavily from Django (notice what I named my library), I wanted to enable developers to write something like this: And have their application derive as much logic and structure as possible from that simple definition (please note, you cannot run the above code unless you have the class properties\u200a\u2014\u200acurrently a Stage-2 proposal\u200a\u2014\u200acompiled to something Node and/or the browser can understand). By design, a Reinhardt model does not assume a specific type of data store, UI, or any other part of the stack; coupling it to a specific breed of technology would hinder its reusability. I was hoping I could easily map a Reinhardt model definition to something that Sequelize, or PropTypes, or any browser could understand, but this would probably be done best with separate, auxiliary libraries. I wrote the very first prototype of Reinhardt with JavaScript, but pivoted quickly to TypeScript (TS), a superset of JavaScript which brings some of the best ideas from languages like Java and C# (at the foremost, static type-checking) to Web Development, making it feel less like the Wild West and more like something a professional (and perhaps paranoid) Software Engineer would write. Since I wanted to build something with the potential to become a successful free and open source project, I saw an opportunity to both learn TS (something I had put off for a long time) and write something more robust. One of TypeScript\u2019s most useful constructs is that of an interface, which enables a developer to define the required and optional properties on an object. An interface can then be used to indicate what a function\u2019s argument may look like, and also impose constraints on the output of the function. Interfaces may also require an object to implement a specific method, so a class which \u201cimplements\u201d an interface without defining a method named in the interface would generate a warning by the TS compiler and\u200a\u2014\u200aif you used the right development tools\u200a\u2014\u200aa visual warning before you compile. Eliminating TS errors sometimes feels like busywork, but if you are not writing throwaway code, it will likely be worth the effort. A surprising features of TypeScript is the cleanliness of the JavaScript output by its compiler, especially if you decide to target more modern environments in the. But one thing I would like to see is an option to have at least some of the type-checking be codified at runtime. While building Reinhardt and then attempting to test its validation logic, I realized I had leaned too heavily on the up-front type-checking, which melts away once the TS has been compiled to JS. This may lead to an inconsistent developer experience between importing the\u00a0.ts files directly vs. relying on the generated\u00a0.js files. Overall, my experience prototyping Reinhardt with TypeScript has been positive, and while my project is still rough around the edges, I expect it to help me clarify and document my code, and hope it will make my library more robust. By the way, did I mention Reinhardt is a free/open source project? Feedback, issues, and pull requests are highly appreciated! The library is fairly small right now, but if you would like a tour through the code base or just want to discuss the project, please email me (my address is on GitHub). By clapping more or less, you can signal to us which stories really stand out. Software Engineer in-training at Fullstack Academy"}, {"by": "prostoalex", "descendants": 0, "id": 17020608, "score": 1, "time": 1525784933, "title": "Decoding the brain's learning machine", "type": "story", "url": "https://www.sciencedaily.com/releases/2018/05/180503142656.htm", "text": "In studies with monkeys, Johns Hopkins researchers report that they have uncovered significant new details about how the cerebellum -- the \"learning machine\" of the mammalian brain -- makes predictions and learns from its mistakes, helping us execute complex motor actions such as accurately shooting a basketball into a net or focusing your eyes on an object across the room. In a summary of the study published on April 16 in Nature Neuroscience, the investigators provide a better understanding of why degenerative diseases that affect the cerebellum cause people to lose control of their movements. Their results demonstrate that the cerebellum is organized in a very different way than current designs of artificial neural networks, which are currently used in machine learning and artificial intelligence. Learning in the cerebellum -- a part of the vertebrate brain located at the back of the skull that directs and regulates movements -- is guided through a process of trial and error. For example, when learning to shoot a basketball, people usually miss many times before getting one shot through the hoop. As the arm moves, the cerebellum makes predictions about the consequences of the action. When the prediction does not match reality -- that is, the ball misses the hoop -- the cerebellum receives feedback from the eyes and the arm to learn from the error, fine-tuning factors such as aim, force and release to make a basket. This trial-to-trial learning from error produces gradual improvements in performance. However, research efforts have fallen short of decoding how the cells in the cerebellum make predictions and how they learn from their mistakes, according to Reza Shadmehr, Ph.D., professor of biomedical engineering and neuroscience at the Johns Hopkins University School of Medicine. \"Trying to understand the neural activity in the cerebellum is a bit like trying to decipher an alien language, a signal from a far-off galaxy,\" he says. To decode the signal, Shadmehr said the team began with the principal neurons in the cerebellum, called Purkinje cells. Purkinje cells communicate through two types of electrical signals: simple spikes, which reflect information regarding the prediction that the cells are making, and complex spikes, which reflect information that is sent back to the Purkinje cells, informing them of the error in their prediction. \"You can think of the simple spikes as the 'student' that makes a prediction and the complex spikes as the 'teacher' that provides feedback,\" says Shadmehr. In a past study, Shadmehr and his research team found that the Purkinje cells organize into small groups of about 50 and together make predictions, sending their output of all the members simultaneously. The neurons that make up these groups share a critical feature: They all receive the same error signal. Therefore, the fundamental computational unit in the cerebellum was not a single Purkinje cell, but a small group that learns together from a common mistake. While earlier work had tried to decipher the simple spikes of individual Purkinje cells, these results suggested that the key to understanding the cerebellum was to organize the Purkinje cells into groups and then count the sum total of simple spikes being produced by the members of the group. By doing so, Shadmehr and his team discovered that the group generated simple spikes that precisely predicted the motion of a monkey's eyes as it gazed around a screen. Having found a way to decipher the language with which the cerebellum made predictions, the next step was to understand how the cerebellum learns from its prediction errors. To investigate this question, the researchers measured cerebellar activity in seven rhesus monkeys fitted with electrodes and trained them to follow a small target with their eyes. The researchers replicated a \"mistake\" by moving the target more quickly than the monkeys could accurately predict, causing them to frequently miss the target. They found that simple spikes measured through the electrodes correlated with the intensity of movement -- more simple spikes meant that the monkeys needed to move their eyes more quickly to catch the target. According to the researchers, the Purkinje cells produced an average of 50-70 spikes per second. The researchers found that the complex spikes did not convey error in the same manner. Regardless of how far off the monkeys' eyes were from the target, the number of complex spikes produced in the cerebellum stayed the same. Instead, they found that the direction of the error affected the probability of generating a complex spike, whereas the magnitude of the error affected the timing of the complex spike. Therefore, the language of prediction was in rate of simple spikes, whereas the language of prediction error was in timing of the complex spike. For example, Shadmehr found that when the monkeys missed the target by 3 degrees of distance, a complex spike appeared after 100 milliseconds, and if the monkeys missed by 8 degrees, a complex spike appeared in 120 milliseconds. \"In short, the 'language' of the teacher is time, while the language of the student is rate,\" says Shadmehr. Shadmehr explains that current AI machines in the world rely on the same basic structure and are composed of many layers of neuronlike units that together make predictions. Layers of such units make calculations, passing results to the next layer until the process reaches the \"output\" layer that makes the final prediction. When an AI machine's prediction is wrong, it experiences an error and, with current AI networks, sends that error back to all units. \"These machines are inspired by the way biological networks learn, but what is interesting is that the architecture of the cerebellum is somewhat different than what has been designed in artificial systems,\" says Shadmehr. Instead of all the cells receiving complex spikes after the error, in the cerebellum the Purkinje cells organize into small groups, each specializing in responding to specific errors. The Purkinje cells appear to be organized based on a preference for error in only a small part of the task space. One of the advantages of the cerebellum's architecture, says Shadmehr, is that it protects memories. When you experience an error to the right, it causes learning by engaging Purkinje cells that register that error. If on the next trial you experience an error to the left, the new error engages a new group of Purkinje cells. As a result, new errors do not erase the memory, but result in two independent memories. Damage to the cerebellum can profoundly impair the ability to make movements, resulting in symptoms that together are called ataxia. A puzzle that has remained unsolved is how the cerebellum translates its predictions into motor commands that guide movements. This final result may make it possible to refine diagnosis of cerebellar disease by measuring learning from error and allow scientists to connect motor learning impairment with the loss of specific groups of Purkinje cells. Other researchers involved in this study include David Herzfeld of the Johns Hopkins University School of Medicine and Yoshiko Kojima and Robijanto Soetedjo of the Washington National Primate Center. This research was supported by the National Institutes of Health (R01NS078311), the National Eye Institute (R01EY019258, R01EY023277), the Johns Hopkins University Science of Learning Institute and the Office of Naval Research (N00014-15-1-2312). Story Source: Materials provided by Johns Hopkins Medicine. Note: Content may be edited for style and length. Journal Reference: Cite This Page: Get the latest science news with ScienceDaily's free email newsletters, updated daily and weekly. Or view hourly updated newsfeeds in your RSS reader: Keep up to date with the latest news from ScienceDaily via social networks: Tell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?"}, {"by": "rglover", "descendants": 0, "id": 17020605, "score": 1, "time": 1525784881, "title": "Clock of the Long Now", "type": "story", "url": "https://en.wikipedia.org/wiki/Clock_of_the_Long_Now", "text": "The Clock of the Long Now, also called the 10,000-year clock, is a mechanical clock under construction, that is designed to keep time for 10,000 years. The project to build it is part of the Long Now Foundation. The project was conceived by Danny Hillis in 1986. The first prototype of the clock began working on December 31, 1999, just in time to display the transition to the year 2000. At midnight on New Year's Eve, the date indicator changed from 01999 to 02000, and the chime struck twice. The two-meter prototype is on display at the Science Museum in London. As of December 2007, two more recent prototypes are on display at The Long Now Museum & Store at Fort Mason Center in San Francisco. The manufacture and site construction of the first full-scale prototype clock is being funded by Jeff Bezos' Bezos Expeditions, with $42 million, and is on land which Bezos owns in Texas.[1]   In the words of Stewart Brand, a founding board member of the foundation, \"Such a clock, if sufficiently impressive and well-engineered, would embody deep time for people. It should be charismatic to visit, interesting to think about, and famous enough to become iconic in the public discourse. Ideally, it would do for thinking about time what the photographs of Earth from space have done for thinking about the environment. Such icons reframe the way people think.\"[2] I want to build a clock that ticks once a year. The century hand advances once every one hundred years, and the cuckoo comes out on the millennium. I want the cuckoo to come out every millennium for the next 10,000 years. If I hurry I should finish the clock in time to see the cuckoo come out for the first time. The basic design principles and requirements for the clock are:[3] No clock can have a guaranteed lifetime of 10,000 years, but some clocks are designed with guaranteed limits. (For example, a clock that shows a four-digit year date will not display the correct year after the year 9999.) With continued care and maintenance the Clock of the Long Now could reasonably be expected to display the correct time for 10,000 years (But being a five-digit year date, carries with it a theoretical accuracy of one-hundred-thousand-years). Whether a clock would actually receive continued care and maintenance for such a long time is debatable. Hillis chose the 10,000-year goal to be just within the limits of plausibility. There are technological artifacts, such as fragments of pots and baskets, from 10,000 years in the past, so there is some precedent for human artifacts surviving this long, although very few human artifacts have been continuously tended for more than a few centuries. Many options were considered for the power source of the clock, but most were rejected due to their inability to meet the requirements. For example, nuclear power and solar power systems would violate the principles of transparency and longevity. In the end, Hillis decided to require regular human winding of a falling weight design for updating the clock face because the clock design already assumes regular human maintenance. However the clock is designed to keep time even when not being wound: If there is no attention for long periods of time the Clock uses the energy captured by changes in the temperature between day and night on the mountain top above to power its time-keeping apparatus.[4] The timing mechanism for such a long lasting clock needs to be reliable and robust as well as accurate. The options considered but rejected as sources of timing for the clock included:[3][5] Most of these methods are inaccurate (the clock will slowly lose the correct time), but are reliable (that is, the clock will not suddenly stop working). Other methods are accurate but opaque (meaning that the clock is difficult to read or understand). Many of these methods are accurate (some external cycles are very uniform over huge stretches of time) but unreliable (the clock could stop working completely if it failed to track the external event properly). Others have separate difficulties. Hillis concluded that no single source of timing could meet the requirements. As a compromise the clock will use an unreliable but accurate timer to adjust an inaccurate but reliable timer, creating a phase-locked loop. In the current design, a slow mechanical oscillator, based on a torsional pendulum, keeps time inaccurately, but reliably. At noon the light from the Sun, a timer that is accurate but (due to weather) unreliable, is concentrated on a segment of metal through a lens. The metal buckles and the buckling force resets the clock to noon. The combination can, in principle, provide both reliability and long-term accuracy. Many of the usual units displayed on clocks, such as hours and calendar dates, may have little meaning after 10,000 years. However, every human culture counts days, months (in some form), and years, all of which are based on lunar and solar cycles. There are also longer natural cycles, such as the 25,765-year precession of Earth's axis. On the other hand, the clock is a product of our time, and it seems appropriate to pay some homage to our current arbitrary systems of time measurement. In the end, it seemed best to display both the natural cycles and some of the current cultural cycles. The center of the clock will show a star field, indicating both the sidereal day and the precession of the zodiac. Around this will be a display showing the positions of the Sun and the Moon in the sky, as well as the phase and angle of the Moon. Outside this will be the ephemeral dial, showing the year according to our current Gregorian calendar system. This will be a five-digit display, indicating the current year in a format like \"02000\" instead of the more usual \"2000\" (to avoid a Y10K problem). Hillis and Brand plan, if they can, to add a mechanism whereby the power source generates only enough energy to keep track of time; if visitors want to see the time displayed, they would have to manually supply some energy themselves. Options considered for the part of the clock that converts time source (for example, a pendulum) to display units (for example, clock hands) include electronics, hydraulics, fluidics, and mechanics. A problem with using a conventional gear train (which has been the standard mechanism for the past millennium) is that gears necessarily require a ratio relationship between the timing source and the display. The required accuracy of the ratio increases with the amount of time to be measured. (For instance, for a short period of time the count of 29.5 days per lunar month may suffice, but over 10,000 years the number 29.5305882 is a much more accurate choice.) Achieving such precise ratios with gears is possible, but awkward; similarly, gears degrade over time in accuracy and efficiency due to the deleterious effects of friction. Instead, the clock uses binary digital logic, implemented mechanically in a sequence of stacked binary adders (or as their inventor, Hillis, calls them, serial bit-adders). In effect, the conversion logic is a simple digital computer (more specifically, a digital differential analyser), implemented with mechanical wheels and levers instead of typical electronics. The computer has 32 bits of accuracy,[2] with each bit represented by a mechanical lever or pin that can be in one of two positions. This binary logic can only keep track of elapsed time, like a stopwatch; to convert from elapsed to local solar time (that is, time of day), a cam subtracts from (or adds to) the cam slider, which the adders move. Another advantage of the digital computer over the gear train is that it is more evolvable. For instance, the ratio of day to years depends on Earth's rotation, which is slowing at a noticeable but not very predictable rate. This could be enough to, for example, throw the phase of the Moon off by a few days over 10,000 years. The digital scheme allows that conversion ratio to be adjusted, without stopping the clock, if the length of the day changes in an unexpected way. The Long Now Foundation has purchased the top of Mount Washington near Ely, Nevada, which is surrounded by Great Basin National Park, for the permanent storage of the full-sized clock, once it is constructed. It will be housed in a series of rooms (the slowest mechanisms visible first) in the white limestone cliffs, approximately 10,000 feet up the Snake Range. The site's dryness, remoteness, and lack of economic value should protect the clock from corrosion, vandalism, and development. Hillis chose this area of Nevada in part because it is home to a number of dwarf bristlecone pines, which the Foundation notes are nearly 5,000 years old. The clock will be almost entirely underground, and only accessed by foot traffic from the east once complete. Before building the public clock in Nevada, the foundation is building a full-scale clock of similar design in a mountain near Van Horn, Texas. The test drilling for the underground construction at this site was started in 2009. The site is on property owned by Amazon.com founder Jeff Bezos, who is also funding its construction. The lessons learned in the construction of this first full-scale 10,000-year clock will inform the final design of the clock in Nevada. The project is supported by the Long Now Foundation, which also supports a number of other very-long-term projects, including the Rosetta Project (to preserve the world's languages) and the Long Bet Project. Neal Stephenson's novel Anathem was partly inspired by his involvement with the project, to which he contributed three pages of sketches and notes.[6][7] The Long Now Foundation sells a soundtrack for the novel with profits going to the project.[8][9] Musician Brian Eno gave the Clock of the Long Now its name (and coined the term \"Long Now\") in an essay;[10] he has collaborated with Hillis on the writing of music for the chimes for a future prototype."}, {"by": "anarbadalov", "descendants": 0, "id": 17020602, "score": 1, "time": 1525784811, "title": "How far should science go to create lifesaving replacement organs?", "type": "story", "url": "https://undark.org/article/dilemma-science-ethics-organ-farming/", "text": "In the United States, the clock is ticking for more than 114,700 adults and children waiting for a donated kidney or other lifesaving organ, and each day, nearly 20 of them die. Researchers are devising a new way to grow human organs inside other animals, but the method raises potentially thorny ethical issues. Other conceivable futuristic techniques sound like dystopian science fiction. As we envision an era of regenerative medicine decades from now, how far is society willing to go to solve the organ shortage crisis?\n DILEMMAS:Of science, ethics, and us. The prospect of creating hybrid animals with human parts and killing them to harvest organs has already raised a slew of ethical questions. \n  I found myself pondering this question after a discussion about the promises of stem cell technologies veered from the intriguing into the bizarre. I was interviewing bioengineer Zev Gartner, co-director and research coordinator of the Center for Cellular Construction at the University of California, San Francisco, about so-called organoids, tiny clumps of organ-like tissue that can self-assemble from human stem cells in a petri dish. These tissue bits are lending new insights into how our organs form and diseases take root. Some researchers even hope they can nurture organoids into full-sized human kidneys, pancreases, and other organs for transplantation.\n \n  Certain organoid experiments have recently set off alarm bells, but when I asked Gartner about it, his radar for moral concerns was focused elsewhere. For him, the \u201creally, really thought-provoking\u201d scenarios involve other emerging stem cell-based techniques for engineering replacement organs for people, he told me. \u201cLike blastocyst complementation,\u201d he said.\n \n  Never heard of it? Neither had I. Turns out it\u2019s a powerful new genetic engineering trick that researchers hope to use for growing human organs inside pigs or sheep \u2014 organs that could be genetically personalized for transplant patients, in theory avoiding immune-system rejection problems. The science still has many years to go, but if it pans out, it could be one solution to the organ shortage crisis. However, the prospect of creating hybrid animals with human parts and killing them to harvest organs has already raised a slew of ethical questions. In 2015, the National Institutes of Health placed a moratorium on federal funding of this nascent research area while it evaluated and discussed the issues.\n \n  As Gartner sees it, the debate over blastocyst complementation research \u2014 work that he finds promising \u2014 is just one of many conversations that society needs to have about the ethical and social costs and benefits of future technologies for making lifesaving transplant organs. \u201cThere\u2019s all these weird ways that we could go about doing this,\u201d he said, with a spectrum of imaginable approaches that includes organoids, interspecies organ farming, and building organs from scratch using 3D bioprinters. But even if it turns out we can produce human organs in these novel ways, the bigger issue, in each technological instance, may be whether we should.\n \n  Gartner crystallized things with a downright creepy example: \u201cWe know that the best bioreactor for tissues and organs for humans are human beings,\u201d he said. Hypothetically, \u201cthe best way to get you a new heart would be to clone you, grow up a copy of yourself, and take the heart out.\u201d Scientists could probably produce a cloned person with the technologies we already have, if money and ethics were of no concern. \u201cBut we don\u2019t want to go there, right?\u201d he added in the next breath. \u201cThe ethics involved in doing it are not compatible with who we want to be as a society.\u201d\n \n  Other options, though, just might be.\n \u201cThe best way to get you a new heart would be to clone you, grow up a copy of yourself, and take the heart out.\u201d \n  Let\u2019s take a closer look at blastocyst complementation. For starters, it\u2019s worth noting that animal hybrids \u2014  including even lab animals that contain human cells or tissues \u2014 are not new. These are called interspecies chimeras, colorfully named after a mythological beast, and researchers have long experimented on this front. But the latest big advance unexpectedly came in a 2010 study, when a Japanese team led by geneticist Hiromitsu Nakauchi created mice that were unable to grow their own pancreases. However, when the rodents were still pre-embryos (at the so-called blastocyst stage), the biologists injected pluripotent stem cells from a rat into them. The pluripotent stem cells, which can transform into any cell type of the body, automatically filled in for the missing organ. Presto: The result was mice with functional rat pancreases.\n \n  Such were the origins of the chimera organ-making technique, and Nakauchi\u2019s group, today based at Stanford University, along with other research labs, are now setting their sights on producing human organs \u2014 by taking pluripotent stem cells that are generated from a person\u2019s skin or blood cells and implanting those into pre-embryos of pigs and sheep.\n \nFor some ethicists, human-animal chimeras, on the face of it, simply violate human dignity. Another worry is a small risk that pig-grown organs might transmit deadly swine viruses into the human population, a hazard that begs careful examination. And then there are fears about other unintended consequences. What if, for example, human stem cells inserted into pig pre-embryos form into human sperm or eggs, and the pig and human gametes could fertilize each other? Or what if human cells incorporate into the animals\u2019 brains \u2014 could that confer \u201chumanized\u201d thoughts or behaviors? Might the pigs even have freakish human-like external features?\n \n  The scientists at the forefront of this research have said that due to biological barriers, these effects shouldn\u2019t happen, and they\u2019re working to build in safeguards to ensure that human stem cells develop only as the desired target organs.\n \n  But perhaps the biggest moral considerations have to do with the ever-controversial issue of animal welfare. If a human liver or pancreas doesn\u2019t integrate well into a chimeric pig\u2019s innards, could that cause discomfort or alter the functioning of neighboring organs? Is it ethical to turn pigs into organ factories, bred solely for human purposes? On the latter, animal rights advocates would say no. But as long as it\u2019s societally acceptable to farm pigs for food, some ethicists point out, it\u2019s hard to argue that killing them for medical uses is wrong. \u201cI think in a world in which people still eat animals daily on a mass scale, using them for organ creation is maybe more ethical than eating them,\u201d David Shaw, a Scottish philosopher at Maastricht University in the Netherlands, told me.\n \n  Of course, in the broader picture, once you start looking at using pigs as organ banks, other conceivable scenarios come up \u2014 and, like Gartner at UCSF suggested, each may present its own moral can of worms.\n \n  Would monkeys or even chimpanzees, our closest evolutionary relatives, provide better hosts for human organs? And if it\u2019s wrong to use non-human primates to such ends, what if, as Shaw and a few other creative thinkers have speculated, it were someday possible to knock out a few key brain-development genes to create a \u201cbrainless\u201d host body? Theoretically, there would be no ethical objection to using brainless bodies of humans (or other primates) as organ farms, Shaw told me, since such entities wouldn\u2019t be \u201cpersons\u201d capable of experiencing consciousness, sentience or suffering. Indeed, an edgy blog post by two other academics discussed the hypothetical alternative of farming organs in brainless humans that are gestated in external artificial \u201cwombs.\u201d\n \u201cI think in a world in which people still eat animals daily on a mass scale, using them for organ creation is maybe more ethical than eating them.\u201d \n  If dystopian scenes from \u201cThe Matrix\u201d leap to mind, you\u2019re not alone. Still, what sounds crazy now might not seem crazy in 30 years. Who knows? Once upon a time, IVF was an alarming and distasteful or sacrilegious notion to many people, yet today, almost no one blinks an eye at test-tube babies.\n \n  In August 2016, nine months after holding a fall 2015 workshop to review the concerns about blastocyst complementation research, the NIH announced its intention to lift its funding ban \u2014 pending a proposed update to its policy restrictions on funding of stem cell science. That proposal drew more than 21,000 public comments, most of them against creating human-animal chimeras; some commenters called the work \u201cimmoral\u201d and \u201cdisgusting,\u201d akin to creating Frankenstein\u2019s monsters.\n \n  With the arrival of the Trump administration, the proposal has stalled, and the federal funding moratorium remains in place. Still, the research continues. But public sensitivities \u2014 such as those expressed in the feedback to the NIH \u2014 may be the ultimate barrier for radical organ-farming technologies, ethicists say. That\u2019s why starting a public conversation sooner, rather than later, to explain and discuss their potential benefits and costs for society is probably a good idea.\n \n  \u201cIt\u2019s important to listen to those concerns and to think about them really carefully and to address them,\u201d Jonathan Kimmelman, a bioethicist at McGill University in Montreal, Canada, told me. It\u2019s also important for people to realize that it usually takes years to decades for a new concept at the lab bench to reach medical clinics \u2014 and by then, the clinical application often looks quite different than the original idea, he added. Ideally, researchers should engage with the public \u201cnot after the science has developed, but as the science, or even before the science and applications, are developing.\u201d\n \n  In the meantime, Shaw, the philosopher, pointed out that expensive new organ-growing technologies will likely be affordable only to the rich. So he suggests that perhaps we can put speculations aside and make a greater effort to increase the supply of donor organs in the here and now, even if they don\u2019t provide perfect matches for recipients.\n \n  \u201cIf everyone registered as an organ donor and spoke to their families and told them they wanted to be organ donors,\u201d he told me, \u201cwe probably wouldn\u2019t be having this conversation.\u201d\n Ingfei Chen is a California-based writer whose stories have appeared in publications including Scientific American, The New York Times, and Spectrum. She is a former Knight Science Journalism fellow at MIT. Email us at dilemmas@undark.org  if you\u2019d like to seek input on a quandary of your own. Undark will talk to experts on ethics, philosophy, or standards of ethical scientific or journalistic practice and share their best wisdom on possible solutions. For those wishing anonymity, we\u2019ll withhold your name from any resulting Q&A items that we publish.  bioethics, blastocyst complementation, chimera, Curiosities, ethics, Ingfei Chen, organ farming, organoids, regenerative medicine Your email address will not be published. Required fields are marked *  Name *  Email *  Website   \n\n   Previous Article"}, {"by": "mlejva", "descendants": 0, "id": 17020591, "score": 1, "time": 1525784677, "title": "Cornucopia", "type": "story", "url": "https://en.wikipedia.org/wiki/Cornucopia", "text": "In classical antiquity, the cornucopia /\u02cck\u0254\u02d0rnj\u0259\u02c8ko\u028api\u0259, \u02cck\u0254\u02d0rn\u0259-/ (from Latin cornu copiae), also called the horn of plenty, was a symbol of abundance and nourishment, commonly a large horn-shaped container overflowing with produce, flowers or nuts.   Mythology offers multiple explanations of the origin of the cornucopia. One of the best-known involves the birth and nurturance of the infant Zeus, who had to be hidden from his devouring father Kronus. In a cave on Mount Ida on the island of Crete, baby Zeus was cared for and protected by a number of divine attendants, including the goat Amaltheia (\"Nourishing Goddess\"), who fed him with her milk. The suckling future king of the gods had unusual abilities and strength, and in playing with his nursemaid accidentally broke off one of her horns, which then had the divine power to provide unending nourishment, as the foster mother had to the god.[1] In another myth, the cornucopia was created when Heracles (Roman Hercules) wrestled with the river god Achelous and wrenched off one of his horns; river gods were sometimes depicted as horned.[2] This version is represented in the Achelous and Hercules mural painting by the American Regionalist artist Thomas Hart Benton. The cornucopia became the attribute of several Greek and Roman deities, particularly those associated with the harvest, prosperity, or spiritual abundance, such as personifications of Earth (Gaia or Terra); the child Plutus, god of riches and son of the grain goddess Demeter; the nymph Maia; and Fortuna, the goddess of luck, who had the power to grant prosperity. In Roman Imperial cult, abstract Roman deities who fostered peace (pax Romana) and prosperity were also depicted with a cornucopia, including Abundantia, \"Abundance\" personified, and Annona, goddess of the grain supply to the city of Rome. Hades, the classical ruler of the underworld in the mystery religions, was a giver of agricultural, mineral and spiritual wealth, and in art often holds a cornucopia.[3] In modern depictions, the cornucopia is typically a hollow, horn-shaped wicker basket filled with various kinds of festive fruit and vegetables. In most of North America, the cornucopia has come to be associated with Thanksgiving and the harvest. Cornucopia is also the name of the annual November Food and Wine celebration in Whistler, British Columbia, Canada. Two cornucopias are seen in the flag and state seal of Idaho. The Great Seal of North Carolina depicts Liberty standing and Plenty holding a cornucopia. The coat of arms of Colombia, Panama, Peru and Venezuela, and the Coat of Arms of the State of Victoria, Australia, also feature the cornucopia, symbolizing prosperity. In the book and film series The Hunger Games, the Cornucopia is filled with weapons, and is the starting point of the Games, and it is also the name of the Anthem of Panem. The horn of plenty is used for body art and at Halloween, as it is a symbol of fertility, fortune and abundance.[4] Coat of arms of Colombia Angel with cornucopia Base of a statue of\nLouis XV of France Coat of arms of\nCopiap\u00f3, Chile Seal of North Carolina Cornucopia as an object used in interior decoration Allegory of peace and happiness of the state. Eirene with cornucopia Coat of arms of\nHuntingdonshire, England Coat of arms of Peru Cornucopia in the Statue of Flora in Szczecin, Poland Coat of arms of Kharkiv, Ukraine"}, {"by": "panic", "descendants": 0, "id": 17020584, "score": 1, "time": 1525784572, "title": "Gaussian prime spirals (2012)", "type": "story", "url": "https://mathoverflow.net/questions/91423/gaussian-prime-spirals", "text": "\r\n                            Stack Exchange network consists of 173 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\r\n                         Imagine a particle in the complex plane, starting at $c_0$, a Gaussian integer, \nmoving initially $\\pm$ in the horizontal\nor vertical directions.  When it hits a Gaussian prime, it turns left $90^\\circ$.\nFor example, starting at $12 - 7 i$, moving initially $+x$, this closed circuit results:\n\u00a0\u00a0\u00a0\u00a0\u00a0\nInstead, starting at $3+5 i$, again $+x$, this (pleasingly symmetric!) closed cycle results:\n\u00a0\u00a0\u00a0\u00a0\u00a0\nHere's another (added later), starting at $5+23 i$:\n\u00a0\u00a0\u00a0\u00a0\u00a0\n(Gaussian primes $a+bi$ off-axes have $a^2+b^2$ prime; on axis, $\\pm(4n+3)$ prime.)\nMy question is, Q0.What's going on? More specifically, Q1. Does the spiral always form a cycle? Q2. Have these spirals been investigated previously? (I am about to step on a plane; apologies for not acknowledging responses!)\n...Later: Q3. Under what conditions is the spiral (assuming it closes) symmetric w.r.t. reflection in a\n  horizontal (as is $12-7 i$), or reflection in a vertical (as is $5 + 23 i$),\n  or reflection in both (as is $3 + 5i$)? Expanding slightly on Greg Martin's answer, the symmetry applies across the imaginary axis as well as the real axis, so the only way a path can avoid closing up is if it crosses at most one axis at most once (and if it does cross an axis, the path -- if extended in the backward as well as forward direction -- will be mirror symmetric with respect to the axis it crosses).  Note that within a quadrant the horizontal (respectively vertical) steps are alternately toward and away from the imaginary (respectively real) axis.   Intuitively the steps, on average, get larger the further you are from the axes.  So if you're in the first quadrant and take a step to the right and then a step up, you expect your next step, to the left, to be somewhat larger than your previous step to the right (and similarly with the next step down, assuming you're still in the first quadrant).  So in a sense the axes are exerting kind of a gravitational tug on sort of a random walk.  Of course, nothing of the sort is literally going on.  Still, it seems hard to imagine the stars (I mean Gaussian primes) aligning to keep a random walk going in perpetuity. It seems more likely one might encounter closed paths that don't have any nice symmetry, such as \n$$(a,b) \\rightarrow (a+4,b) \\rightarrow  (a+4,b+8) \\rightarrow (a-2,b+8) \\rightarrow$$ $$(a-2,b+4) \\rightarrow (a+2,b+4) \\rightarrow (a+2,b+6) \\rightarrow (a,b+6) \\rightarrow (a,b)$$\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n(Sorry, if someone could replace that with a picture, that would be helpful.)  As \"unknown\" pointed out, there are certainly closed square paths that stay in the first quadrant.  There are also rectangles, such as the $2\\times4$ rectangle with $8+13i$ for its lower left hand corner and the $2\\times6$ one starting at $14+19i$.  (I spotted these in a picture of Gaussian primes of norm less than 1000 in the paper \"A Stroll Through the Gaussian Primes\" by Gethner, Wagon, and Wick.)  One might expect a souped-up (supped-up?) $k$-tuple conjecture to predict the existence of any closed-path pattern that isn't forbidden by the usual suspects.  (Part of the souping up, though, is that not only are there primes at the specified corners, but everything else along the edges is composite.) All in all, a nice problem -- and the spirals, reminiscient of Celtic knots, are really lovely! Stan Wagon, coauthor of the award-winning article\n\"A Stroll through the Gaussian Primes,\"\nbecame intrigued, and sent me these two stunning images.\nThe first is a different rendering of the 2,956-cycle I posted earlier:\n \u00a0\n\nThe second is a\nRorschach-like\ncycle of\n316,268 primes he found:\n \u00a0\n\n(seed: $232+277 i$). [Apologies for the loss of resolution converting for this forum.] Soon there will be a Mathematica Demonstration Project on this topic; it currently awaits approval. Update. The Mathematica Demo is available at this link: \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nUsing a modification of this code, Stan has found a cycle of length 3,900,404. Seed: $107 + 992 i$.\n Just for your amusement,\nhere is a spiral of length 2,956 primes that crosses just one axis, and hits no\nprime on that axis.\nI encountered it twice, once in a horizontal orientation (seed: $12+28 i$)\nand once in a vertical orientation (seed: $43 + 55 i$).\nThat it should occur twice is a consequence of the symmetry of the conditions\nthat render a Gaussian integer prime.\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n Agreed, nice spirals. This is not meant as a complete answer. Heuristically and experimentally, it is very likely that there are infinitely many $x,y\\in\\mathbb{Z}$ such that $x^2+y^2$, $(x+2)^2+y^2$, $x^2+(y+2)^2$ and $(x+2)^2+(y+2)^2$ are simultaneously prime. (For example, PARI gave $136$ such $(x,y)$ with $1\\leq x,y\\leq 1000$ and $1330$ such $(x,y)$ with $1\\leq x,y \\leq 5000$.) Such a result will give Gaussian primes at the corners of a square of side length two and hence infinitely many of the simplest type of cycle. Though it may not directly solve your question, it should be noted that you do get infinitely many squares with Gaussian primes at the corners if you allow the lengths of the sides of the squares to vary, due to the following result: T. Tao, The Gaussian primes contain arbitrarily shaped constellations, J. d.Analyse Mathematique 99 (2006), 109--176. Also, heuristically, if you believe the kind of heuristics behind the Hardy-Littlewood $k$-tuple conjecture, the same kind of reasoning should give you the conjectural asymptotics for  $$\n\\sum_{\\substack{x,y\\in \\mathbb{Z}\\\\1 \\leq x,y\\leq N}}\\Lambda(x^2+y^2)\\Lambda((x+2)^2+y^2)\\Lambda(x^2+(y+2)^2)\\Lambda((x+2)^2+(y+2)^2).\n$$ Note that if you start above the real axis, and then one of the steps takes you below the real axis, then the subsequent path will be the mirror image (complex conjugate) of the start of the path. In your second picture, this accounts for the whole left half of the spiral (starting at $3+5i$ and pausing at $3-5i$). Then, if the continuation happens to cross the real axis again, the rest of the path will close up into a closed loop. (This ignores the case where the point on the real axis itself is a Gaussian prime.) So your question has to do with the (equally mysterious) question of when such paths will hit Gaussian primes of the form $m+ni$ where none of $m$, $m+i$, ..., $m+(n-1)i$ are prime. I wanted to report on a variation, Gaussian prime random walks.\nAs before\nstart with any point in the complex plane $c_0$. Walk in the $+x$ direction\nuntil you hit a Gaussian prime.\nUnlike the original question, which requires a $90^\\circ$ counterclockwise turn,\nturn left or right by $90^\\circ$ with equal probability. Continue turning randomly $\\pm 90^\\circ$ at every Gaussian prime hit, until some Gaussian prime is hit for a\nsecond time. Call that a Gaussian prime loop. Below is an example:\n *\n\"It's unknown whether there are infinitely many Gaussian primes of the form $n+i$ with $n \\in \\mathbb{Z}$. So starting at $N+i$ and moving $+x$, we cannot exclude the possibility of hitting no prime.\"\n   \r\nBy posting your answer, you agree to the privacy policy and terms of service. asked 6 years, 1 month ago viewed \n8,131 times\n active yesterday \r\nsite design / logo \u00a9 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0\r\n                            with attribution required.\r\n                    rev\u00a02018.5.8.30316\n"}, {"by": "john58", "descendants": 0, "id": 17020583, "score": 1, "time": 1525784556, "title": "Apple is worth $945B \u2013 more than ever", "type": "story", "url": "https://www.recode.net/2018/5/7/17327866/apple-worth-945-billion-stock-market-value", "text": "The most valuable public company in the world just got more valuable. Apple\u2019s market cap reached an all-time high today of $945 billion. Apple reported totally fine second-quarter earnings last week. But what really sent the stock up was Warren Buffett\u2019s announcement Friday that his company Berkshire Hathaway would be purchasing an additional 75 million shares in Apple, giving it a 5 percent stake. The stock is currently trading at $186, up 10 percent since the beginning of the year.  As of the end of 2017, Vanguard Group and BlackRock were Apple\u2019s biggest shareholders.  Sign up for our Recode Daily newsletter to get the top tech and business news stories delivered to your inbox. Sign up for our Recode Daily newsletter to get the top tech and business news stories delivered to your inbox. Google will be showcasing all the AI, Android and smart home updates. Amazon\u2019s Tim Stone is in. Vollero is out. The company\u2019s internal investigation as well as the federal investigation are ongoing. First Sony, now Warner Music Group. A Verge affiliate site"}, {"by": "doener", "descendants": 0, "id": 17020576, "score": 1, "time": 1525784486, "title": "A Portrait Series featuring the people shaping Berlin\u2019s crypto ecosystem", "type": "story", "url": "https://salon.thefamily.co/berlincryptocapital-a-portrait-series-featuring-the-people-shaping-berlins-crypto-ecosystem-74686e81af45", "text": "Decentralization is in Berlin\u2019s DNA on multiple levels. Berlin has one of the world\u2019s most vibrant political histories, and as mentioned by numerous people, that history shapes the way the city thinks, talks & acts. In the past century, Berlin basically got unbundled, notably after WWII. It happened politically, through the L\u00e4nder re-organization and Berlin separation, but also economically: the car industry went to Stuttgart and Munich, press and insurance to Hamburg, finance to Frankfurt, etc. In a sense, Berlin is already the capital of a decentralized country, fertile ground for the crypto economy \ud83d\ude42. Then Berlin became the center of the Cold War (1947\u201391), \u201ca huge geopolitical struggle, its citizens being tugged at by different governments halfway across the world\u201d, as Robert from Polkadot told me. This rampant conflict bred one of the greatest state surveillance initiatives in history: the Stasi (1950\u201389). Headquartered in Berlin, the Stasi was the official state security service of the GDR (East Germany). It was so infiltrated into life in the GDR that one out of every 63 East Germans was collaborating with it. By the time East Germany collapsed in 1989, the Stasi counted 91,015 employees and another 173,081 informants. They would drill tiny holes in apartments and hotel rooms to film citizens with special video cameras. Just look at this one\u200a\u2014\u200athat\u2019s some serious 1950s-spying skills! No wonder why privacy & censorship resistance are key topics around Berlin\u2026 Internationally, Berlin brought Soviet and NATO powers closer together than anywhere else and the city became the world\u2019s spy capital, with both sides trying to gather info about the other. As it became impossible to pass from East to West following the erection of the Berlin Wall (1961), espionage shifted from people-focused to tech-focused. The Teufelsberg listening station is still a vestige of Berlin\u2019s spying history, even if it\u2019s now a counter-culture hot-spot. There has been so much spy-related history that Berlin even has a spy museum! With a bit of historical context, it\u2019s easier to understand how the Berlin counter-culture was born. Squatting spots, musical experiments, anti-establishmentarianism, Berlin has it all according to Joshua from Vaultoro and Brian from Epicenter. One must note that vacant buildings & military service conscription also helped Berlin attract flocks of creative young people, wired with a sense of newfound freedom & responsibility to create a better future for the \u201clost generation\u201d, as Pawel from Point9 puts it. This going-around-the-system mindset is not only rooted in political activism but also in technological activism. Most of the people from the Berlin crypto scene linked the emergence of the movement with the foundation of the Chaos Computer Club, the world\u2019s oldest hacking group. Worried about the rising fascism and a possible tech-enabled Stasi 2.0, these hacktivists grew from a handful to now 5,500 members that reached 11,000 attendees at its 2016 annual events. They\u2019ve been introducing people to data privacy through their crypto-parties (no, they\u2019re not connected to the Kitties one\u2026) where they guide participants in starting to use Tor, CloudFlare, PGP & DuckDuckGo. As a matter of fact, Greg from IPDB identified the Tor project as being \u201clargely Berlin-based\u201d and a \u201ccommunity that has been smashing at the edges of cyberculture and building up the radical side of the internet\u201d. Trent from Ocean Protocol added that Wikipedia has its largest non-US engineer office here, too. That sounds very Berlin-y to me \ud83d\ude0e That\u2019s how Berlin became the new ideological center of the decentralized world. The city suffered from the deep absence of privacy and developed ideologies & strong communities in which these questions have been central for a very long time. As Robert from Polkadot puts it, \u201cthe people making the crypto-currencies scene have been in Berlin for decades\u201d. And that\u2019s why after the NSA scandal, lots of security experts flew towards Berlin for political reasons, as mentioned by Zoe from Neufund. For Ele from OSCoin, if Berlin is the ideological center of this new technological movement, it\u2019s because there\u2019s a strong upside to \u201cnot being in Silicon Valley, as it forces us to rethink some stuff and creates some space for us to think about certain things differently. We have our own space to create our own things.\u201d It\u2019s a view shared by lots of people around Europe, and Martin from Gnosis agrees: \u201cIt\u2019s quite natural that Ethereum didn\u2019t crystalize in the Valley as its main purpose is about changing the way the economy works, not just optimizing it.\u201d And talking about differences with San Francisco, folks in Berlin regularly talk about prices. For Robert from Nakamoto, low-rents & cheap Berlin cost-of-living enabled lots of early believers to #HODL. He actually lived in a car for 2 years so as not to sell his bitcoins, \u201ctoo busy to spend the money anyway!\u201d This was a key thing for lots of early pioneers, being able to afford a decent living while exploring new horizons. Berlin is economically attractive not only because it\u2019s cheap but also because of taxes. According to J\u00f6rg from Bitwala, Germany was\u2014in 2012\u2014the 1st government to regulate crypto, declaring it as a Unit of Account. \u201cIt\u2019s tax-free if you hold more than 1 year\u201d, he told me. This early regulatory move triggered a wave of people moving to Berlin from all over the world. The city\u2019s attraction doesn\u2019t stop at taxes! Berlin is a truly international city, owned by no one (Brian from Epicenter). \u201cBerlin is a completely blank slate\u201d as Peter from the Web3 foundation puts it. By nature, the city became the European capital with the widest range of interests (Ele from OSCoin)\u200a\u2014\u200acomputer science, economics, politics, arts, history\u2026 The city mixes people from lots of different fields. Like Bitcoin, Berlin is \u201cthe result of different ideas from diverse fields being put together in such a magical way\u201d (Ele from OSCoin). As Berlin isn\u2019t dominated by any one industry, so the crypto-scene isn\u2019t geared towards any, as for example London would be for blockchain enterprise financial applications. Berlin has this inclusive international mindset (Cassidy from Chlu) for multiple reasons: It\u2019s a decentralized world but as Radoslav from Bitbond puts it, \u201cWe\u2019re human beings after all, we are wired to socialize with people whom we share common interests with.\u201d People in Berlin definitely shared an interest in privacy, but guess what they also shared? Beers! Almost everyone in Berlin roots their \u201cA-ha!\u201d crypto moment to Room77, which in May 2011 became the world\u2019s first brick-and-mortar establishment to accept bitcoin. The bar also started hosting the Berlin Bitcoin meetup in 2013, and as every German TV station mentioned it, it drew lots of folks. Business school drop-outs, anarchists, artists, consultants, everybody could share a beer & talk crypto. The 1st mobile wallet was developed by a guy who was tired of bringing his laptop to the counter to pay his tab (Brian from Epicenter)! And even if lots of people are working remotely around the world, even if most of the teams working on protocols look like the Tower of Babel, the scene needs to gather around some physical spaces. And that\u2019s why Martin launched Full Node, Europe\u2019s largest blockchain co-working space! \u201cDecentralized or not, it made a lot of sense to be close to those you\u2019re dependent on\u201d he commented, while Gnosis was hosted in the Ethereum office. To be fair, Ethereum played a key role in Berlin\u2019s crypto growth. Their Berlin office really became a global hub, Fabian told me. Most of the EthCore Devs are there, and that brought lots of projects to the city. A pretty impressive list of projects from the Ethereum ecosystem are in Kreutzberg, one of the city\u2019s hottest neighborhoods: Gnosis, Raiden, Parity, Web3 foundation, Truebit, Polkadot, Cosmos, enough to attracts lots more in the future. To say the least, I\u2019m pretty excited about the Berlin crypto scene. This combination of political history, ambition & international context gives me hope that the city will keep attracting top talents into its net. I\u2019m also confident that Berlin\u2019s inclusive community will make the best out of them! Of course it\u2019s just starting and like any ecosystem its success will ultimately be measured by the success of its members. On that note, one thing that\u2019s still missing is the emergence of a local scaled-up crypto-company like Ripple or Coinbase, which are now clocking in at ~500/600 employees\u2014as wisely mentioned by Brian from Epicenter. This is the next step, so let\u2019s #BUIDL\u00a0:) On our side, we\u2019ll keep on organizing dinners, events & bridges among crypto-ecosystems in Europe! By clapping more or less, you can signal to us which stories really stand out. Director, Berlin at @_TheFamily\u200a\u2014\u200aNow shooting #BerlinCryptoCapital \ud83d\udcf8 Education, Unfair Advantages & Capital for European Founders"}, {"by": "dcsadmin", "descendants": 0, "id": 17020564, "score": 4, "text": "I&#x27;ve decided to bite the bullet and release Bitbox.co and get some feedback. I&#x27;ve been coding for years and one of the things I constantly see companies do is write and rewrite a document storage system, so I decided to write one for them. Bitbox.co uses cloud storage (currently Azure) to store documents but can be coded to use any cloud service. Documents can be public or private and URLs to that document can be used directly in HTML. There are currently a list of WebAPIs for developers to use as a tool, or build an entire system around.<p>This is an MVP and it&#x27;s simple. Upload a file, get a token; like a valet or coat check. When you want that file back, send the token and you get a byte stream. There is also a built in file converter, so you can can upload a docx file and retrieve it as a pdf by appending .pdf to the URL, for example: https:&#x2F;&#x2F;www.bitbox.co&#x2F;files&#x2F;1.pdf, or perhaps a tiff: https:&#x2F;&#x2F;www.bitbox.co&#x2F;files&#x2F;1.tif. Files can be marked as public or private, and account owners can invite other users to join their team with read, write or admin privileges.<p>There are some directions I want to possibly go:<p><pre><code>  - Allow users to select their own cloud store (Azure, AWS, etc) rather than the built in store.\n  - Allow users to replicate their files between cloud stores, so you would store your documents on all your cloud stores in Azure or AWS, etc.\n  - Folder structures for organization\n  - Integration with third party platforms like WordPress or SurgarCRM\n  - Cookbook for code examples on how to connect through the WebAPI in various languages.\n  - Local install so developers can use their internal SMB shares to store files.\n  - File level security, and &#x2F; or folder level security for users &#x2F; files.\n</code></pre>\nThis was designed for IT departments without a strong development team to save money. Does anyone have a need for something like this?<p>www.bitbox.co&#x2F;Landing&#x2F;HN", "time": 1525784323, "title": "Bitbox.co \u2013 Cloud Storage for Developers", "type": "story"}, {"by": "ilamont", "descendants": 0, "id": 17020562, "score": 2, "time": 1525784274, "title": "Amazon Froze My Account and I Still Don\u2019t Know Why", "type": "story", "url": "https://bardsandsages.com/juliedawson/2018/05/06/amazon-froze-my-account-and-i-still-dont-know-why/", "text": " Where do I begin\u2026 On May 2nd, I got an accusatory email from Amazon, claiming that one of my Society of Misfit Stories ebooks had \u201cmisleading metadata\u201d and had been deactivated. Now before I get too far into this story, for those of you that are not tuned in to the soap opera that has become Amazon publishing as of late, metadata abuse is a major problem on Amazon. If you ever came across a book with a title like In Love With the Werewolf Billionaire (billionaire alpha shifter romance bestseller book three Sexy Werewolf Billionaire Series) then you know things like keyword stuffing and bad metadata is a thing. But the thing is, my metadata is pristine. Because, you know, I\u2019ve been involved in publishing for over a dozen years now and understand professional norms. So, the title and author name in the metadata for EVERY BOOK matches the title and author on the cover, and both match the title and author on the title page. Because, that is what you are supposed to do. And while the ebook was in Select, there was no weirdness going on with it. Each ebook in the series is literally: title page, story, one-page author bio. THAT IS IT. No links to competitors. No stuffing of books within other books to increase page reads. No funny coding to trick the system into thinking more pages have been read than were actually read. Title page, story, author bio. That is ALL that is in each entry. There was some vague reference to the name of a famous author or personality. No, the email did not NAME the famous personality, and when I used Amazon search to see if there was another author with the same name as my author, nothing came up. So I sent Amazon an email that I didn\u2019t understand what they were talking about. The author name was correct. Please reactivate the story. A short time later, I tried to log into my KDP account to upload the newest Misfit story and\u2026I was frozen out of my account. I couldn\u2019t even access KDP support. I had to go through regular Amazon support to reach a person, who forwarded my complaint to KDP support. Thus begin the bizarre story of my argument with the Bots of Amazon. Most people have suspected for a while now that Amazon was relying way too much on automation. The story you are about to read is evidence of just how ridiculous this has gotten. The following day, I received this email: During a quality assurance review of your catalog, we found that you have published books with intentionally misleading metadata. Here are some examples of the books you have submitted: The Society of Misfit Stories Presents\u2026[I have removed the title to avoid embarrassing the affected author] As stated in our metadata guidelines, we don\u2019t accept content that is meant to advertise, promote, or mislead, because such content may lead to inaccurate or overwhelming search results or impair our readers\u2019 ability to make good buying decisions. As a result, we have temporarily suspended your account and we request that you respond to content-review@amazon.com with the following declaration: \u201cI confirm that I have read and will comply with the Content Guidelines [https://kdp.amazon.com/self-publishing/help?topicId=A2TOZW0SV7IR1U] and [https://kdp.amazon.com/self-publishing/help?topicId=A294SHSUYLKTA6], and that I will remove any previously published books that do not meet these guidelines.\u201d Until we receive a response from you regarding this issue, your account will remain blocked. If you would like to review the KDP Content Guidelines, please visit: https://kdp.amazon.com/self-publishing/help?topicId=A2TOZW0SV7IR1U. So instead of explaining what they think I did, they send me a notice requiring me to promise not to do again what I have no idea what I have done. So I email them AGAIN asking them to please explain what, exactly, they think I did. Because I am very familiar with the TOS and for the life of me I have no idea what I am being accused of. A few hours later, I get the following reply: Hello, For information regarding your account, please reference the message that we sent you on May 3, 2018: we request that you respond to content-review@amazon.com with the following declaration: \u201cI confirm that I have read and will comply with the Content Guidelines [https://kdp.amazon.com/self-publishing/help?topicId=A2TOZW0SV7IR1U] and [https://kdp.amazon.com/self-publishing/help?topicId=A294SHSUYLKTA6], and that I will remove any previously published books that do not meet these guidelines.\u201d To reinstate your account please follow the instructions provided in the above email. Amazon.com At this point, I send ANOTHER email asking them what they think I did. I can\u2019t fix something that I don\u2019t know what the problem is. I also forwarded the whole thing to Jeff Bezos\u2019 email address, as sometimes he will get involved or, at least, whomever answers his emails gets involved. Because I NEED TO TALK TO A HUMAN BEING! On May 4th, I get the following response: Hello, We have temporarily suspended your account and we request that you respond to content-review@amazon.com with the following declaration: \u201cI confirm that I have read and will comply with the Content Guidelines  and , and that I will remove any previously published books that do not meet these guidelines.\u201d Until we receive a response from you regarding this issue, your account will remain blocked. If you would like to review the KDP Content Guidelines, please visit: https://kdp.amazon.com/self-publishing/help?topicId=A2TOZW0SV7IR1U. Best Regards, Camila H Best Regards,  Ian S Bangs head on table A short time later, I DID get an email from someone at KDP Executive Customer Relations, that they were investigating my problem and would get back to me. FINALLY! A human being was involved! Then, on the 5th, I got another generic email from Amazon, making the same, identical statement demanding that I promise not to do what I have no idea what they think I did ever again. Now at this point, your Sithiness here is ready to Force Choke someone. Because it is so painfully\u2026.HILARIOUSLY\u2026.obvious that no human being is reading my email queries that I decide to set a trap for the bot. So at 6 PM Saturday night, I sent the following email to their content-review address:  As per the request below: \u201dI confirm that I have read and will comply with the Content Guidelines [https://kdp.amazon.com/self-publishing/help?topicId=A2TOZW0SV7IR1U] and [https://kdp.amazon.com/self-publishing/help?topicId=A294SHSUYLKTA6], and that I will remove any previously published books that do not meet these guidelines.\u201d As I have repeated the requested statement, please unfreeze my account.  I am repeating this statement at your request. This is not an admission of guilt, as you have not actually informed me of what I an allegedly guilty of. I am complying with an arbitrary request to repeat information in an automated email. Repeating this statement in no way acknowledges the validity of the accusations Implied, since the accusations have never actually be clarified in any way, shape, or form. So, what do you think are the chances that the bot scanned the response for the required phrase and then unfroze my account without a human being ever reading it? If you answered \u201cHighly likely\u2026YOU WOULD BE CORRECT! Because by Sunday morning, my account was unfrozen, and I got a lovely, accusatory bot response informing me not to do it again or I could lose BOTH my KDP and Createspace accounts. Still have no idea what I did, however. Since the offending story was reinstated with no changes to it. I started publishing in 2004, before the Kindle was a thing. I started with mobipocket.com and was part of the initial transition into KDP when it launched. I was an early adopter of the Createspace platform. I have been publishing with Amazon for over a decade and have always strived to present professional material. To have a robot accuse me of unethical behavior and demand that I promise not to do it again is insulting and borderline libel. Anyone who has ever worked with me knows I am one of the most open and transparent publishing in the indie community. To have an automated algorithm make an arbitrary decision that I broke some unexplained rule is enough to make me breath fire. So what does all this mean? For starters, it means I will no longer be making any Bards and Sages Publishing titles available exclusively through Select. All current Society of Misfit Stories locked into the 90-day exclusivity will remain available in Select until the 90-day window expires. Future Society of Misfit Stories entries will be published at Smashwords.com. This means they will also be available in epub and PDF formats, as well as mobi format. They will still be removed from sale after the six-month window.  Secondly, I\u2019ve already stopped using Createspace for new releases. I\u2019m going to start transitioning all of our older titles off Createspace platform. The books will still be available in print on Amazon, but they will no longer be under Amazon\u2019s bot-tastic thumb where they will be able to lock me out of my own books. Finally, while I will continue to upload ebooks to Amazon for now, I\u2019m going to pursue other affiliate programs and stop driving traffic to Amazon listings entirely. In truth, Amazon has never been my primary market anyway, and if they don\u2019t care enough about their publishers to have actual people make decisions that impact author livelihoods, I don\u2019t know how much more business I want to pursue with them. I don\u2019t expect Amazon will even notice or care about these changes. I\u2019m not a seven-figure seller, and therefore I don\u2019t matter to them. That is apparent from the treatment I received this weekend. Unfortunately, the treatment I received this weekend is indicative of how they treat everyone that is not a seven-figure seller. Amazon is not a friend of authors. They see us as interchangeable commodities to be used up, worn out, and replaced: like widgets.  You must be logged in to post a comment. Your email:  Julie Ann Dawson is an author, editor, publisher, RPG designer, and advocate for writers who may occasionally require the services of someone with access to Force Lightning (and in case it was not obvious, a bit of a geek).  Her work has appeared in a variety of print and digital media, including such diverse publications as the New Jersey Review of Literature, Lucidity, Black Bough, Poetry Magazine, Gareth Blackmore\u2019s Unusual Tales, Demonground, The Philadelphia Inquirer, and others.  In 2002 she started her own publishing company, Bards and Sages.  The company has gone from having two titles to over one hundred titles between their print and digital products.   In 2009, she launched the Bards and Sages Quarterly, a literary journal of speculative fiction. Since 2012, she has served as a judge for the IBPA's Benjamin Franklin Awards. "}, {"by": "uptown", "descendants": 0, "id": 17020554, "score": 1, "time": 1525784190, "title": "Facebook will not accept referendum ads from advertisers outside of Ireland", "type": "story", "url": "https://www.facebook.com/notes/facebook-dublin/facebook-will-not-be-accepting-referendum-related-ads-from-advertisers-based-out/10156398786998011/", "text": ""}, {"by": "jorymackay", "descendants": 0, "id": 17020547, "score": 1, "time": 1525784106, "title": "How MapMe founder Ben Lang prioritizes projects", "type": "story", "url": "https://blog.rescuetime.com/ben-lang-interview-consistency/", "text": "RescueTime Blog Helping you stay happy and productive in the modern workplace It can often feel like there isn\u2019t enough time in the day. Between work, side projects, spending time with friends and family, learning new skills, and basic maintenance (don\u2019t forget to eat and sleep!) our daily 24 hours gets filled quicker than we\u2019d like. Which probably explains our current cultural obsession with productivity. However, getting the most out of our days isn\u2019t simply about doing more. But about doing the right things. For Ben Lang, prioritization, focus, and time management have been issues that are constantly top of mind. Because they\u2019ve had to be. From founding and running his own company, to working as a full-time marketer for AI-powered help desk, Spoke, to wrangling time for his multiple side projects, events, and online communities, Ben\u2019s days are especially packed. Which begs the question: How do we get the most out of our days? Especially when we have so much we want to get done. In this interview, we spoke to Ben about his daily routine, how he prioritizes the work that matters most, and the importance of consistency when it comes to hitting your long term goals. We\u2019ve written about the importance of having a morning routine in the past. But talking to Ben, the importance of starting your day off right becomes especially clear: \u201cI wake up at 7am every day. Weekends too. And start with something physical like a game of tennis or yoga before spending an hour on my computer going through my current projects and getting prepared for the day.\u201d \u201cI wasn\u2019t always like this and there was definitely a hard learning period at first where I\u2019d get up later or sleep in on the weekends. But what I discovered was that it wasn\u2019t until I started to be very consistent about waking up to my alarm, every single day, that I started to see results. \u201cThose first hours are my most productive and I was wasting them by not being committed to a schedule.\u201d As the old saying goes,\u00a0\u201cYou overestimate what you can accomplish in a week, but underestimate what you can accomplish in a year.\u201d Yet when most people talk about productivity, they discuss the things you can do in the moment. Like blocking distracting websites, prioritizing your workday, and protecting yourself from unnecessary interruptions. Talking to Ben, however, it seems like the most important things you should optimize for are outside the workday. Things like getting exercise, eating right, and sleeping enough. Or, the sort of general wellness that bleeds into the rest of our day but gets ignored when we typically talk about productivity. With the right start to your day, the next question is what to do once the day has started? When you\u2019re juggling multiple projects this isn\u2019t as easy as it sounds. First, you need to decide what the right work is. Then, find ways to protect yourself from everything else that wants to take your attention away from working on it. \u201cWhen you\u2019re passionate about so many different ideas and projects, how do you balance all your commitments and feel like you\u2019re actually making progress on them?\u201d \u201cThat\u2019s been a struggle for me, because we can all go ahead and work on a million things but not move forward with any of them. So the question then becomes not what do you want to do, but what do you want to actually finish?\u201d In this way, Ben says that consistency is more important than perfection. Rather than obsessing over all the things we could do. It\u2019s better to simply put the time in every single day to work on what is most important to you. Yet while consistency makes sure that we\u2019re spending some time every day on our most important work, how do we make the most of that time? \u201cI use products like RescueTime to see where I\u2019m spending my time and measure myself. And the weekly emails are super helpful in seeing what I\u2019ve been spending my time on. But, I think I\u2019m still in the phase of trying different productivity methods out.\u201d While he\u2019s still perfecting his personal productivity methods, Ben suggested a few paths to help protect your focused time: There will always be things that want to take your attention away. But by using the right tools, being organized, and committing to consistency we can put ourselves on the right track towards doing more focused work when it matters. When you have multiple projects and interests on the go all at once, you more than likely will be moving around a fair bit. And few things kill our productivity like being in an uncomfortable work environment. For Ben, who has worked in Tel Aviv and San Francisco as well as throughout Southeast Asia, he quickly discovered that having a consistent work environment was a major productivity booster. \u201cFor me, one of the most important things is to have a very comfortable working environment. Which makes it hard to just go to a cafe and do work, because I want a comfortable chair and the right monitor height and all those things.\u201d But we can\u2019t always be working in the perfect situation. Which is why Ben suggests investing in the tools you need to make an environment comfortable for your working style. \u201cI love my Roost laptop stand and can\u2019t go somewhere and work without it. I also bring a mouse and keyboard with me if I\u2019m working out of the office. Noise is also a significant factor in my ability to focus, and I\u2019ve become a huge convert to my Airpods as they do a great job of drowning out background noise.\u201d Lastly, as someone who has spent time both leading and working for companies, Ben has a unique view on the connection between control and time management. Yet, while it might seem like you need total control over your time to be the most productive with it, Ben explains how\u00a0there\u2019s actually freedom in working for someone else: \u201cI\u2019ve definitely been on both sides and can say that it\u2019s a very different experience. When you\u2019re a founder, your company is your sole focus and everything revolves around it. \u201cWhereas when you work for a company, you have space to think and spend time on your passion projects. You get the privilege to be able to think about things that excite you and that you\u2019re passionate about on evenings and weekends and anytime outside of the work day.\u201d We\u2019ve talked before about the liberation of limitations, and how sometimes having less time to work on projects makes us more productive. And this seems to be the case with Ben. Consistency is almost a by-product of constraints. The less time we have to do something, the more we want to use that time wisely and effectively. If you\u2019re the type of person to get excited about every new idea that comes up, you\u2019ve probably faced a lot of the dilemmas Ben talks about. Where do you find time to do it all? How do you prioritize what work is most important? How do you make sure you\u2019re able to do the most with the limited time you have? While these are questions that can take a lifetime to figure out, talking to Ben one thing became incredibly clear: consistency and commitment will always take you further than feeling paralyzed by wanting to do it all at once. If you want to find out more about Ben, you can follow him on Twitter or ProductHunt. And check out his work with IT Kit and Spoke\u2014a company that\u2019s using AI to help empower Human Resources and IT professionals.  Want to learn more about spending your time well and doing more meaningful work? Get our latest blog posts in your inbox every week.    Jory MacKay Jory MacKay is a writer, content marketer, and editor of the RescueTime blog. Read More From  Interviews  Want to learn more about spending your time well and doing more meaningful work? Get our latest blog posts in your inbox every week.   "}, {"by": "ntang", "descendants": 0, "id": 17020543, "score": 1, "time": 1525784054, "title": "Everyone is getting rich except you", "type": "story", "url": "http://blairreeves.me/2018/01/21/everyone-getting-rich-except/?resubmit=hn", "text": "Blair Reeves You're in the right place. In the last several weeks, we\u2019ve seen an enormous amount of chatter about market valuations. The Dow hit a record 26,000 points the other day, only two weeks after hitting a previous record of 25,000. Bitcoin traded above $20,000 not long ago, then crashed to somewhere less than half of that, and has since partially recovered. If you work anywhere remotely connected to the tech industry, you\u2019ve probably heard a lot about all of this. It\u2019s impossible not to notice the large amounts of money sloshing around out there. I started writing a post about Bitcoin, and cryptocurrencies, but then realized that what I was really talking about was the weaknesses humans have for risk. The pernicious thing about bubbles \u2013 whether in the stock market, or cryptocurrency or elsewhere \u2013 is that they create a lot of overnight geniuses out of early speculators. They also spawn a class of Explainers, who shape and evangelize a bullish narrative out of every bubble with a clear logical conclusion: to invest, now. You see this happening on CNBC every single day, as well as in the legion of private \u201ccrypto\u201d chat groups popping up all over the place. But first, indulge me in a little story about craps.  One perk of working at IBM was that I\u2019d get out to Vegas about once or twice a year for a few days for one of those giant tech mega-conferences. (This is the ideal amount of time to spend in Las Vegas.) When I did so, I\u2019d always carve out a little time \u2013 often late at night \u2013 to go play craps. Craps is my game \u2013 I love it. I don\u2019t play any other games in the casino. I never go overboard or anything, but always make a point to bring a few hundred bucks to go have fun at the Vegas craps table when I have the opportunity. I very keenly remember one of my early craps games in particular. I\u2019d been playing for about two hours. In that time, I\u2019d been up, then down, then up again, and then, gradually, dwindled my chips down to basically nothing. I\u2019d lost the $300 or so that I came to the table with, and was ready to hang my head in defeat and call it a night. Finishing my drink and putting on my jacket, I placed my last bets \u2013 probably something on Don\u2019t Pass and, as one of my signature moves, I put some chips on what\u2019s called an All Bet. This is basically a bet that, in consecutive rolls of the dice, the shooter will hit a 2, 3, 4, 5, 6, 8, 9, 10, 11 and 12 before rolling a 7 (which is, of course, the most likely number). All Bets are a sucker\u2019s bet \u2013 a really long shot \u2013 which is why the payout is something like 175:1. I then watched, in growing amazement shared around the table, as the shooter did precisely that. When the final number rolled, the table went up in a cheer. I made $350 from a $2 bet. From being utterly tapped out, I was officially right back \u201cup\u201d for the night. At this point, a part of my brain was longing to get settled again, order another drink and try again to win big. Instead, I did the sensible thing: I tipped the dealer, flicked a $10 chip at the shooter with my thanks, and called it a night. I went home having made a little money in Vegas. Everyone who gambles sensibly will agree that this is the right thing to do; unfortunately, even for a reasonably sensible person, I can attest that it isn\u2019t exactly easy. I think about this experience a lot in the context of the ever-present chatter today about rising values for everything in sight \u2013 the stock market, crypto, housing, companies \u2013 and what we mean about \u201cinvesting\u201d in them. There is a small group of people out there who are sitting on enormous holdings today in Bitcoin (and a handful of the other valuable cryptocurrencies). Bloomberg estimates that about 1,000 people own fully 40% of the BTC in existence, and most of them know each other and coordinate. As the joke goes\u2026 Haha, I feel sorry for all you losers who missed out on the Bitcoin train. You should've bought in years ago, like me: A perfectly normal man who coincidentally hoarded a virtual currency during a time when it's only use was for sex trafficking and purchasing organs. \u2014 Shane (@Shanehasabeard) December 8, 2017  There were, indeed, very few plausibly legitimate reasons to buy Bitcoin up until very recently, and that group of original hoarders contains a lot of people with very weird ideas. But more importantly, it\u2019s also a group of people for whom \u201cinvesting\u201d $10,000 in a magic internet money scheme was not only an option, but seemed like a good idea. And if you were buying $10,000 of Bitcoin, chances are good you were also buying other speculative \u201cassets\u201d as well, like gold, silver, canned food or healing crystals. No one, as recently as just a few months ago, could\u2019ve possibly credibly predicted that Bitcoin would explode in value as it has. (If you had put down $10k into Bitcoin in just the middle of 2016, you\u2019d have made something like a 48,000% return by now.) Yet, like myself at the craps table, these guys were putting down money that they could presumably afford to lose on a ridiculously long-shot bet. That isn\u2019t investing. It\u2019s betting, better known as speculating. (Not that I have anything against speculating! In fact, I dabbled in it a bit myself. I made just a few hundred bucks gambling on Bitcoin. We\u2019re paying for NYC childcare now, you know. I\u2019ve cashed out now, though.) I am personally of the opinion that Bitcoin is probably here to stay. It obviously has no intrinsic value, but then, neither does gold nor silver, and they\u2019ve been used as stores of value forever. The blockchain technology underlying Bitcoin has tremendous potential, and the promise of a decentralized, anonymous, internet-based currency among other global reserve currencies is obvious. But it\u2019s equally obvious that much of Bitcoin\u2019s current value is the result of hucksters pumping it like a penny stock to the unsophisticated. We\u2019re at the \u201cYouTube celebrities hawking $49 Bitcoin newsletters\u201d stage of this bubble, which does not augur for great times to come. If anything, the bubble in crypto seems like a smaller companion of the broader, rapid expansion of equities values. As I mentioned earlier, the stock market is hitting new all-time-highs every few months (or weeks) now. I keep a little Google Sheet full of tech company stock price data, and most or all have increased by 50-80% in the last year. In fact, the Tech category is almost singlehandedly responsible for most of the gains in the S&P 500, with just 5 companies (Google, Apple, Amazon, Facebook and Microsoft) contributing almost half the growth. Homo economicus would interpret this as a sign that investors are increasingly confident in these companies\u2019 future revenue potential \u2013 and indeed, they\u2019re not wrong to do so. These are good companies. But I think the enormous rise in these firms\u2019 market caps is also explained by a large amount of stock speculation. This is good for these firms\u2019 employees and, certainly, executives, who probably don\u2019t mind the boost, but it is creating a large group of bagholders who are buying in at very high values and will be left in the bathtub when this cycle slows down again. Google is a tremendous company, but has their actual enterprise value really increased 40+% in the last 12 months? Has Facebook\u2019s? Count me skeptical. It\u2019s not hard to see a lot of this rise driven by people hoping to ride the train to even higher values and turn a quick profit. It is an age-old lesson of the stock market that no one beats the market over the long haul. No one can \u201cpick stocks\u201d or time the market. There is an entire industry of people who try, and yet, mountains of economic research (including Warren Buffett\u2019s famous bet) repeatedly demonstrate that passive index funds beat stock-picking, every time. The same is true of early-stage investing. Many tech venture capitalists are celebrated as geniuses for much the same reason those early Bitcoin \u201cinvestors\u201d are \u2013 they\u2019ve managed to make a lot of money from the outsized performance of a few early investments. Yet the truth is probably closer to being in the right place at the right time. If 90% of a VC fund\u2019s investments lose money, and only 2-5% make significant returns (a pretty typical distribution), are those investors actually good at predicting the future? Or are they simply good at spraying enough money at enough targets in a market seeing long-term, cyclical growth \u2013 and fortunate enough to have the pre-existing wealth to do so? Of course, the monied classes have always enjoyed investing opportunities that the rest of us don\u2019t. Your typical retail investor (like me) does not inhabit the same world of potential returns that Warren Buffett, Marc Benioff or Jeff Bezos do. (This is why I have not launched my own spaceflight company.) Similarly, the elite VC class equally has access to information, people and market intelligence that we normals do not. This cozy ecosystem is partly to blame for the hyperconcentration of personal wealth in Silicon Valley and the country as a whole, and equally the reluctance of many VC-funded companies to go public and share their destinies with retail investors. But perhaps the biggest advantage that wealth confers in investing is the ability to do it at all. \u201cNormal\u201d people do not have tens of thousands of dollars to \u201cinvest\u201d in questionable magic internet money schemes (nor would doing so be a responsible choice for most people), much less a million or two to experiment with angel investing. This is how wealth begets more wealth. Early investors in Facebook or Salesforce or Google were no more \u201cgeniuses\u201d than the guys who made $10,000 bets on Bitcoin in 2014. They could not see into the future. I don\u2019t mean to suggest that investors like these didn\u2019t do their due diligence, only that analysis like that can provide very limited information about what the future holds. Post-hoc narratives get crafted for success, but rarely failure. This feeds the mythology of \u201cgenius\u201d investors getting rich with their sagacity and wisdom, and the FOMO that narrative engenders: the creeping feeling that \u201ceveryone\u201d is getting rich \u2013 except you, because you\u2019re not as smart. Market FOMO is what pumps every bubble \u2013 everyone rushing to get a piece of the action before the party is over. This is true whether it\u2019s the stock market, cryptocurrency prices, housing or anything else. As I said, I am actually a believer in the long-term potential of Bitcoin, and certainly for the technology industry as a whole; but the market values we see today are more related to buyer psychology than underlying value. That won\u2019t last forever. \u00a0 Related posts: \u00a0  \n I send out a semi-regular update with the more interesting stuff I've read and written. Learn more about it and read old issues, or just sign up below. I won't spam you - promise. \n\n \n \n\n\t\t\t\t\tVisit website\t\t\t\t\n"}, {"by": "throwaway2016a", "descendants": 0, "id": 17020541, "score": 1, "time": 1525784015, "title": "On entrepreneurship and Canada", "type": "story", "url": "https://avc.com/2018/05/canada/", "text": "I\u2019m in Canada today to attend a board meeting. I love Canada. It\u2019s a kindler gentler more welcoming version of the US. And it\u2019s increasingly an important place to be for the tech sector. About 10% of USV\u2019s active portfolio is in Canada. And 30% of our last ten investments are in Canada. We have portfolio companies in Toronto, Waterloo, and Vancouver. We also know that Montreal, Quebec, and Ottawa are attractive places to invest. Canada has a lot to offer as a home for a tech company or a second office for a tech company. The government is enthusiastic about tech companies and provides an R&D credit to tech companies. I don\u2019t think this credit is so financially significant that it should determine where you locate your company, but it is a sign of the government\u2019s enthusiasm for tech. More importantly, the talent pool in Canada is rich. Canadians are well educated and there are a number of very strong engineering schools in Canada. All of our portfolio companies that have engineering teams in Canada claim they get higher quality and retention in those teams than the ones they operate in the US. And, if course, Canada makes it easy for highly skilled workers to immigrate to Canada. In a time where high skilled immigration to the US has essentially been stopped, Canada is a great option to locate a team and recruit the smartest people in the world to it. What once was the game plan for tech in the US is now the game plan for tech in Canada. Finally, entrepreneurship is alive and well in Canada. We have met and work with so many ambitious and agressive founders in Canada. And it\u2019s a short plane ride from NYC and SF, depending on where in Canada you want to go. We also work with a number of great investors from Canada. They are supportive and engaged and very much on top of the important trends in tech. So I\u2019m bullish on Canada and have been since we started investing here almost ten years ago. And unlike the US, Canada has the wind behind it\u2019s back in tech right now.  \nMay 2, 2018 \u2013 VC & Technology\n \nTweet\n"}, {"by": "aravindhsriram", "descendants": 0, "id": 17020534, "score": 2, "time": 1525783973, "title": "Exchanges ordered to pull Chinese smartphones over security risks", "type": "story", "url": "https://nakedsecurity.sophos.com/2018/05/08/pentagon-orders-military-exchanges-to-pull-chinese-smartphones-over-security-risks/", "text": "The Pentagon has banned the sale of Chinese phones at military exchanges over security risks, it said on Wednesday. Spokesman Maj. Dave Eastburn told Stars and Stripes that the Department of Defense\u2019s (DOD\u2019s) undersecretary for personnel and readiness issued a ban of \u201call Huawei and ZTE cellphones, personal mobile internet modems and related products from locations worldwide.\u201d More from his emailed statement: Given the security concerns associated with these devices, as expressed by senior US intelligence officials, it was not prudent for the Department\u2019s exchange services to continue selling these products to our personnel. Military personnel haven\u2019t been banned from using the Chinese phones yet, but that may well come: Eastburn said the DOD is \u201cevaluating the situation\u201d to see whether more security measures such as an outright ban might be needed. In the meantime, they should keep an eye on the headlines, he said: Servicemembers should be mindful of the media coverage about the security risks posed by the use of these devices, regardless of where they were purchased. This is par for the course. Chinese companies Huawei and ZTE have a history of being telecoms non gratae. In 2012, the US House of Representatives issued a report recommending that the firms be banned because of concerns over spying. A year-long investigation had shown that the companies had maintained close ties to the Chinese Communist Party and People\u2019s Liberation Army back home while trying to expand their US businesses.  ZTE was also found to have violated an American embargo on technology sales to the Iranian government. In 2010, ZTE helped to send over software and hardware from US companies including Oracle, Microsoft and Cisco for use in building what was described as a $130m, nationwide surveillance system. Lawmakers on 9 January, 2018, introduced the Defending US Government Communications Act (H.R. 4747), which would prohibit federal agencies from contracting with an entity that uses telecom gear or services from Huawei, ZTE, or any other entity thought to be under China\u2019s thumb. A companion bill, the Defending U.S. Government Communications Act, was filed in the Senate on 7 February. Also in February, the heads of the FBI, NSA and CIA all testified to Congress that they and their organizations don\u2019t use Huawei or ZTE phones or products. They also warned others against doing so. The hearing influenced the Pentagon\u2019s decision to ban Huawei and ZTE phone sales from military exchanges, according to what Eastburn indicated to FCW, a publication that covers the business of federal technology. During a 2 May Pentagon news conference, Navy Secretary Richard Spencer gave more details on the Chinese phone ban: according to FCW, he alluded to testimony from a 19 April Senate Armed Services committee hearing, where it was revealed that the DOD had put a recent contract award on hold when officials realized Huawei would be one of the subcontractors. FCW quoted Spencer: The mobile phone ban was due to the location devices more than anything else \u2013 the ability to be located. That\u2019s definitely worth worrying about. We\u2019ve already seen troop location given away inadvertently: In November, fitness app Strava posted its impressive Global Heat Map, which logs the activity history of the software\u2019s tens of millions of active users. That\u2019s a lot of data: users jogged or cycled along 17bn miles and three trillion GPS data points over two years. As we reported in January, in short order, a student looked more closely at Heat Map countries such as Afghanistan and Syria, where he noticed vast dark areas dotted with small islands of user activity\u2026 which he tweeted about\u2026 and which other users pointed out might coincide with the location of US military personnel in places the DOD doesn\u2019t necessarily want made public. That was inadvertent intelligence gathering done by taking a close look at publicly available location data. With the Chinese phone ban and other regulations and pending bills, the US government is of course concerned with purposeful data theft. Huawei spokespeople have repeatedly denied that their devices carry security risks. Nonetheless, Eastburn said, the company\u2019s equipment might pose \u201can unacceptable risk to [the] department\u2019s personnel, information and mission.\u201d During the February hearing, FBI Director Christopher Wray testified that Huawei\u2019s products enable the Chinese government to covertly gather or alter sensitive corporate and military information. The concern about the company\u2019s products first focused on routers, switches and other high-bandwidth commercial products, and later expanded to include consumer mobile phones, which are already banned for most official government use. Huawei also makes personal mobile internet modems, called pucks, which in recent years it\u2019s sold to US troops at a coalition base near Irbil, the capital of Iraq\u2019s Kurdish region. Military.com reports that some soldiers may have purchased similar devices made by ZTE. Follow @LisaVaas\nFollow @NakedSecurity \n Lisa has been writing about technology, careers, science and health since 1995. She rose to the lofty heights of Executive Editor for eWEEK, popped out with the 2008 crash and joined the freelancer economy. Alongside Naked Security Lisa has written for CIO Mag, ComputerWorld, PC Mag, IT Expert Voice, Software Quality Connection, Time, and the US and British editions of HP's Input/Output. Fill in your details below or click an icon to log in: \n\n\t\t\tYou are commenting using your WordPress.com account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Google+ account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Twitter account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n \n\n\t\t\tYou are commenting using your Facebook account.\t\t\t\n\t\t\t\t(\u00a0Log\u00a0Out\u00a0/\u00a0\n\t\t\t\tChange\u00a0)\n\t\t\t\n\n Connecting to %s  \n\n  "}, {"by": "matryer", "descendants": 0, "id": 17020512, "score": 1, "time": 1525783598, "title": "Create a macOS Application from an executable (like a Go binary)", "type": "story", "url": "https://github.com/machinebox/appify", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Create a macOS Application from an executable (like a Go binary)  To install appify: It will create a macOS Application. If you want to build a Go backed web based desktop application, check out our machinebox/desktop project."}, {"by": "ac360", "descendants": 0, "id": 17020505, "score": 3, "time": 1525783527, "title": "Introducing CloudEvents 0.1 \u2013 An open effort by Google, Microsoft, IBM and more", "type": "story", "url": "https://medium.com/@austencollins/introducing-cloudevents-a758c62c76bf", "text": "Last week at CloudNativeCon & KubeCon, a specification was announced that aims to help people and applications handle event data, called CloudEvents. CloudEvents\u2019 scope is simple: provide a consistent set of metadata which you can include with event data to make events easier to work with for publishers, middleware, subscribers & applications in general. In short, it\u2019s a standard event envelope. Here\u2019s what version 0.1 of the spec looks like in an example event represented in JSON: The scope of CloudEvents is pretty humble, however many believe it has big potential. It\u2019s being incubated within the CNCF and an increasing number of industry stakeholders have been actively contributing to the project since January. These include Google, Microsoft, IBM, VMware, SAP, Oracle, Huawei, Alibaba, Red Hat & more. Further, after version 0.1 of the specification was announced last week, Microsoft announced their immediate support for CloudEvents natively for all events in Azure, via Azure Event Grid. Other vendors have started incorporating it just as quickly. What\u2019s the big deal? Why is this important and why is everyone getting involved? I can\u2019t speak for everyone, but as a developer who helped conceive CloudEvents, gather the initial stakeholders, and hand off the effort to the CNCF\u2019s Serverless Working Group, I can chime in on why I\u2019m excited about CloudEvents and why it can raise the tide. In 2015, I made an open-source project called the Serverless Framework. It helped the world realize we can use new serverless services (e.g. AWS Lambda) to build entire applications. These serverless applications are easier to make, easier to maintain and incredibly efficient overall. If you want to move fast, increase innovation and reduce overhead, there is no better option right now than the serverless application. Since 2015, I\u2019ve witnessed many developers and orgs use serverless architectures to deliver incredible amounts of amazing work. At the same time, I\u2019ve seen many struggle with event-driven design, which is inherent in serverless architectures since FaaS functions are event-driven. For example, developers have a hard time discovering events they can work with, understanding event schemas, mocking events for development, testing events and event-driven workflows, observing event-driven workflows, tracing events as they are transported across environments, evolving event data safely, correlating events, transporting events via common protocols, ensuring event delivery, and the list goes on\u2026 What\u2019s most frustrating is event-driven design and serverless technologies enable developers to reach unprecedented levels of productivity, yet our tooling isn\u2019t designed to help us manage this new level of output. We can each provision a thousand AWS Lambda functions right now, which will scale massively and cost nothing until they\u2019re executed. This is incredible power, but I don\u2019t believe we are ready to reason about this new level of output. That is, until we have better tools. This is a problem which I founded Serverless Inc. to solve. CloudEvents is part of the solution. Consistent event metadata creates a stable foundation on top of which we can build better tools for developing, running and operating serverless & event-driven architectures. Events have been around for ages, but our future will contain more events and event-driven applications than ever before. The digital world is storming into our human reality. IoT devices and sensors are proliferating, monitoring us and our environments. At the same time, our applications are becoming more distributed and the quantity of our integrations is growing. As a result, we\u2019re increasingly publishing events, leveraging event-driven design patterns and transporting events across environments. Meanwhile, serverless computing (originally known as event-driven computing) can process events at scale on a pay-per-execution basis. It has never been easier to write code to react to anything that can possibly happen. This is a powerful incentive to create more event-driven applications. Suddenly, the world is richer with events than ever, awaiting developers with vision to put them to productive use. Unfortunately, as the quantity of events and event-driven integrations increase, there still is no consistency in how publishers describe events. Developers looking to easily work with event data have to first learn platform-specific terms and semantics. Some platforms even format events differently across their own services (*cough* AWS *cough*). Meanwhile, the ability to transport events is hindered as logic and infrastructure have no consistent information which they can use to make smart decisions to handle and route events. Future versions of CloudEvents will focus on routing and tracing events, making event-driven patterns and integrations easier. Lock-in is a key concern in serverless architectures. A common question in the serverless community is how to make serverless logic portable. I\u2019m not convinced we should prioritize moving serverless logic around. The economics of serverless computing change the conversation. Serverless logic can reside everywhere with little overhead and cost. Serverless functions can be provisioned simultaneously across regions and providers. Events can now be consumed by one function, or multiple functions at the same time, more easily than ever. I favor offering developers a uniform experience for provisioning and managing the lifecycle of serverless logic, over making it portable. Meanwhile, our serverless lock-in concerns should be directed at the gravity of data. Events bring anti-gravity to data. Our applications currently focus on collecting and protecting static data \u201cat rest\u201d, but this doesn\u2019t allow apps to be as agile and extensible as we\u2019d like. If our architectures prioritize it, they can first keep data \u201cin flight\u201d in the form of events, and leverage serverless functions to consume events easily and concurrently, to deliver valuable outcomes. CloudEvents\u2019 common metadata makes events easier to route, fan out, trace, replay and generally keep \u201cin flight\u201d. They are more portable, more liquid, easier to transport across environments. We also have specs for mapping CloudEvents metadata into existing protocols (HTTP, AMQP, MQTT, and HTTP WebHooks are in PRs) as appropriate for each protocol. Network bandwidth, cost and latency are still major challenges. But CloudEvents simple metadata can bring anti-gravity to data in a lot of use-cases today. Lastly, is the increasing desire for vendor choice. Serverless compute has always been only half of the value. The other half of the value comes from the other services platforms provide which developers can use to deliver innovative outcomes with their serverless functions. We see this every day from the vantage point of the Serverless Framework. The vast majority of serverless applications built on AWS Lambda use other serverless services AWS provides, like S3, DynamoDB, Kinesis or Rekognition. Who cares about your serverless compute service. Developers and businesses need other serverless services that offer easy, high-level abstractions to help them innovate. I expect cloud providers to offer significantly more serverless services in the near future. Vendors (especially nascent ones) have an opportunity in serverless, if they decrease emphasis on trying to copy each other\u2019s services, and instead innovate new serverless services that offer unique, best-in-class, value propositions. With CloudEvents plus serverless and event-driven architectures, developers can leisurely distribute data across vendors and clouds to take advantage of the unique value they provide. CloudEvents helps level the playing field. At CloudNativeCon, I did a live demo of a massive multi-cloud scenario using CloudEvents. The demonstration involved integrating 11 different serverless compute providers, which to their credit, all contributed to the development of the demo. The scenario was completely over the top, but it showed what\u2019s possible. In the architecture, a CloudEvent is published from AWS S3 and another from Azure Blob Storage. Both CloudEvents are then distributed via new types of event routers to 11 serverless functions across 11 different vendors. Each function used a proprietary service their respective platform offers to perform an action based on the event to create an valuable outcome. The results were then logged to a Twitter feed. It was incredible. CloudEvents\u2019 consistent metadata can be generically tracked, routed, and dispatched by infrastructure, which gives us the opportunity to build smarter pipes. Microsoft\u2019s Event Grid is one of the first to provide a new take on an event router with a serverless focus, and it can especially help build event-driven and serverless applications on the Azure platform. The other pioneer in the space is an open-source project which my company has been building for over a year. It\u2019s called the Event Gateway and it uses CloudEvents as the default event format. CloudEvents envelopes are one-way letters. Its metadata enables the creation of complex workflows, which can include request/response. So we designed the Event Gateway to offer both event-driven (pub/sub, async) and request-driven (request/response) flows. You can send custom CloudEvents into the Gateway and route them to multiple serverless functions in a pub/sub pattern. You can also expose HTTP endpoints and receive HTTP requests (which the Gateway converts to HTTP CloudEvents) and respond to them synchronously with serverless functions. Plus, the Gateway makes it easy to integrate with multiple serverless compute options as well as other sources and sinks, making it the ultimate event-driven integration infrastructure, for the serverless and multi-cloud era. You can use the Event Gateway today via our recently released Hosted Event Gateway Beta. You can also deploy the open-source project. Kelsey Hightower gave a great demo at CloudNativeCon on running the Event Gateway on Kubernetes. There is a clear opportunity for new event-driven middleware to help us build the next generation of applications and integrations. More importantly, both of these new event routers are clear examples of what CloudEvents\u2019 simple metadata helps enable. As the CloudEvents specification advances, so will the tooling and ecosystem around it. Of course, this is just the beginning of CloudEvents. It\u2019s remarkable the effort has such significant industry support already. More importantly, CloudEvents must be owned by the community in order for it to be successful, which is why it was important to hand over to the CNCF. The CNCF provides a neutral ground where industry titans and startups alike can collaborate on solutions for the greater good. The CNCF is also exceptional at fostering community. Dan Kohn, Kelsey Hightower, Chris Aniszczyk & more have done a tremendous job there. Now, as we move higher up the stack and shift the focus of software development tools from infrastructure to outcomes, I have only one question for them... When will the Serverless Foundation be established? \ud83d\ude0f Ultimately, this is not about vendors or the CNCF. What matters most is that we continue to make things simpler, more accessible, and democratize the power to create software, regardless of location, gender, race, age, background, resources, power, etc. Serverless, CloudEvents and community can do this. Shout-out to the great CloudEvents contributors. Dream team. Check out the CloudEvents spec for more info and ways to contribute. We\u2019d love to start seeing some end users from various verticals involved. Also this Twitter thread by Clemens Vasters rocks. By clapping more or less, you can signal to us which stories really stand out. A founder."}, {"by": "anarbadalov", "descendants": 0, "id": 17020499, "score": 1, "time": 1525783365, "title": "How the chicken nugget became the true symbol of our era", "type": "story", "url": "https://www.theguardian.com/news/2018/may/08/how-the-chicken-nugget-became-the-true-symbol-of-our-era?CMP=share_btn_tw", "text": "This is what happens when you turn the natural world into a profit-making machine. By  Raj Patel and  Jason W Moore \n\nTue 8 May 2018 01.00\u00a0EDT\n\n The most telling symbol of the modern era isn\u2019t the automobile or the smartphone. It\u2019s the chicken nugget. Chicken is already the most popular meat in the US, and is projected to be the planet\u2019s favourite flesh by 2020. Future civilisations will find traces of humankind\u2019s 50 billion bird-a-year habit in the fossil record, a marker for what we now call the Anthropocene. And yet responsibility for the dramatic change in our consumption lies not so much in general human activity, but capitalism. Although we\u2019re taught to understand it as an economic system, capitalism doesn\u2019t just organise hierarchies of human work. Capitalism is what happens when power and money combine to turn the natural world into a profit-making machine. Indeed, the way we understand nature owes a great deal to capitalism. Every civilisation has had some rendering of the difference between \u201cus\u201d and \u201cthem\u201d, but only under capitalism is there a boundary between \u201csociety\u201d and \u201cnature\u201d \u2013 a violent and tightly policed border with deep roots in colonialism. First taking shape in the era of Chistopher Columbus, capitalism created a peculiar binary order. \u201cNature\u201d became the antonym of \u201csociety\u201d in the minds of philosophers, in the policies of European empires, and the calculations of global financial centres. \u201cNature\u201d was a place of profit, a vast frontier of free gifts waiting to be accepted by conquerors and capitalists. This was a dangerous view of nature for all sorts of reasons, not least because it simultaneously degraded human and animal life of every kind. What we call \u201ccheap nature\u201d included not only forests and fields and streams, but also the vast majority of humankind. In the centuries between Columbus and the industrial revolution, enslaved and indentured Africans, Asians, indigenous peoples and virtually all women became part of \u201cnature\u201d \u2013 and treated cheaply as a result. When humans can be treated with such little care, it\u2019s not surprising that other animals fare even worse under capitalism, especially the ones we end up paying to eat. Animals have been at the epicentre of five centuries of dietary transformation, which sharply accelerated after the second world war. The creation of the modern world depended on the movement of cattle, sheep, horses, pigs and chickens into the new world, reinforcing the murderous advance of microbes, soldiers and bankers after 1492. Capitalism\u2019s \u201cecological hoofprint\u201d, to use food scholar Tony Weis\u2019s well-turned phrase, has become radically globalised ever since. In the half-century after 1961, Weis tells us, per capita meat and egg consumption has doubled, and the number of slaughtered animals leapt eightfold, from eight to 64 billion. To those with a romantic view of where their food comes from, uncooked meat appears to be a raw ingredient rather than a processed one. Quite the opposite. Feed and oilseed crops form part of what Weis terms \u201cthe industrial grain-oilseed-livestock complex\u201d. Markets for grain made it possible for meat not just to become cheap food, but also to back financial instruments. Futures contracts in pork bellies, for instance, in turn require the uniformity, homogenisation and industrialisation of the crops they transform. Raw meat in the supermarket is, in other words, cooked up by a sophisticated and intensive arm of capitalism\u2019s ecology. Where there\u2019s profit, there\u2019s every incentive to realise it efficiently. Modern meat-production systems can turn a fertile egg and a 4kg bag of feed into a 2kg chicken in five weeks. Turkey production times almost halved between 1970 and 2000, down to 20 weeks from egg to 16kg bird. Other animals have seen similar advances through a combination of breeding, concentrated feeding operations and global supply chains. The consequences of the sustained rise in meat consumption are a planetary affair too: 14.5% of all anthropogenic carbon dioxide (CO2) emissions are from livestock production. The environmental consequences of meat production are, of course, external to industrial agriculture\u2019s bottom line. Nature is merely the pool from which animals are drawn and factory farmed, and the dump into which their, and our, waste disappears. The danger lies in believing the division between nature and society is real, in seeing \u201cfactory farming\u201d as an environmental question and \u201cfactory production\u201d as a social question. Social questions are environmental questions, and vice versa. Chickens don\u2019t turn into nuggets by themselves. Capitalists need cheap work. With the European invasion of the new world in 1492, that labour presented itself in the bodies of indigenous people. By the late 16th century, when Spaniards were desperately trying to revive silver production at the great silver mountain of Potos\u00ed, in present-day Bolivia, they began using the word naturales to refer to indigenous people. Through hard work and prayer, those indigenous people, and enslaved Africans, might find divine redemption through work and perhaps even, one day long in the future, entry into society as equals. Work was never meant to be fun. Consider the etymology of the French travail and the Spanish trabajo, each a translation of the English noun \u201cwork\u201d: their Latin root is trepaliare, \u201cto torture, to inflict suffering or agony.\u201d But the way work works has changed. For millennia, most humans survived through more or less intimate relations with land and sea. Even those who didn\u2019t were closely connected to the tasks and objects of labour. Human survival depended on holistic, not fragmented, knowledge: fishers, nomads, farmers, healers, cooks and many others experienced and practised their work in a way directly connected to the web of life. Farmers, for instance, had to know soils, weather patterns, seeds \u2013 in short, everything from planting to harvest. That didn\u2019t mean work was pleasant \u2013 slaves were often treated brutally. Nor did it mean that the relations of work were equitable: guild masters exploited journeymen, lords exploited serfs, men exploited women, the old exploited the young. But work was premised on a holistic sense of production and a connection to wider worlds of life and community. In the 16th century, that began to shift. The enterprising Dutch or English farmer \u2013 and the Madeiran, then Brazilian, sugar planter \u2013 was increasingly connected to growing international markets for processed goods, and correspondingly more interested in the relationship between work time and the harvest. International markets pushed local transformations. Land in England was consolidated though enclosure, which concurrently \u201cfreed\u201d a growing share of the rural population from the commons that they had tended, supported and survived on. These newly displaced peasants were free to find other work, and free to starve or face imprisonment if they failed. This history is alive and well in the modern chicken nugget. Poultry workers are paid very little: in the US, two cents for every dollar spent on a fast-food chicken goes to poultry workers. It\u2019s hard to find staff when, according to one study in Alabama, 86% of employees who cut wings are in pain because of the repetitive hacking and twisting on the line. To fill the gaps in the labour force, some chicken operators use prison labour, paid at 25 cents an hour. In Oklahoma, chicken company executives returned to a colonial fusion of work and faith, setting up an addiction treatment centre in 2007, Christian Alcoholics & Addicts in Recovery. With judges steering addicts to treatment instead of jail, the recovery programme had a ready supply of workers. At CAAIR, prayer was supplemented with unpaid work on chicken production lines as part of a recovery therapy. If you worked and prayed hard enough for the duration of your treatment, you\u2019d be allowed to re-enter society. CAAIR\u2019s recruits were predominantly young and white, but the majority of poultry workers are people of colour. Latinx immigrants are a vital force in US agriculture, and the delivery of their cheap work was made possible by class restructuring on two fronts. One, in the US, was a strong movement in the 1980s by newly aggressive meat-packing firms to destroy union power and replace unionised workers with low-wage immigrant labour. The other was the destabilisation of Mexico\u2019s agrarian order after 1994 by the North American Free Trade Agreement (Nafta), which resulted in flows of cheap immigrant labour \u2013 unemployed workers displaced by capitalism\u2019s ecology from one side of the US border to the other. A line on a map between two states is a powerful abstraction, one that has been used recently by the far right to recruit and spread fear, and for much longer by capitalists in search of ever cheaper and more profitable workers. Under capitalism, national territories, locally owned land and new migrating workers are produced simultaneously. With migrant workers came elite fears of the itinerant poor. In 17th- and 18th-century England, this panic resulted in harsh laws against vagabondage, and the development of charities to ameliorate the worst effects of enforced destitution. Threats of imprisonment moved the poor into waged work, an activity that took the intelligence, strength and dexterity of humans and disciplined them to productive labour using another modern invention: a new way of measuring time. If the practice of labour shapes capitalism\u2019s ecology, its indispensable machine is the mechanical clock. The clock \u2013 not money \u2013 emerged as the key technology for measuring the value of work. This distinction is crucial because it\u2019s easy to think that working for wages is capitalism\u2019s signature. It\u2019s not: in 13th-century England only a third of the economically active population depended on wages for survival. That wages have become a decisive way of structuring life, space and nature owes everything to a new model of time. By the early 14th century, the new temporal model was shaping industrial activity. In textile-manufacturing towns like Ypres, in what is now Belgium, workers found themselves regulated not by the flow of activity or the seasons but by a new kind of time \u2013 abstract, linear, repetitive. In Ypres, that work time was measured by the town\u2019s bells, which rang at the beginning and end of each work shift. By the 16th century, time was measured in steady ticks of minutes and seconds. This abstract time came to shape everything \u2013 work and play, sleep and waking, credit and money, agriculture and industry, even prayer. By the end of the 16th century, most of England\u2019s parishes had mechanical clocks. Spain\u2019s conquest of the Americas involved inculcating in their residents a new notion of time as well as of space. Wherever European empires penetrated, there appeared the image of the \u201clazy\u201d native, ignorant of the imperatives of Christ and the clock. Policing time was central to capitalism\u2019s ecology. As early as 1553, the Spanish crown began installing \u201cat least one public clock\u201d in its major colonial cities. Other civilisations had their own sophisticated temporal rules, but the new regimes of work displaced indigenous tempos and relationships with the natural world. The Mayan calendar is a complex hierarchy of times and readings from the heavens, offering a rich set of arrangements of humans within the universe. Spanish invaders respected it only to this extent: they synchronised their colonial assaults to sacred moments in the calendar. As social historian EP Thompson observes in his seminal study Time, Work-Discipline and Industrial Capitalism, the governance of time follows a particular logic: \u201cIn mature capitalist society all time must be consumed, marketed, put to use; it is offensive for the labour force merely to \u2018pass the time\u2019.\u201d The connection of specific activities to larger productive goals didn\u2019t allow for time theft, and the discipline of the clock was enforced by violence across the planet. Teaching the value and structure of capitalist time to new subjects was a key part of the colonial enterprise. One settler noted in 1859 that Indigenous Australians \u201cnow \u2026 have the advantage of dating from the \u2018Nip Nip,\u2019 or Settlers\u2019 yearly regular shearing time. This seems to supply them with a mode of stating years, which before they had not. Months or moons then satisfied them.\u201d But the regulation of time was also a focus of resistance. Another settler wrote in a diary: \u201cThis evening there was a grand Korroberry [sic, for corroboree, an exuberant, possibly spiritual, gathering] \u2013 I endeavoured to dissuade them, telling them that it was Sunday \u2013 but they said, \u2018black fellow no Sunday.\u2019\u201d Why the resistance? Because they knew full well that their labour was the object of theft, that colonists were appropriating their work. Fights over the regulation of time continue even now. On US poultry lines, there is a federal law limiting the speed at which birds are processed: 140 birds per minute. The industry is lobbying to eliminate the limit, so that it can compete with factories in Brazil and Germany, where the rate is nearer 200 bpm. Worries about higher rates of food contamination and worker injury are being outweighed by the certain profit from more dead chickens. Capitalism has always experimented with every available kind of labour system simultaneously. A sugar plantation in 1630s Brazil, for example, would be easily recognisable as a modern industrial operation in, say, the Bangladeshi textile industry. Just as autoworkers on the line assemble simplified, interchangeable parts and fast-food workers manufacture standardised burgers, so did African slaves work specialised jobs in a simplified landscape of sugar monoculture. Behind the modern factory, there has always been a layer-cake of exploitation. Managers of factories were salaried more than the workers, who worked with raw materials acquired through various kinds of peonage and natural resource exploitation, and all of them depended on free domestic labour, usually from women. The global factory depends on a global mine, a global farm, and a global family. Hence the persistence today of slavery. One UN agency, the International Labour Organization, estimates there are 40 million people in slavery today, the majority of whom are women, many in forced marriages. Wartime work camps in, say, the Democratic Republic of the Congo supply the rare-earth metals such as tantalum that power the physical infrastructure behind the virtual economy. But just as management looks to find new ways to generate profit, so workers find ways to resist. Early capitalism\u2019s great commodity frontiers \u2013 of sugar, silver, copper, iron, forest products, fishing and even cereal agriculture \u2013 were zones of experimentation in strategies of labour control in Europe and its colonies, and always spaces of conflict. Strikes, rebellions, negotiations and resistance characterised the application of capitalist work disciplines. Every resistance by labour was a new reason to bring in machines. Modern work regimes and technologies emerged from the crucible of experiments, strategies and resistances of early modern workers. Worker unrest in factories and slave rebellions, past and present, are linked not just because they are expressions of resistance, but because they are protests against the ecology of capitalism. Every global factory needs a global farm: industrial, technological and service enterprises rely on the extraction of work and cheap nature to thrive. The apps on your iPhone, designed in Cupertino, California, might have been coded by self-exploiting independent software engineers, and the phone itself assembled in draconian workplaces in China, and run on minerals extracted in inhumane conditions in the Congo. Modern manufacturing relies on layered, simultaneous and different regimes of work. And in response to every act of resistance against it, capitalism has moved the frontiers of work yet again. Hegemony over workers has been aided by cheap food, and the promise of a chicken in every pot. Cheap food has been central to the maintenance of order for millennia. But in the ecology of capitalism, that order has been maintained through planetary transformation. Since the 15th century, some land has become the exclusive domain of specific kinds of crops and crop systems: fields of monocultures designed to bring in flows of cash. Other areas were reserved to house those humans who had been excommunicated from those lands, to be better placed at the service of capitalists in cities. It was always a socially unstable geography, with low industrial wages supported by lower peasant wages supported by free gifts from nature, women and the colonies. After the revolutions of the 19th and 20th centuries offered workers the promise of alternatives to exploitation, capitalist fears of urban uprising and communism reached fever pitch. To allay this existential dread, governments and foundations did not address inequality or exploitation. Instead, they funded the development of crops that would grow abundantly enough to provide cheap food and curb urban hunger. That it was urban, and not rural, hunger that troubled policy makers is vitally important. Food and employment for people in rural areas \u2013 where most of the world\u2019s hunger was concentrated \u2013 were of little concern. Hunger began to matter politically only when the poor came to the cities and translated it into anger, and thence potentially into insurrection and a challenge to the rule of cheap nature. It\u2019s here \u2013 in the bourgeois concern about that rule and its need for worker quiescence \u2013 that we find the origin of what came to be known as the Green Revolution. The aim was to breed varieties of cereals that might flow freely through urban areas. But the revolution wasn\u2019t simply an agronomic transformation. It required more than magic seeds. In order for farmers to grow the crops, national governments had to subsidise the purchase of crops through agricultural marketing boards, to lay the infrastructure for irrigation, and to suppress political dissent around alternative food systems. The Green Revolution of the early- to mid-20th century was a package of reforms designed to prevent the revolutionary political goal of many peasants\u2019 and landless workers\u2019 movements: comprehensive land and agrarian reform. If you squint, it\u2019s possible to see the Green Revolution as a success. Globally, grain output and yields (the amount of output per unit area) more than doubled \u2013 between 1950 and 1980. India\u2019s wheat yields shot up by 87% between 1960 and 1980, similar to what American corn farmers experienced in the two decades after 1935. A rising share of all this food was traded on the world market, with global grain exports increasing by 295% during the 1960s and 70s. If these are the metrics of success, then the political commitment to making food cheap through state subsidy and violence worked. But the prodigious output did not reduce hunger. Wheat production in India soared, but the amount that Indians ate hardly improved. Hunger, particularly in an economy dependent on agriculture, doesn\u2019t end if people remain poor: it doesn\u2019t matter how much grain there is if you can\u2019t afford to buy it. Indeed, it is a global phenomenon that from 1990 to 2015, prices of processed food rose far less than those of fresh fruits and vegetables, and that in almost every country today, the poorest part of the population can\u2019t afford to eat five fresh fruits or vegetables a day. Although workers in countries belonging to the Organisation for Economic Cooperation and Development (OECD) saw an increase in their share of national income after the second world war, that reversed in the 1980s. This was a direct consequence of anti-labour policies that scholars aptly call \u201cwage repression\u201d. Given consistently low wages in the neoliberal era, it mak0es sense to look at cheap food as cheap not merely relative to wage costs but directly in terms of price. When we do, it emerges as no accident that one foodstuff whose price has fallen dramatically is chicken in Mexico \u2013 a direct consequence of Nafta, technology and the US soybean industry. Nafta originally excluded agricultural goods, but they were included at the insistence of the Mexican government, which wanted to \u201cmodernise\u201d its peasantry by moving them from agriculture into urban circuits of industry. The strategy worked: Mexico\u2019s campesino (\u201cpeasant farmer\u201d) agricultural economy buckled, as evinced by the El Campo No Aguanta M\u00e1s (\u201cthe countryside can\u2019t take it anymore\u201d) protests that spread throughout the country in 2003. Circuits of migration and pools of labour for US agriculture were the result. But at least the chicken was cheap. Here we come to an important point about cheap food regimes: they guarantee neither that people are fed nor that they are fed well \u2013 as the global persistence of diet-related ill health and malnutrition can attest. Capitalism\u2019s agricultural frontiers continue to press against the world\u2019s peasants, who provide 75% of the food in large parts of the global south. But while the present is bleak, with agricultural frontiers pushing through Amazonia and displacing peasants around the world, in the 21st century a new wrinkle has appeared that will fatally undermine capitalism\u2019s five century-long food regime: climate change. The imagery of the frontier lends itself to thinking only about land. But the past two centuries have witnessed a very different kind of frontier movement: the enclosure of the atmospheric commons as a dumping ground for greenhouse gas emissions. In the 21st century, agriculture and forestry (which includes land clearance for cash cropping) contribute between a quarter and a third of greenhouse gas emissions. This is inevitable, because they\u2019re profoundly energy- intensive, and have become more so. That\u2019s a big problem, because there are no more atmospheric commons to enclose, and no obvious way to keep the costs of climate change off capitalism\u2019s ledgers. Nowhere is this clearer than in the faltering global farm, whose productivity growth has been slowing, just as it did for English farmers in the middle of the 18th century. Agro-biotechnology\u2019s promise of a new agricultural revolution has so far been worse than empty \u2013 failing to deliver a new yield boom, creating superweeds and superbugs that can withstand glyphosate and other poisons, and sustaining the cheap food model that is driving the ongoing state shift in the world\u2019s climate system. Climate change represents something much more than a closing frontier \u2013 it is something akin to an implosion of the cheap-nature model, bringing not the end of easy and cheap natures, but a dramatic reversal. As a growing body of research demonstrates, climate change suppresses agricultural productivity. \u201cClimate\u201d refers to extremely diverse phenomena, including drought, extreme rainfall, heat waves and cold snaps. Soy, the paradigmatic neoliberal crop, has already experienced what agronomists call yield suppression as a result of climate change. How much remains a matter of debate, but many analyses land somewhere in the area of a 3% reduction in growth since the 1980s \u2013 a value of $5bn per year from 1981 to 2002. Worse, climate change promises absolute declines. Each 1C increase in average annual global temperature is accompanied by a greater risk of dramatic effects on global farming. Agricultural yields will decline between 5% and 50% (or more) in the next century, depending on the time frame, crop, location and extent to which carbon continues to be pumped into the air at today\u2019s prodigious rates. World agriculture will absorb two-thirds of all climate change costs by 2050. That means that both the climate and capitalism\u2019s agricultural model are in the midst of an abrupt and irreversible moment of change. There is little reason to imagine that climate change won\u2019t break the modern food system. Worse, industrial food production is a breeding ground for pandemic disease, and reasoned analysis suggests that the kind of concentrated animal-feeding operations that bring us cheap meat will also bring viruses that could decimate the human population. Again, this is nothing new. Just as early-modern climate change and the plague brought about the end of feudalism and the beginning of capitalism, so we face a future in which climate change and a vulnerability to big systemic shocks augur a dramatic end for capitalism\u2019s ecology. We\u2019re astute enough students of history to see that what follows capitalism might not be better. Around the world, fascism has emerged from liberalism\u2019s soil. Yet precisely as capitalism\u2019s bills come due, communities are both resisting and developing complex and systemic responses at capitalism\u2019s frontiers. Around each of the seven cheap things that make capitalism possible \u2013 nature, work, care, food, energy, money and lives \u2013 there are movements that are developing alternatives. Whether in a globally reviving labour movement, in the Movement for Black Lives\u2019 demands around food, reparations and local economic sovereignty, or the feminismo campesino y popular (\u201cpopular peasant feminism\u201d) developed by the La Via Campesina peasant movement in Latin America to bring together concerns around food, care, nature and work, movements are both fighting and developing intersectional alternatives. John Jordan, an activist and co-founder of the UK\u2019s Reclaim the Streets movement, argues that resistance and alternatives are \u201cthe twin strands of the DNA of social change\u201d. That change will need resources and space to develop. If we are made by capitalism\u2019s ecology, then we can be remade only as we in turn practise new ways of producing and caring for one another together \u2013 a process of redoing, rethinking and reliving our most basic relations. Adapted from A History of the World in Seven Cheap Things by Raj Patel and Jason W Moore, published by Verso on 22 May, and available to buy at guardianbookshop.com \u2022 Follow the Long Read on Twitter at @gdnlongread, or sign up to the long read weekly email here."}, {"by": "pm3310", "descendants": 1, "id": 17020498, "kids": [17020617], "score": 2, "time": 1525783353, "title": "Show HN: Sagify - Train and deploy ML/DL models on AWS SageMaker made simple", "type": "story", "url": "https://github.com/Kenza-AI/sagify", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. \nSign up\n \n              Use Git or checkout with SVN using the web URL.\n             If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download GitHub Desktop and try again. Go back If nothing happens, download Xcode and try again. Go back If nothing happens, download the GitHub extension for Visual Studio and try again. Go back Build Status master:   A command-line utility to train and deploy Machine Learning/Deep Learning models on AWS SageMaker in a few simple steps! For detailed reference to Sagify commands please go to: Read the Docs sagify requires the following: At the command line: You're going to clone and train a Deep Learning codebase to evaluate arithmetic additions on up to 3 digit integers. Clone repository: Optionally, if you want to use Python 2.7 replace the value of REQUIRED_PYTHON and PYTHON_INTERPRETER in test_environment.py and Makefile, respectively, to python2. Create environment: Don't forget to activate the virtualenv after the creation of environment by executing workon deep-learning-addition. Install dependencies: Generate training and validation data: Type in deep-learning-addition for SageMaker app name and make sure to choose your preferred Python version, AWS profile and region. A module called sagify is created under src/. The structure is: As a Data Scientist, you only need to conduct a few actions. Sagify takes care of the rest: Hence, Copy .npy files from data/processed/ to sagify/local_test/test_dir/input/data/training/ Replace the TODOs in the try..except of train(...) function in sagify/training/train file with: and after the import traceback at the top of the file, add: The body of try..except should look like: Replace the body of predict(...) function in sagify/prediction/predict.py with: and replace the body of get_model() function in ModelService class in the same file with: It's time to build the Docker image that will contain the Deep Learning Addition codebase: The path to requirements.txt is necessary to be specified so that all the required dependencies are installed in Docker image. If you run docker images | grep deep-learning-addition-img in your terminal, you'll see the created Deep Learning Addition image. Time to train the Deep Learning model in the newly built Docker image: This step takes ~5 minutes in a MacBook Pro Early 2015 3.1 GHz Intel Core i7. Finally, serve the model as a REST Service: Run the following curl command on your terminal to verify that the REST Service works: It will be slow in the first couple of calls as it loads the model in a lazy manner. Voila! That's a proof that this Deep Learning model is going to be trained and deployed on AWS SageMaker successfully. Now, go to the Usage section in Sagify Docs to see how to train and deploy this Deep Learning model to AWS SageMaker! Initializes a sagify module This command initializes a sagify module in the current working directory or under SRC_DIR, if optional flag --dir is specified. --dir SRC_DIR or -d SRC_DIR: Directory to create sagify module Builds a Docker image This command builds a Docker image from code under the current working directory or under SRC_DIR, if optional flag --dir is specified. A REQUIREMENTS_FILE needs to be specified in order to install all required dependencies in Docker image. --requirements-dir REQUIREMENTS_FILE or -r REQUIREMENTS_FILE: Path to REQUIREMENTS_FILE --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides Executes a Docker image in train mode This command executes a Docker image in train mode. More specifically, it executes the train(...) function in sagify/training/train inside an already built Docker image (see Build command section). --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides Executes a Docker image in serve mode This command executes a Docker image in serve mode. More specifically, it runs a Flask REST app in Docker image and directs HTTP requests to /invocations endpoint. Then, the /invocations endpoint calls the predict(...) function in sagify/prediction/predict.py (see Build command section on how to build a Docker image). --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides Pushes a Docker image to AWS Elastic Container Service This command pushes an already built Docker image to AWS Elastic Container Service. Later on, AWS SageMaker will consume that image from AWS Elastic Container Service for train and serve mode. --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides Uploads data to AWS S3 This command uploads content under LOCAL_INPUT_DATA_DIR to S3 under S3_TARGET_DATA_LOCATION --input-dir or -i: Local input directory --s3-dir or -s: S3 target location --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides Executes a Docker image in train mode on AWS SageMaker This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in train mode --input-s3-dir or -i: S3 location to input data --output-s3-dir or o: S3 location to save output (models, reports, etc). Make sure that the output bucket already exists. Any not existing key prefix will be created by sagify. --ec2-type or e: ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/ --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides --hyperparams-file or -h: Path to hyperparams JSON file --volume-size or -v: Size in GB of the EBS volume (default: 30) --time-out or -t: Time-out in seconds (default: 24 * 60 * 60) Executes a Docker image in serve mode on AWS SageMaker This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in serve mode --s3-model-location or -m: S3 location to to model tar.gz --num-instances or n: Number of ec2 instances --ec2-type or e: ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/ --dir SRC_DIR or -d SRC_DIR: Directory where sagify module resides"}, {"by": "ibobev", "descendants": 0, "id": 17020485, "score": 1, "time": 1525783211, "title": "Std::iterator is deprecated", "type": "story", "url": "https://www.fluentcpp.com/2018/05/08/std-iterator-deprecated/", "text": "Hello, my name is Jonathan Boccara. I'm your host on Fluent C++. I have been a C++ developer for 6 years, working for Murex which is a major software editor in the finance industry. My focus is on C++ and particularly how to write expressive code. I'm happy to take your feedback, don't hesitate to drop a comment on a post, follow me or get in touch directly ! Jonathan Boccara's blog Popular Follow Recent Posts C++17 has deprecated a few components that had been in C++ since its beginning, and std::iterator is one of them. If you don\u2019t have C++17 in production, you\u2019re like most people today. But one day or the other, your will have it, most likely. And when that day comes, you\u2019ll be glad you anticipated the deprecation of such components, and stopped using them well in advance. Let\u2019s see how std::iterator was used, why it was deprecated, and what to use instead. std::iterator\u00a0was used to specify the traits of an iterator. What does that mean? Generic code that uses iterators, such as the STL algorithms which use them intensely, needs information about them. For example, it needs the type of the object that the iterators refer to. To obtain this information, the STL requires that the iterator it operates on must define a type called value_type. To illustrate, consider the algorithm std::reduce. One of its overloads takes two iterators and returns the sum of the objects contained between those two iterators: \nThis should output 15, which is the sum of the elements inside numbers. But what if the collection of number was empty? \nWhat should this code output? The spec of std::reduce\u00a0says that it should return an object of the type of elements, value constructed (which essentially means, constructed with {}). So in our case that would be int{}, which is 0. But how does std::reduce\u00a0know that the type of the elements of the vector numbers\u00a0is int? Indeed, it has no connection with the vector, as it only interacts with its iterators coming from the begin\u00a0and end\u00a0functions. This is why iterators must provide a ::value_type, which is, in this case, the value of the elements of the vector. So int. Another example of required information is the capabilities of the iterator: is it just an input iterator, that supports ++\u00a0but should not be read twice? Or a forward iterator that can be read several times? Or a bidirectional that can also do --? Or a random access iterator, that can jump around with +=, +, -=\u00a0and -? Or an output iterator? This piece of information is useful for some algorithms that would be more or less efficient depending on those capabilities. Such an algorithm typically has several implementations, and chooses one to route to depending on the category of the iterator. To achieve this routing, the STL requires that iterators provide a type called iterator_category, that can be either one of: Finally, the other types than value_type\u00a0and iterator_category required by the STL on iterators are: Which makes up 5 types to define. All the iterators in the standard library comply by this (static) interface. If you need to implement your own iterator, you also need to provide those types. If you want to\u00a0access those types on a given iterator, you may think that you can rely on the iterator to provide the 5 types. And to be able to call Iterator::value_type\u00a0for example. This is mostly true, but there is one exception: when the iterator is in fact a pointer. Some STL implementations use a pointer to stand for the iterator of a vector (indeed, pointer arithmetics does a fine job of +=, and other usual iterator manipulations). And it is also the case for iterating over an C-style array. In such cases, you can\u2019t just do something like int*::value_type, since pointer don\u2019t have nested types! To cover that case, the convention is not to call ::value_type\u00a0or ::iterator_category\u00a0directly, but rather to add a level of indirection. This level of indirection is a template called std::iterator_traits, that exposes the same 5 types. If the template type Iterator of std::iterator_traits<Iterator>\u00a0is not a pointer, then the types of std::iterator_traits\u00a0are just forwarded to those of the Iterator. For example: \nis defined as \nBut if the template type is a pointer, say T*, then std::iterator_traits<T*>::value_type\u00a0is hardcoded as T, and std::iterator_traits<T*>::iterator_category\u00a0is hardcoded as std::random_access_iterator_tag. std::iterator\u00a0is a helper to define the iterator traits of an iterator. std::iterator is a template, that takes 5 template parameters: \nThose 5 names sound familiar, right? Those template types correspond to the 5 types required by the STL on iterators. The job of\u00a0std::iterator\u00a0is to expose those types. Here is one possible implementation of std::iterator: \nstd::iterator\u00a0allows an iterator to define this 5 types, by inheriting from std::iterator\u00a0and passing it those types (at least the first 2 since the other 3 have default values): \nBy inheriting from std::iterator, \u00a0MyIterator\u00a0also exposes the 5 types. This all seems very useful, so why deprecate this functionality? The important thing to note is that the deprecation only concerns std::iterator. So it does not concern the types that the STL expects from an iterator, and neither does it concern the idea that an iterator should provide information to the code that uses it. What is deprecated is the technique of inheriting from std::iterator\u00a0to define those types. That\u2019s it. The rest stays, including std::iterator_traits\u00a0for example. Now, what\u2019s wrong with std::iterator? At least one thing that is wrong with it is that the iterator that inherits from it provides the 5 types without being explicit about which one is which. For instance: \nThis code doesn\u2019t say to which type of the interface (value_type, reference_type\u2026) each of the types passed corresponds. A more explicit way to go about it is to write the using declarations (or typedefs if you\u2019re before C++11) directly inside of the iterator: \nAnd this is how we\u2019re expected to define the types exposed by our iterators now. You may have noticed that std::iterator\u00a0had default template parameters: \nWhich meant that, if there wasn\u2019t a specificity on the last 3 types that forced you to define them, you could get away with defining just the first two: \nNow, to my knowledge, this is no longer possible: you have to write the 5 types definitions in full inside your iterator. I consider this to be a regression. I may well be wrong and if you see it a different way, I\u2019m glad to hear your view on that. Output iterators, such as std::back_inserter\u00a0(or, to be more accurate, the iterator generated by that function), also have to expose certain types. In particular their iterator_category\u00a0is std::output_iterator_tag, and the other types are void. My understanding as to why the last 4 types must be void\u00a0is that they are not used anyway. With std::iterator, we used to define output iterators\u00a0this way: \nWe used to fill out the types in std::iterator\u00a0with void, just for the sake of putting something. When I learned about the deprecation of std::iterator\u00a0and the new way of providing the types, I first thought that it would be more convenient for defining output iterators. Indeed, the only type that matters is the iterator category, and I thought we could just forget about specifying the other types: \nAnd then I realized that this was completely wrong. Indeed, some platforms won\u2019t accept your code if you don\u2019t define the 5 types. So you still have to go and define the 4 aliases to\u00a0void: \nIf you\u2019re interested, we now get into more details about why some platforms will let you get away with only the std::iterator_category and some won\u2019t. And if you don\u2019t feel getting into such details right now you can hop onto the conclusion. But the bottom line is that, if you want your iterator code to be portable, you need to define the 5 types. So, how come some platforms force you to write the 5 types even if you don\u2019t use them all? On libstdc++, used by gcc If you peek into libstdc++, used by gcc, you\u2019ll see that std::iterator_traits\u00a0is implemented as: \nThis implies that, as soon as you try to access one member, such as ::iterator_category\u00a0for example, the whole structured and all its typedefs are instantiated. If one of them doesn\u2019t exist, this leads to a compilation error. On libc++, used by clang And if you go look into libc++, used by clang, you\u2019ll observe that std::iterator_traits\u00a0has a different implementation: \nThe typedefs are not directly inside iterator_traits. Instead, they are in its base class. And this makes a whole difference: if you try to use one of those typedefs in your code (say,\u00a0::iterator_category\u00a0for instance), your code will compile even if another one (say,\u00a0::value_type) is missing. To be honest, I don\u2019t know which language rule explains that difference. If you know, now is a good time to share your knowledge in the comments section. In any case, the bottom line is that one of the major platforms won\u2019t let you get away with it, so do specify all 5 types to stay away from such portability issues. std::iterator\u00a0is deprecated, so we should stop using it. Indeed, the next step after deprecation could be total removal from the language, just like what happened to std::auto_ptr. But contrary to std::auto_ptr, the alternative to std::iterator\u00a0is trivial to achieve, even in C++03: just implement the 5 aliases inside of your custom iterators. And even if your code doesn\u2019t use the 5 of them, do define them to make sure your code stays portable. Now, you may wonder, does it really happen that we create iterators? To answer that question, I invite you to have a look at Smart Output Iterators! Related articles:  Copyright text 2017 by Fluent C++. \u00a0\u00a0-\u00a0\u00a0Designed by Thrive Themes | Powered by WordPress "}, {"by": "aaron_p", "descendants": 0, "id": 17020482, "score": 1, "time": 1525783192, "title": "Google promises better verification of political ad buyers in US", "type": "story", "url": "https://www.theguardian.com/technology/2018/may/04/google-verification-political-ad-buyers-us-election", "text": "Associated Press in San Francisco \n\nFri 4 May 2018 16.16\u00a0EDT\n\n\nLast modified on Fri 4 May 2018 17.36\u00a0EDT\n\n Google says it will do a better job of verifying the identity of political ad buyers in the US by requiring a copy of a government-issued ID and other information. In a blogpost, the Google executive Kent Walker said the company would also require the disclosure of who is paying for the ad.  He also repeated a pledge he made in November to create a library of such ads that will be searchable by anyone by this summer: \u201cWe\u2019ll also release a new Transparency Report specifically focused on election ads. This Report will describe who is buying election-related ads on our platforms and how much money is being spent. We\u2019re also building a searchable library for election ads, where anyone can find which election ads [were] purchased on Google and who paid for them.\u201d Google\u2019s blogpost stops short of declaring support for the Honest Ads Act, a bill that would impose disclosure requirements on online ads, similar to what is required for television and other media. Facebook and Twitter support that bill. Google says applications under the new system will open by the end of May, with approval taking up to five days."}, {"by": "philk10", "descendants": 0, "id": 17020478, "score": 1, "time": 1525783167, "title": "Shaping Your Company Culture Using the Culture Cycle", "type": "story", "url": "https://spin.atomicobject.com/2018/05/06/culture-cycle-examples/#.WvGaf4XlTh0.hackernews", "text": "Software Consultant and Developer at Atomic Object Grand Rapids A few weeks ago, I wrote an introduction to the Culture Cycle, a model for understanding how culture changes over time. Today, I\u2019d like to give two examples of the Culture Cycle in action, specifically in the workplace. The first example is a real-life success story viewed through the Culture Cycle lens, and the second example is hypothetical. Hopefully, they will inspire you to play a more active role in your company\u2019s culture! As a reminder, the Culture Cycle diagrams four levels of culture\u2014Individuals (I\u2019s), Interactions, Institutions, and Ideas\u2014all of which shape one another continuously over time. In this article from The Washington Post, restauranteur Erin Wade describes how her company was able to largely eliminate instances of customers harassing the restaurant\u2019s staff.  A few years ago, Wade \u201creceived a flood of emails from staff labeled \u2018harassment\u2019\u201d which exposed a persistent problem with customers acting inappropriately. The restaurant responded by implementing a color-coded alert system for identifying and dealing with harassment. Through this system, staff can notify managers about harassment (e.g. \u201cI have an orange at table five\u201d), and the color determines how the manager should respond. The system has been hugely successful and has helped the company move closer to their vision of \u201ca restaurant culture that is inclusive, diverse, and counter to many damaging industry norms.\u201d Wade and her team almost certainly weren\u2019t thinking about the Culture Cycle when they implemented this system. They probably didn\u2019t have company culture on their minds at all. They were simply looking to solve an important problem at their restaurant. Still, I think it\u2019s interesting to look at the problem and solution in terms of the Culture Cycle, since it touches on each of the Four I\u2019s. To begin, Wade states her restaurant\u2019s goals of inclusivity, diversity, and activism within the industry. This absolutely fits the bill for a cultural Idea, especially since it has implications about how the world should be, not just how the restaurant should work. Their solution was to take action at the Institutional level. They did not try to directly change the way customers and staff interact, and they did not try to directly change their customers, even though they were the\u00a0root of the problem. Nor did they resort to firing managers for being unresponsive.  Making a policy change was the right type of action because it effectively helped to re-align Individuals and Interactions with the company\u2019s cultural Ideas. Of course, they could have made this policy decision while also trying to change the people involved, but they didn\u2019t have to do so. That\u2019s because the idea behind the policy change \u201ctrickled down\u201d over time to Individuals and Interactions. The single policy change actually affected Individuals in a number of ways. Obviously, it made the staff feel more comfortable because it helped to guard against unwanted interactions. It affected management by providing a default reaction when staff experience harassment, which, in turn, made staff-manager interactions less uncomfortable. Lastly, it changed the behavior of customers, and maybe even drove some away (which is a good thing in this case). It\u2019s also very possible that the policy attracted new customers who are more in tune with the restaurant\u2019s culture, especially after receiving press like the story in The Washington Post. The effects don\u2019t end with the restaurant\u2019s Individuals, either. Their Culture Cycle is right in the middle of the dining industry\u2019s culture, which of course has its own cycle. Wade notes that the restaurant industry has seen a shift in how people tolerate harassment, and she and her team have contributed to that movement while shaping culture in the process. Suppose that your company\u2019s new goal is to promote sustainability within the organization. The Idea is to become less wasteful and more environmentally friendly. Most people like the idea of sustainability, but changing wasteful behaviors can be burdensome or even downright challenging.  Placing recycling bins throughout the office is a good start, but becoming truly sustainable takes significant time and energy. How can the Culture Cycle be used to break this problem apart? Since businesses are already a type of Institution, that\u2019s a convenient and familiar place to start. Some changes may involve the environment, i.e. your office space. For example, adding recycling bins to the office, putting locally-sourced snacks in the pantry, upgrading to an efficient HVAC system, or installing solar panels on the roof are all institutional actions. Other institutional solutions may be policy-based. Wade\u2019s solution in the previous story is a great model of a policy-based solution. One example for this scenario would be requiring everyone to turn off their monitors and lights when they leave the office. Another would be to pay for bicycle maintenance for employees who bike to work. These policies are essentially carrots and sticks to nudge people\u2019s behavior toward a goal. They\u2019re technically extrinsic motivators, but they can have a real effect on culture over time. Now let\u2019s look at how Individuals and Interactions could affect this initiative toward sustainability. After all, most people in most companies don\u2019t have the authority to change\u00a0institutions on a whim. But as an individual, you create the basis for culture\u2013company policies don\u2019t mean anything if you don\u2019t follow them.  The advice here is pretty straightforward: To support a culture of sustainability, act sustainably! Use the aforementioned Institutions correctly, and encourage your peers to do the same. If you\u2019re already supporting the cause, then it\u2019s up to you to reflect that in your Interactions. This doesn\u2019t just mean telling your coworker that their Hummer is contributing to climate change. It means setting a good example and actively looking for ways to make your own actions more sustainable. Conversely, people who\u00a0don\u2019t promote sustainability are going to hinder the process. This is especially true if they\u2019re influential within the company. If someone who\u2019s well-regarded pays no attention to the recycling bins and jokes about how much food they throw away, their coworkers will feel less bad about doing the same thing.  Luckily, given the way culture works, if everyone else already has a sustainable mindset, then the wasteful person probably won\u2019t be very influential. And if everyone already agrees with that person, then the company probably wouldn\u2019t be promoting sustainability. But if everyone\u2019s behavior is more evenly split, the company needs to enforce its new policies to get the desired results.  In fact, two of the most powerful cultural tools are hiring and firing. Neither of those is an easy process. But being careful with the former can help to avoid cultural mismatches which lead to the latter. Obviously, thinking about the Culture Cycle isn\u2019t the golden ticket to making everyone behave the way you want. But from the perspective of an employee or business owner, considering the Four I\u2019s can be a great first step for tackling a Culture-centered problem. And simply focusing on the right level of culture, as Wade did at her restaurant, can yield big results.  I hope that these examples can serve as a guide for problem solving through a cultural lens. If you have any stories about changes in your organization\u2019s culture, please tell us about it in the comments! Your email address will not be published. Required fields are marked * Comment  Name *  Email *  Website  \nDon't subscribe\nAll\nReplies to my comments\n Notify me of followup comments via e-mail. You can also subscribe without commenting.  \n\n   Atomic does more than talk about software. We also make it \u2014 for clients large and small in all kinds of industries. \n\n\t\t\tAtomic is a software design + development consultancy.\n\t\t"}, {"by": "iafrikan", "descendants": 0, "id": 17020471, "score": 1, "time": 1525783085, "title": "6 ways to boost your landing page conversion rates", "type": "story", "url": "https://www.iafrikan.com/2018/05/08/6-ways-to-boost-your-landing-page-conversion-rates/", "text": "Many startups and businesses in Afrika neglect the importance of landing page conversion rates. A typical scenario is that a founder or entrepreneur will explain their mobile and web requirements to their designers and developers, and then they, in turn, give them exactly what they want without any consideration for optimizing conversion rates (especially on the landing page). This comes about as a consequence of lacking experience and knowledge regarding conversion rates, thus everybody sticks with what they know best. Entrepreneurs are typically experts at what they do, designers and developers also. Neither of them is marketers. A conversion rate is the percentage of your site visitors that end up performing the action you require them to, to reach your website business goals. These goals may vary including but not limited to: As such the landing page is the most important aspect of your site. Optimizing your landing page conversion rates can increase your online marketing profitability. You need to have a clear understanding of your desired conversion action, whether it is to sell, download, fill a form, or simply a click-through to another page. Only the desired conversion action should be on the landing page, all other clickable links or choices should preferably be removed on the landing page. Your site visitors have arrived from somewhere. Their expectation was set before they even landed on your page. Ensure that you understand the context from which they arrived. It's critical to align the content of your landing page with their intent and expectations. Reduce visual distractions as much as possible. Avoid using a wide variety of font styles, colors, and sizes on your page. Remove images and interactive rich-media content unless it directly supports your conversion goal and is a clearly superior way of conveying important information. Bland landing pages are often the best at converting visitors, surprisingly. Unless you work for an internationally recognized company, you have no brand-strength or credibility with your online audience. Do everything possible to reduce anxiety on your page by using safe shopping seals and other indicators of your trustworthiness. The logos of trade associations, acceptable payment methods, and money-back guarantee seals can all be powerful ways to make your site visitors feel safer. People want to feel an attraction for your product or service. Site visitors are most likely to convert if they see logos of well-recognised client\u2019s brands and media sources that have covered or mentioned your company. Display prominently testimonials from existing customers. Make sure you tell your site visitors what to do next, spell out exactly what will happen if someone fills out the form or clicks on the desired link. Let your call to action always stand out, avoid clutter in the area around it. In summary, when you increase you landing page conversion rate, your business will grow faster than you might expect. So, you enjoy the content we publish on iAfrikan.com? Do you find the information and insights we share useful? You can now contribute to iAfrikan.com and help us to continue doing what we do best for you by sending as little as $10 via Credit Card/PayPal or send us some Bitcoins. A media brand that produces content, podcasts, events and  publications about Afrika's burgeoning science, technology and innovation sectors and how they affect culture, business and day to day life on the continent. We'd love to hear from you. \n \u00a9 iAfrikan Digital (Pty) Ltd all rights reserved. About | Privacy Policy | Terms of Use | Partnerships & Advertising"}, {"by": "meejay", "descendants": 0, "id": 17020470, "score": 1, "time": 1525783075, "title": "7 Major Problems of Existing Centralized Social Media Platforms and a Solution", "type": "story", "url": "https://steemit.com/socialmedia/@ukg/7-major-problems-of-existing-centralized-social-media-platforms-and-how-doo-address-them", "text": "Howdoo is a decentralized social platform based on blockchain technology. It aims to address the social and economic problems that exist in the social platforms. We are going to visit the 7 major problems that exist and learn how Howdoo is planning to address those. Social media platforms collect valuable user data such as interests, preferences and activities via the actions made during user interaction within the platforms. Social media platforms categorize this big data and sell them to the marketers. This way such platforms obtain enormous revenues. Therefore, this revenue model is essential to existing centralized social platforms. Facebook reported advertising revenues of $9,16 billion in Q2 of 2017. Solution: Howdoo will give users and communities freedom of choice whether they want to see advertisements or not. They will have opportunity to accept advertisements in return of payment. Similarly, users and communities would have full control over their personal data. Besides, they will have control over the types of advertisement they want to see. In addition, a feedback mechanism to be provided will also incentivize advertisers to respect users\u2019 requirements. Content creators are the most important element of social media platforms. The quality or popularity of the contents created determines the number of people attracted to the platform. The traffic occurred on any social platform is an essential indicator for advertisers since it is directly correlated to the number of audiences that can see their advertisements. However, in todays centralized platforms, established players increasingly demonetizing the contents created by content creators with no notification or option to appeal. Moreover, there is a growing concern with shrinking subscriber numbers, and established platforms prefer sponsored content and increase the level of editorial control pressure on content creators. Solution: Howdoo proposes an incentive-based reward program. The program encourages masses to get involved in the platform. It will also reward content creators according to their active contributions. We will transfer the business models and user benefits associated with social media via offering a share of revenue (60 to 70%) to the original content creators. Since advertising is the main revenue stream for the existing social platforms they treat their users as passive consumers of content and advertisement. For this reason, a central body determines who, where and when to advertise. Meanwhile advertisers offer distinct commercial packages to distribute their messages. On the contrary users have no control on the advertisement traffic. Solution: Howdoo will provide users and communities payments for the advertisements they have agreed to consume. In addition, users as individual or communities will have two options either adjusting the amount of advertisement they are going to consume or not to see any at all. Those who accept to consume advertisements will be rewarded by the percentage (depends on the overall proof of contribution score) of advertisement fees paid in \u03bcDoo tokens by advertisers. \nCyber-bullying and trolling are major problems of the centralized social platforms. Since there are no strict deterrent regulations ruling, people often easily offend each other. Besides, centralized platforms are not capable to regulate content in a timely and fair fashion. Frankly, expecting a single body to control the content globally is not possible, especially in a global user base where there are differences in culture, context and generations exist. Solution: Howdoo will provide users with the power of content moderation by creating a fair, decentralized and financially incentivized social network.\n\nContributions will be judged as positive or negative according to a mixture of objective and subjective measures, namely service quality and judgements of other community members respectively. Moderation of both user-created contents and advertisements will be the responsibility of users. As a result, community will decide if a content is offensive or inappropriate in the context of their members\u2019 culture, interests, sense of humour, and opinions \u2013 and penalize anti-social behaviour. Domestic fund transfers are easy to implement, settled at once and free of charge. On the other hand, international fund transfers are expensive and time-consuming processes since there are numerous foreign exchange rate fees, and service fees charged by providers or middleman and long clearing times. There are two billion adults and 160 million small businesses that do not have any banking access. Teller services such as Western Union and MoneyGram allow money to be transferred without a bank account but with very high service fees. Solution: The platform provided by Howdoo will enable people to send money around the world easily similar to sending message or media. The service provided will be independent of conventional banking facilities and provide services for both banked and unbanked users. Providing money transfer within a social network will provide peer-to-peer financial services such as loans. Because online costumers provide feedback that is irrelevant to the needs of the individual, it is challenging to social media marketing professionals to engage with them. Besides there are online marketplaces that fail to link individuals with brands and their products. This is a huge missed opportunity for brands and sellers to nurture a community of like-minded customers, and to create a shared sense of commitment and loyalty to their offering. Solution: Online marketplace and social network are combined inHowdoo which provide a platform for brands and entrepreneurs to sell their products into communities and enable them to develop more personalized relationships with those who show an interest. Such a communication would enable sellers to place a direct value to their goods which is better than relying on unpredictable advertisement revenue. Besides, encouraging users to provide feedback on the relevance and effectiveness of campaigns, advertisers will be able to concentrate their efforts towards people who are most interested in their products and services. Besides the concerns of user hate, adblocking software and high level of fraud, there are transparency and brand association problems. All these problems result in deficient audit perception as a result advertisers could never be sure that their products are being shown and what else their products are appearing next to. Solution: Since blockchain based Howdoo platform gives power to advertisers to hold accurate audits of each location their advertisements reach. Such transparency enables brands to review their activity, accurately assess any result and to continuously adjust their future positions. These capabilities promote the level of control over brands and also enables advertisers to optimize their activity by audience and location. Howdoo is ready to disrupt social network industry with addition of payment options via cryptocurrency. You would check the details at project website. https://howdoo.io/"}, {"by": "mspoonyg", "descendants": 0, "id": 17020466, "score": 3, "time": 1525783023, "title": "I Won't Buy the Oculus Go or Lenovo Mirage Solo (and You Shouldn't, Either)", "type": "story", "url": "https://www.tomsguide.com/us/standalone-vr-headsets-problems,review-5395.html", "text": "No more wires! You don't need a phone! I still don't care! This past week, the stand-alone VR headset era began with the launch of the Oculus Go and Lenovo Mirage Solo. Both gadgets fix two of the biggest problems with virtual reality \u2014 and they both offer unique advantages. But, despite our fairly positive reviews, neither headset addresses the primary reasons why these things tend to lose their novelty faster than you can say \"early adopter.\" Oculus GoOf the two, the cheaper $199 Oculus Go has the potential to reach the most shoppers. The design is lighter than Lenovo's headset and less bulky, and it offers access to more than 1,000 apps and games. Oculus, part of Facebook, also deserves credit for trying to make virtual reality less of a solitary experience. If your friend or family member has the headset, they can join you in a virtual apartment of sorts, where you both can watch photos and movies and talk to each others' avatars. You can even walk up to a table and play a card game together. I tried it during a demo, and it was pretty neat to be interacting with someone in London, even though I was sitting in New York City. The Oculus Go also benefits from having built-in speakers, while the Lenovo Mirage Solo forces you to plug in earbuds. However, while the Oculus controller is pretty easy to use, you can't really move around in virtual reality land. Or dodge. Or duck. Only the pricier $399 Lenovo lets you do this with its six degrees of freedom via WorldSense's motion-tracking technology. Lenovo Mirage SoloThe Lenovo Mirage Solo also packs more power than the Oculus Go, thanks to the former's Snapdragon 835 processor, compared to the latter's aging Snapdragon 821 chip. And while the Solo doesn't offer as many social apps as the Go, you can beam your VR experience to the nearest TV with a Chromecast plugged in. That way, your friends and family can get a taste of your adventures. Too bad the Google Daydream-powered Mirage Solo has only a quarter of the content of the Oculus Go (about 350 titles). Overall, these new headsets feel like a necessary evolution of virtual reality, not the leap forward the category needs for them to become must-have devices. For one, you wouldn't want to be seen in public using either the Go or Solo. Cutting the cord from a geeky headset doesn't cut out the geek part. Second, virtual reality is still waiting for killer apps, or at least titles and franchises that are household names. Where is the Call of Duty in VR? Or Fortnite? Or Star Wars (no, an add-on to Battlefront on the PSVR doesn't count). To me, it feels like publishers and developers like EA and Epic are forever dragging their feet, waiting for true mass adoption before they commit more resources.  MORE: Hands-On with Oculus Go's First Games These stand-alone headsets have another issue, and that's the fact that no one under 13 is supposed to use them. This is the same health-related warning that comes with other headsets, because childrens' eyes are still developing. It's hard to indoctrinate the next wave of VR heads when they can\u2019t participate. Of course, virtual reality has uses other than entertainment and games, such as taking virtual tours and watching educational content. And Lenovo makes it pretty easy to create your own VR content with its Mirage Solo Camera. But it costs $299 for this accessory, and it captures only a 180-degree view, while you can pick up a great 360 camera like the Insta360 for the same price. Not all hope is lost when it comes to stand-alone VR headsets, however. The upcoming Oculus Santa Cruz promises better performance and more immersion via its six degrees of freedom and motion controllers. That will be pricier than the Go for sure, but the premium should be worth it, assuming the quality is that much better. Still, as someone who has extensively played with the Gear VR, only to place it in a drawer, it's hard to envision a different fate for these new headsets.  Credit: Shaun Lucas/Tom's Guide  \nSee Also  :  5 Reasons to Buy Oculus Go (and 3 Reasons to Skip It) \n\n\n\n\n Mark Spoonauer is the editor in chief of Tom's Guide and has been covering technology for more than 15 years. When he's not obsessing over the latest gadgets or appearing on TV, you'll find him running with his smartwatch. Follow him on at @mspoonauer."}, {"by": "aaron_p", "descendants": 0, "id": 17020459, "score": 2, "time": 1525782901, "title": "Key Challenges in Logistics and Transportation Business: How Apps Can Help", "type": "story", "url": "https://www.appfutura.com/blog/key-challenges-in-logistics--transportation-business-how-apps-can-help/", "text": "In the modern digital world, the logistics and transportation industry has to innovate in order to conquer challenging frontiers. The industry understands the importance of digitalization for tackling challenges and improving efficiency. In the past couple of years, technology has crept into the logistics industry. Several tech trends have influenced the growth in the logistics industry. Some of the trends that are transforming the industry include: Mobile App Solutions: Tackling Critical Challenges in Logistics Industry The app ecosystem has facilitated processes in almost every industry. Just like any other industry, the logistics and transportation sector also has peculiar challenges. Achieving cost-efficiency is the prime-focus of every logistics company. Reducing the cost of operations requires managing various routine aspects efficiently. Logistics Business Challenges Through data analytics and tracking, mobile apps can handle critical challenges. According to an industry report by PwC, data analytics will be of high importance during the next five years. A mobile app can check operations, manage data and help in achieving cost-efficiency. Owing to digitalization, the sector has access to data like never before. The logistics & transportation industry can use data to improve the performance and delight customers in a better way. Enterprise Mobility Solutions: Streamlining Operations, Enhancing ROI According to Digital Strategy Consulting Research, 60% of workers are using apps for work-related activities and this has led to an increase in the productivity. Enterprises have gained 240 employee hours each year i.e. 6 additional weeks per employee after deploying mobility solutions. Companies are able to track resources, get real-time data, manage data, track routes, ensure vehicle safety and delight end-customers by providing accurate information about their cargo. Mobile apps have facilitated communication, improved reporting and enabled management to make informed decisions. A connected network is capable of sharing real-time information, leading to better operations. An improvement in operations has a positive impact on customer delight and revenues in the long term. Solving Logistics & Transportation Business Issues through Mobility Solutions The disruptive powers of technology can help the industry in solving critical challenges. Through efficient deployment, mobile apps can shape the future of logistics industry. Inventory Management at Fulfillment Centers The success of a logistics enterprise is determined by its inventory management abilities. With the help of an automated inventory management app, accessing real-time cargo information is easier. Integrating cloud capabilities in a mobile app will enable the management to access inventory data. This will ensure better grip over cargo as well as storage facilities at different locations. Documentation & Paperwork Handling Cargo handling is a time-intensive process that requires a lot of paperwork and documentation. Through logistics management app, a company can digitize records easily. Digitization promises better management than a paper-based system and also improves customer satisfaction. Fleet & Labour Management Labour and fleet are the greatest assets of a logistics company. No operation can be complete without involvement of these two elements. So, in order to improve operations and ROI, these need to be managed, optimally. GPS-enabled mobile apps can help in keeping a record of miles driven, fuel utilized and vehicle health. Mobile solutions can connect to the vehicle and pass on notifications for vehicle maintenance and repairs. Such solution can also ensure seamless communication with drivers across geographical territories. Delivery Schedule Optimization Optimization is an important aspect when it comes to cost reduction. Mobile apps can help in enhancing business efficiency by passing on deep insights into transportation routes. These can be used to optimize deliver schedule by selecting routes that reduce fuel consumption and time required. With smart optimization for delivering schedules, customers\u2019 satisfaction can be improved, which leads to better revenues in the long run. Manpower Training Manpower training remains one of the most burning challenges when it comes to digitizing operations. According to Logistics Industry Report by PwC, 50% of the owners and managers say that the lack of digital culture and training is the biggest challenge in the sector. Mobile apps can help in removing this bottleneck, too. Intuitive training modules can be developed and made accessible for drivers, supervisors and other employees anywhere, anytime. This will enhance personal skills and capabilities that will reflect in the improvement of overall efficiency. When it comes to building smart mobile apps, Intuz is a leader. Intuz can create highly-functional mobile apps that improve operations and boost productivity. The company has developed numerous mobile applications that benefit businesses at various levels. A robust app development process gives Intuz edge over others. So, if you want to succeed in the logistics industry, you should adopt a tech-driven approach and contact mobility experts at Intuz, now. Find more top mobile app development companies worldwide on AppFutura. Intuz is an industry-leading mobile app development & cloud consulting company, serving the global market since 2008. We offer end-to-end solutions for enterprise mobility, IoT, AR/VR, AI, Wearables & App marketing. As a true partner for Startups & Enterprises, we bring our years of insights and domain expertise that will add a lot of value to your app solution. We use Cookies to improve and perform a navigation analysis on the website. If you continue browsing, we understand you are accepting its use. You can get more information or learn how to change the settings in our Cookies Policy. AppFutura is the community for mobile app developers where they can meet people or companies looking to hire the best firms to develop an app project. Copyright 2018 - AppFutura.com"}, {"by": "aaron_p", "descendants": 0, "id": 17020456, "score": 1, "time": 1525782873, "title": "Google News is being reborn with faster load times and a focus on video", "type": "story", "url": "http://www.trustedreviews.com/news/google-news-redesign-3468753", "text": "Google is working on a new version of Google News that will incorporate\u00a0elements of Newsstand and YouTube, according to a new report.\u00a0 Sources reportedly familiar with Big G\u2019s plans told\u00a0digital outlet\u00a0AdAge that it\u2019s aware there are a number of Google services where news is hosted, and it\u2019s looking to bring them under the same umbrella with the updated build of Google News. The sources say that Google will bake the core features from Newsstand into Google News, later scrapping the standalone application, while also integrating the dedicated News section of YouTube into the new version of the News client. To be clear: YouTube will emerge unscathed, with the service continuing to function the exact same as it does now\u00a0\u2013 no sections will be added or removed. And that makes a lot of sense seeing as it\u2019s one of the firm\u2019s most popular products. Google has already briefed some of its largest publishers on the impending changes and is expected to tell the world about them at its Google I/O 2018 Developer Conference, which commences on May 8 and runs through May 10. Related: Google I/O 2018 \u201cIt\u2019s a consolidation of all the ways you can interact with news on Google,\u201d one publishing executive told AdAge.\u00a0\u201cThere are a lot of Google services where you find news, and what they\u2019re trying to do is bring it all under one brand.\u201d Do you think Google News is in need of a redesign? Let us know your thoughts over on Facebook or Twitter @TrustedReviews."}, {"by": "forrestbrazeal", "descendants": 0, "id": 17020451, "score": 1, "time": 1525782820, "title": "How to decide when DynamoDB is right for you", "type": "story", "url": "https://read.acloud.guru/why-amazon-dynamodb-isnt-for-everyone-and-how-to-decide-when-it-s-for-you-aefc52ea9476", "text": "By 2004, Amazon\u2019s business was already stretching the limits of their Oracle database infrastructure. In order to scale the growing business, AWS designed an award winning internal key-value store\u200a\u2014\u200aAmazon Dynamo\u200a\u2014\u200ato meet their performance, scalability, and reliability requirements. Amazon Dynamo now underlies much of Amazon.com and defined an entirely new category of key-value store databases\u200a\u2014\u200a\u201cNoSQL\u201d. In 2012, AWS announced the availability of DynamoDB as a fully managed NoSQL data service to customers with the promise of seamless scalability. As Dynamo celebrates its tenth anniversary, AWS should consider a companion service called \u201cWhynamoDB\u201d. Every time a developer attempts to provision a new DynamoDB table, the service would pop-up in the AWS Console and simply ask: \u201cWhy?\u201d The answer to \u201cwhy use DynamoDB\u201d isn\u2019t as straightforward as the marketing promise of seamless scalability. Over the past few weeks, I interviewed a number of engineers and developers about their experiences with the database service. As great as DynamoDB is\u200a\u2014\u200aand as rousing as its success stories are\u200a\u2014\u200ait has also left plenty of failed implementations in its wake. In order to understand what causes some DynamoDB implementations to succeed and others to fail, we need to examine the essential tension between DynamoDB\u2019s two great promises\u200a\u2014\u200asimplicity and scalability. I really can\u2019t overstate how easy it is to start throwing data in DynamoDB. The AWS team has done a great job of abstracting away complexity\u200a\u2014\u200ayou don\u2019t have to log into a management studio, you don\u2019t have to worry about database drivers, you don\u2019t have to set up a cluster. To get started with DynamoDB, just turn a knob for provisioned capacity, grab your favorite SDK, and start slinging JSON. With that feature set, it\u2019s no wonder that DynamoDB is especially attractive to \u201cserverless\u201d application developers. After all, a lot of serverless apps start as prototypes, prioritizing speed of delivery and minimal configuration. Why mess with a relational datastore when you don\u2019t even know yet what your final data model is going to look like? At this point, we need to make a key distinction\u200a\u2014\u200ano pun intended. DynamoDB may be simple to interact with, but a DynamoDB-backed architecture is absolutely not simple to design. DynamoDB is a key-value store. It works really well if you are retrieving individual records based on key lookups. Complex queries or scans require careful indexing and are tricky or straight-up inadvisable to write\u200a\u2014\u200aeven if you don\u2019t have a terribly large amount of data, and even if you have some familiarity with NoSQL design principles. That last part is the kicker, of course\u200a\u2014\u200athere are an awful lot of developers who don\u2019t know much about NoSQL compared to classic relational database design. Moreover, prior NoSQL experience isn\u2019t always a net positive. I spoke to a few engineers whose teams were burned when they brought a bunch of expectations from MongoDB, a document database, to their DynamoDB implementation. So when you combine inexperienced devs, the lack of a clear plan for how to model a dataset in DynamoDB, and a managed database service that makes it really easy to ingest a lot of unstructured data\u200a\u2014\u200ayou can end up with a solution that spirals out of control even at a small scale. Lynn Langit, a cloud data consultant with experience in all three of the big public clouds, has seen enough of these botched implementations to be justifiably wary of businesses relying on NoSQL solutions like DynamoDB. When I interviewed Lynn recently for the \u201cServerless Superheroes\u201d series, she shared a story about moving a client from DynamoDB to Aurora\u200a\u2014\u200athe AWS homegrown relational database service\u200a\u2014\u200aeven though the AWS reference architecture for their project used DynamoDB. A relational database will do most anything you need at small scale. It might take a little longer to set up initially than DynamoDB, but the well-established conventions of a SQL implementation can protect you from a lot of wasted time down the road. This isn\u2019t because DynamoDB is worse technology\u200a\u2014\u200abut because it is new to you, and things that seem \u201ceasy\u201d and \u201cconvenient\u201d will absolutely bite you if you do not understand them. Now explore the other end of the spectrum\u200a\u2014\u200agreat big DynamoDB tables. For this article I interviewed happy customers getting sub-second latency with billions of records in their DynamoDB tables. DynamoDB promises consistent performance at essentially infinite scale, limited only by the physical size of the AWS cloud. Without exception, these customers are right in the center of DynamoDB\u2019s canonical use case\u200a\u2014\u200adoing key-value lookups on well-distributed records, avoiding complex queries, and most importantly, limiting hot keys. Dealing with hot keys is undoubtedly DynamoDB\u2019s best-known \u201cgotcha\u201d. The issue with hot keys is well explained in many places including the DynamoDB developer\u2019s guide documentation. Although DynamoDB can scale indefinitely, your data isn\u2019t stored on a single, magic, ever-expanding server. As your data grows larger than the capacity of a single DynamoDB shard, or \u201cpartition\u201d (up to 10 GB), it gets divided into chunks, with each chunk living on a different partition. If you have a \u201chot\u201d key in your dataset\u200a\u2014\u200aa particular record that you are accessing frequently\u200a\u2014\u200ayou need to be sure that the provisioned capacity on your table is set high enough to handle all those queries. The \u201cgotcha\u201d is that you can only provision DynamoDB capacity at the level of the entire table\u200a\u2014\u200anot per partition\u200a\u2014\u200aand the capacity is divided up among the partitions using a fairly wonky formula. As a result, your read and write capacity on any given record is a lot smaller than your overall provisioned capacity. So if your application is using too many RCUs on a single key, you either need to over-provision all the other partitions (expensive), generate a ton of \u201cThroughput Exceeded\u201d errors (not ideal), or figure out how to decrease access to that key. One takeaway here is that DynamoDB isn\u2019t necessarily suited to datasets that have a mix of hot and cold records. But at sufficiently large scale, every dataset has such a mixture. You could split up the data into different tables, of course\u200a\u2014\u200abut if you do that, you\u2019ve lost the scalability advantage that DynamoDB was supposed to be providing in the first place. A blog was recently published on this subject called \u201cThe Million Dollar Engineering Problem\u201d. It showed how Segment substantially decreased their AWS bill by fixing hot key-related DynamoDB over-provisioning. The most interesting part of that article is the \u201cheatmap\u201d graphics showing exactly which partitions were the troublemakers. Now, if you read the fine print, those cool graphics came from AWS\u2019s internal tools, not from any monitoring Segment was able to do on their own. In other words, somebody from Segment had to get on the phone with the DynamoDB team in order to get observability into their database problems. Even at that point, their strategy for blocking the offending keys was a matter of wrapping DynamoDB calls in a try/catch\u200a\u2014\u200aand executing custom trace logic if a particular key tripped a throughput exception. In effect, Segment had to fight the hot keys problem with a blindfold on, and this is where we get back to the tension between simplicity and scale. DynamoDB is designed as a black box with very few user-accessible controls. This approach makes it easy to use when you are just getting started. But at production scale\u200a\u2014\u200awhen edge cases rule your life\u200a\u2014\u200asometimes you desperately need more insight into why your data is misbehaving. You need a little bit of compassionate complexity. This is not a problem with the architecture of Dynamo. It\u2019s a problem with what AWS has chosen to expose through the service of DynamoDB. At this point, we haven\u2019t even touched on the issue of backups and restores\u200a\u2014\u200asomething DynamoDB doesn\u2019t support natively and which gets awfully tricky at scale. The inability to back up 100TB of DynamoDB data was apparently a big reason why Timehop recently moved off the service altogether. So if DynamoDB is just one of many plausible options at small scale, and has limited viability as a service at large scale\u200a\u2014\u200awhat is it good for? If you ask AWS, almost anything. After all\u200a\u2014\u200aWerner Vogels says the original Dynamo design could handle about 90% of Amazon.com\u2019s workloads. With the exception of certain special cases like BI analytics or financial transactions, it\u2019s true that you can redesign just about any application to move the business relationships out of the database, store state in a K/V table, and use event-driven architecture to your heart\u2019s content. But as my computer science professor used to say, it\u2019s also true that \u201cjust because you can, doesn\u2019t mean you should.\u201d If you don\u2019t fully understand why you are using DynamoDB at the outset, you\u2019re likely to end up like Ravelin spinning your wheels through several code rewrites until finally landing on a solution that more or less works\u200a\u2014\u200abut you still kind of hate. This is why Lynn Langit has more or less abandoned NoSQL as a solution for small and medium-size businesses. It\u2019s why Timehop moved from DynamoDB to Aurora, and why another well-known company I interviewed has moved to \u201ca giant ElasticSearch cluster\u201d. It\u2019s also why DynamoDB has gobs of case studies from satisfied customers at famous brands. Not because one of these technologies is uniformly better than another\u200a\u2014\u200abut because the engineers in each company, with their specific use cases and levels of expertise, were able to deliver business value most quickly and effectively with differing solutions. At some point, Amazon may announce the release of the WhynamoDB service that asks \u201cwhy you are provisioning a DynamoDB table\u201d. In preparation for the launch, I\u2019ve created this handy decision tree that guides you through the WhynamoDB service. What\u2019s your experience and thoughts on DynamoDB? I\u2019d be interested in hearing your thoughts in the comments below! If you enjoyed this article, be sure to check out my FaaS and Furious comic series. You can follow on Twitter where I\u2019m @forrestbrazeal. By clapping more or less, you can signal to us which stories really stand out. Cloud architect @Trek10inc, words and cartoons @acloudguru. Previously cloud infrastructure @Infor. Opinions here are mine. The #1 community-sourced collection of cloud computing and serverless articles curated and published by A Cloud Guru. We are on a mission to teach the world to cloud."}, {"by": "mhb", "descendants": 1, "id": 17020444, "kids": [17020568], "score": 2, "time": 1525782709, "title": "Google Bans Bail Bond Ads, Invites Regulation", "type": "story", "url": "https://marginalrevolution.com/marginalrevolution/2018/05/google-bans-bail-bond-ads.html#comments", "text": "Google: Today, we\u2019re announcing a new policy to prohibit ads that promote bail bond services from our platforms. Studies show that for-profit bail bond providers make most of their revenue from communities of color and low income neighborhoods when they are at their most vulnerable, including through opaque financing offers that can keep people in debt for months or years. Google\u2019s decision to ban ads from bail bond providers is deeply disturbing and wrongheaded. Bail bonds are a legal service. Indeed, they are a necessary service for the legal system to function. It\u2019s not surprising that bail bonds are used in communities of color and low income neighborhoods because it is in those neighborhoods that people most need to raise bail. We need not debate whether that is due to greater rates of crime or greater discrimination or both. Whatever the cause, preventing advertising doesn\u2019t reduce the need to pay bail it simply makes it harder to find a lender. Restrictions on advertising in the bail industry, as elsewhere, are also likely to reduce competition and raise prices. Both of these effects mean that more people will find themselves in jail for longer. As with any industry, there are bad players in the bail bond industry but in my experience the large majority of providers go well beyond lending money to providing much needed services to help people navigate the complex, confusing and intimidating legal system. Sociologist Joshua Page worked as a bail agent: In the course of my research, I learned that agents routinely offer various forms of assistance for low-income customers, primarily poor people of color. It\u2019s very difficult for those with limited resources to get information, much less support, from overburdened jails, courts, or related institutions. Lacking attentive private attorneys, therefore, desperate defendants and their friends and families turn to bail companies to help them understand and navigate the opaque, confusing legal processes. \u2026In fact, even when people have gone through it before, the pretrial process can be murky and intimidating\u2026.[A]long with walking clients through the legal process, agents explain the differences between public and private attorneys and the relative merits of each. Discussions regularly turn to the defendant\u2019s case: Is the alleged victim pressing charges? Will the case move forward if he or she does not? When is the next court date? If convicted, what\u2019s the likely punishment? Any chance the charges will get dropped? \u2026In a classic 1975 study, sociologist Forrest Dill argued: One of the key functions performed by attorneys in the criminal process is to direct the passage of cases through the procedural and bureaucratic mazes of the court system. For unrepresented defendants, however, the bondsman may perform the crucial institutional task of helping to negotiate court routines. Dill\u2019s observation still rings true: bail agents and administrative staff (at least in Rocksville) act as legal guides for defendants who do not have private attorneys\u2014and at times they provide this help to defendants with inattentive hired counsel. They provide information about court dates and locations, check the status of warrants, contact court staff on defendants\u2019 behalf (especially when the accused have missed court or are at risk of doing so), and, at times, drive defendants to their court dates. These activities help clients show up for court, thereby protecting the company\u2019s investments. The bail agents are not purely altruistic, they are in a competitive, service business and it pays to help their clients with kindness and care. When I asked one bail agent why he was so polite to his clients and their relations\u2013even when they had jumped bail\u2013he told me, \u201cwe rely a lot on repeat business.\u201d Ian Ayres and Joel Waldfogel also found that the bail bond system can (modestly) ameliorate judicial racial bias. Ayres and Waldfogel found that in New Haven in the 1990s black and Hispanic males were assigned bail amounts that were systematically higher than equally-risky whites. The bail bondpersons, however, offered lower prices to minorities\u2013meaning equal net prices for people of equal risk\u2013exactly what one would expect from a competitive industry. My own research found that defendants released on commercial bail were much more likely to show up for trial than statistical doppelgangers released by other methods. Bounty hunters were also much more likely than the police to capture and bring to justice people who did jump bail. The bail bond system thus provides an important public service at no cost to the public. In addition to being wrong-headed, Google\u2019s decision is disturbing because it is so obviously a political decision. Google has banned legal services like bail bonding and payday lending from advertising on Google in order to curry favor with groups who have an ideological aversion to payday lending and the bail system. Google is a private company so this is their right. But every time Google acts as a lawgiver instead of an open platform it invites regulation and political control. Politicians on both sides will see that Google\u2019s code is either a quick-step to political power without the necessity of a vote or a threat to such power. Personally, I don\u2019t want to see greater regulation but if, for example, conservatives decide that Google doesn\u2019t represent their values and threatens their interests, they will regulate. Google\u2019s decision to use its code as law is an invitation to politicization. Moreover, Google is throwing away its best defense against politicization\u2013the promise of neutrality and openness. This should be used in economics 101 across the country. Great post Alex. But this is odd.  \"We made this decision based on our commitment to protect our users from deceptive or harmful products, but the issue of bail bond reform has drawn support from a wide range of groups and organizations who have shared their work and perspectives with us, including the Essie Justice Group, Koch Industries, Color of Change and many civil and human rights organizations who have worked on the reform of our criminal justice system for many years.\" Why does Koch not support bail bonds? That is from the link provided by google. Are you possibly confusing bail with an industry that profits from providing bail bonds? As can be seen from this - 'Technology behemoth Google is revving up its support for bail reform and it's getting help from one of the most influential groups in Washington \u2014 the Koch political network. Google is partnering with Koch Industries, the multibillion-dollar conglomerate owned by political financiers Charles and David Koch, in an event Tuesday to push for changes to the bail code within the United States, according to an invitation obtained by CNBC. The event is expected to take place in Washington and, among others, leaders of the Koch network are expected to attend. Several will be making comments at the event. Mark Holden, general counsel and senior vice president of Koch Industries and chairman of the board of directors for Freedom Partners, one of the network's nonprofit political advocacy groups, confirmed the event. He explained that he's hoping to see more due process before someone has to pay a bail fee that at times can exceed $20,000. \"Our desired state is that after people are arrested, there should be a risk assessment done, a determination if they are a risk to public safety\" and then a decision should be made on whether they should be detained and pay bail, Holden said. He noted this was not the first time Google and the Kochs had teamed up to push for criminal justice reform.' https://www.cnbc.com/2018/05/07/google-and-koch-brothers-team-up-for-bail-reform.html I'm guessing that Prof. Tabarrok was not invited to attend. I noticed about a couple of years ago Koch broke ties with their traditional (white) libertarian base and started partnering with more American liberal groups like ones that cater to African-Americans.  This might be part of that trend. One problem is that with bonds available bail must be higher.  The system comes to use the ability to bond as ability to pay.  It is the free market outcome.  Including therefore a steady income for bondsmen even as everyone shows up for court.  A rent. Personally I think the answer would be less bail and more voluntary tracking.  But that would cost cities money up front.  New systems.  And for which constituencies? Good observation. The block behind the criminal courthouse here in NYC are several bail bondsmen while inside the criminal courthouse are defendants primarily \"people of color.\" So the upshot of Google's decision is that defendants will be less able to shop around -- not that there will be fewer people of color as defendants or that the several bail bondsmen around the corner will price their products more competitively. Sure, Google is taking action \"in order to curry favor\" .... just as you are with this post. Funny thing is, you're both sucking up to the left -- by proposing completely opposite things.    Sane people find this hilarious. It's so challenging to claim the moral high ground these days, isn't it?    If a business serves primarily people of color, is it racist for making a profit off of them?    Or is it racist to decry the business, because it is providing a service to them?     At least we can all agree that the reason the business exists is of course -- wait for it -- racism!    But does that mean the business should be celebrated, or destroyed?    Only economists can tell us!     Please keep this up.    The best part is that it has gotten you so wound up that you think your \"repeat business\" anecdote helps your cause, when it does the opposite. I honestly have no idea what any of the above means. Can someone please translate? Hm I thought it was very clear. Are you being obtuse or are you just dense? I favor the former but either way, I am not going to oblige your trollish request. This is so annoying. I disagree with everything that guy wrote, but you\u2019re making me want to defend him by pulling this shit. You understand that banning ads is not banning look-up, right? Interesting choice, to allow search (as does Yelp, BTW), but not ads. the point of ads is that they show up first when you search so this policy is just harassing people who are searching for bail bonds The natural results for \"bail bonds my town\" would be bondsmen as well.  They might be more established firms though, and this might restrict entry to the business. This: Whatever the cause, preventing advertising doesn\u2019t reduce the need to pay bail it simply makes it harder to find a lender.  As someone that has been on the wrong side of the bail desk (a long time ago), you can *literally* be reduced to using a bondsman whose number you can see outside of the Sheriff's office or the equivalent of an index card with five variations of AAAAAAA1! Bonds taped to a wall in a rural Louisiana county. It may not make it harder to find a good one. Without ads the top result is the natural one, the company with the highest page rank. And as I say, Yelp ranks them by user experience. I beg to differ.  How many Yelp reviews are you going to find for bondsmen in Opelousas, Louisiana, that are under six months old?   Pull your head out of your metro area bias and understand you can't find a Yelp review less than a year old for the most popular Mexican restaurant in Jackson, Mississippi (as was the case the last time I visited there; a large town by Mississippi standards, btw). I'm not a person that ever clicks on ad links for anything, but I also didn't have this resource at 3AM in the middle of nowhere when someone was in jail.  The ad \"resource\" is the Google magic of knowing what businesses are open when, which ones serve your particular area of the county or parish (it's not as simple as matching zip codes, and often times the arrestees don't know *exactly* where they're at, especially when arrested on the interstate.  This also becomes a problem as different arresting agencies can take you to different facilities even if you're arrested in the exact same place.) So, no, it *does* make it harder for the low income, low information user to have more choices made available.  You're talking about people (I'm talking about poor trash, not any particular race of trash) that show up late to court, and can't understand how to get their electric bills paid on time at the local grocery store, etc.   It's not supposed to be the job of the elites to decide what books (i.e. resources) are in the available information library of the poor.  And let's not kid ourselves, the poor of *any* color are most likely to be impacted by this restriction of information dissemination.  Yet another sign of the continued infantilization of the lower class folks by Techno Social Justice Warriors. More information is better only when it suits your agenda.  Be intellectually consistent. I am not really coming from that direction.  I am more inclined to think \"kids today\" can navigate the web just fine, and \"missing ads\" won't slow them down much.  Heck, ask your phone \"bail bonds\" and default search will be location based. Maybe the follow-on question for Alex should be: If we all did that test, should the fact that we searched \"bail bonds\" be in our permanent record?  Or for sale? \"Kids these days\" don't form the bulk of who ends up in jail in rural areas. You don't have your phone in your possession when you get booked.  You're relying on someone else to figure this stuff out, someone that is not at your location. Google shows ads for abortion services in Jackson, MS.  Abortion is disproportionately used by people of color in the area.  Meh?  It also shows ads for casinos and gambling, which all of us math smarties know is just a tax predominately on the poor.  Meh Meh? a lot of virtue signaling is about showing that you are not poor by supporting policies the hurt poor people Just seems so weird.  Bail bonds help people get out of jail. Google continues to go off the deep end with its politics. Every day I get closer to de-googling myself. Wait, Google does not want to take an advertiser's money, but does nothing to change search results? And this is a problem for a libertarian why? 'In addition to being wrong-headed, Google\u2019s decision is disturbing because it is so obviously a political decision.' And this is a problem for a libertarian why? 'But every time Google acts as a lawgiver instead of an open platform it invites regulation and political control.' No company is required to accept any and all advertisers - and using the term 'open platform' is another one of those fascinating examples of having absolutely no understanding of what a term means. A lack of understanding underlined by this - 'Politicians on both sides will see that Google\u2019s code ....' Google's code has nothing to do with what advertising business it accepts or rejects, as clearly 'code of conduct' is not what you mean when misapplying Lessig's several decades old thinking - 'This regulator is code--the software and hardware that make cyberspace as it is. This code, or architecture, sets the terms on which life in cyberspace is experienced. It determines how easy it is to protect privacy, or how easy it is to censor speech. It determines whether access to information is general or whether information is zoned. It affects who sees what, or what is monitored. In a host of ways that one cannot begin to see unless one begins to understand the nature of this code, the code of cyberspace regulates.' https://harvardmagazine.com/2000/01/code-is-law-html What business an advertiser decides to reject has basically nothing to do with Lessig's writing - which is from 2000, by the way. \"But every time Google acts as a lawgiver instead of an open platform it invites regulation and political control.\" This isn't intuitive to me. By regulating its own platform, isn't Google demonstrating that it doesn't need external regulation? Conversely, if Google didn't moderate its ad platforms, they would be full of harmful content and certainly invite regulation. It is fair to say that determining bail bond ads to be harmful to the public isn't something that people of all political stripes would agree to. But Google is presumably claiming to do so in order to reduce harm. Well, why talk about an advertiser when we can see the real effect of a real law on a real web site - 'You can still find furniture or a roommate on Craigslist. But ads seeking romance or sexual connections are no longer going to be available, after Craigslist took down the \"personals\" section Friday for its U.S. site. The company says it made the change because Congress has passed the Fight Online Sex Trafficking Act, meant to crack down on sex trafficking of children. It was approved by a landslide in the Senate earlier this week, as NPR's Alina Selyukh has reported, but has been met with criticism by free speech advocates and sex workers. As Craigslist wrote, the law seeks \"to subject websites to criminal and civil liability when third parties (users) misuse online personals unlawfully.\" \"Any tool or service can be misused,\" Craigslist said. \"We can't take such risk without jeopardizing all our other services, so we are regretfully taking craigslist personals offline. Hopefully we can bring them back some day.\" The site added: \"To the millions of spouses, partners and couples who met through craigslist, we wish you every happiness!\"' https://www.npr.org/sections/thetwo-way/2018/03/23/596460672/craigslist-shuts-down-personals-section-after-congress-passes-bill-on-traffickin Strange how that seemed to slip under the radar of this web site. The objection to the bail bond system is that it encourages courts to require bail (i.e., the courts and the bail bondsmen are co-dependent): absent the bail bondsmen, courts are more likely to choose alternatives.  As an economist, Tabarrok  usually sees through the lens of incentives. This is one instance in which his preference for freedom of choice (i.e., letting bail bondsmen advertise on Google) outweighs his training as an economist. I can't say he is wrong, just a bit myopic. To be clear, bail does provide an incentive for the accused to appear in court, but the point of those who object to the bail bond system is that alternatives aren't tried. But in its own way, it kinda privatizes the process of pretrial accountability. I\u2019m sure bondsmen are far better and more cost effective at preventing their clients from jumping than anything the government could do. I think Google just went for the \"less bad\" path, because very few people want to see these ads: https://youtu.be/bdfoojXzuWI  So, put on Google's shoes for a min, they got 3 alternatives: Alternative A is that Google let everyone see these ads. Eventually, people stops laughing and complain to Google for getting ads aimed at \"criminals\". Also, quite probably bail agents can't fund an ad campaign that reach people nationwide. Alternative B is that Google target ads to the intended audience: poor people of color. The implementation of alternative B takes 1e-9 seconds to be leaked to the press and Google is blamed for racism, collaborating with low status bail agents, etc, etc...... Alternative C: ban these ads, get applause from some group, derision from Mr. Tabarrok while safely navigating the treacherous waters of public opinion. Alternative C allows Google to run their business with minimal controversy. Busted Bunny was good.  But yes, I agree. Isn't the argument with payday lenders that the people consuming their service aren't equipped to properly understand the agreement they are making.  As such they end up in a worse situation then if the option weren't allowed. We don't let children make certain decisions because we don't think they have the mental capacity to do so responsibly.  With the poor (who are generally impulsive, low IQ, and easily manipulated) we often try to prevent them from being taken advantage of by high IQ manipulative people.  Debt with high rates of interest is a classic case of where the poor often don't grasp the long term consequence of their agreements, and the businesses lending to them usually do their best of make it as confusing and non-transparent as possible (lots of fine print). Without payday lending people might just borrow and spend less money, which might be good for them and the rest of us (prices would come down). P.S. We can believe that the poor can understand the choices they make in the grocery store or other market context, and still believe that people who can't do algebra might not understand complex lending agreements. So basically Google collapsed in the face of some pressure from a few Hard Left political groups - and the Koch Brothers.  It is inevitable that they will push for regulation soon.  It is the only way they can grow a spine.  They will be shaken down by everyone now.  Now that people know they can be.   The only solution is to push the Feds to regulate them so that they can say \"we would love to but we are not sure how that would affect our legal position.   We are already fully complying with whatever toothless regulation we lobbied for\". Better yet they can push for a binding code of conduct that not only allows them to do pretty much what they want to - and ban pretty much anyone they like - but they can try to hobble their competition by getting them included too.  Best of all if the process is expensive and time consuming.  That will cripple upstarts like Duck Duck Go. I would be sympathetic but given Google is happily banning all the voices on the Right they can, they have regulation and everything else coming to them. Google is already politicized. Conservatives have already decided that Google doesn\u2019t represent their values and threatens their interests. Also Facebook and Twitter. Google is probably at the point: In for a penny, in for a pound. They are pretty much daring congress to regulate them.  Regulation is not necessarily a bad thing at their stage of growth, it may reduce competition and lock them in as the leader. It will depend how Congress regulates them. I would not bet on politicians making a wise choice. Alex Tabarrok\n Email Alex\n Follow @atabarrok Tyler Cowen\n Email Tyler\n Follow @tylercowen"}, {"by": "wyclif", "descendants": 0, "id": 17020441, "score": 1, "time": 1525782660, "title": "Why Spotify Will Never Make Money", "type": "story", "url": "http://fortune.com/2018/05/03/spotify-earnings/", "text": "Spotify announced its first quarterly results as a public company on Tuesday, and the market was not impressed. Shares of the music streaming service fell over 7%, even though the company met its goals for customer and revenue growth. Unfortunately for Spotify, Wall Street was hoping it would beat expectations \u00e0 la Facebook or Google (which regularly surpass stated targets) and when it didn\u2019t, investors sulked. Those investors better get used to disappointment. That\u2019s because unlike other tech companies, music streaming services face a fundamental business problem they can\u2019t overcome. That problem is the price of music or, in this context, what accountants call \u201ccost of goods sold.\u201d The term refers to materials a company must acquire to sell its product\u2014for instance, the flour and sugar a baker must buy to sell donuts. In the case of streaming services like Spotify, they are uniquely dependent on a single supplier\u2014the music industry\u2014to provide them the goods for their product. And that supplier has both a monopoly and deep political influence to protect it. Over the last ten years, the music industry has waged a relentless war in Congress and the courts to force streaming companies to pay more for the music they play. The justifications for the payments are often shaky\u2014for instance, record labels and 1960s musicians sued Pandora over and over to pay an unprecedented royalty on pre-1972 recordings (essentially new money for old rope) until the company capitulated. But regardless of whether such payments are justified, what matters is the music industry has the clout to demand them. The industry has a phalanx of lawyers to push their case in court, and is effective at charming Congress with lobbyists and celebrity star power. The likes of Spotify and Pandora are badly outgunned. The upshot is, no matter how many subscribers they add, the companies will never enjoy the fat profits of other tech firms. Right now, the streaming services have yet to make any money and, if they ever do, it\u2019s a safe bet the music industry will find a way to claw it back in the form of higher royalties. It\u2019s much like the baker being totally beholden to a flour supplier that raises its prices every time donuts are on the verge of being profitable. To see how this plays out from an investor\u2019s perspective, take a look at the share price of Pandora, Spotify\u2019s older cousin, over the past five years: And lest anyone think the music industry is going to let up, a new piece of legislation known as the Music Modernization Act just passed the House of Representatives. While the bill would do much to clean up a messy royalty collection system, it also contains some blatant giveaways for the industry, including new types of copyright that would last 144 years. It will be Spotify (and consumers) that will have to pay for those goodies. Meanwhile, Spotify\u2019s long-term prospects are bleaker still, given the company is also competing with the likes of Apple and Amazon, which can afford to lose money on their music offerings as part of a larger customer retention strategy. None of this is to say Spotify is a bad company. Its CEO Daniel Ek has created a transformative new entertainment experience enjoyed by 170 million active users. But until streaming companies are able to gain the influence of the music industry, they will never make any money. On Thursday morning, Spotify shares (spot) were trading around $153, well off their recent high of $170."}, {"by": "kgraves", "descendants": 0, "id": 17020432, "score": 1, "time": 1525782491, "title": "ai.google", "type": "story", "url": "https://ai.google/", "text": "https://ai.google/ FAILED"}, {"by": "netgate", "descendants": 0, "id": 17020431, "score": 1, "time": 1525782476, "title": "Design Patterns in Modern C++: Reusable Approaches for Software Design", "type": "story", "url": "https://itbook.store/books/9781484236024", "text": "Be the first to know about new IT books, upcoming releases, exclusive offers and more. Java Design Patterns Pro Design Patterns in Swift Django Design Patterns and Best Practices Mastering JavaScript Design Patterns Learning JavaScript Design Patterns SQL Server Integration Services Design Patterns, 2nd Edition "}, {"by": "imartin2k", "descendants": 0, "id": 17020414, "score": 1, "time": 1525782218, "title": "System Failures: Planned Obsolescence and Enforced Disposability", "type": "story", "url": "https://medium.com/disruptive-design/system-failures-planned-obsolescence-and-enforced-disposability-f0fab3e597f3", "text": "We live in a world made up of complex interconnected systems\u200a\u2014\u200anatural, industrial, and social. Industrial systems involve all the manufactured goods and services that fill our lives, social systems are the intangible social scaffolding that humans have developed over time to function as a society, and ecosystems are all the natural services that keep the Earth alive. The intersection of these systems is often where our social and environmental issues occur. In this series of essays, I will be exploring some of these complex \u2018system failures\u2019, exposing the reinforcing systems that we have accidentally created, and posing the challenge of how we can redesign the negative externalities and find pathways to a more regenerative future. I want to start this series with one of the most prominent and destructive system failures: the absurdity of intentionally designing things to have reduced value, break or be wasted and the cycles of enforced disposability that this creates. Waste in all its incarnations\u200a\u2014\u200aocean plastic waste, street litter, curbside trash collection, air pollution\u200a\u2014\u200athese are all human inventions. Nothing in nature is \u2018waste\u2019 everything has a purpose and is cycled through ecosystems to contribute and increase value over time. It is through the human designed social and technical systems that we have created systems that intentionally produce waste, things that can not be reintegrated into nature in productive ways. As a result of this, we have had to build special technical facilities to house our discarded products. Landfills are an externality to bad design. They are costly, often become a toxic soup of chemicals, can leach these back into nature, produce methane (a 25 times more potent greenhouse gas than carbon dioxide) and literally stink. Our current economic system incentivizes waste generation and encourages producers to manipulate their products to be used up and then discarded for something new in increasingly shorter cycles. This has bread the practice of planned obsolescence in many of our industries, from technology to toothpaste we see the active integration of failure and reduced lifespans. Our daily lives are now predominantly scripted and defined by single-use throwaway stuff. Think of how many of your normal daily interactions involve an enforced aspect of disposability. The food industry is one of the worst\u200a\u2014\u200acups, plates, bags, packaging, cutlery\u200a\u2014\u200abut recently, design for disposability has moved into the medical, transport, and government sectors just as much. Many of the goods and services we all rely on are created with the specific intent to lose value over time so that the consumer is stuck in an enforced consumption service cycle, which increases value for the producer, but not for the customer nor the planet. And the cost of dealing with all of this reduced value stuff is placed on the customer and local governments in the form of funding local waste management services. Daily options for obtaining basic needs from food to water over the last two decades have slowly been swapped from a reusable user experience to a crappy plastic or paper single-use disposable option. For many it feels cheap because it is, and it feeds into the speedy convenience fuelled lifestyles currently dominating societies. But the long-term costs are much greater than the immediate cost cutting and time-saving perceived benefits. The burden of disposability is shifted away from the producer and onto the consumer through taxes and costs to governments in cleaning up the damage that this creates. Look at the collective systems loss of the ocean due to the plastic waste disaster\u200a\u2014\u200aa situation with complex remedies due to the \u2018tragedy of the commons\u2019 that the oceans fall victim to. Or just look at the global recycling crises that has recently surfaced as a result of China rightfully refusing to take the world's waste! Australia\u2019s, the UK\u2019s, the USA\u2019s and many parts of Europe\u2019s recycling systems are falling apart under the recent ban on importing contaminated recycling streams. Given that until recently, over 50% of the worlds plastic waste went to China for processing, many countries have now had to face the ramifications of their cheap disposability addiction. The big issue with the up trend to \u2018make it recyclable\u2019 as a solution to disposability has validated the production of single-use product streams. In many advanced recycling markets, we see an increase in the net use of disposable products, which has a collective loss of value of raw materials and ultimately produces bigger ecological issues. The big thing is that it also costs the consumers rather than the producers, and it\u2019s the local governments that have to fit the bill of waste removal and processing. The wastefulness of our everyday experiences in the world has become so normal, it now takes more energy to question how it has become this way than it does to just accept it as a part of life. Disposability is an absurd business model that was originally encouraged as a way of increasing consumption for the benefit of the entire economy, but it is now used as a manipulative tactic to keep consumers locked into enforced consumption cycles where you have to pay for upgrades, buy the newest version, or accept the limited use option. As I have written about before, this is all very much by design. The systems of disposability permeating our lives are a product of economic incentives and the systems archetype of a race to the bottom, offering the cheapest price tag by the producer and the most convenient solution to the \u2018consumer\u2019, at the cost of the society and the planet. But whilst it may seem like cheaper products are better for the consumer, the net gain is always skewed towards the producer. Over time, individuals have to spend more on the collection of disposable products as more are required to achieve the functional needs. Since developing a systems mindset, I can\u2019t help but notice systems designed to maximize waste all around us. I see the pointless plastic stirrers in the cocktail glass in the fancy bar, the disposable chopsticks in the sit-down restaurant, the plastic wrapped plastic cutlery in the cafeteria with ceramic plates\u2026 so much of our lives involves unintentional waste production, and it\u2019s a trend that needs to be redesigned. I recently was at an opening event for a conference at a lovely museum, and they had real wine glasses but used expanded polystyrene disposable coffee cups for the soup they were serving for dinner. Clearly they would be washing the wine glasses\u200a\u2014\u200athey must have had other reusable glasses on hand, so why can\u2019t they be used for the soup? 1000 people were implicated in the absurdity of normalized disposability; they kept talking about how crappy the cups were and didn\u2019t know what to do with them. The most infuriating thing about all of this is that in nearly every one of these cases, a reusable option is possible and just as easy, but for some reason, no one thought about the disconnect. Disposability has become so normal that often no one thinks about it anymore. The blatant lack of long-term economic and environmental considerations of the impacts of disposability-focused service design is infuriating. It might be cheaper for some to swap from reusable to disposable in the short term (no washing up costs for example), but the cost of landfilling waste is always going up, and the collective impact of ocean plastic waste along with all the other shitty by-products of these decisions (or lack thereof) is causing systems-wide negative impacts that we are all now implicated in trying to resolve. These are all symptoms of a very sick system, and the prognosis is not good\u200a\u2014\u200aunless we fix the systemic issue of normalized hyper-disposability. By far one of the biggest environmental and social issues we face right now is the ramifications of disposability, the devaluing of materials, and the perpetual normalization of waste built into every part of our daily lives. It\u2019s not fair to expect individual consumers to fix this mess. We no longer have consumer sovereignty in so many of our day-to-day interactions with companies; even government agencies and services have taken the easy road and moved to forceful waste production. On a global scale, we have quickly replaced things that have continued and sustained value for newly designed things that are literally created to be valueless. You get take out that includes several disposable plastic forks, and you think, \u201cOh well, they are not worth anything anyway.\u201d In New York City, there is some weird cultural norm of putting a paper bag inside a plastic bag when getting a delivery. It makes no sense. In San Francisco, the eco community bans plastic bags, but then the Whole Foods supermarkets offer TRIPLE paper bags as a precaution, since single paper bags (and apparently double bags) are not strong enough to deal with all the heavy tin cans and bottles. This doesn\u2019t solve any problems\u200a\u2014\u200ait increases them. In my 2014 TED talk, I explain how material impacts are based on functional unit delivery. Just because a material is biodegradable, such as paper, does not give it instant environmental benefits\u200a\u2014\u200ait\u2019s the system-wide impacts that determine the degree of impact. Paper bags are worse than plastic when you consider the functionality of it across its entire life, but really the issues need to be addressed at the service delivery and systems design level. Hyper-disposability is the real-life incarnation of cultural absurdity. As a society, we have come to accept that things made from non-renewable resources, that will live for a very very long time, like plastic and high tech electronics, are literally worthless. For all the things we have turned into disposable useless crap in our lives, we will eventually have to pay the price in the loss of ecosystems services and through the collective guilt of messing up the things we all appreciate, like the oceans\u2026which is the one defining difference that our planet has to all other planets and the main ingredient that made life possible, where over 50% of our oxygen comes from, and where we get a large amount of our food products! Last New Years Eve, I sat on a street and cried in frustration at the magnitude of the plastic waste left behind after revellers had gathered to watch the fireworks. Mountains of cups and bags and straws and now unloved plastic party glasses were all just thrown on the ground for someone else to deal with as the families departed for home. At 3am, an army of hired city street cleaners came through, and I watched in awe as they quickly gathered up the discards of other people\u2019s moments of happiness into a ginormous pile of single-use crap, and then a truck swiftly came along and carried it away. Had the festivities been disrupted by rain, then the party goers leftover refuse would have yet again all been whisked away into the oceans. Several years ago I was on a design challenge in Thailand, looking at the issue of rapid transitions by street food sellers from natural packaging, like banana leaves, to expanded polystyrene disposable packaging. The impacts were immediate and immerse in the mega city of Bangkok, where there were very few municipality waste collection services and a culture of throwing the (what was once biodegradable) packaging on the ground. In the floods of 2011, one of the suspected culprits was plastic packaging blocking the drains, as the extra water could not escape out into the ocean. We live in an interconnected world, and all water eventually leads to the oceans. Allow me to demonstrate several other everyday scenarios where the tragedy of planned obsolescence and disposability has become a ubiquitous phenomenon: 1. Long-Haul Transport In economy class on long-haul transport like planes and trains, you have no choice but to accept the plastic-wrapped disposable pile of small portioned crappy food and drinks issued to you. Everything beyond the rations of overpacked processed food stuffs is designed for disposability. Don\u2019t try to console yourself with the recycling content of it either, as most airlines do not recycle (they usually have to incinerate everything), and when they do, the separated waste streams with the amount of recovered content pales in comparison to the amount that is produced. The same with trains, though often considered the more sustainable travel option, you will be lucky to get a reusable anything, even in the more executive classes. This is another point of constant contention. Why is it that in business class, everything is reusable, but in economy, it is a plastic waste wonderland? We are reminded of our socio-economic position by the cheapness of the disposable items. 2. Water The global trend towards bottled water use is staggering. Since 2000, bottled water consumption has doubled each year per capita in the US. One million disposable water bottles are used every single minute of every single day around the world. Germany, Norway, and soon the UK have container deposit legislations that incentivize people to return drink vessels and get a refund on their drink bottles. This type of system is designed with thicker glass or plastic bottles so that they can be easily washed and reused by manufacturers, dramatically reducing waste but still allowing for convenience. Another important simple fix to the water bottle problem is to put in water-filling stations in public places so people can refill a reusable bottle. 3. Overpriced Technology Yes, your cell phone and sexy brand name headphones are designed for disposability too. Electronic waste is a global pandemic, with almost 50 million tons of it being trafficked around the world for some hazardous \u2018recycling\u2019 in often emerging economies where environmental legislation is lax. Technology contains so many different materials, and some are valuable (like gold and copper), while others are not. So, the incentive is to just get to the valuable materials in whatever ways possible. Cell phones are often shredded up into tiny pieces, a conglomerate of toxic and benign materials, just to get to the gold. Producers could definitely design for disassembly so that it is quick and easy to get the good stuff out, but they don\u2019t. Instead they design to lock the consumer out and make it impossible to repair old items. Why? So people are forced to buy a new one on average every 2 years. France announced it is now a crime to design planned obsolescence into products, but the norm is pervasive across the industry. Certainly there needs to be more effective design approaches, but also investment by tech companies in the development of advanced recycling and recapturing facilities, especially in markets where processing is already happening and where there is an economic reliance on e-waste handling. Disposability is a disease that has infected our societies and polluted our planet. It is not normal nor necessary to have so much pointless stuff. The ramifications of this increase in waste have led to the pollution of our oceans, the logging of our forests, and the messiness of our city streets. Recycling is no match for this beast of a social norm, as it further motivates the production of wasteful products. The insidious nature of these social norms where disposability, single-use, and entitled access to new stuff makes it extremely difficult for people to opt out of the hyper-disposable lifestyles that plague the Earth with waste. I consider myself a very considered consumer. I carry a reusable water bottle, don\u2019t get disposable coffee cups, have my own fork and straw with me. I refuse things I don\u2019t need and avoid using disposable chopsticks or napkins when dining in places that seem to throw these things at you. But even then, I find it extremely difficult to have people understand the reason why I am refusing to partake in the normalized disposability. Raised eyebrows or \u2018difficult customer\u2019 looks are a plenty. These are all social conformity practices that further perpetuate the problem. I often have store clerks say to me, \u201cBut the paper bag is recyclable\u2026\u201d when I opt to stuff the 6 items at the grocery store into my handbag instead of taking the free bag. The mindlessly discarded waste of our modern lives desperately requires reflection and questioning on a societal level. Waste is a human created design flaw. It is a solvable system failure that addressing it will bring about significant collective society value. Everything is interconnected on this planet. Our collective choices have impacts, and our disposable economy needs to be shifted to a circular one. The goal of design-led systems change is to be able to deeply understand the connections and relationships that maintain a system; these are usually reinforcing feedback loops in all directions that take time to decipher and explore When looked at from a systems perspective, the only real solution to this design flaw is the shift to a post disposable society, one where we reinstate the value in consumer goods and find closed-loop production and delivery services that design out disposability. No matter how you try to spin it, if your company produces products that are designed to be used once and then discarded, you are part of one of the biggest social issues to infect our planet. Take examples like like this coffee cup library system and this reusable packaging trial in NYC. Approaches like this fit within the growing circular economy movement where business and governments around the world are taking responsibility to design products that fit within a service model that maximizes benefits to the system. These approaches circularize the supply chain by designing for the full life-cycle impacts right from the start. We all have the power to demand post disposable products and help transition to a future that is not plagued by single-use products and cheap disposable crap. If we are to address the tragic contributions that these design flaws have have made to the planet, then we must design new systems that make the old disposability-focused approach obsolete. \u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014\u200a\u2014 \u2014 I have developed a series of support tools for designing for a post disposable future. You can download a free activation kit here and find out more about our global design challenge briefs here. If you want to learn more about the design-led systems change or the Circular Economy, you can take my introductory class here. In June, I will be releasing a new handbook on Circular Design. All of the illustrations by Emma Segal By clapping more or less, you can signal to us which stories really stand out. UNEP Champion of the Earth, Designer, Sociologist, Sustainability Provocateur, TED Speaker, PhD, Experimental Educator, founder @unschools & @disruptdesign.co Going against the grain; disrupting the status quo. This curated collection of articles explores the themes of disruptive design, sustainability, cognitive science, gamification, social innovation, positive change interventions and the systems that connect it all."}, {"by": "camtarn", "descendants": 0, "id": 17020390, "score": 2, "time": 1525781936, "title": "The vetting files: How the BBC kept out \u2018subversives\u2019", "type": "story", "url": "http://www.bbc.co.uk/news/stories-43754737", "text": " Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp LinkedIn Copy this link These are external links and will open in a new window For decades the BBC denied that job applicants were subject to political vetting by MI5. But in fact vetting began in the early days of the BBC and continued until the 1990s. Paul Reynolds, the first journalist to see all the BBC's vetting files, tells the story of the long relationship between the corporation and the Security Service. \"Policy: keep head down and stonewall all questions.\" So wrote a senior BBC official in early 1985, not long before the Observer exposed so many details of the work done in Room 105 Broadcasting House that there was no point continuing to hide it. By that stage, a policy of flatly denying the existence of political vetting - not just stonewalling, but if necessary lying - had been in place for five decades. As early as 1933 a BBC executive, Col Alan Dawnay, had begun holding meetings to exchange information with the head of MI5, Sir Vernon Kell, at Dawnay's flat in Eaton Terrace, Chelsea. It was an era of political radicalism and both sides deemed the BBC in need of \"assistance in regard to communist activities\". These informal arrangements became formal two years later, with an agreement between the two organisations that all new staff should be vetted except \"personnel such as charwomen\". The fear was that \"evilly disposed\" engineers might sabotage the network at a critical time, or that conspirators might discredit the BBC so that \"the way could be made clear for a left-wing government\". And so routine vetting began. From the start, the BBC undertook not to reveal the role of the Security Service (MI5), or the fact of vetting itself. On one level this made sense, bearing in mind that the very existence of the Secret Service remained a secret until the 1989 Security Service Act. Over the years, some BBC executives worried about the \"deceptive\" statements they had to make - even to an inquisitive MP on one occasion. But when MI5 suggested scaling back the number of jobs subject to vetting, the BBC argued against such a move. Though there were some opponents of vetting within the corporation, they had little influence until the Cold War began to thaw in the 1980s. This is how the system worked. Vetting was brought into play once a candidate and one or two alternatives labelled \"also suitable\" had been selected for a job. The alternatives served a useful purpose. If the first choice was barred by vetting, the appointments board moved easily on to the second. The candidates were told only that \"formalities\" would be carried out before an appointment was made. This sounded harmless enough; it would allow time to follow up references, perhaps. Candidates did not know that \"formalities\" meant vetting - and was, in fact, the code word for the whole system. A memo from 1984 gives a run-down of organisations on the banned list. On the left, there were the Communist Party of Great Britain, the Socialist Workers Party, the Workers Revolutionary Party and the Militant Tendency. By this stage there were also concerns about movements on the right - the National Front and the British National Party. A banned applicant did not need to be a member of these organisations - association was enough. If MI5 found something against a candidate, it made one of three \"assessments\" in a kind of league table: The BBC procedure was in principle never to employ someone in Category \"A\", though a few did get through the net. This contradicted its public position that the BBC controlled all appointments. In theory it did. In practice it gave that choice to MI5 in Category \"A\" cases. If staff came under suspicion only after they had been employed by the BBC or applied for transfer to a job that needed vetting, an image resembling a Christmas tree was drawn on their personal file. This \"tree\" was an important part of the process. The BBC maintained a \"Staff Transfer List\" which named staff who needed to be checked if they were to be promoted. A tree added to the file alerted the administration that this was a security case. Also written on to their file was a so-called \"Standing Reminder\". This stated: \"Not to be promoted or transferred (or placed on continuous contract) without reference to [Director of Personnel].\" So keen was the BBC to maintain secrecy that it secretly removed the Standing Reminder from someone's file if they went to an Industrial Tribunal, which had the power to call for personal files. It was also agreed to (misleadingly) explain away a stamp on a file saying \"Normal Appointments Formalities Completed\" by pretending that it referred to \"routine procedures, Next of Kin, Pension etc\". The Christmas tree was eventually dropped in 1984 because it was said to attract too much attention. It attracted a great deal of attention when the Observer described it in 1985. The day after publication, someone hung some Christmas decorations on the door handle of Room 105 in Broadcasting House, from where the system was run. An interview given in 1968 by BBC director general Sir Hugh Greene shows the BBC's policy of denial and obfuscation in action. To a reporter from The Sunday Times in February Greene blithely and misleadingly declared: \"We have a staff of 23,000 and in that community we have people of all descriptions, including what you call pansies\" - the word had apparently been used by the reporter - \"and also communists. But that's none of my business. We don't conduct an inquisition on people who join the BBC.\" It was true that neither the BBC nor MI5 used homosexuality as a reason to block someone, but stopping the employment of communists was very much part of Greene's business and, if not the BBC itself, then the Security Service did conduct inquisitions. The files reveal that BBC tactics for handling this interview had been drawn up by MI5 itself - known delicately as the \"College\" in BBC memos. In advance of the interview, and in a rare departure from prevailing policy, a BBC official had suggested to \"College\" that \"the need for vetting no longer existed, in peacetime\". The BBC indicated that it was willing to admit publicly that staff involved in plans for broadcasting in time of war - called \"emergency defence work\" - and any aliens employed were checked. However, a BBC memo records: \"College do not favour reference to vetting in any way whatever.\" To underline the point, an MI5 official telephoned to say that \"no direct admission of vetting should be made\". If pressed, the BBC could admit that \"something of this sort\" was carried out \"in relation to War Planning purposes\" and \"where aliens were concerned\". Nevertheless, the Security Service \"would prefer that as little reference should be made to this subject as possible\". MI5 suggested that questions could be diverted from Greene to someone on the \"personnel side of things\". And \"perhaps stress could be laid on a stiff recruitment procedure and the fact that references are taken up very thoroughly\". This last phrase was taken up by the BBC very thoroughly itself, and it appears in many of the BBC responses over the following years. Cleverly ambiguous, it implies that the references taken up were those given by an applicant. In reality, the references were supplied by the Security Service. Greene followed the MI5 line. He told the Sunday Times that he would not answer any questions about vetting but would leave that to a senior subordinate. The paper seems to have gone along with this. There is no questioning of Greene on this matter in the published interview in the Sunday Times. However, Greene's nominated spokesman, the director of administration, John Arkell, did not quite follow the script with which he had been provided. At first he repeated the denial: \"We don't ask about religion or politics.\" However he then wobbled. \"If someone is a communist it's of no relevance, unless, of course, he is working in a particularly sensitive area.\" The Sunday Times reporter then asked Arkell whether he had confidence in British security vetting and Arkell wobbled again: \"In this imperfect world perhaps sometimes someone suffers,\" he replied, and the implication was clear, though the paper did not pursue the point. Arkell then hurriedly repeated the standard denial: \"I must point out that security vetting isn't a prerequisite for getting a job at the BBC.\"  In fact for about 6,000 BBC jobs at the time, it was. Arkell's comments led to some raised eyebrows. One BBC official accused him of making \"an open avowal\" of vetting. However, Arkell himself was pleased with his performance and encouraged a BBC colleague to use it \"to get a bit of credit for the BBC with MI5\". When this resulted in a letter from the Security Service congratulating Arkell for \"standing up so creditably to the questioning\" the critical BBC official fell into line, commenting merely that Arkell's denial had to be maintained \"unvarnished, unglossed and unexpanded\". Greene's refusal to answer questions was no surprise to insiders. Although he had been a liberalising influence in the BBC since becoming director general in 1960, he was firmly in favour of vetting. Not long after taking over, he had led a BBC delegation in talks with the Home Office, which was asking why so many BBC applicants had to be vetted. MI5 was worried that it was open to being sued by individuals, since its directive required it to concentrate only on \"countering threats of subversion and sabotage\". It wanted to vet only the applicants for a limited number of jobs. But Greene resisted any change. The BBC actually argued for more vetting, to prevent infiltration by \"subversives\", but felt that it could publicly admit to checking some key staff. MI5 wanted to lessen the burden placed on it by vetting - but insisted on almost total secrecy. It took some time for the argument to be resolved. The Security Service managed to square its activities with its directive and the BBC removed 528 staff from the vetting system. Among these were 81 staff in the Make-up and Wardrobe Department, 20 in the Gramophone Department and 21 in the Library. Sixteen staff in Religious Broadcasting were also excluded, though the BBC could still request vetting for any individual there. Thus were such staff no longer regarded as dangers to the state. Banned applicants did not know why they had been turned down, though they might have guessed. One notorious case involved the journalist and broadcaster, Isabel Hilton (who later received an OBE for her reporting). She was refused a job in BBC Scotland in 1976 because, she believes, she was guilty by association with a member of the Communist Party at Edinburgh University - a fellow member of the university's China-Scotland organisation. After unprecedented protests from the BBC executive who wanted to employ her, Alastair Hetherington, she was eventually offered the job. But it was too late, she had gone elsewhere. She was later told apologetically by Michael Hodder, the last BBC official who acted as liaison with the Security Service, that it had all been a \"mistake\", but the episode still angers her. \"I still feel indignant. It's the lack of accountability that bothers me and the fact that nobody in the BBC ever apologised, explained - or made any public statement in my defence or to acknowledge their error,\" she says. \"They went into an institutional defensive huddle without regard for what their actions might have done to me, my reputation, my career etc - nobody in the BBC took responsibility or seemed to feel that they should make any move to repair any damage. I felt it was a squalid way to behave and I still do. \"More seriously, beyond the particulars of my own case, I felt that the BBC had betrayed public trust by promoting a system in the UK by which the secret police were licensing and blacklisting journalists. Whenever I hear the BBC boasting about its fine traditions of journalism, I feel a minor stab of outrage.\" Hilton did eventually work for the BBC, presenting the World Tonight on Radio 4 in the 1990s, and later the Radio 3 arts programme, Night Waves. Another candidate who was rejected on MI5's advice was Tom Archer, who worked as a BBC freelancer in Bristol in the 1970s, but was turned down when he applied for staff jobs from 1979 onwards. Archer says he had been \"an active socialist at university\", but this was something the BBC usually ignored as youthful enthusiasm. There had to be another reason, and an editor in Bristol who wanted to employ him, Robin Hicks, discovered it: Archer was being blocked because a close relative had allegedly joined the Socialist Workers Party. Hicks protested but to no avail. Archer's career flourished outside the BBC, however - at Channel 4 and Granada - and he eventually worked his way back. In 2008, he became controller of BBC factual programmes - based in Bristol. \"I was angry and even frightened at the time,\" he says, casting his mind back to 1979. \"I feared that everything would be blocked off for me. We were a young married couple. I sent back the video recorder and sold the car. They did it in a secret cack-handed way. \"It was of course a complete triumph when I went back.\" At the same time Tom Archer's job applications were being rebuffed, a senior BBC appointments official was arguing that it was time for vetting to end. In December 1979, Hugh Pierce pointed out that over a recent two-year period only 22 people had been excluded out of thousands vetted. He said, therefore, that \"the process of vetting could be reduced\". He doubted if the 22 could have done much damage, as \"any personal bias... could have been spotted and checked.\" He recommended continued vetting for those with access to official secrets and in the BBC World Service, where many foreign staff were employed. Beyond that, he said, \"We should abandon forthwith the current requirement to vet wholesale categories of applicants. We should replace a rather crushing machinery by a more flexible service.\" The last line of his 10-page report was prophetic. He warned that if the scale of vetting became publicly known, it \"would be grounds for ridicule and vilification\". His recommendation was not followed and his prophecy came to pass when the Observer story was published in August 1985. Despite the rejection of Pierce's proposal to reduce vetting significantly, steps were taken before long to further cut the number of staff subject to it. Since the start of the policy, journalists had always been included in the system, but a review in 1983 resulted in about 2,000 posts being removed from the list - including some junior editorial jobs - bringing the total number down to 3,705. The man who conducted this review, the BBC official in charge of liaison with MI5, was Brig Ronnie Stonham, a former Royal Signals officer, who also produced an updated \"defensive brief\". The first line of this was the usual blanket denial: \"It can be stated categorically that BBC staff are not subject to a security clearance as a prerequisite for employment.\" It is hard to reconcile this with reality, as Stonham himself says in his report that in 1982 1,287 names were sent to MI5 for \"counter-subversion\" clearance. At the top of the BBC support for vetting was clearly waning, however. The vice-chairman of the Board of Governors, Sir William Rees-Mogg, had already questioned it before the Observer broke the story that broke the system. \"It operates, unknown to almost all BBC staff, from Room 105 in an out-of-the way corridor on the first floor of Broadcasting House - a part of that labyrinth on which George Orwell modelled his Ministry of Truth in Nineteen Eighty-Four,\" wrote reporters David Leigh and Paul Lashmar. \"The legend on the door - 'Special Duties-Management' - gives little away,\" the story went on. \"Behind that door sits Brigadier Ronnie Stonham.\" The headline - Revealed: How the BBC vets its staff. This time, there were facts and case histories which made the standard denials useless. Stonham's boss, director of personnel Christopher Martin, had initially tried the standard denial routine with Leigh and Lashmar, but then got the Home Office to agree a new line - a public acknowledgment that vetting had taken place, but was now being reduced. There were some who marvelled that the BBC had managed to keep the secret for so long - it is evident from the anxiety expressed in the files that the corporation would have been hard pressed to hold its line if it had been pushed really hard by the press. \"This story is 50 years old and it has taken the press that long to find it,\" said the BBC director general at the time, Alasdair Milne. The Observer's revelations brought about a major change.  Almost immediately, Ronnie Stonham recommended confining counter-subversion vetting to senior editorial staff but BBC management went further. In October 1985 the BBC announced publicly that vetting would in future be applied only to a few operational people at the very top, to those who would run emergency broadcasting (which meant the then secret wartime broadcasting system in the event of nuclear war) and to those staff in the BBC World Service who were thought to be vulnerable to hostile infiltration. All vetting of staff not in those categories would cease. But behind the scenes there was still resistance from some quarters. A rearguard action was fought to keep specialist domestic and foreign correspondents on the vetting list, on the grounds that \"the BBC's credibility depended on their integrity\". A dodge had to be devised, and so correspondents were quickly reassigned to a list of those who had access to restricted government information - an access they in fact did not have. The upshot was that vetted staff were reduced to 1,400 in the domestic services and 793 in the World Service. The system was further refined in 1990, following the Security Service Act, under which all vetting in the BBC stopped except for those who would be involved in wartime broadcasting and those with access to secret government information. Then, two years later, the wartime broadcasting system was stood down, so vetting was further cut back. The BBC will not say whether any staff are vetted these days. \"We do not comment on security issues,\" a spokesperson said. But any residual vetting, of people needing access to classified information for emergency planning for example, would be open and known to the person. There is no more secrecy as once there was.  By the time the wartime broadcasting system was wound up, Stonham had retired, and his role as MI5 liaison had been taken by a personnel officer from the news division, Michael Hodder, a former Royal Marine. Hodder oversaw the residual vetting and dealt with a few cases informally in the World Service. There was an employee in the Burmese Section who was giving the names of dissidents to the Burmese Embassy in London. Another was the case of a Saudi employee who turned out to be on the payroll of both the BBC and the Saudi embassy. A third involved an applicant for a job in the Arabic Service, who was related to a notorious terrorist. It was Hodder who saved the files for history. He ignored an instruction to destroy them and crated them up in a safe for delivery to the BBC Written Archives Centre. He did shred all Security Service material on staff that the BBC held. However, he ensured that one personal file was kept - that of Guy Burgess, who worked for the BBC during the war.  The BBC even put his file online in 2014 but of course in this case the vetting had failed - and there was nothing in the file of Guy Burgess to indicate that he was in in fact a Soviet spy. Paul Reynolds was a BBC correspondent from 1978 to 2011. Rail firms admit \"anomalies\" in the system mean passengers are not always offered the cheapest fare.                   The incredible story told through testimony Sign up for our newsletter"}, {"by": "romefort", "descendants": 0, "id": 17020376, "score": 1, "time": 1525781758, "title": "Pyception", "type": "story", "url": "https://www.youtube.com/watch?v=UXd0EDy7aTY", "text": ""}, {"by": "tambourine_man", "descendants": 0, "id": 17020369, "score": 1, "time": 1525781561, "title": "Vue Mastery \u2013 VueConf US", "type": "story", "url": "https://www.vuemastery.com/vueconf", "text": "The first ever VueConf US took place in New Orleans on March 26-28, 2018. Vue Mastery is the sole destination for VueConf US 2018 conference videos. We'll be releasing videos over the next few weeks as we receive them. The creator of Vue kicked off the first VueConf.US by talking about the growth of Vue, recent project releases, the maturing eco-system, the Vue-Loader rewrite, Vue CLI 3, upcoming releases of Vue.js, and initiatives to improve the stability and evolution of the library. We talk about what we've learned in 1 year of using Vue in production. How we mix in Vuex, webpack, and other tools. How we write tons of code and get from idea to production so quickly using VueJS as our workhorse. How we get crap done at GitLab. Learning a JavaScript framework can look intimidating if you come from a traditional design background. But Vue has some of the best features for designing with motion. Come see how to translate your motion design into Vue-powered prototypes! In this talk Ed explains how to structure a test suite, and how to write effective unit tests for Vue components. We\u2019ll cover key use cases for these functions within a Vue.js application: we\u2019ll accept payments with stripe, we\u2019ll gather geolocation data from Google Maps, and more! Making it work with Vue and Nuxt and simplifying how to leverage this paradigm to be a workhorse for your application. Vue.js ships with a built-in reactive system to help easily manage data and state throughout your app, but RxJS streams allow you to control complex async situations through streams. This lecture will cover how Vue.js integrates with RxJS and walks through the common problems taking this approach can solve. Need a mobile app to offer different, yet complimentary functionality, while retaining shared code between your website and mobile app? Welcome to the beautiful world of Vue and NativeScript, which, paired together make for a great user experience. As a Vue consultant and member of the core team, Chris will share some of the lesser-known features and patterns you can use to improve your apps and amaze your colleagues. Style Guides & Pattern Libraries are great tools. Without dedicated teams and budgets to build a centralized system, how can we build patterns into our code, using Vue components and pre-processors to iterate and automate a design system in an agile process? One of TypeScript's goals is to make sure that JavaScript users of all communities can use the language. Over the last year, the TypeScript team has put effort in to making TypeScript & Vue work together much more seamlessly. Come find out a bit about that work, and how you can benefit from TypeScript today! As a React developer learning Vue, adapting to the \u201cVue way of doing things\u201d is a challenge that requires a sound understanding of the philosophy behind Vue. We will examine the nuances of the two frameworks and cover common mistakes that React developers make when switching to Vue. Writing an Universal Application with Vue might be hard, this talk will show common problems with server-side rendering and how to deal with them. It will also show how Nuxt.js solves most of these problems for you. Building and documenting a component library at your company can be a huge hassle, but is a great way to keep your code consistent and clean among all of your projects. Storybook and Vue take most of the pain out of this, and allow you to develop a set of core components and document them in no time! Vue Single File Component is ideal for sketching out UI components, animation, interaction prototypes and data visualization. I present a mini Vue SFC based framework that prescribes setup, languages and coding styles in exchange for the best prototyping experience. To be released on:\u00a0May 10, 2018 Listen to these 5-minute lightning talks on a range of Vue topics. We also help produce the Official Vue.js News. It's a free community resource where we curate the most impactful Vue.js news and tutorials. Offered up in both text and audio versions. Consider subscribing today. As the ultimate resource for Vue.js developers, Vue Mastery produces weekly lessons so you can learn what you need to succeed as a Vue.js Developer."}, {"by": "crankylinuxuser", "descendants": 1, "id": 17020367, "kids": [17020529], "score": 1, "time": 1525781540, "title": "Game theory analysis, proposed solution to Homelessness", "type": "story", "url": "https://www.reddit.com/r/nomorenicksleft/comments/4g79ja/homelessness_game_theory_and_asshole_municipal/", "text": "Sign up and stay connected to your favorite communities. Ok. So the title will need some work before I submit this to an economics journal (haha!). I've been thinking about the problem of homelessness in the United States for a few months now. It bugs me that it should be an intractable problem, those should be rare and involve hard physics. It also occurred to me that if the problem were solvable for one city government or another, then the cheapest solution for other cities would be to buy one-way bus tickets to the city that solved homelessness. (Note: that already happens occasionally and those that do it go so far as to defend the practice.) It's a variation on prisoner's dilemma. This tends to discourage any municipality from attempting to solve homelessness. As soon as they succeed, suddenly they'd be inundated by new arrivals. It's difficult to estimate how many would arrive and over the course of how many years, but also rather likely that the budget margins for such a project will be thin enough that (for smaller cities) even just dozens more could endanger the viability of the program itself. Paradoxically, homelessness might be more solvable if city and county governments can actually deny (some of) the homeless the benefits of the program. If they can turn away those who have been sent by other cities on one-way bus tickets, the homeless have no reason to agree to leave... they don't get much (or any) benefit from going elsewhere. They stay where they are and pressure that city to deal with their own homeless population. Of course, this doesn't actually give us any clues for how to solve homelessness itself (flaming truckloads of cash launched out of giant trebuchets), but it does make it a more manageable problem I think. Now this idea isn't without its own perils. If a city government can deny some, would it not be cheaper to deny all of the homeless? That's mostly what already occurs. So we'd have to also define just which homeless are their responsibility. And it's necessary to define it in such a way that they are responsible for almost all (more than 90% certainly, probably more than 95%) of their existing homeless population, excluding mostly only those bussed in from elsewhere (should that happen). Hypothetical example city of 175,000 people. This isn't a sprawl city, rural townships with small populations here and there, but county governments take care of the rest. Let's say that they have 500 homeless (the city I'm from has a few tens of thousands more population, but fewer than 500 homeless). They're not the first to implement this policy in the United States, so other cities may already be considering sending a horde their way. They may or may not have a recent \"homelessness\" census to rely on (cities usually don't like to pay for that, considering that the results might be embarrassing). The qualifications for homelessness assistance need to look something like the following: The homeless person has inhabited in the city or surrounding area for a total of 18 months or longer in the past 2 years (as of the date of implementation) The homeless person was born in the city or surrounding area and resided there for at least 12 years prior to reaching the age of majority The homeless person's parents were both born in the city or surrounding area and resided there for 2/3rds or more of their natural lives, or if still living, 2/3rds of their lives as of the date of qualification determination The homeless person resided in the city or surrounding area for 2 years prior to becoming homeless through eviction, building condemnation, natural disaster, or similar circumstances The homeless person worked in the city or surrounding area for for 4 years prior to becoming homeless through circumstances relating to layoffs or employment termination without cause The homeless person is the legal spouse or dependent of another who qualifies The first rule is important because many homeless people may not qualify in any city in the whole of North America, but shouldn't be ignored or otherwise short-shrifted. This obligates the municipal government to qualify even those who haven't ever had strong connections to any particular city or region, but disqualifies any who come in after the date. The second and third rules qualify anyone with strong hometown ties to the area. The fourth and fifth rules, of course, suggest that anyone who becomes homeless after having a home in the city is the responsibility of said city. And the sixth rule is necessary for those homeless families where otherwise some members would qualify but not others. Any of these rules by itself would qualify a person to receive benefits from a homeless program. If the homeless person would qualify in multiple cities, they still wouldn't be disqualified. With that in mind, there probably needs to be one more rule that disqualifies: The homeless person has arrived from another city where hypothetically similar rules would qualify them for benefits there I need to word the above better, but it's late and I'm groggy. Imagine cities A and B existed, with a homeless person who would qualify for benefits in either according to the above rules. B though buys them the bus ticket, sends them to A. City A then justifiably disqualifies them from benefits, as City B should never have stooped to sending them away. The homeless person returns to B (hitchhiking, hoofing it, riding the rails). City B can now disqualify them, as they have arrived from A. If someone can formulate that a bit better to protecting against bussing without mangling the intent, say so. I'm generous with the credit. Now, with that in mind, the implementation can commence. The city that enacts these rules would need to conduct a proper census of the homeless. I'm not an expert on those (I'll leave it to the polisci nerds to have figured that out). The census should be as accurate as possible, and should pre-assess (when practical) whether the homeless person qualifies and by what rules. But also if they do not qualify, which rule is the closest to allowing them to qualify. You see, if the example city has 500 homeless and only 320 of them qualify with the rules as I've written them, then the rules need to be adjusted. They need to be adjusted permissively to qualify all 500 people, or if that's not really possible, then at least 95% of them (very little wiggle room on that number). The nature of each rule wouldn't change, but the numbers can be adjusted to be more permissive... rule one would be adjusted to be \"6 months of the past 2 years\", if that will bump the 320 up to 460. Similar modifications of the other rules would be acceptable, in whatever combination gets them to within a few percent of the total. Once these adjustments were made, no further modifications should be acceptable if those are less permissive. Having modified the second rule to be \"8 years of 18 as a child\", it is not morally acceptable to change it to \"16 of 18 years\" a decade later. As for actual qualification, that's a function of wonderful red-tape bureaucracies for others to figure out. Probably enough paperwork that if the trees had been made into lumber instead that could have built sufficient housing to solve the problem. Social workers would be involved, obviously, but government seems to have already settled all the basic questions as to what evidence is sufficient (for example, I imagine arrest records for the homeless would establish when they were inhabiting the city for rule #1). If most or all local governments were to implement these rules in good faith, it's still possible that some homeless people would qualify in none of them. Within the United States, I suggest that these people should be a problem for either the state governments to solve or perhaps the Federal government. I wish I had better ideas on how the problem of homelessness itself might be solved, but that's beyond me. As a libertarian I'm supposed to chirp \"free market yay!\" or some such other bullshit, but I haven't chugged enough koolade to believe that would actually solve the problem. I have no good ideas. This, however, is a good way to determine just which local governments are responsible for any given homeless person and I think that might make a difference. no comments yet Be the first to share what you think! 19 Subscribers 5 Online "}, {"by": "imartin2k", "descendants": 0, "id": 17020365, "score": 1, "time": 1525781486, "title": "Biased Facial Recognition \u2013 A Problem of Data and Diversity", "type": "story", "url": "http://www.skynettoday.com/content/news/face-recog/", "text": "A great example of how misleading and click-baity article titles can cause hype and often diminish well-researched articles. Joy Buolamwi, a Rhodes scholar at MIT Media labs found that facial recognition systems 1 did not work as well for her as it did for others. The models just did not seem to recognize her compared to her fairer skinned friends. This led her to investigate facial recognition systems in the context of people of color. In her recently published paper and project - \u201cGender shades\u201d, she evaluated commercially available gender identification systems from Microsoft, IBM, and the chinese company Mobvii. The results showed that all three systems performed poorly for people of color compared to fairer skinned subjects. To make things worse, the systems performed abysmally for darker-skinned women, failing to correctly identify them up to 34% of the time. Looking for what might be the cause of this performance disparity led Joy to examine two commonly used datasets 2 of faces. She found both datasets to be composed of a higher number of lighter skinned subjects that led to an imbalance 3 in the datasets, which correspondingly caused the disparity in the systems\u2019 performance. To help correct the lack of balanced datasets she has put together a more ethnically-diverse one for the community to use. The media reacted to the results of the paper with headlines like \u201cPhoto algorithms ID white men fine\u2014black women, not so much\u201d (WIRED) and \u201cFacial Recognition Is Accurate, if You\u2019re a White Guy\u201d (NYT). The skewed accuracy appears to be due to underrepresentation of darker skin tones in the training data used to create the face-analysis algorithms. The algorithms aren\u2019t intentionally biased, but more research supports the notion that a lot more work needs to be done to limit these biases. An algorithm trained exclusively on either African American or Caucasian faces recognized members of the race in its training set more readily than members of any other race. Microsoft and IBM were both given access to Joy Buolamwi\u2019s research and the newly created dataset. Microsoft\u2019s claimed to have already taken steps to improve the accuracy of the facial recognition technology. IBM went a step further and published a white paper replicating the results from the paper and showing the improvements in the systems after re-training on a balanced dataset. .@jovialjoy sent a pre-print of their paper to the companies after #FAT2018 acceptance. Face++ didn't respond. MS sent a response. IBM had the best response -- replicated the paper internally and released a new API yesterday. Wow. New API classified darker females at 96.5% While most of the NYT article focused on the research itself, it closed with this great line \u201cTechnology\u201d, Ms. Buolamwini said, \u201cshould be more attuned to the people who use it and the people it\u2019s used on.\nYou can\u2019t have ethical A.I. that\u2019s not inclusive,\u201d she said. \u201cAnd whoever is creating the technology is setting the standards.\u201d As this line suggests, the issue is more nuanced than just collecting better data; the larger and more complicated underlying problem is that of lacking diversity and inclusivity within the tech industry. In facial analysis, \"supremely white data\" gave a false sense of technical progress by leaving out the majority of the world --@jovialjoy #FAT2018 #AI #MachineLearning Is it inevitable that these data-driven systems will reflect the biases and prejudices of our society? Or are the people designing the systems not trying to prevent this bias? If we examine the technical details, neither of those statements are quite true and the situation turns out to not be so dire. As most of the media coverage correctly points out, this is a problem caused by lack of balanced data. As IBM\u2019s white paper shows, when these systems are trained with datasets that are well balanced they are very accurate. The important question to ask is why these systems were not trained with balanced datasets to begin with. The answer seems to be (as most of the articles suggest) that balanced datasets were not considered during the design of the systems. The lack of consideration is, in significant part, due to a lack of diversity in the AI community itself. On a broader note, face recognition might actually be the least of our worries with regards to the bias and fairness of AI systems. There are worrying examples of bias and discrimination in criminal sentencing, hiring, and mortgage grants. To make matters worse, recent research has shown that when multiple AI systems are used together they can amplify bias, even if each of the systems is bias-free. Although challenges exist, solutions to these challenges are actively being explored; a fast growing community of researchers are now hard at work on these solutions with formal approaches for ensuring Fairness, Accountability and Transparency in AI systems. It may be fair to say that some of the best minds in the field have set their sights on tackling these challenging problems! Be woke. @mrtz's epic class on fairness in #MachineLearning is a must read for everyone using ML. https://t.co/nfoiJtoDZE pic.twitter.com/AvSrL9HduF Excited to see 54 submissions to FATML 2018, up 12.5% from last year even though @fatconference just happened. However, most of these researchers attack these problems from a technical perspective. As Kate Crawford points out in her excellent keynote at the NIPS 2017 conference, bias cannot be considered a purely technical problem. \u201cBias is a highly complex issue that permeates every aspect of machine learning. We have to ask: who is going to benefit from our work, and who might be harmed? To put fairness first, we must ask this question.\u201d As Moustapha Ciss\u00e9 of Facebook AI Research says, the datasets are a result of the problems being considered by the designers. If we focus on problems that are important to only a specific population of the world, the end result is these unbalanced datasets. These datasets then, are used for training the models that make biased and unfair decisions. To ensure that a wide variety of problems are considered, there is also a need for a more diverse community of researchers and engineers in AI. Looking forward to seeing @jovialjoy and @timnitgebru present their paper on bias in facial recognition systems at #FAT2018 this weekend\u2014so much research needed to ensure AI is inclusive https://t.co/KeL0Aq6stx So what of the media coverage of this problem? While the content of the news articles is well articulated and on point, hyperboles such as the ones in the headlines are harmful and misleading. In the short-term, as pointed out in this great blog post by researchers at Google, AI experts need to be aware of undesirable biases that might exist in the systems they are designing and look to mitigate them. Further, system designers should be mindful of all possible users of their system. The onus is on the AI community to be more inclusive and work towards greater diversity in datasets and in the community itself. Contrary to what some headlines said, the algorithms in question are not the issue. None inclusive datasets produced skewed results, but the same models showed remarkable accuracy when trained on a more racially diverse and balanced dataset. However, this is still a warning sign pointing at an underlying problem: lack of diversity in the AI community. In the short-term, researchers and engineers need to be aware of all their users and the fact that the systems they build may contain possible biases. While fast growing research in the fields of fairness, accountability and transparency provides hope for a technical solution, the AI community needs to also make significant efforts to become more inclusive and diverse. A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source.\u00a0\u21a9 A dataset is a collection of data. All Machine Learning systems are \u201ctrained\u201d on a dataset. If you\u2019re confused or intimidated by the term machine learning - watch this video\u00a0\u21a9 When the dataset consists of more examples belonging to one group than the others it leads to an imbalance. For example, if the datasets consisted of more males than females it would be considered an imbalanced dataset.\u00a0\u21a9"}, {"by": "bgnm2000", "descendants": 0, "id": 17020336, "score": 1, "time": 1525781106, "title": "Show HN: No Zero Days - Foster personal growth, and be someone you\u2019re proud of", "type": "story", "url": "https://www.producthunt.com/posts/no-zero-days", "text": "Live a happier, healthier, and more content life, by fostering personal growth and service to others.  It works by keeping the qualities you admire at top of mind, every day. Helps you to hit your next business milestone Find time for your goals A calendar that finds time for your daily goals Closing the loop on ROI for marketers A brand new goal-setting app Accountability partner bot to set monthly goals. View your email subscriber goal & tips in Chrome's new tab Track your daily goals"}, {"by": "tomduncalf", "descendants": 0, "id": 17020330, "score": 1, "time": 1525780981, "title": "Visual Studio Live Share Public Preview", "type": "story", "url": "https://code.visualstudio.com/blogs/2018/05/07/live-share-public-preview", "text": "Version 1.23 is now available! Read about the new features and fixes from April. May 7, 2018 Amanda Silver, @amandaksilver We are excited to announce the public preview of Visual Studio Live Share! At Connect last November, we showed how Live Share enables real-time collaborative editing and debugging from the comfort of your favorite tools. Since then, we\u2019ve worked with thousands of developers worldwide, resolved hundreds of issues, and addressed top feature requests such as adding support for Linux. We\u2019ve benefitted greatly from all the feedback thus far, thank you! Today, we\u2019re excited to announce that every developer using Visual Studio and Visual Studio Code can get started with Live Share today! When talking with developers, the need for better collaboration tools is clear, and isn't limited to any particular programming language or app type. To empower the diverse and increasingly polyglot developer community, we\u2019ve brought Live Share to all the languages and platforms.  Whether you\u2019re building a Python web app, a Go microservice, a React Native mobile app or an Ethereum smart contract, you can use Live Share. Everything from Go to Definition, code fixes/refactorings (\"lightbulbs\"), build errors, and debugging sessions are shared with guests, which ensures that everyone stays productive no matter what you\u2019re working on. Even better, all of this works without requiring guests to have any language extensions, dependencies, or SDKs installed! You can instantly invite others to work with you and allow them to share the context from your development environment. Modern development commonly makes use of command-line tools to perform tasks such as build and running unit tests. Because the command-line represents such a core component of the common developer workflow, it\u2019s critical that participants within a Live Share session can use them, without requiring a separate tool. To further support your collaborative debugging sessions, and address one of our top feature requests, Live Share now allows hosts to share terminals with their guests.  When you share a new terminal, it is read-only by default. This allows everyone to see the real-time results of any command you run without needing to expose access to your machine. Additionally, you can choose to make a shared terminal writeable, which enables a fully collaborative experience between you and your guests which executes in your environment. This makes pair programming much easier, especially when seeking help with environment configuration. We want collaboration to become simpler and more natural for everyone so that ad-hoc interactions can occur more frequently. However, developers also need to be confident that when they share their source code, they have the necessary control and visibility into who has access, and what files they can see. To improve this balance between simplicity and security, Live Share provides two capabilities:   These enhancements represent some of our most frequently requested items so you can collaborate with confidence, without adding unnecessary friction. For more information on how to share your projects securely, refer to the documentation. We are excited for you to give Visual Studio Live Share a try! Please give us feedback how we might improve the experience for real-time collaborative development even further. If you'd like to learn more, you can refer to the Visual Studio Live Share documentation. Additionally, if you have any questions, encounter any issues, or have feature requests, please don\u2019t hesitate to let us know. Happy Collaborating!\r\nAmanda, @amandaksilver"}, {"by": "cerberusss", "descendants": 0, "id": 17020314, "score": 4, "time": 1525780666, "title": "Fairphone new cases and Android 7 announcement", "type": "story", "url": "https://www.fairphone.com/en/2018/05/08/keeping-your-phone-longer-refreshed/", "text": "   English     Bas  Founder, CEO  8 May 2018   Long-Lasting Design     Bas  Founder, CEO  8 May 2018   Long-Lasting Design  There\u2019s nothing better than the feeling of a new phone in your hand\u2026 except for the feeling of an old familiar phone that you\u2019ve kept and cared for all these years. So, we\u2019re excited to announce that nearly three years after the introduction of the Fairphone 2, we\u2019re not releasing a new model just yet. Instead, we\u2019re aiming to extend the life cycle of the phone as much as we can.  Device longevity relies both in hardware and software. We\u2019re adding new see-through covers to our collection of protective cases. A transparent cover lets your phone do the talking, telling the Fairphone 2 story by highlighting and protecting its unique modular hardware. Device longevity relies on healthy software too: we\u2019re also working hard to update the Fairphone 2\u2019s operating system, so your phone can tell the story of fairness for even longer. The new transparent case for the Fairphone 2 \u00a0 Right now, we\u2019re hard at work to get Android 7.1 \u201cNougat\u201d into the hands of existing and new Fairphone 2 users this summer. Crucially, it means we\u2019ll be able to keep our users safer for longer: Nougat will enable us to keep providing monthly security updates and patches. Android 7 also brings Doze Mode 2.0, a feature that aims to extend battery life by putting inactive apps on standby. However, it\u2019s smart about keeping fitness and tracker apps going when they really need GPS or WiFi. At Fairphone, one of our primary goals is device longevity. So for us, it made sense to face the time and resource challenges that this update required head on, and try as much as we could to make it happen. We want to keep your phone working for as long as possible so you don\u2019t need to replace it as often. Secure, up-to-date and useful software is essential to achieving this goal.\u00a0Releasing software updates for higher versions of Android \u2013 further than Android 6 \u2013 with the Fairphone 2 chipset is more challenging and success for the work on the release of newer versions is not guaranteed. That\u2019s why, despite the ongoing challenges, we\u2019ve been working hard with a community of Open Source Android developers and external parties to try to offer software updates, and will happily be rolling out Android 7 this summer. Now, you may be wondering, \u201cwhy not Android 8, \u2018Oreo\u2019?\u201d. Android 8 and 9 are entirely new levels of complexity from Android 7, far more different than 7 was to 6. Therefore, attempting the upgrade to 7 was the best option. This update will be the second major update to the Fairphone 2, but it will also mean that Fairphone is one of the few manufacturers on the market that released an update to Android 7 with a Qualcomm Snapdragon 801 chipset. Our work on this update will be publicly available and could, in theory, be adapted to the same chipset on other phones. \u00a0 So, along with upgrading your Fairphone 2, how about giving it a fresh new look? Back in 2015, when we launched the Fairphone 2, we were sure that allowing our community to take a peek at what is inside their phones was a good way to kickstart a conversation, and a great incentive to dive deeper into how our products are made. The design of our first transparent covers changed eventually to make them more resistant and durable when we first introduced the slim cases, but we didn\u2019t include a transparent option. Now we\u2019re excited to bring back two see-through cases: transparent and black translucent, to show off the Fairphone 2\u2019s modular, made-to-be-opened design. The new protective cases have the same design as the current slim cases, meaning they\u2019re durable and long-lasting. We hope that these new cases spark an interest in learning more about how your products are made, as well as great conversations about how we can keep having a positive impact. The more people who hear the story of Fairphone, the more we can spread the word about fairer electronics and create real change.  I\u2019m an open source fan. Which is to say, I believe so much in the principles behind fully transparent, fully shareble, fully user-tweakable software that I\u2019m willing to sacrifice some personal convenience for the sake of the community benefit, longer-living phones, and security open source has to offer. Today, I\u2019m happy to announce a major [\u2026]  The word is out! We have unveiled the latest accomplishment in our journey for fairer electronics and long-lasting design. By introducing two brand new camera modules, the Fairphone 2 has become the first smartphone with a core function that owners can upgrade by themselves. Now that the dust has slowly started to settle, we want [\u2026]   Except where otherwise noted, content on this site is licensed under a Creative Commons License.    Except where otherwise noted, content on this site is licensed under a Creative Commons License.  Fairphone website uses cookies. They allow us to improve the site, to analyze your use of our website anonymously, as well as allowing ads to show. By continuing to browse the website, you are agreeing to our use of cookies. For more information, click here."}, {"by": "yaroon99", "descendants": 0, "id": 17020299, "score": 14, "time": 1525780435, "title": "Get coffee with Sujan Patel, Master of SaaS", "type": "story", "url": "https://blog.salesflare.com/sujan-patel-of-mailshake-d0facff292e3", "text": "I\u2019m Jeroen from Salesflare and this is Founder Coffee. Every two weeks I have coffee with a different founder. We discuss life, passions, learnings,\u00a0\u2026 in an intimate talk, getting to know the person behind the company. For this fourth episode, I talked to Sujan Patel of Mailshake, Pick, ContentMarketer, Narrow, Linktexting, Quuu, Ramp Ventures, Web Profits,\u00a0\u2026 does this list even end? Sujan is a serial, but even more so a parallel entrepreneur, who likes to keep pushing his own limits to the extreme. He knows all about building startups and is a great guy to have a coffee with. Welcome to Founder Coffee. Prefer listening? You can find this episode on: Jeroen: Hi Sujan, great to have you on Founder Coffee. Sujan: Yeah, hey thanks for having me. Very, very excited to talk to you. Jeroen: Me too. You\u2019re founder of a whole bunch of companies, can you give us the list? Sujan: Yeah, there\u2019s a bunch. So to keep it simple, I\u2019m a managing partner at Ramp Ventures. We own six SaaS companies, we operate five. So founder of Mailshake, or cofounder of Mailshake, and Narrow.io, Pick.co, Voila Norbert. There\u2019s probably a few I\u2019m forgetting here Linktexting.com, but yeah, I run a couple SaaS companies essentially. And I also run a marketing agency, a digital marketing agency called web profits. So two, Ramp Ventures and the agency are two very different businesses. But yeah, I love helping people and working on growths and so, I guess I\u2019ve doubled down over and over to the point where I have six different things going on. Jeroen: How do you manage all of these things at once? Sujan: Yeah, so it\u2019s quite difficult to be honest, but also it\u2019s also not as bad. So one thing I do to help me manage multiple things is, I only focus for every single company, you\u2019ll notice for our SaaS companies you\u2019ll see a common trend between not the products but sometimes the industry, but more importantly, the tactics, the strategies, the channels we use to grow. They\u2019re all very similar. So that just means, when we do it once, we master it, we figure out a process and we figure out a way to do it over and over again and apply it to other companies. Sujan: So I\u2019ll give you an example, we\u2019re working on onboarding improvements for and we\u2019re also testing a referral program. We\u2019re testing a referral program in one our companies that\u2019s the less risky one, Narrow.io, see how that works. And getting people to be, getting people activated, I know the pitfalls, I\u2019ve set up lots of different referral programs, I know the pitfalls, they usually don\u2019t work as well as you want, most people don\u2019t actually do anything and what not. Use the program. So we\u2019re going to test it out. And if it works, you know we can apply it to other companies. Sujan: Mailshake has 12,000 customers I definitely don\u2019t want to do any tests on that one because that\u2019s risky there\u2019s a lot of blowback. So when we do things like this, it improves our efficiency and it also improves our effectiveness because we\u2019re not scaling things that are half-bake ideas, we\u2019re scaling things that are proven and we have metrics behind it. It helps us kind of do our jobs better. The other thing is, I think working on multiple projects, multiple businesses, switching gears all the time is something that I enjoy doing. I\u2019ve always worked in, prior to this I\u2019ve worked on the agency side, I had a previous agency, so always used to kind of switching gears all the time. And so, I kind of just got used to it and even before that, I worked at companies where I was in charge of a marketing channel specifically for multiple products. Sujan: And so just got used to really moving around. And so I can kind of go into one business for a day, do a bunch of things, or implement a bunch of things that may take a week or two to get data or even get developed, and then come back to it a week later to check on what\u2019s happening, how it went. Jeroen: Yep. Is Mailshake the bigger one in the portfolio? Sujan: Yeah, Mailshake\u2019s one of the bigger ones. At least the most [inaudible 00:04:01] one. Jeroen: Yep. Is there any bigger idea behind the portfolio? Sujan: Well, I think the bigger idea is to buy and grow SaaS companies. You\u2019ll one thing with the portfolio is that they\u2019re kind of an evolution, it\u2019s an evolution over the last three years. So 2015 started with Mailshake and Narrow, and our goal was can we even do this? Every year, can we even grow a business? Next year, we proved ourselves we could, it worked, they grew. So we\u2019re like, okay, let\u2019s go do this but we found that obviously it takes a while to build stuff, so let\u2019s go buy something that\u2019s there. Sujan: Got one, tested it, can we grow one that we didn\u2019t build? The answer\u2019s yes, so in 2017 we bought Norbert and we\u2019re growing that. It\u2019s grown over 2x in the six, seven months we\u2019ve owned it and what not. But so yeah, the theory, the concept here is that we\u2019re not neccesarily focused on any specific industry, although I know the sales and marketing space the best so that kind of naturally we get a lot more stuff in that space. But yeah I mean we\u2019re looking at HR, we\u2019re looking at kind of really anything and our mantra or kind of mission is just to grow companies to get them to size. Jeroen: Yep. So how did you get into building, growing, buying SaaS companies? How did that start? Sujan: Yeah, so that started, you know I\u2019ve always been infatuated with SaaS and software. I think when I was running my first agency called Single Grain, I worked with lots of different sales, not sales, SaaS companies and software companies, helped them grow. And I was like, man, I really want to this. So while I was doing my agency, my first one, I was trying to build some stuff on the side. It always failed. I went to startup weekend, found a developer, try to build an idea but they were kind of crappy. Reality is, I was just kind of not that good at being a software guy. I was really good as a marketer, I was good as a top of the funnel guy but I knew how to grow businesses. And so I would always get hired to work on growing businesses, like we worked with Salesforce, helped with Crazy Egg, Kissmetrics, and seeing all these companies kind of grow, I\u2019m like wow. Sujan: Meanwhile, I\u2019m chugging away, and agency margins are not the greatest- Jeroen: No. Sujan:\u00a0\u2026 an agency is a grind, right? I mean, I love it doing it very differently this time around but you know selling hours and time and so it\u2019s fairly difficult to do. And so my point being is that I was always envious on the software side. But I tried and failed a couple of times and so I was like, okay I need to get like some education in this thing. And so I was like, I sold my agency, my first one, end of 2013, and I had some pretty good runway. And so I was like, I\u2019m going to go get my master\u2019s, I\u2019m going to go do nothing. Sujan: I got to see the mobile space, built some mobile apps and try to grow those, was able to grow it. Didn\u2019t really make much money off of it, got some lawsuit due to a copyright, pretty much I bought an app from somebody and they had a copyright issue of the image, of their main image and they failed to mention that in acquisition. I ended up getting a big, fat fee. I killed all my profits, it sucks. But anyways, I ended up working and going to work for a company called When I Work.com as the head of marketing, VP of marketing and it was a client of mine. And so my goal was to, after Single Grain, to go and do something and learn the software space. Sujan: Like not learn it like read about it, I mean like live it, breathe it. And I was like I\u2019m going to give myself five years, I\u2019m going to go through an exit as an employee, I\u2019m going to take a break from entrepreneurship and that lasted like maybe six or seven months. And that\u2019s when I ran into my cofounder, well ran into my cofounder Jared from Narrow and hunted down my cofounder Colin from Mailshake and we\u2019re like, I found two doves they both love, agreed to work with me and we work together well. So I was like, well let me hedge my bets. One of these got to work, right? I have two chances of success now. And I started moonlighting in the software space after I felt like I got really, really comfortable because I knew the marketing side, but what I didn\u2019t know is the customer support, how to. Sujan: Like, I knew all the theories I\u2019m not like an idiot I was reading all this stuff, but like when you get in there and you live it and breathe it, it\u2019s very different. The sales side, the customer success, the operations, the development, all the other stuff that I think are very, very difficult to do and are equal parts of running a company when compared to marketing. Jeroen: How did you actually get into marketing in the first place? Like you worked for big companies, helping to grow them, how did that come about? Sujan: Yeah, so I got into marketing through my cousin Neil Patel. He was in high school and I think that was like my first year of college or last year like summer between college and high school and he\u2019s like, \u201cSujan, you got to check out this like this SEO thing.\u201d And I was like, \u201cWhat\u2019s SEO?\u201d So first of all he was like, \u201cYou got to check out the Internet, you can make a lot of money online.\u201d I\u2019m like, \u201cThat sounds like a scam.\u201d And then he\u2019s like, \u201cNo, no, seriously, [inaudible 00:09:42]\u201d And I\u2019m like, \u201cOkay.\u201d So I got into search engine optimization, he was like I\u2019ll show you how to do it and we should, I don\u2019t know, we\u2019re young kids right. It turns out he told me about a lot of stuff around SEO, pointed me in the right direction, didn\u2019t really teach me much, unfortunately Neil\u2019s style of training is putting you in the deep end. Sujan: Which is great that I learned the hard way and so in college I was just doing SEO consulting on part-time, instead of getting a part-time job or whatever. And as SEO evolved I kind of became more of a T-shaped marketer, learned other marketing channels, PPC, social media, I mean I got into SEO before social media existed, right. That was kind of the start of like Dig and other things like that so. So anyways, it was very early on, and it kind of just expanded to be more of a T-shaped all arounded marketer. Jeroen: Yeah, so straight from college you went into having your own agency? Sujan: Well, actually yes, I was in college when I started Single Grain. But it, we tried to do it, it worked well as a part-time college gig but not so well as a full-time job or making a living off of this. So I ended up getting a job, putting kind of on hold, getting a job at an agency and just kind of working my way up from like entry-level SEO person to like head of SEO, director of SEO whatever in two years. And I realized this workforce thing is not for me. I need to go do my own stuff and so I restarted the Single Grain business and kept going. Jeroen: Yep. Why was the workforce thing not for you? Sujan: You know it was one is like I went from like entry-level employee to director-level person in two years and I kind of hacked my way there. I kind of, every move I made I took a leap and I got lucky at the timing because it was hot so people were investing into it. But ultimately I felt like there was a ceiling of how much money I could make. I was making, I was like 22, 23 making well over, well into the six figures and you know my bonus, with my bonus and rev share and all that stuff I had with my job, I was making, I was clearing almost 200k or if not a little bit more, I forget now. Sujan: But I was like, I look at the wages for like what the next two, three levels would be and they weren\u2019t that much better. And I was like, okay, so I have no college degree because I dropped out. I could grind this out and kind of work my way up, but all I\u2019m going to be doing is going and doing less marketing and being in more meetings and upside isn\u2019t that much greater than what I have now and I\u2019m 23. I can always go back to this if I ever wanted to so I was like, I want to make more money, I want to make millions and at the time I was like, I want to do, I want to make money and I want to do things I like, which is marketing. And not being in meetings and PowerPoint presentations, which is what I believe the life of an executive marketing person is, at any large company. Sujan: So anyways, that\u2019s what brought me into doing my own business. I ended up convincing, crazy story, convincing my day job, to they were downsizing and laying off a few people, so I was like this is a good time to like leave. So I convinced them to be my first client at the agency and I locked in a year contract with them and also got them to pay me more than my salary. And I was like, \u201cLook, you guys are laying off people. You\u2019ve just laid off like four or five people in the SEO team that I was running. There\u2019s three people left on the team, I could do all the work of all these, not neccesarily the people that are on the team, but the people you just let go and myself and you could de-risk the position and move me to contractor. I want to start an agency anyways.\u201d And so yeah, that worked really, really well and I gave us a year to either fail or succeed, and it ended up working out. Jeroen: Yeah, so that was kind of to start off on your own, was that with the intention of only growing your own agency or already thinking about products? Sujan: At the time I was not thinking about really products, I was thinking more about what businesses can I get into. So I was like very rudimentary of like, what are the businesses that I can even start? What do I have the skill sets for? And so we did some affiliate, I was doing affiliate marketing and lead generation sites. And I was in the travel space, I did a lot of SEO and marketing for the travel space. And so I was like, oh, maybe we can work with in the lead gen for insurance in travel, and so I built out some sites and they worked pretty well for some time. So anyways, that was the extent of my product experience. It was like working and promoting someone else\u2019s product. Jeroen: Yep, cool. And now it\u2019s with your different startups are any of those VC funded or are they all bootstrapped? Sujan: So we\u2019re, I\u2019m a partner in Quuu, Quuu.co. And that is a funded startup. Can\u2019t, we haven\u2019t raised like a Series A or anything still like seed and early stage funding. But yeah, so that\u2019s the only one. That\u2019s also one that we don\u2019t run through Ramp Ventures just because we\u2019re a partner, there\u2019s an awesome team that\u2019s behind it based in the UK. Jeroen: Yep, with Daniel [Campa 00:15:31]. Sujan: Yep. Jeroen: Yep. Is that a conscious choice not to raise money? Sujan: Yeah, absolutely. I\u2019ve been involved in, when I work as an executive sitting in board meetings, I\u2019ve worked with lots of different VCs, they actually refer us quite a bit of business and I\u2019ve seen the insides of a SaaS company, working with VCs. And realistically I didn\u2019t want to have a job that I was forced to do for 10 years. That\u2019s how I look at getting investment. The reason I say 10 years or failure really, is that if you\u2019re taking $500, $1,000, $1 million, $20 million, $100 million, whatever it is, the bigger it is the longer the commitment. And VCs don\u2019t want to see things do well, they want to see things explode. Or implode. And so that means you have to hire fast, grow as a team, do things that I think of, if you didn\u2019t have the money it would be irresponsible to do the same things. Sujan: And I want to build something sustainable and so yeah, we could have easily raised money for Mailshake, or any other venture, any I could have joined some firms as a ER, jumped on as a cofounder of a business but I wanted to have my cake and eat it too. So I wanted to make money now but I also want to make a lot of money later on a potential exit or as it grows. And so, I figure I\u2019ve got enough capital to kind of get things off the ground. I\u2019m, for the last few years I run an agency because that\u2019s how I make a living. And I don\u2019t need to take any money off of SaaS stuff, so I\u2019m not a burden on the cost. Sujan: So, it kind of allows us to grow without anything else. And you know one of the things I realized is that I don\u2019t want to take a shot at $1 billion or $100 million or this crazy, big company. I don\u2019t want to take one shot in 10 years and then if I look at my life, I got maybe two, three shots before I\u2019m like, I\u2019ve got to give up on this and go do something like a get a job or whatever. I want to take more than that. I want to get, I want to do it quicker. And I think doing it quicker means no one kind of telling me what to do. Sujan: Obviously we\u2019ve got some advisors, my partner in Ramp Ventures is an ex-VC, and so he\u2019s very, very experienced on the capital raising and that front and exiting multiple businesses. But we didn\u2019t want to do that ultimately. Because I wanted to be able to, if I see, let\u2019s say we\u2019re 30% of our goal for our company and somebody offers us a ridiculous amount of money, or even a semi-ridiculous amount of money, or even a reasonable amount of money, I want to be able to consider that and make my own decision, not have a board or outside people tell me that I can or cannot do that. Jeroen: Yep. When you buy a business, what is exactly the goal you have in mind when you do that? Like where do you see the business going? Sujan: My goal, yeah my goal is to really to try to grow the business. Ultimately I won\u2019t buy a business that I don\u2019t think I can grow. And I want to understand who the customers are, I want to look at can we make the product better? Where are the strengths in the products? And so but ultimately you know I\u2019m looking at a business and I ask myself, ultimately can I 5 to 10X this business? If I can, then I usually dig deeper, you know looking at the competition, looking at what\u2019s out there and what the strengths and weaknesses are, talking to customers. Really it\u2019s awesome to see this, get this detail and go into the weeds on this because it helps you understand more and more businesses and what the strengths are, is making me better at running my own. Jeroen: Yep. For those with a small SaaS business out there that might be interested in selling, what kind of businesses are you looking for? Sujan: Yeah, so we\u2019re looking for really businesses that are somewhere between 100K ARR to a million. Or a few million. And want to, really want to just grow them, right, they want to, they could be in, have to be SaaS, first of all, but they could be in any industry, really just want to make sure that we can grow them. So if anyone out there knows anyone that are interested, let me know. Jeroen: Cool. What business do you spend most of your time on now? Sujan: It ranges. Right now in the last few months I\u2019ve been kind of serving as a customer success person for Mailshake. I\u2019m kind of known for like going into the different businesses, I\u2019ll go and try a rollout. So for example, we want to test out if customer success is the right hire for us, or should we hire a salesperson? I want to see if, we get hundreds of customers, new customers a month signing up. I want to see if engaging them actually does anything. I mean I know obviously it will, talking to customers is definitely very, very valuable, but do customers want to talk to us? What are the KPIs of this role? What [inaudible 00:21:28] the meeting? All these little nitty, gritty details that I think we\u2019re not quite ready for that role but I can serve as that role. Sujan: You know, last year I was doing a lot of the marketing for Mailshake, the content marketing we\u2019ve hired for that, so we\u2019re pretty much serving as roles that are potentially coming up. And I learn a lot about the position and then either we\u2019ll move on with it or hire somebody to kind of manage that. But I\u2019m also doing onboarding and things with other businesses. We\u2019ve got a really, really awesome team which helps us execute. So I\u2019m not in the weeds on building a new, for like example in Norbert, we\u2019re expanding the product to add in a few more, add in a few more things, bells and whistles to what it does. It\u2019s fallen behind over the years to some of the competitors. Sujan: So we\u2019re kind of expanding to catch-up. I don\u2019t need to neccesarily be in the day to day of those things, but that was something myself and my partner Bob worked on three months ago. He\u2019s kind of carrying that and I\u2019ll kind of, once it\u2019s live, now it\u2019s my turn to do more of the marketing for it. I can go in and out of businesses fairly easily and I usually go in and out of businesses on a daily basis. So like I worked on Pick yesterday. We worked on all of the onboarding and some of the, some of the technical stuff to kind of get the email and marketing automation set up. That\u2019s going to take two days for my dev to do, I\u2019m going to, now I\u2019m going to back to Mailshake and kind of move on and do that role. Jeroen: Yep. So if I understand it well, you focus very much on experimenting, improving, and then delegating that or? Sujan: Yeah, yeah. Delegating and also could be just the in some cases it\u2019s not doing things. Choosing not to do things. So for example, we\u2019re working on, we use the ICE framework. Impact, Confidence, Ease of Implementation in rating all of our ideas. A lot of times we decide not to do things. Like I already have the product roadmap for the next year, year and a half for Mailshake. I know exactly what we\u2019re going to do and build for Norbert. Some of these things we decided not to do immediately and we prioritize and that comes from extreme discipline, but also comes from having resource and budget constraints. It forces you, those two things have leveled me up as a human being, as well as a marketer. Because having those constraints really forces you to think about, oh is your genius new idea or like the 50 things you could do to grow the company, the best things to do to grow the company? Or run the company or whatever. Sujan: I think, most marketers and also most founders, they get really carried away with the fact that they can do things. But that doesn\u2019t neccesarily mean you should do things or those are the right things to do. And so, I always go through the exercise of like, is this the absolute right thing to do? We usually move, that means we usually move slower than most, and we don\u2019t do a lot. But, that means we end up executing better. Sujan: You know, one of the things I really believe in, and I\u2019m not an expert in but part of what makes Mailshake, Pick and some of our other companies really great is the UX, like I\u2019m looking at this as like, I\u2019m an end consumer, is this the easiest, simplest thing to do? And if we can get that right or perfect, then we win. Because it doesn\u2019t matter if a tool or a competitor whatever has more features, functionality, cheaper whatever, if ours is the easiest to use, I\u2019ve found that that\u2019s going to be the thing that makes people love the tool, [inaudible 00:25:50] spread the word, stay on it longer for a customer, all sorts of other things. Jeroen: Mm-hmm (affirmative), right. When you look at the different tools you have in your portfolio, the common thread is probably ease of use. Sujan: Yeah, exactly, ease of use and they\u2019re simple. I think that\u2019s kind of our ethos, specifically for Mailshake but really for Ramp Ventures as well is that we want to make things as easy to use as possible for our demographic. And that means saying no to building out features, saying no to doing things that make things complicated. We get feature requests all the time and it\u2019s, I don\u2019t want to say no, and I never immediately say no, I take note, we have a list, but when we go think about how we can grow and I think a lot of customers, I\u2019ll give you an example. A lot of customers are like, we really want to build a CRM. We want to do everything inside of Mailshake and I\u2019m like, actually guys, if you did everything inside Mailshake, you\u2019re going to be ineffective and you\u2019re going to be confused as hell. Sujan: And so I\u2019m making, you\u2019re telling me, I know customers, they want the functionality but it\u2019s really, when you try and go an execute it, I\u2019d rather go and integrate with you guys because that\u2019s what you, core functionality is, then try out a big old [inaudible 00:27:13] ourselves because we\u2019re never going to be a great all in one tool. I don\u2019t know any great all in one tools. If you look at HubSpot, you look at Salesforce, people don\u2019t, you don\u2019t hear people like, I love that I can do everything in HubSpot. No, they\u2019re like, I use HubSpot because I use HubSpot for leads, I use it for this lead scoring side, I use it\u2019s CRM, like they use it because it does everything, but they don\u2019t love it because it does everything. Jeroen: No, no. Sujan: And it can\u2019t do everything great. Jeroen: I agree. It tends to make the solution very complicated, difficult to navigate. It has some, obviously some advantages but it doesn\u2019t make it fun or like a joy to use it. Sujan: Yeah, exactly. And I found that fun and enjoyable to use is actually the thing that is the marketing engine. So in the early days of Mailshake I talked to a few customers and I was talking to every single customers I can get my hands on that would be willing to talk to me. And one of the things that somebody said to me, Mailshake we have three different use cases or personas. We have marketers use it for link building content, you know whatever. I know that customer. [inaudible 00:28:39] is one part of that flow but the sales audience is probably a bigger segment. And a third was kind of like HR, recruiting side of things, or PR it\u2019s kind of a hodge podge of other things but like it\u2019s an individual in the organization. But the salesperson, I was talking to somebody who runs like a seven, eight person sales team and they just signed up. At that time he was our first like eight person account and eight user account is I\u2019ve got to talk to this guy. Who is this guy? Sujan: I talked to him and he\u2019s like, he told me that he uses Mailshake even though it doesn\u2019t have a lot of the functionality that he needs because he goes and puts the software in front of a salesperson and without any explanation or maybe even just like a video that he created, two minute video, they can get it and be effective immediately. And he\u2019s like, that is why I love this thing. And he\u2019s like I don\u2019t care if this costs 20 bucks or $5,000 a month, you\u2019re making seven of my team more productive and the net gain on efficiency is in the tens of thousands. Jeroen: Mm-hmm (affirmative), yeah, yeah most people are not really tech savvy, so you need to make it super easy for them to become productive. What is it exactly that gets you in like flow state? What are the things that keep you going? Sujan: You know I get really excited like so, I get really excited about doing like, getting a win or seeing the results of hard work. So like, I\u2019ll give you an example and this is not even a monetary change or win. We implemented a, just this past Monday, we implemented this form, came up with an idea when somebody signs up as a, somebody uses Mailshake successfully, well we had to define what that means, so for us that means they\u2019ve sent out one campaign, right? And in order to send a campaign you kind of need to set up your email, write the email, figure out who to contact, there\u2019s like five or six things that involved, right? Sujan: And we wanted to survey, like we have 12,000 customers, I don\u2019t know, I\u2019ve talked to hundreds and hundreds of them, and some of them more intimately than others, actually probably a 1,000 now, but that\u2019s still 11,000 people I haven\u2019t talked to. And I was like, what the heck do the people use it for? And I know there\u2019s like dozens of use cases for it but I wanted to hear it from people\u2019s mouth. And so we implemented this like, this forced feedback form that\u2019s after you send a campaign, it asks you like, \u201cWhat are you using Mailshake for?\u201d And it\u2019s like, \u201cTell us more so we can make the software better for you and more tailored\u201d and whatever. Sujan: And so like we literally and we also force every single customer that hasn\u2019t like that has sent a campaign ever before to receive this too. And there\u2019s no way to x out of this, so in the first hour, we got 500 people. Five hundred responses. Within the first day we ended up getting like 1,300 and now we\u2019re pacing at like 2,500. This has been like three days ago. It\u2019s exciting first of all to see all the quantitative and well as qualitative information back from how people are using it. Some people are giving us feature requests. Some people are just saying like, this is what I\u2019m doing and I integrate with this other company, this is my workflow. Sujan: And also, we weren\u2019t, one of our metrics, a metric we don\u2019t really track is like daily active users, because I don\u2019t care. I look at campaigns sent. And we look at also number of emails that we send per day. But what I was most excited about this was like, holy crap we got a lot of feedback. And also we found out that there\u2019s frickin 2,000 people or over 1,000 people that login to our product every single day. If I were to have looked at the data, and we have tracked this but it doesn\u2019t show those numbers, it doesn\u2019t show nearly that high. So I was just excited that this came back, we got feedback. Sujan: You know for Norbert, another thing that excites me, Norbert we increased the conversion by 2x already or sorry 3x. So it was exciting to see the movement, that needle go up from one percent to two percent to three percent from pay to, from free trial to paid. So those are not the actual numbers but I\u2019m just giving you random numbers. Jeroen: Yeah, yeah, yeah. Sujan: But it was exciting to see all those things. And then, just kind of seeing those numbers and that\u2019s why I like working on multiple companies. Because while one is kind of maybe struggling the others ones like, I can tinker and poke around like I\u2019m really excited because I\u2019m getting my hands dirty with the referral program again, it\u2019s been six, seven months since I\u2019ve done it. For one company I\u2019m working on in-app kind of personalized workflows, for another company and I\u2019m working on customer success. For another one giving people feedback. So like I get gratification in three different ways, and they I think, but now I have different mechanisms to get this feedback, I\u2019m kind of addicted, so. But it\u2019s fun. Jeroen: Yeah, you are a really lucky, lucky founder I think. Sujan: Yeah. Jeroen: In terms of balancing this with your personal life, how does that work? Sujan: Yeah, so that\u2019s a good question. So I think for the longest time, I was trying to find my work-life balance. So last two and a half years, so we\u2019ve been doing Ramp Ventures now for it hasn\u2019t had the same, it hasn\u2019t had the name Ramp Ventures but we\u2019ve been doing this kind of model for the last three years. And I can say for the first two and a half years, there was no work-life balance. Jeroen: No. Sujan: There was just work. It was my life. And try to survive it right. And I think a lot of this was also do to me having fun doing this too, right? I found it truly passionate. But I think now we\u2019ve been able to in the last six months, hire build out the team, hire for the things that my founder, my cofounder and I were doing manually or solves that can be outsourced, including customer support. That is another thing that we\u2019ve now finally been able to get off our plates. So then we could proactively reach out. Sujan: So now I have a work-life balance and what I try to do is exercise in the morning. That when I exercise in the morning, I wake up early, by the time my true workday starts, I\u2019m usually caught up on emails, I\u2019m not like trying to go like catch up on anything and then focus. In the evenings I usually clock off around like four, three or four o\u2019clock. So it\u2019s really different like, signing off, off work, leaving the office at like 3:00 or 4:00 is really weird for me. Sujan: Or it sounds weird for people but that\u2019s because I found that the last few hours of a typical workday, I\u2019m completely and utterly useless. Like I\u2019m tired, I\u2019m like browsing Facebook and looking at Amazon and texting my wife, well what\u2019s for dinner and where do you want to go or what do you want to make, whatever it is. I\u2019m not really focused. And so I sign off early, actually come home or sometimes I kind of just go do something fun. Sometimes I watch a movie, whatever and I come home, spend time with my wife, family, eat, dinner. And I kind of [inaudible 00:36:36] back on again, after, in the evening, where I have this like less distraction of, I\u2019ve got my one or two hours to get in my zone and usually in that one or two hours of doing something I come up with something good. I do things much faster than would have taken me longer during the workday where there\u2019s a lot of kind of distractions. And realistically my best and most productive days are Sunday mornings. Sujan: I wake up early Sunday mornings, I usually knock out one of the biggest, hardest thing I have which helps me and then organize kind of what I have going on. And during the weekdays, my goal at my job, my role is really to help my team remove hurdles and bottlenecks and make sure that they have everything they need to be successful in their roles. So I\u2019m not actually doing a whole lot as an individual contributor during the work hours. Jeroen: Yep, yep, I saw on Facebook recently that you really start working out, is that working for you? Is that- Sujan: Oh yeah. Jeroen:\u00a0\u2026 do you feel a difference in energy? Sujan: Absolutely. I\u2019ve got, I wake up earlier, I work harder and it\u2019s all because I actually exercise. And this is, this is not my first time doing this, this is actually my second time. I\u2019ve actually for the last two years just got distracted and fell off the wagon of eating healthy and exercising regularly. I was doing it for five years before, but I kind of, I moved cities, I moved time zone cities and started traveling and doing a lot of speaking and working much, much harder. Sujan: So I kind of was like, you know what I can stop doing this. And one by one everything kind of faded away but like exercise has helped me a lot. And I mean, just even like 20 minutes running or like going to the gym like it doesn\u2019t matter what you do, getting your heart rate up has helped me kind of be happier. And it\u2019s a, I kind of sound like a little infomercial here but it\u2019s such a simple thing that works so well. You just have to get your ass out of bed or off the couch or office chair and do it. Jeroen: Yeah, no I totally agree. I also just started running again two weeks ago, makes me feel so much better. Sujan: Yeah. Jeroen: You just mentioned you moved cities, where are you based now? Sujan: I\u2019m in Austin. I- Jeroen: Austin, Texas? Sujan: Yep, Austin, Texas. I\u2019m from LA, lived in San Francisco for five years and then kind of made my way to Austin. Jeroen: Why did you move to Austin? Sujan: It\u2019s a little mini San Francisco, it\u2019s a mini little tech area. Lots of good food, nightlife, and I like it because it\u2019s not always all tech focused. There\u2019s probably a lot of non-tech things, music, and fun so. I like the work-life balance of the community, oh sorry of my surroundings and again that kind of forces me to be outside of that bubble. I think in San Francisco, I love that place, I always say like that\u2019s where I grew up, that\u2019s where I kind of learned my chops, but I think you kind of live in this bubble. Everyone\u2019s a founder, everyone\u2019s doing something cool, whatever and so getting out of that and seeing what the rest of the world looks like is a, I think more important than anything else. Sujan: And funny thing is, I connected with more people the six to nine months after I left San Francisco than I did the whole time I was there. Jeroen: With people in San Francisco or? Sujan: Yeah, with people in San Francisco. I met, I even went and hung out with people more, I had more meetings with people in San Francisco than I did the five years that I was there. And it was because I made a concentrated effort. And when I was there for like, let\u2019s say five or six days for a conference or speaking or like something, I would make sure that I connected with everybody that I wanted to connect to. Whereas, when I was there I\u2019d be like\u00a0\u2026 I\u2019m busy right now, I\u2019ll do it next month, or next week and that kind of turned into never. Or just never making, being proactive to connect with people. And so, over the years one of my things, I like to daily I don\u2019t do it every day these days but usually three or four times a week, is connect with someone new or have a meaningful conversation. Sujan: I remember a few weeks, a month ago now we had our, we had a great conversation, no agenda, just to meet each other, get to know each other. And I have those types of conversations two or three times a week at least and those things have been great because it opens my eyes. I learn, I can share my knowledge and it\u2019s a whole lot of fun. Jeroen: Yep. I\u2019m also learning a lot now doing these calls, it\u2019s really amazing. You think Austin is a good place to have your startup? What other cool startups are based in Austin? Sujan: Yeah, I think, there\u2019s, Austin\u2019s a pretty good place. There\u2019s Book in a Box is a good startup, Sumo.com and Noah Kagan and that group is here. There\u2019s HomeAway, you know one of the older startups. Dell, which is not neccesarily a startup anymore and some larger companies. But yeah, I mean, there\u2019s a decent amount of startups. Not as much as you would think. And not always like software businesses. There\u2019s companies like Able or LawnStarter subscription lawn care business. Lots of very different kinds of businesses. And I think because it\u2019s a smaller community, I think everyone gets, it\u2019s more intimate and they get to know each other and so. I\u2019ve gotten, I have a monthly CMO breakfast we have and there\u2019s like five to seven people that attend with all the VPs or executives, executive marketers in the area. So I think my network is much, much tighter here so I don\u2019t have as many relationships, but I have a lot more meaningful and stronger ones. Sujan: But I think, I used to think, oh you have to be in a certain area. But, you don\u2019t, you could be anywhere. I think it\u2019s being about, it\u2019s about really going after who you want to connect with and figuring out how to connect with them. And you\u2019ll find that it\u2019s actually not the location that is the hindering factor, it\u2019s either you or your lack of initiative to connect with that person. Jeroen: Totally agree. Wrapping up, what\u2019s the latest good book you\u2019ve read and why did you choose to read it? Sujan: I am reading the book Sapiens. Sapiens I don\u2019t know why I\u2019m reading it. I\u2019m not. I would say it\u2019s interesting, it\u2019s an interesting book. I chose to read it because every one of my friends kept telling to check- Jeroen: Yeah, exactly. Sujan:\u00a0\u2026 it out. I actually didn\u2019t, I was like, sometimes I blindly take recommendations for books of people I respect. And again like that book is just really interesting learning about humanity and human beings and what not. I would say the book I most recently read that I love is the book Principles by Ray Dalio. He\u2019s the guy who kind of, I can\u2019t even explain what he does, but it\u2019s about investing and kind of money management and what not and work-life. Awesome book. Like one of my favorites, but yeah. That\u2019s the one I would recommend. Jeroen: Yeah, I\u2019ll put it on my list. Is there anything you wish you had known when you started out? Sujan: No. You know why? It is because even if I knew it, I would make another mistake and I wish I would have, then I would fast forward to this interview and wish I knew that mistake. So I learned all these lessons the hard way and made all these mistakes and honestly that\u2019s kind of what made me get to where I am. And so I\u2019m fine with going that route over and over again. Jeroen: Cool. Thanks for being on Founder Coffee, it was really interesting. I\u2019ll send you over a package of some actual Founder Coffee in the next few weeks. Thanks again. Sujan: My pleasure, thanks for having me. Enjoyed it? Read Founder Coffee interviews with other founders. We hope you liked this episode. If you did, review us on iTunes! \ud83d\ude0d For more hot stuff on startups, growth and sales \ud83d\udc49 subscribe here \ud83d\udc49 follow @salesflare on Twitter or Facebook Comment  Name *  Email *  Website       "}, {"by": "kawera", "descendants": 0, "id": 17020289, "score": 2, "time": 1525780278, "title": "Living with the consequences of climate change", "type": "story", "url": "https://newhumanist.org.uk/articles/5299/", "text": "Want some offline reading? Get our magazine Climate change is already here: we must stop debating deniers and start making tough decisions about what we will save. \u2013\u00a0\u00a0 By\r\n\nWill McCallum\n\r\n\u00a0\u00a0\u2013\r\n\nMonday, \r\n19th\n\nMarch\n2018\n\n\n This article is a preview from the Spring 2018 edition of New Humanist. Deep purple is not a colour normally seen on the weather report. In 2013, responding to a heatwave deemed \u201cvirtually impossible\u201d without climate change, the Australian Bureau of Meteorology introduced a new colour to their heat index to depict the 50-degree-plus temperatures they were facing in a summer that broke 123 different temperature records across the country. Some of these records have been broken several times since. Climate change is making its mark on the world, and new maps are needed to chart the changes \u2013 from the now annual deep purple in Australian weather reports to the retreating ice of the Arctic. In the echo chamber that is my social media feed there was a fresh storm over the New Year. Donald Trump, yet again, provoked anger by claiming that a cold snap on the east coast of the US showed that global warming was a liberal hoax. Cue indignant posts by scientists and campaigners, along with mainstream media coverage . The unfortunate fact is that we are already experiencing climate change, and we need to stop getting dragged into endless debates with sceptics. Instead, we need to focus on how we\u2019re going to cope with the changes ahead of us. That is not to say that environmentalists should abandon their campaigns to reduce our reliance on fossil fuels, or to limit polluting industrial activity \u2013 we have much we can and must still do to avoid the worst effects of climate change. Recent literature, such as a 2017 paper in Nature, suggests we have just a 5 per cent chance of keeping warming within 2 degrees, an amount that will already see sea levels rise by an estimated 50cm and render much of the UK\u2019s coastal flood defences useless. But each fraction of a degree of warming beyond this could spell the end for whole communities, cities and even some countries. The New Zealand government is considering a new climate-change refugee visa in order to welcome those forced to leave South Pacific islands by rising sea levels. The challenge, then, is to do both \u2013 to adapt to an already changed world and to take action against it getting worse. Collectively, we have some difficult decisions to make about what we choose to save and what we should accept we are going to lose. We need to reflect on what the changed world we are facing is going to look like. This isn\u2019t easy for anybody. People in my field of work \u2013 I\u2019m head of oceans campaigns for Greenpeace \u2013 are trained not to dwell too long on the scary, or even the uncomfortable. The communications advice is to focus on the positive: stories of hope, of our ability as humans to innovate and collaborate, to avoid doomsday scenario descriptions. But does the avoidance of honest discussion hinder our ability to make urgent decisions now? As researchers warned in the journal Proceedings of the National Academy of Sciences last July, we are already living through the sixth mass extinction. Yet when I mentioned to a friend that I wanted to write an article about it, her response was typical: \u201cbut aren\u2019t so many creatures adapting quite well? And technology is so good that surely we can save the rest.\u201d We are quick to tell ourselves the stories we want to hear. \u201cOne of the penalties of an ecological education,\u201d said the conservationist Aldo Leopold, recently quoted in an article by George Monbiot, \u201cis that one lives alone in a world of wounds.\u201d In trying to deal with these wounds, first we convince ourselves of our ability to save everything \u2013 and when that fails, we move on without critically examining our failures. Our adaptability as humans is a strength, but it can also be a weakness. Last year the media reported a triumph \u2013 North Sea cod was certified as sustainable by the Marine Stewardship Council. Cod stocks plummeted by nearly 90 per cent between 1976 and 2006, devastating fishing communities and galvanising us against overfishing. The recent certification suggests we are on the right track, but it was disappointing to see it so uncritically celebrated, given that stocks are still roughly half their historic levels. The bigger change is not its recovery, but our ability, eagerness even, to redefine what is normal, and even sustainable. To cope with our future in a more honest and constructive way, perhaps we need storytelling that unpacks the difficulties of our age. There is not a complete dearth. The Archers does its bit, broadcasting the reality of an agricultural industry struggling to deal with changes in its environment \u2013 increased flooding and soil degradation two recent examples. But we continue to tell ourselves \u2013 via a mass media often unable to deal with nuance \u2013 that technology will fix things; or that climate change will be mitigated by benefits such as a thriving English wine industry. The flipside are apocalyptic stories that portray our future world as entirely barren. (See Cal Flyn\u2019s piece on page 46 for more on the apocalypse in popular culture.) Elements of this are true: in just a few years the Arctic may be free of all ice cover during the summer months, warned the \u201cSnow, Water, Ice and Permafrost in the Arctic\u201d assessment last April. The home of polar bears and narwhals, creatures that have enchanted children for generations, forever lost. Such an event will no doubt be greeted with eulogies by poets and geographers, but perhaps what we need instead are more stories that show us how to deal with profound change. Last summer, New York Magazine published an article by David Wallace-Wells entitled \u201cThe Uninhabitable Earth\u201d. In it he outlined what many scientists would deem a worst-case scenario for climate change, its opening sentence setting the scene: \u201cIt is, I promise, worse than you think.\u201d The article went viral, and widespread condemnation from the experts followed: Wallace-Wells had gone too far \u2013 a description of the human body cooking from inside out; subheadings like \u201cThe End of Food\u201d \u2013 and the article was likely to frighten readers into inaction. But the environmentalist and blogger David Roberts pointed out that since the article was being read by millions, it evidently had something other climate-change literature lacked. A touch of honesty, perhaps. Nevertheless it was too apocalyptic for my taste. Both over- and understating the truth squeezes out the necessary nuanced debates we should be having about our future. Rising sea levels, expanding desertification and an increase in the frequency of extreme weather events mean we have already denied future generations the same quality of life we have enjoyed. The least we can do is first to not let it get even worse; and second to learn how to make difficult decisions as a society. What kind of decisions do I mean? Every area of policy-making will be affected \u2013 from public health, with the likely spread of infectious diseases, to insurance regulation struggling to deal with bigger, more frequent hurricanes and floods \u2013 our fast-changing world requires new rules. No area, however, is changing as fast as wildlife protection. According to a 2014 paper in Conservation Biology, with an estimated 10,000 species going extinct every year, we are experiencing a rate of species extinction up to 10,000 times higher than what would be considered \u201cnatural\u201d. Species that have evolved over millions of years in a relatively stable climate are for the most part struggling to keep up with the pace of change. Many of them will not make it. With our help, some of them might. As one of the most diverse habitats on the planet, coral reefs will be among the worst hit. The fate of our coral reefs, often characterised as underwater gardens of paradise (the earliest written reference to coral is in a Chinese historical text from the first century BCE), is front-page news. Mass bleaching \u2013 the killing of coral by rising water temperatures \u2013 provides news outlets with shocking \u201cbefore and after\u201d images. I can\u2019t count the number of \u201cSave the Reef\u201d campaigns I have seen or been a part of \u2013 save them from fishing, from pollution, from mining \u2013 the list goes on. Are such campaigns worthwhile? Is it possible to still save coral, given that ocean temperatures are already set to rise beyond what may make them habitable? The answer \u2013 maybe we can save some. Such uncertainty presents us with challenging decisions: whether to expend significant capital putting the world\u2019s coral ecosystem off-limits to human activity so that it might regenerate; whether to invest in technologies like the recently reported \u201ccoral sperm bank\u201d in the hope that at some point in the future we\u2019ll be in a position to artificially restore the reefs. This is before we even think about how to prevent the plastic polluting our oceans from flooding on to the reefs. Conservation was already filled with these complex decisions, but the prospect of a world altered by climate change is only making them more difficult. How to judge what is ecologically valuable or culturally significant enough to preserve? In general, solipsism rules. Any ecologist can make the case for the species or habitat they are an expert in. Speak to any campaigner and they will relay all the reasons to support their particular cause. And most policy makers will tell you why the suggestions of these scientists and campaigners are not possible given the resources available. Put all three together and you risk years of point-scoring, at the end of which no decision is made. It is tempting to advocate a purely rational decision-making model. This could include criteria against which policy-makers could undertake cost-benefit analyses to inform decisions. Such a model might sound appealing, but is it realistic given the extensive gaps in any evidence portfolio? We know less about the seabed than about the surface of the moon, so how are we meant to make these judgements, given the unknowns? An evidence base could probably be constructed to justify protecting pretty much anything from the smallest creatures at the bottom of the food chain to the apex predators keeping everything else in check. In his work on rational decision-making models, the political philosopher Peter Allen discusses how such a narrow approach may also preclude more radical policy, the kind of transformative action that we\u2019re desperately in need of. Ideas such as Kate Haworth\u2019s doughnut economics, the sharing economy and decentralised decision-making should all be explored to help us be innovative. A \u201crational\u201d model also denies the less tangible cultural and social factors that play a role in conservation decisions. My own reaction to the RSPB\u2019s 2017 report that the Scottish Crossbill is facing extinction due to climate change served as a reminder that our cultural identity remains highly influential in how we approach conservation. The report made me feel miserable, but my misery was at the prospect of losing the only bird to be found in Britain and nowhere else rather than any ecological assessment of what it might mean for the pine forests of the Highlands. Evidence-based policy should still of course be the guiding principle. Not least because our lack of meaningful discussion about the future means that we lack the emotional intelligence to allow us to make good decisions. As an example, one can look to the example of the vaquita \u2013 the world\u2019s smallest dolphin. Last year saw its population fall to below 30. The cause of its decline has been known for years: illegal fishing for the totoaba fish, whose swim bladder is highly prized in traditional Chinese medicine. One of the first petitions I was involved in at Greenpeace was to \u201csave the vaquita\u201d \u2013 at that time there were still several hundred left and it felt like a realistic prospect. Now, in a bizarre twist, the Mexican government is considering the use of military-trained dolphins to round up the few survivors and keep them safe in captivity. All historical evidence suggests they won\u2019t do better in captivity than in the wild and that the genetic pool of just 30 animals is unlikely to be large enough for population recovery. If action had been taken sooner this absurd situation could probably have been avoided. Instead we are witnessing irrational, last-minute action by a government concerned about its popular image. Wouldn\u2019t these resources be better deployed securing the future of another species under threat? Do we throw everything we can at saving the Scottish Crossbill because it has the word Scottish in its name? Maybe, but if we do let\u2019s be honest that it has as much to do with nationalism as with ecology; and that it will come at the expense of concentrating efforts elsewhere. The challenge of our generation is a daunting one, with no easy avenue. We know what is coming, but we are currently unable to pull back and understand the bigger picture. We may not be able to save everything, but it is likely we can halt the most radical changes \u2013 the ones that will destroy all that is familiar to us. But we will only be able to do this if we take seriously our situation, come to terms with what we will lose, and start talking frankly about where we need to concentrate our resources. We should allow ourselves to be hopeful, to be inspired by the possibilities of technology and science \u2013 but not at the expense of dealing with the consequences of our actions. This article was brought to you by New Humanist, a quarterly journal of ideas, science and culture. To support our journalism, please subscribe. Chemistry, Biology, Physics: Three scientists talk through big recent developments in their fields. The argument from design is often thought of as the most modern of objections to evolution. But Charles Darwin anticipated right from the start, says James Randerson The Rationalist Association is independent, irreverent & non-profit. We are supported by our members. The Rationalist Association is a registered charity in England No 1096577 a company limited by guarantee No 4118489 All content \u00a9 of the RA or its respective author. This site uses cookies. By continuing to browse the site you are agreeing to our use of cookies. Cookies and how we use them Design & front-end development by Mr & Mrs OK. Web system by SimplicityWeb."}, {"by": "indescions_2018", "descendants": 0, "id": 17020285, "score": 1, "time": 1525780224, "title": "The Cutting Room Floor: Unearthing Unused Content from Video Games", "type": "story", "url": "https://tcrf.net/The_Cutting_Room_Floor", "text": "\n  The Cutting Room Floor is a site dedicated to unearthing and researching unused and cut content from video games. From debug menus, to unused music, graphics, enemies, or levels, many games have content never meant to be seen by anybody but the developers \u2014 or even meant for everybody, but cut due to time/budget constraints.\n Feel free to browse our collection of games and start reading. Up for research? Try looking at some stubs and see if you can help us out. Just have some faint memory of some unused menu/level you saw years ago but can't remember how to access it? Feel free to start a page with what you saw and we'll take a look. If you want to help keep this site running and help further research into games, feel free to donate.\n Developer: Nintendo\nPublisher: Nintendo\nReleased: Unknown (disk image dumped in 2010), Famicom Disk System\n A The Legend of Zelda Prototype was dumped and released on December 25th, 2010. It's a late build of the game: the game can be completed, but the Second Quest has just barely been started. Nevertheless, there are a great amount of room alterations, both on the overworld and in the nine dungeons. Some of the game's iconic enemies have different designs, shops have different items in stock, and a significant amount of music hasn't been composed yet. Overall, the prototype is a lot more forgiving than the final version: it's much easier to get rupees, and enemy placement and difficulty isn't quite as evil. It's worth checking out for anyone interested in learning about the game's development.\n Read more...\n \n Want to contribute? Not sure where to begin? Visit the Help page for everything you need to get started, including...\n We also have a sizable list of games that either don't have pages yet, or whose pages are in serious need of expansion. Check it out!\n "}, {"by": "imartin2k", "descendants": 1, "id": 17020281, "kids": [17020426], "score": 1, "time": 1525780111, "title": "Spotify\u200a\u2013\u200aEurope\u2019s first real Aircraft Carrier", "type": "story", "url": "https://blog.creandum.com/spotify-european-tech-feb561a1f6ed", "text": "Spotify just released their first quarter report last week, showing performance at the high end of earlier guidance. We thought it was important to once again reflect on the huge importance of Spotify\u2019s contribution to the European tech scene. The Spotify direct listing is probably the most significant event in the European VC and Tech industry in the past 20 years, and for Sweden, perhaps in the last 100 years. It was the third largest Tech listing in the US, which has shown Europe can not only create BUSD \u201cunicorns\u201d, but now a multi-billion publicly listed consumer brand that engages hundreds of millions of users daily. Spotify sits firmly beside Facebook, Instagram and Netflix on the front page of consumers\u2019 phones. It\u2019s a milestone for European tech. It\u2019s proof we can now truly compete with Silicon Valley. It\u2019s Europe\u2019s first \u201cAircraft Carrier.\u201d Exits do more than create cash returns to founders and investors, they also drive the tempo of the startup scene. Historically European companies have sold out too early to US companies, but with Spotify remaining independent in Europe, you can expect a massive impact on the region. We\u2019ve seen this already; over the years Spotify has acquired dozens of companies\u200a\u2014\u200alike Tunigo and Soundtrap. And perhaps the most important one was the early acquisition of The Echo Nest, a Massachusetts-based music intelligence platform whose technology has led to Spotify\u2019s smart playlists and enhanced discovery. European companies acquiring in America is more common today, but just four years ago it was unique that Spotify was acquiring American tech and talent, and not the other way around. Outside of acquisitions, Spotify\u2019s listing will boost entrepreneurship in many ways. Now that Spotify is public, many current and ex-employees can more easily cash in on options and shares. A significant part of this will come back as investments in new startups and teams. Many current employees will be able to think about trying their own wings outside Spotify, as we have seen over the years backing teams with roots within for instance Spotify. And others will become angel investors or even start their own VC Fund. Soon, the tech press will be talking about the \u201cSpotify Mafia\u201d just like the narrative PayPal and Skype. And Spotify, like Klarna and iZettle, has become a huge magnet that is attracting international talent to work in Stockholm. At Klarna more than 60% of their employees come from 52 different countries. Spotify has similar numbers. This is of huge importance to create a flowerbed for new startups and innovation based in Sweden. It\u2019s only in the last 5 years that investors have been rewarded for investing in European VC when compared to US counterparts. The European VC ecosystem is much younger than the US, perhaps 25 years (vs 50 years in the US), and suffered from poor timing. Many of the early European VC funds were created in the late 90s and failed miserably with the dot com bust where over 50% of the funds disappeared. There were only a few pension funds and institutional investors who were brave enough to continue to invest in VC, and for Creandum these included the 6th AP fund and Skandia, who have been backers ever since Creandum was funded in 2003. This is including Fund III raised in 2012 when none of the Creandum funds had seen an exit (although Fund II did have some interesting companies like, for example, iZettle and Spotify). With Spotify now public these investors are well rewarded for taking this risk and supporting us and other VCs. The Creandum Fund II includes not only Spotify, but also iZettle, Linas Matkasse, Cint, Videoplaza and several others. It will probably be one of Europe\u2019s best performing funds ever. It will be a huge payback that hopefully will go back to the ecosystem, backing VC funds but more importantly contribute to the Swedish society through pensions and new start ups etc. The importance of a thriving stable entrepreneurial ecosystem for the benefit of our society cannot be shown in a better way. So why is Spotify Europe\u2019s first real \u201caircraft carrier\u201d? It\u2019s a term we\u2019ve used to describe tech giants that have a huge impact on the local startup scene. We call them Aircraft Carriers because these tech giants are indestructibly large platforms with strong moats and drive forward in the ecosystem. They attract talent, acquire companies and launch partnerships. It is also a platform for new innovation, new ideas and startups. We know for sure that Spotify will not be the only Swedish Aircraft Carrier\u200a\u2014\u200alater this year iZettle may go public, and we would not be surprised if Klarna did as well. Which of the others in Creandum\u2019s portfolio will be the next one? Johan Brenner is a General Partner at Creandum. We believe any industry can be disrupted by the right entrepreneur. Check out our website or follow Creandum Family on Medium to learn more. By clapping more or less, you can signal to us which stories really stand out. A look at European tech and US expansion"}, {"by": "bartoszhernas", "descendants": 0, "id": 17020276, "score": 6, "time": 1525780075, "title": "Spotify trying to lock-in users just after IPO", "type": "story", "url": "https://hrn.as/ySpLV", "text": "Spotify has recently IPO\u2019ed, and its stock is available publicly to buy. The company is now under even closer watch from the public market, as it tries to beat the competition and compete in the market where it doesn\u2019t really have any great distribution models. Apple owns iOS ecosystem, and Apple proved how great customer acquisition channel that can be. Apple Music has recently passed 38 million\u00a0paying subscribers. How hard is it for Google to pack their Music Streaming Service into different paid products? Not hard apparently: Google Music is available for free as part of YouTube Red. I couldn\u2019t find exact numbers for Google Music, but the point still stands:\u00a0Spotify does not own any substantial distribution channel.\u00a0In other words, why would I stay with Spotify when I can: I still use Spotify, because I prefer it\u2019s UX, sharing capabilities, pricing, music recommendation etc. This may not be enough though to win the streaming services battle. ( I highly recommend Ben Thompson\u00a0articles about Spotify) \u2026or at least trying. When we started FreeYourMusic, our goal was really simple, we wanted to move our carefully curated playlists to Apple Music, just to test how it works. Since then we have been helping thousands of users with their playlists migrations across various music services. Back in the time when we started writing the code, we checked for what Spotify SDK terms of services would allow us. Spotify SDK allowed us to use user-data, and as long as we did not charge for the app that allows listening to the music, it was all fine.\u00a0Not anymore\u2026 Spotify recently wrote to us, saying that we have to stop using their official SDKs for integrating Spotify with competing services. Dear Mike, It\u2019s come to our attention that the service associated with your Spotify developer account, Stamp, is in violation of Spotify\u2019s Developer Terms of Use. According to its website, Stamp provides a service that allows users to \u201cmove[] tracks and playlists across various services.\u201d Stamp is in violation of Spotify\u2019s Developer Terms, which prohibit developers from integrating Spotify Content (including metadata) with third-party services, or building products or services that promote competition with the Spotify Service, among other things. Stamp\u2019s use of the Spotify logo in its app and on its website also misleadingly suggests Spotify\u2019s endorsement of Stamp\u2019s services.\u00a0 We ask that you please discontinue such infringing use of Spotify\u2019s trademarks. Please confirm in writing that you will discontinue: (i) any violations of Spotify\u2019s Developer Terms and (ii) unauthorized trademark use within 30 days. If we do not receive a response or if no corrective action is taken, we may be required to disable your API key and access to the Spotify developer platform. Thank you. Spotify Developer Compliance Team As it turns out, they have just changed their Developers Terms of Use, and now they explicitly\u00a0try to destroy apps like STAMP. We believe that user data fully belongs to the user, and the user should have full control over it. If I want to migrate from Spotify, I should be free to export all my playlists to Apple Music. This is not the case with music streaming services unless you use FreeYourMusic.com\u2019s app STAMP. FreeYourMusic supports multiple services, across which, users can easily migrate their songs. Some of them are handled with official API (eg. Apple Music, YouTube) and some are simulating to be the official webapp\u00a0(like Tidal or Google Play Music). The pros of using official API is that user experience is much better: if you are logged in to Spotify on your mobile, you are also logged in automatically in STAMP. Unfortunately\u00a0we had to stop using official Spotify SDK, and instead, we are simulating their webapp. Everything works as previously, except login page, now you have to log in, even after being logged in on Spotify App. We will continuously fight with music data lock-in, as it drives the companies to offer better and better products. Finally Taylor Swift is available on Spotify, but if it won\u2019t be, I want to be able to migrate to service where she is. Spotify, we don\u2019t own MP3\u2019s anymore and sometimes we loose music when you pull it out of your library. Don\u2019t steal also our playlists, which you are using to train your AI for the recommendation system. You do not gain anything from it, I will still move, if you want it or not.  "}]